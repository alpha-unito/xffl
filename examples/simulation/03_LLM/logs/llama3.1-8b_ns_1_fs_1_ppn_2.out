[38;20m2025-08-01 00:43:30,053 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
[38;5;39m2025-08-01 00:43:30,069 | xffl.cli.simulate |    DEBUG | Using current virtual environment: /leonardo_scratch/fast/uToID_bench/xffl/.venv[0m
[38;5;39m2025-08-01 00:43:30,069 | xffl.cli.simulate |    DEBUG | New local simulation xFFL environment variables: {'XFFL_VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv', 'XFFL_WORLD_SIZE': '2', 'XFFL_NUM_NODES': '1', 'MASTER_ADDR': 'lrdn2379.leonardo.local', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;5;39m2025-08-01 00:43:30,069 | xffl.cli.simulate |    DEBUG | Updated xFFL environment: {'XFFL_VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv', 'XFFL_WORLD_SIZE': '2', 'XFFL_NUM_NODES': '1', 'MASTER_ADDR': 'lrdn2379.leonardo.local', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;20m2025-08-01 00:43:30,071 | xffl.cli.simulate |     INFO | Running local simulation...[0m
[38;5;39m2025-08-01 00:43:30,071 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn2379.leonardo.local: ssh -oStrictHostKeyChecking=no lrdn2379.leonardo.local " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=2 XFFL_NUM_NODES=1 MASTER_ADDR=lrdn2379.leonardo.local XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py -dbg -m llama3.1-8b -d clean_mc4_it --seed 42 -fs 1 -b 5 -w /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs -csv llama3.1-8b_ns_1_fs_2.csv "[0m
[38;5;39m2025-08-01 00:43:44,987 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-08-01 00:43:44,987 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-08-01 00:43:45,315 | xffl.distributed.distributed_state |    DEBUG | Setting Symmetric Federated Scaling with sizes (1, 1)[0m
[38;5;39m2025-08-01 00:43:45,350 | xffl.distributed.distributed |    DEBUG | [Rank 1]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn2379.leonardo.local
                    Master port=29500
                    Rank=1
                    World size=2
                NODE:
                    Node local rank=1
                    Node local size=2
                    Node rank=0
                    Node world size=1
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1)
                    Federated rank=1
                    Federated world size=2
                MESHES:
                    FSDP=DeviceMesh('cuda', [1], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1]
                    Replica group=None
                    Federation=[1]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:1
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:1 cuda_stream=0xbf5b140>, <torch.cuda.Stream device=cuda:1 cuda_stream=0xbf78380>, <torch.cuda.Stream device=cuda:1 cuda_stream=0xc4bf0f0>, <torch.cuda.Stream device=cuda:1 cuda_stream=0xc4bf3f0>)
                [0m
[38;5;39m2025-08-01 00:43:45,350 | xffl.distributed.distributed |    DEBUG | [Rank 0]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn2379.leonardo.local
                    Master port=29500
                    Rank=0
                    World size=2
                NODE:
                    Node local rank=0
                    Node local size=2
                    Node rank=0
                    Node world size=1
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1)
                    Federated rank=0
                    Federated world size=2
                MESHES:
                    FSDP=DeviceMesh('cuda', [0], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1]
                    Replica group=None
                    Federation=[0]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc299950>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2b6b90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc7fe3a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc7fe6a0>)
                [0m
[38;5;39m2025-08-01 00:43:45,350 |         __main__ |    DEBUG | Rendez-vous time: 0.36 seconds[0m
[38;5;39m2025-08-01 00:43:45,350 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-01 00:43:48,373 | xffl.distributed.distributed |    DEBUG | [Rank 1]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-01 00:43:50,012 |         __main__ |    DEBUG | Model loading time: 4.65 seconds[0m
[38;5;39m2025-08-01 00:43:50,013 |         __main__ |    DEBUG | Training llama3.1-8b: 8030.26 million trainable parameters[0m
[38;5;39m2025-08-01 00:43:50,013 | xffl.distributed.distributed |    DEBUG | [Rank 0]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-01 00:43:54,917 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-08-01 00:43:55,931 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-08-01 00:43:55,931 |         __main__ |    DEBUG | FSDP wrapping setup time: 5.92 seconds[0m
[38;5;39m2025-08-01 00:43:56,011 |         __main__ |    DEBUG | Dataset loading time: 0.08 seconds[0m
[38;5;39m2025-08-01 00:43:56,011 |         __main__ |    DEBUG | train set size: 4085342 samples[0m
[38;5;39m2025-08-01 00:43:56,012 |         __main__ |    DEBUG | train dataloader size: 510667 mini-batches[0m
[38;5;39m2025-08-01 00:43:56,012 |         __main__ |    DEBUG | val set size: 5311 samples[0m
[38;5;39m2025-08-01 00:43:56,012 |         __main__ |    DEBUG | val dataloader size: 2655 mini-batches[0m
[38;5;39m2025-08-01 00:43:56,012 |         __main__ |    DEBUG | Dataloaders creation time: 0.00 seconds[0m
[38;5;39m2025-08-01 00:43:56,012 |         __main__ |    DEBUG | Learning rate adjusted to: 5e-05[0m
[38;5;39m2025-08-01 00:43:56,022 |         __main__ |    DEBUG | Total setup time: 11.03 seconds[0m
[38;5;39m2025-08-01 00:43:56,022 |         __main__ |    DEBUG | GPU RAM allocated before training: 16.06 GB[0m


[38;20m2025-08-01 00:43:58,847 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.71/557.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:01,625 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 554.80/556.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:04,242 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.87/611.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:06,768 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.72/610.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:44:09,542 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.11/556.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:12,319 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 554.65/556.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:14,845 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.10/611.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:17,370 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.60/611.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:44:20,143 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.45/556.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:22,918 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.04/556.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:25,444 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.05/611.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:27,972 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.71/611.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:44:30,749 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.81/555.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:33,525 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.87/556.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:36,052 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.13/610.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:38,580 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.37/611.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:44:41,353 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.26/556.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:44,128 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.86/556.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:46,655 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.62/611.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:49,181 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.86/611.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:44:51,958 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.59/555.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:54,733 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.55/556.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:57,260 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.88/610.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:59,788 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.66/610.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:02,566 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.83/555.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:05,344 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.58/555.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:07,871 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.87/611.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:10,396 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.34/611.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:13,174 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.05/555.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:15,947 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.25/557.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:18,475 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.46/610.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:21,001 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.04/611.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:23,779 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.05/556.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:26,555 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.49/556.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:29,083 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.57/610.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:31,610 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.84/610.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:34,386 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.36/556.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:37,162 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.73/556.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:39,688 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.45/610.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:42,216 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.74/610.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:44,994 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.71/555.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:47,772 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.33/555.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:50,299 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.51/610.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:52,826 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.62/610.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:55,605 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.32/555.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:58,382 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.75/555.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:00,909 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.14/610.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:03,436 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.15/610.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:06,211 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.14/555.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:08,987 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.98/556.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:11,513 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.24/610.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:14,039 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.05/610.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:16,815 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.26/556.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:19,590 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.97/555.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:22,116 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 612.00/610.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:24,641 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.05/611.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:27,419 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.51/555.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:30,197 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.43/554.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:32,725 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.59/610.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:35,252 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.11/610.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:38,029 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.68/555.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:40,806 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.71/555.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:43,333 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.64/610.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:45,860 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.88/610.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:48,635 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.58/556.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:51,411 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.76/556.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:53,936 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.30/611.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:56,462 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.97/611.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:59,241 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.03/554.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:02,018 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.44/555.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:04,547 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.60/610.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:07,076 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.33/610.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:47:09,852 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.42/556.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:12,627 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.55/556.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:15,153 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.69/611.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:17,681 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.67/610.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:47:20,460 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.09/555.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:23,235 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.83/556.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:25,762 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.91/610.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:28,290 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.86/610.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:47:31,066 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.74/556.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:33,846 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.12/555.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:36,371 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.42/611.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:38,898 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.53/611.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:47:41,676 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.16/555.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:44,456 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.80/553.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:46,983 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.99/610.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:49,509 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.82/611.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:47:52,288 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.96/555.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:55,067 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.13/555.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:57,595 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.36/610.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:00,122 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.23/610.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:48:02,899 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.85/555.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:05,678 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.13/555.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:08,206 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.29/610.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:10,732 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.58/611.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:48:13,513 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.64/553.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:16,294 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.38/554.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:18,792 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.19/617.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:21,290 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 618.96/617.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:48:24,073 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.23/554.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:26,855 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.77/553.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:29,353 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 618.91/617.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:31,851 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.34/617.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:48:34,634 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.28/553.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:37,418 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.74/553.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:39,916 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.35/616.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:42,413 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.65/617.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:48:45,193 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.51/554.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:47,976 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.47/553.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:50,475 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.20/617.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:52,973 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.50/617.06 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:48:55,756 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.42/554.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:58,540 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 554.70/553.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:01,038 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 618.92/617.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:03,536 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.31/617.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:49:06,318 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.13/554.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:09,100 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.17/554.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:11,597 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.56/617.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:14,093 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.76/617.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:49:16,872 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.46/555.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:19,654 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.82/553.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:22,150 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.51/617.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:24,645 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.90/618.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:49:27,424 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.68/555.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:30,204 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.19/554.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:32,699 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.74/618.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:35,195 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.61/617.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:49:37,975 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.81/553.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:40,758 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.59/553.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:43,256 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.03/617.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:45,752 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.60/617.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:49:48,536 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 554.98/553.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:51,318 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.10/554.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:53,817 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.04/616.91 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:49:56,314 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.57/617.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:49:59,096 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.30/554.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:01,877 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.00/554.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:04,375 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.04/617.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:06,873 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.19/616.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:50:09,656 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.27/553.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:12,438 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.94/553.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:14,935 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 620.05/616.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:17,432 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.69/617.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:50:20,216 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.56/553.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:22,999 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.56/553.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:25,495 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.56/617.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:27,992 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.54/617.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:50:30,773 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.33/553.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:33,553 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.09/554.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:36,051 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.40/616.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:38,548 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.58/616.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:50:41,329 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.90/554.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:44,108 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.40/555.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:46,605 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.51/617.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:49,100 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 620.15/617.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:50:51,881 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.68/554.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:54,663 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.40/554.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:57,160 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.78/617.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:50:59,656 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.61/617.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:51:02,437 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.06/553.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:51:05,220 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.79/554.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:51:07,716 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.80/617.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:51:10,213 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.51/617.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:51:12,991 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.91/554.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:51:15,774 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.03/554.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:51:18,269 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.79/617.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:51:20,765 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.67/617.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:51:23,544 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.73/554.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:51:26,320 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.77/555.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:51:28,817 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.70/617.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:51:31,314 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.59/617.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:51:34,095 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.78/554.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:51:36,874 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.94/554.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:51:39,370 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.81/617.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:51:41,865 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 620.14/617.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:51:44,647 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.92/553.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:51:47,426 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.45/554.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:51:49,925 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.10/616.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:51:52,422 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.48/617.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:51:55,207 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.25/553.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:51:57,988 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.91/553.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:00,486 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.34/617.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:02,982 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.17/617.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:52:05,764 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.29/554.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:08,545 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 556.16/554.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:11,041 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.93/617.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:13,535 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 620.02/617.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:52:16,316 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.78/554.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:19,095 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.23 (max/real adjusted throughput: 555.41/555.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:21,593 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.41/617.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:24,089 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 619.79/617.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:52:27,251 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.19/482.66 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:52:30,582 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.02/482.45 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:52:33,574 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.82/536.05 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:52:36,580 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 609.87/538.13 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:52:39,902 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.24/482.10 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:52:43,232 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.18/482.27 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:52:46,227 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.52/537.70 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:52:49,229 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.97/535.76 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:52:52,553 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.80/481.36 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:52:55,876 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.44/483.02 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:52:58,878 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.09/535.00 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:53:01,881 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 614.57/536.73 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:53:05,201 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 591.16/481.42 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:53:08,528 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.79/483.03 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:53:11,535 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 607.84/534.22 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:53:14,536 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.37/536.42 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:53:17,856 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.19/482.48 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:53:21,177 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.91/484.11 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:53:24,174 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 609.58/539.15 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:53:27,174 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.89/535.89 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:53:30,498 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.90/482.53 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:53:33,823 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.20/482.77 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:53:36,821 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.31/537.61 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:53:39,817 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 609.46/538.91 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:53:43,139 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.56/483.04 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:53:46,460 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.75/483.25 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:53:49,464 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.81/535.40 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:53:52,465 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.21/537.34 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:53:55,791 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.86/481.14 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:53:59,115 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.57/481.95 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:54:02,116 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 609.87/537.58 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:54:05,125 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 608.41/536.76 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:54:08,455 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.02/480.64 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:54:11,777 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.20/482.76 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:54:14,776 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 609.44/538.08 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:54:17,771 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 609.59/539.49 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:54:21,091 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 591.12/482.03 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:54:24,410 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.79/483.47 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:54:27,424 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 607.75/533.86 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:54:30,425 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 614.23/533.02 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:54:33,758 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.52/483.76 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:54:37,084 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.08/483.07 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:54:40,089 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.07/536.01 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:54:43,099 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 607.90/535.47 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:54:46,429 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.80/481.04 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:54:49,751 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.58/482.72 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:54:52,756 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.50/535.69 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:54:55,764 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 607.61/534.77 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:54:59,082 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.96/483.30 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:02,412 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 591.84/483.38 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:05,413 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 609.57/536.70 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:55:08,412 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.06/537.25 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:55:11,730 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.41/483.59 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:15,061 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.69/482.42 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:18,068 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 609.44/535.77 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:55:21,067 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 607.91/538.94 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:55:24,386 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.83/483.19 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:27,710 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.53/482.60 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:30,724 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 606.85/534.61 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:55:33,726 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 608.47/537.08 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:55:37,048 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 591.11/481.61 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:40,372 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.44/482.20 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:43,375 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 609.21/537.16 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:55:46,372 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 609.63/537.73 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:55:49,696 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.59/482.39 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:53,015 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.60/482.76 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:56,013 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.39/538.02 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:55:59,007 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 612.81/536.58 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:56:02,330 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.81/482.57 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:05,655 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.50/483.00 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:08,650 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 613.97/538.06 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:56:11,640 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.77/538.49 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:56:14,963 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.77/482.32 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:18,288 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.22/482.95 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:21,277 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 613.95/538.66 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:56:24,272 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 607.90/539.95 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:56:27,597 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.95/483.05 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:30,916 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.97/483.63 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:33,914 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 609.36/537.61 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:56:36,916 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.50/535.22 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:56:40,238 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.56/483.32 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:43,561 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.56/482.86 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:46,570 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 609.00/534.59 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:56:49,573 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 609.04/535.20 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:56:52,900 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.79/482.79 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:56,223 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.11/482.16 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:59,228 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.14/536.62 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:57:02,226 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.85/536.80 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:57:05,548 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.20/482.67 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:57:08,869 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.10/483.28 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:57:11,878 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 607.29/535.95 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:57:14,881 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 609.77/535.53 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:57:18,202 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.15/483.04 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:57:21,524 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.44/483.15 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:57:24,520 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 615.29/535.98 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:57:27,521 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.17/535.34 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:57:30,285 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 593.67/588.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:57:32,897 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.47/589.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:57:35,428 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.87/608.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:57:37,960 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.91/607.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:57:40,574 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.91/588.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:57:43,188 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 593.12/587.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:57:45,720 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.93/607.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:57:48,252 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.53/607.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:57:50,866 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 593.22/588.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:57:53,478 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.31/589.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:57:56,008 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.87/608.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:57:58,539 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.32/608.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:58:01,150 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.68/589.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:03,762 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.65/589.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:06,292 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.97/608.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:08,823 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.61/608.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:58:11,436 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.81/588.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:14,049 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.69/588.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:16,580 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.57/608.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:19,111 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.95/608.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:58:21,725 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.88/588.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:24,340 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.04/589.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:26,872 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 612.34/607.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:29,404 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.77/607.90 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:58:32,016 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.68/590.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:34,633 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 590.49/588.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:37,163 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 612.20/607.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:39,694 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 612.20/608.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:58:42,310 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.64/588.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:44,930 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 590.63/587.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:47,463 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 612.14/607.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:49,994 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 612.50/607.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:58:52,605 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.75/589.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:55,215 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 593.29/589.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:57,747 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.88/607.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:00,278 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.57/607.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:02,892 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.51/588.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:05,504 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.61/588.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:08,037 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.46/607.74 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:10,572 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.79/607.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:13,190 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 590.65/588.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:15,806 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.77/588.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:18,341 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.23/606.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:20,873 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.49/607.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:23,486 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.34/588.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:26,102 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.01/587.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:28,634 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.86/607.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:31,167 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.57/607.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:33,782 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.70/588.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:36,397 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.72/588.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:38,929 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.52/607.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:41,460 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 612.39/607.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:44,073 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.91/589.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:46,688 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.13/589.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:49,220 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.04/607.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:51,750 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 612.64/608.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:54,365 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.35/589.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:56,979 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.94/588.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:59,510 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.67/607.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:02,041 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.57/608.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:00:04,654 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.72/589.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:07,269 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.10/588.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:09,799 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 612.30/608.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:12,332 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.51/607.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:00:14,949 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.66/587.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:17,564 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.34/589.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:20,098 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.54/607.06 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:22,631 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.62/607.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:00:25,247 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.21/587.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:27,862 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.93/588.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:30,395 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.85/607.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:32,929 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.60/607.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:00:35,545 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.47/587.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:38,160 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.88/588.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:40,692 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.74/607.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:43,225 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.47/607.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:00:45,843 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 590.47/589.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:48,457 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.08/588.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:50,991 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.10/607.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:53,524 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.73/607.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:00:56,141 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 590.97/588.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:58,759 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.36/587.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:01:01,292 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.98/607.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:01:03,825 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.25/607.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:01:06,438 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 593.55/589.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:01:09,052 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.08/589.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:01:11,585 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.75/607.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:01:14,119 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.23/607.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:01:16,733 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.46/589.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:01:19,345 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 593.25/588.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:01:21,879 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.80/607.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:01:24,412 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.86/606.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:01:27,030 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.41/587.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:01:29,649 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 590.34/587.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:01:32,181 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 612.04/607.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:01:34,714 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.07/608.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:01:37,892 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.73/482.31 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:01:41,220 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.35/482.14 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:01:44,193 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.37/540.48 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:01:47,175 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 612.83/542.21 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:01:50,496 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.72/482.38 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:01:53,828 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.33/481.59 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:01:56,813 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.89/539.12 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:01:59,787 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 614.83/542.14 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:02:03,107 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.77/482.20 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:02:06,436 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.36/482.35 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:02:09,416 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.73/542.78 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:02:12,402 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 608.44/541.33 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:02:15,729 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.37/480.75 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:02:19,058 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.42/481.65 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:02:22,039 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.06/541.66 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:02:25,017 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 612.37/541.72 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:02:28,341 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.44/481.51 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:02:31,674 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 587.83/481.41 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:02:34,672 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.99/540.18 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:02:37,644 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 615.82/541.45 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:02:40,969 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.40/481.17 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:02:44,301 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 587.91/481.69 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:02:47,280 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.55/541.89 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:02:50,261 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.71/542.23 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:02:53,594 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.23/479.37 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:02:56,930 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 586.15/482.34 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:02:59,906 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.42/542.70 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:03:02,892 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 612.13/538.08 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:03:06,222 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.82/479.93 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:03:09,551 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 587.68/482.93 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:03:12,524 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 614.82/541.49 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:03:15,498 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 615.19/540.49 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:03:18,823 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.65/481.50 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:03:22,156 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.86/481.47 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:03:25,135 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 612.49/541.22 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:03:28,120 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 612.55/538.04 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:03:31,449 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.14/481.82 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:03:34,779 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.05/480.97 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:03:37,754 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 614.53/541.41 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:03:40,730 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 614.89/541.18 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:03:44,060 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.87/480.32 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:03:47,391 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.83/480.71 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:03:50,371 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.23/541.73 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:03:53,364 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.78/540.64 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:03:56,698 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.68/480.83 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:04:00,027 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.14/481.81 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:04:03,009 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.27/541.67 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:04:05,994 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.24/539.99 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:04:09,322 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.02/480.17 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:04:12,651 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 587.93/482.25 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:04:15,628 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 614.45/540.48 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:04:18,608 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.20/540.08 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:04:21,932 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.69/481.23 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:04:25,275 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.88/481.45 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:04:28,255 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 613.33/540.49 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:04:31,234 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.28/540.61 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:04:34,559 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 591.04/480.73 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:04:37,884 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.11/481.77 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:04:40,865 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.94/542.61 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:04:43,838 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 614.78/540.96 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:04:47,170 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 587.78/480.86 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:04:50,496 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.77/481.99 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:04:53,490 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.83/541.08 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:04:56,477 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.23/538.19 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:04:59,800 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.04/482.13 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:05:03,127 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.25/481.88 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:05:06,103 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 613.76/542.28 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:05:09,083 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.64/541.10 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:05:12,410 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.42/481.56 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:05:15,744 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.70/480.91 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:05:18,727 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 612.39/540.16 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:05:21,703 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 614.75/539.35 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:05:25,026 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.70/481.80 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:05:28,354 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.08/481.69 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:05:31,335 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.70/542.23 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:05:34,306 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 615.72/541.44 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:05:37,628 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 591.79/482.05 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:05:40,956 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.49/481.27 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:05:43,938 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 610.83/541.00 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:05:46,914 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.29/542.84 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:05:50,241 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.71/481.54 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:05:53,574 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 587.94/480.55 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:05:56,556 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 615.32/538.64 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:05:59,538 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 614.85/538.02 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:06:02,862 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.64/481.83 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:06:06,188 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 589.23/481.93 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:06:09,166 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 615.03/541.64 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:06:12,144 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 614.20/539.35 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:06:15,481 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 590.39/480.81 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:06:18,809 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.33/482.35 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:06:21,782 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 615.96/540.97 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:06:24,758 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 612.57/542.22 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:06:28,087 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.42/481.36 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:06:31,416 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.27 (max/real adjusted throughput: 588.06/481.85 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 01:06:34,386 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 615.42/542.75 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 01:06:37,369 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.24 (max/real adjusted throughput: 611.13/539.89 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 01:06:40,138 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.61/587.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:06:42,757 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 590.39/588.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:06:45,297 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.76/605.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:06:47,835 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.41/605.34 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:06:50,451 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 593.21/587.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:06:53,072 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.77/587.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:06:55,610 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.22/605.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:06:58,149 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.35/605.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:07:00,766 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 593.80/586.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:03,384 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.00/587.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:05,922 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.53/605.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:08,460 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.64/605.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:07:11,077 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.22/587.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:13,698 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.10/586.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:16,236 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.30/605.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:18,775 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.62/605.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:07:21,396 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.87/586.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:24,017 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.93/586.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:26,557 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.56/605.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:29,096 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.10/605.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:07:31,719 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 590.58/586.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:34,340 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.33/586.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:36,879 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.27/605.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:39,417 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.12/605.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:07:42,037 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.18/586.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:44,657 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.21/586.91 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:47,195 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.38/605.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:49,736 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.24/605.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:07:52,354 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.68/587.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:54,976 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.27/586.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:07:57,513 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.32/605.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:00,051 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.63/605.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:08:02,668 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.87/587.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:05,288 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.28/586.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:07,826 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.26/605.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:10,363 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 612.15/605.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:08:12,984 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.94/586.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:15,610 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 590.23/586.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:18,150 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.51/605.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:20,689 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.64/605.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:08:23,310 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.36/585.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:25,931 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.11/586.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:28,469 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.45/605.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:31,005 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.74/606.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:08:33,623 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.88/587.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:36,239 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 594.29/586.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:38,776 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.68/605.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:41,313 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.47/605.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:08:43,932 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 593.26/586.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:46,553 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.23/586.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:49,090 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.31/605.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:51,628 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.54/605.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:08:54,245 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.41/588.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:56,864 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 593.00/586.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:08:59,401 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.39/606.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:01,941 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.88/605.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:09:04,559 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.68/587.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:07,181 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 590.67/586.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:09,718 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.49/605.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:12,257 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.78/605.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:09:14,876 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 593.57/586.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:17,495 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.87/587.06 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:20,035 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.73/605.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:22,573 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.90/606.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:09:25,192 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.65/586.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:27,808 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 593.26/587.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:30,346 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.38/605.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:32,884 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.46/605.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:09:35,505 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.09/586.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:38,122 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 593.70/586.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:40,661 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.74/605.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:43,200 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.89/605.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:09:45,820 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.27/587.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:48,441 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 590.84/587.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:50,980 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.75/605.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:53,520 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.79/605.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:09:56,139 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.53/586.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:09:58,763 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 590.39/586.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:10:01,302 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.83/605.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:10:03,842 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.97/605.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:10:06,464 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.19/586.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:10:09,085 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.61/586.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:10:11,626 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.46/605.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:10:14,165 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.54/605.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:10:16,786 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.64/585.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:10:19,411 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 590.44/585.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:10:21,950 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.30/605.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:10:24,490 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.72/605.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:10:27,112 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.02/585.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:10:29,738 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 590.77/585.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:10:32,277 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.17/605.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:10:34,817 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 610.73/605.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:10:37,440 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 592.61/584.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:10:40,062 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.22 (max/real adjusted throughput: 591.88/586.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:10:42,600 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.22/605.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:10:45,138 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.21 (max/real adjusted throughput: 611.72/605.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:10:45,138 | xffl.distributed.aggregation |     INFO | Dumping benchmarking results to /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs/llama3.1-8b_ns_1_fs_2.csv[0m
[38;20m2025-08-01 01:10:49,800 | xffl.cli.simulate |     INFO | Total simulation execution time: 1639.73 seconds[0m
[38;20m2025-08-01 01:10:49,800 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
