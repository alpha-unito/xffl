[38;20m2025-08-01 00:43:31,086 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
[38;5;39m2025-08-01 00:43:31,122 | xffl.cli.simulate |    DEBUG | Using current virtual environment: /leonardo_scratch/fast/uToID_bench/xffl/.venv[0m
[38;5;39m2025-08-01 00:43:31,123 | xffl.cli.simulate |    DEBUG | New local simulation xFFL environment variables: {'XFFL_VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv', 'XFFL_WORLD_SIZE': '4', 'XFFL_NUM_NODES': '1', 'MASTER_ADDR': 'lrdn2567.leonardo.local', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;5;39m2025-08-01 00:43:31,123 | xffl.cli.simulate |    DEBUG | Updated xFFL environment: {'XFFL_VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv', 'XFFL_WORLD_SIZE': '4', 'XFFL_NUM_NODES': '1', 'MASTER_ADDR': 'lrdn2567.leonardo.local', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;20m2025-08-01 00:43:31,125 | xffl.cli.simulate |     INFO | Running local simulation...[0m
[38;5;39m2025-08-01 00:43:31,125 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn2567.leonardo.local: ssh -oStrictHostKeyChecking=no lrdn2567.leonardo.local " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=4 XFFL_NUM_NODES=1 MASTER_ADDR=lrdn2567.leonardo.local XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py -dbg -m llama3.1-8b -d clean_mc4_it --seed 42 -fs 1 -b 5 -w /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs -csv llama3.1-8b_ns_1_fs_4.csv "[0m
[38;5;39m2025-08-01 00:43:45,991 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-08-01 00:43:45,991 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-08-01 00:43:45,991 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-08-01 00:43:45,991 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-08-01 00:43:46,405 | xffl.distributed.distributed_state |    DEBUG | Setting Symmetric Federated Scaling with sizes (1, 1, 1, 1)[0m
[38;5;39m2025-08-01 00:43:46,454 | xffl.distributed.distributed |    DEBUG | [Rank 2]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn2567.leonardo.local
                    Master port=29500
                    Rank=2
                    World size=4
                NODE:
                    Node local rank=2
                    Node local size=4
                    Node rank=0
                    Node world size=1
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1)
                    Federated rank=2
                    Federated world size=4
                MESHES:
                    FSDP=DeviceMesh('cuda', [2], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3]
                    Replica group=None
                    Federation=[2]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:2
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:2 cuda_stream=0xcb6d6b0>, <torch.cuda.Stream device=cuda:2 cuda_stream=0xcb8a8f0>, <torch.cuda.Stream device=cuda:2 cuda_stream=0xd0d24c0>, <torch.cuda.Stream device=cuda:2 cuda_stream=0xd0d27c0>)
                [0m
[38;5;39m2025-08-01 00:43:46,454 | xffl.distributed.distributed |    DEBUG | [Rank 0]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn2567.leonardo.local
                    Master port=29500
                    Rank=0
                    World size=4
                NODE:
                    Node local rank=0
                    Node local size=4
                    Node rank=0
                    Node world size=1
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1)
                    Federated rank=0
                    Federated world size=4
                MESHES:
                    FSDP=DeviceMesh('cuda', [0], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3]
                    Replica group=None
                    Federation=[0]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc6d2a40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc6efc80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc36dc0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc370c0>)
                [0m
[38;5;39m2025-08-01 00:43:46,454 | xffl.distributed.distributed |    DEBUG | [Rank 3]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn2567.leonardo.local
                    Master port=29500
                    Rank=3
                    World size=4
                NODE:
                    Node local rank=3
                    Node local size=4
                    Node rank=0
                    Node world size=1
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1)
                    Federated rank=3
                    Federated world size=4
                MESHES:
                    FSDP=DeviceMesh('cuda', [3], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3]
                    Replica group=None
                    Federation=[3]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:3
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:3 cuda_stream=0xc6fa830>, <torch.cuda.Stream device=cuda:3 cuda_stream=0xc717a70>, <torch.cuda.Stream device=cuda:3 cuda_stream=0xcc5f2f0>, <torch.cuda.Stream device=cuda:3 cuda_stream=0xcc5f5f0>)
                [0m
[38;5;39m2025-08-01 00:43:46,454 | xffl.distributed.distributed |    DEBUG | [Rank 1]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn2567.leonardo.local
                    Master port=29500
                    Rank=1
                    World size=4
                NODE:
                    Node local rank=1
                    Node local size=4
                    Node rank=0
                    Node world size=1
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1)
                    Federated rank=1
                    Federated world size=4
                MESHES:
                    FSDP=DeviceMesh('cuda', [1], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3]
                    Replica group=None
                    Federation=[1]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:1
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:1 cuda_stream=0xb8a5410>, <torch.cuda.Stream device=cuda:1 cuda_stream=0xb8c2650>, <torch.cuda.Stream device=cuda:1 cuda_stream=0xbe0a110>, <torch.cuda.Stream device=cuda:1 cuda_stream=0xbe0a410>)
                [0m
[38;5;39m2025-08-01 00:43:46,454 |         __main__ |    DEBUG | Rendez-vous time: 0.46 seconds[0m
[38;5;39m2025-08-01 00:43:46,454 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-01 00:43:50,816 | xffl.distributed.distributed |    DEBUG | [Rank 3]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-01 00:43:50,832 | xffl.distributed.distributed |    DEBUG | [Rank 2]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-01 00:43:51,049 | xffl.distributed.distributed |    DEBUG | [Rank 1]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-01 00:43:52,086 |         __main__ |    DEBUG | Model loading time: 5.62 seconds[0m
[38;5;39m2025-08-01 00:43:52,087 |         __main__ |    DEBUG | Training llama3.1-8b: 8030.26 million trainable parameters[0m
[38;5;39m2025-08-01 00:43:52,087 | xffl.distributed.distributed |    DEBUG | [Rank 0]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-01 00:43:59,091 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-08-01 00:43:59,091 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-08-01 00:43:59,091 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-08-01 00:43:59,091 |         __main__ |    DEBUG | FSDP wrapping setup time: 7.00 seconds[0m
[38;5;39m2025-08-01 00:43:59,091 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-08-01 00:43:59,255 |         __main__ |    DEBUG | Dataset loading time: 0.16 seconds[0m
[38;5;39m2025-08-01 00:43:59,255 |         __main__ |    DEBUG | train set size: 4085342 samples[0m
[38;5;39m2025-08-01 00:43:59,255 |         __main__ |    DEBUG | train dataloader size: 255333 mini-batches[0m
[38;5;39m2025-08-01 00:43:59,255 |         __main__ |    DEBUG | val set size: 5311 samples[0m
[38;5;39m2025-08-01 00:43:59,255 |         __main__ |    DEBUG | val dataloader size: 1327 mini-batches[0m
[38;5;39m2025-08-01 00:43:59,255 |         __main__ |    DEBUG | Dataloaders creation time: 0.00 seconds[0m
[38;5;39m2025-08-01 00:43:59,255 |         __main__ |    DEBUG | Learning rate adjusted to: 2.5e-05[0m
[38;5;39m2025-08-01 00:43:59,287 |         __main__ |    DEBUG | Total setup time: 13.30 seconds[0m
[38;5;39m2025-08-01 00:43:59,287 |         __main__ |    DEBUG | GPU RAM allocated before training: 16.06 GB[0m


[38;20m2025-08-01 00:44:00,851 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1642.65/1642.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:02,263 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1641.14/1642.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:04,126 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1703.50/1701.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:05,488 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1699.82/1702.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:44:06,898 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1644.57/1640.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:08,310 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1638.90/1642.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:09,673 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1699.43/1702.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:11,035 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1703.34/1701.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:44:12,445 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1645.41/1639.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:13,853 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1644.33/1648.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:15,213 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1700.69/1705.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:16,572 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1706.86/1703.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:44:17,983 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1643.01/1639.74 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:19,392 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1644.55/1647.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:20,754 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1701.52/1702.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:22,114 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1705.09/1702.74 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:44:23,524 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1643.03/1645.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:24,932 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1645.09/1645.74 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:26,295 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1699.17/1700.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:27,656 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.02/1701.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:44:29,066 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1644.99/1642.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:30,475 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1641.23/1647.91 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:31,837 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1701.53/1703.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:33,199 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.74/1700.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:44:34,609 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1642.44/1644.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:36,018 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1640.80/1647.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:37,376 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.60/1707.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:38,736 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1703.32/1702.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:44:40,146 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1643.60/1644.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:41,556 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1644.33/1642.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:42,915 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.64/1708.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:44,274 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1706.57/1703.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:44:45,684 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1644.47/1642.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:47,093 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1644.26/1645.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:48,452 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1703.20/1707.15 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:49,811 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1705.39/1706.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:44:51,221 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1647.26/1644.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:52,628 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1645.90/1648.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:53,989 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.57/1701.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:55,349 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.10/1705.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:44:56,756 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1645.61/1650.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:58,166 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1643.45/1646.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:44:59,526 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1703.38/1705.34 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:00,885 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1703.51/1708.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:02,294 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1643.54/1645.74 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:03,702 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1643.33/1648.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:05,063 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1705.17/1701.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:06,426 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1698.32/1700.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:07,836 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1641.81/1644.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:09,243 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1644.96/1649.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:10,604 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.18/1702.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:11,963 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.79/1706.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:13,372 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1642.82/1645.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:14,782 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1641.05/1643.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:16,146 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1699.86/1699.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:17,508 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.96/1700.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:18,920 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1639.57/1640.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:20,331 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1639.50/1641.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:21,692 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.62/1703.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:23,052 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.25/1703.74 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:24,461 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1641.34/1645.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:25,873 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1642.98/1642.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:27,232 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1703.57/1706.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:28,592 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.91/1703.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:30,003 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1639.58/1647.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:31,414 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1640.20/1643.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:32,776 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1700.06/1704.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:34,136 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.63/1700.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:35,546 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1641.95/1644.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:36,959 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1638.09/1641.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:38,323 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1700.01/1700.90 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:39,686 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1699.89/1701.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:41,098 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1641.93/1642.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:42,508 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1645.43/1641.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:43,874 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1700.39/1694.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:45,237 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1698.92/1702.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:46,648 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1639.95/1644.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:48,057 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1645.00/1643.06 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:49,417 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1705.77/1705.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:50,775 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1705.12/1707.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:52,185 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1644.50/1642.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:53,594 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1643.69/1644.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:54,955 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.87/1700.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:56,316 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.07/1704.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:45:57,727 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1640.02/1643.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:45:59,135 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1645.03/1644.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:00,498 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1700.92/1700.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:01,857 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1705.48/1704.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:03,267 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1642.41/1645.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:04,679 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1638.54/1644.91 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:06,039 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1705.72/1701.91 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:07,399 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1703.47/1704.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:08,807 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1644.12/1647.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:10,218 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1643.80/1644.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:11,579 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.35/1701.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:12,943 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1699.06/1697.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:14,361 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1639.93/1628.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:15,781 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1635.02/1630.15 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:17,140 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1714.03/1698.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:18,501 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1709.06/1699.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:19,921 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1636.45/1630.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:21,340 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1638.11/1628.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:22,697 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1711.40/1706.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:24,057 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1707.58/1703.06 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:25,473 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1641.83/1632.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:26,889 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1641.52/1632.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:28,249 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1707.45/1702.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:29,609 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1706.92/1705.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:31,024 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1640.78/1633.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:32,441 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1639.56/1632.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:33,801 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1707.08/1703.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:35,157 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1718.47/1703.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:36,570 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1646.27/1637.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:37,987 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1645.31/1627.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:39,347 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.93/1702.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:40,708 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1706.91/1700.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:42,124 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1642.59/1631.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:43,541 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1640.05/1630.74 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:44,901 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1706.76/1700.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:46,261 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1706.17/1706.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:47,676 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1642.15/1634.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:49,094 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1642.15/1629.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:50,456 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1700.23/1704.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:51,815 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1705.25/1705.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:53,230 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1642.60/1631.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:54,645 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1642.70/1636.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:56,005 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1709.38/1703.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:46:57,364 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1712.01/1698.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:46:58,780 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1640.44/1634.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:00,197 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1641.08/1629.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:01,557 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1709.73/1699.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:02,917 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1709.29/1698.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:47:04,335 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1638.62/1630.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:05,753 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1640.21/1631.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:07,115 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1706.08/1695.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:08,477 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.94/1697.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:47:09,894 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1640.06/1630.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:11,311 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1641.55/1632.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:12,671 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1707.18/1703.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:14,033 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1709.00/1698.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:47:15,451 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1642.53/1628.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:16,865 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1642.85/1633.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:18,226 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1706.99/1696.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:19,589 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1705.07/1697.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:47:21,007 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1637.40/1631.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:22,423 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1641.77/1632.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:23,785 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1708.16/1697.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:25,147 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1699.23/1702.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:47:26,564 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1641.05/1629.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:27,982 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1640.39/1630.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:29,345 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1703.01/1698.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:30,709 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.52/1695.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:47:32,127 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1638.58/1629.06 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:33,545 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1640.10/1630.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:34,906 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1706.16/1698.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:36,270 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1703.48/1695.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:47:37,687 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1642.96/1626.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:39,106 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1639.58/1629.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:40,468 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1706.76/1699.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:41,830 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.83/1700.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:47:43,248 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1633.84/1633.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:44,663 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1641.24/1634.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:46,024 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.80/1703.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:47,387 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.70/1699.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:47:48,805 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1642.35/1626.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:50,221 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1639.86/1634.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:51,581 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1706.98/1701.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:52,943 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1705.81/1698.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:47:54,360 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1640.91/1630.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:55,777 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1639.84/1630.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:57,136 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1711.25/1702.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:47:58,500 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1703.27/1697.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:47:59,920 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1640.42/1626.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:01,340 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1633.60/1628.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:02,703 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.70/1698.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:04,062 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1709.33/1705.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:48:05,480 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1640.73/1629.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:06,899 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1639.47/1628.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:08,263 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1697.40/1700.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:09,623 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1708.00/1702.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:48:11,039 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1641.56/1634.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:12,454 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1640.43/1633.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:13,814 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1709.78/1702.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:15,174 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1707.44/1703.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:48:16,587 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1645.76/1635.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:18,003 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1639.58/1634.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:19,359 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1714.43/1705.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:20,716 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1710.29/1708.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:48:22,129 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1644.32/1636.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:23,544 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1644.93/1634.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:24,902 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1713.34/1704.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:48:26,259 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1711.69/1707.34 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:48:28,406 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1720.57/1211.16 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:48:30,894 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1722.15/1210.44 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:48:33,231 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1698.60/1283.14 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:48:35,540 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1695.69/1301.28 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:48:38,025 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1726.23/1209.33 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:48:40,507 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1728.91/1211.34 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:48:42,820 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1698.73/1298.84 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:48:45,130 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1702.57/1295.14 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:48:47,617 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1728.18/1212.23 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:48:50,092 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1721.79/1211.58 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:48:52,415 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1699.04/1291.18 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:48:54,713 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1699.60/1310.43 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:48:57,207 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1724.04/1209.73 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:48:59,700 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1726.23/1211.30 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:49:02,019 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1699.45/1292.74 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:49:04,309 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1704.10/1306.74 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:49:06,803 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1724.68/1209.21 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:49:09,302 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1728.73/1210.58 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:49:11,634 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1704.11/1290.36 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:49:13,937 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1706.66/1301.65 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:49:16,436 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1723.26/1209.88 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:49:18,931 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1721.07/1209.73 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:49:21,263 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1703.05/1302.37 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:49:23,576 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1705.22/1301.10 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:49:26,067 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1723.73/1210.02 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:49:28,561 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1727.25/1210.71 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:49:30,880 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1704.02/1290.79 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:49:33,182 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1702.72/1302.26 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:49:35,665 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1726.51/1210.15 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:49:38,149 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1728.72/1211.50 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:49:40,463 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1708.78/1292.91 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:49:42,766 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1703.19/1302.59 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:49:45,249 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1729.35/1208.36 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:49:47,731 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1731.54/1209.73 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:49:50,026 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1705.02/1309.47 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:49:52,323 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1703.42/1297.63 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:49:54,802 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1725.13/1211.24 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:49:57,285 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1732.88/1209.55 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:49:59,617 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1704.41/1296.86 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:50:01,940 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1700.13/1282.69 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:50:04,433 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1725.75/1211.45 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:50:06,922 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1719.49/1208.79 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:50:09,241 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1701.42/1287.30 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:50:11,545 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1705.34/1293.64 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:50:14,037 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1723.88/1211.03 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:50:16,527 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1728.61/1210.60 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:50:18,843 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1701.51/1300.04 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:50:21,141 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1699.29/1303.73 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:50:23,637 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1720.02/1208.98 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:50:26,127 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1718.28/1211.36 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:50:28,438 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1704.71/1302.97 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:50:30,748 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1700.26/1298.24 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:50:33,232 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1722.33/1209.55 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:50:35,725 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1725.70/1208.75 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:50:38,039 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1701.55/1299.32 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:50:40,341 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1707.80/1296.36 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:50:42,834 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1729.67/1211.02 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:50:45,317 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1726.50/1209.96 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:50:47,637 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1706.12/1299.67 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:50:49,950 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1706.78/1300.11 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:50:52,438 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1725.86/1211.08 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:50:54,937 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1724.23/1209.42 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:50:57,253 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1703.72/1303.22 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:50:59,562 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1700.49/1304.97 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:51:02,048 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1721.77/1211.51 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:51:04,544 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1724.88/1211.64 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:51:06,872 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1707.87/1295.43 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:51:09,189 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1699.97/1292.32 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:51:11,673 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1730.28/1210.89 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:51:14,172 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1723.51/1210.27 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:51:16,481 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1705.74/1299.30 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:51:18,801 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1704.02/1297.90 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:51:21,290 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1724.22/1209.14 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:51:23,788 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1722.80/1210.46 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:51:26,114 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1703.80/1302.13 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:51:28,419 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1705.03/1295.94 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:51:30,906 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1727.68/1210.52 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:51:33,398 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1722.79/1210.60 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:51:35,728 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1702.33/1294.73 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:51:38,041 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1701.09/1294.79 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:51:40,535 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1728.16/1210.22 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:51:43,034 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1725.00/1210.01 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:51:45,362 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1704.03/1296.57 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:51:47,668 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1703.50/1296.85 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:51:50,161 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1729.77/1207.86 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:51:52,657 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1721.94/1210.33 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:51:54,973 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1703.23/1291.33 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:51:57,287 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1706.22/1293.43 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:51:59,773 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1730.66/1211.00 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:52:02,264 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1722.92/1210.83 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:52:04,581 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1702.92/1307.40 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:52:06,905 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1700.59/1306.52 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:52:09,395 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1724.15/1210.39 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:52:11,893 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1717.59/1210.68 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:52:14,216 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1704.77/1295.25 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 00:52:16,506 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1698.25/1301.90 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 00:52:18,220 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1669.41/1652.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:19,572 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1723.74/1705.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:20,945 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1696.34/1681.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:22,316 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1701.45/1679.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:52:23,669 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1722.57/1697.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:25,020 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.66/1707.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:26,393 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1697.68/1677.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:27,764 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1698.13/1684.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:52:29,115 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1728.44/1702.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:30,467 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1727.11/1701.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:31,838 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1697.66/1681.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:33,207 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.20/1680.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:52:34,561 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1719.24/1704.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:35,914 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1721.08/1704.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:37,286 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1699.25/1679.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:38,660 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1700.49/1673.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:52:40,012 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1725.79/1702.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:41,361 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1727.06/1710.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:42,731 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1700.04/1681.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:44,103 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1700.52/1678.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:52:45,455 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1724.60/1704.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:46,806 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1724.33/1705.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:48,173 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.88/1685.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:49,545 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1697.09/1680.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:52:50,895 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.34/1707.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:52,246 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1725.21/1704.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:53,617 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1700.76/1680.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:54,989 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1699.76/1678.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:52:56,342 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1720.17/1705.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:57,693 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.73/1708.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:52:59,063 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.64/1683.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:00,435 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1701.96/1679.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:53:01,785 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1725.43/1709.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:03,132 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1727.93/1711.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:04,500 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1705.11/1681.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:05,870 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1706.62/1679.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:53:07,220 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1728.48/1708.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:08,571 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.31/1702.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:09,942 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1695.50/1684.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:11,316 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1695.98/1682.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:53:12,665 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1730.02/1708.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:14,017 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1723.87/1705.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:15,387 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1701.11/1682.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:16,758 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1699.06/1682.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:53:18,108 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.04/1709.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:19,461 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1723.60/1703.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:20,834 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1697.68/1678.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:22,205 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1701.88/1680.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:53:23,558 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1724.00/1700.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:24,912 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1721.28/1705.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:26,285 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1699.52/1679.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:27,657 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1698.91/1679.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:53:29,011 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1717.53/1705.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:30,366 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1719.60/1702.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:31,738 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1696.36/1680.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:33,109 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.33/1677.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:53:34,461 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1727.11/1705.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:35,815 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1719.30/1704.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:37,188 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1695.68/1679.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:38,561 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1696.19/1680.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:53:39,913 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1723.28/1706.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:41,264 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1723.58/1706.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:42,637 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1697.60/1678.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:44,010 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1699.64/1675.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:53:45,364 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1723.51/1701.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:46,713 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.94/1709.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:48,084 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1700.97/1681.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:49,452 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1705.37/1684.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:53:50,800 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1728.50/1712.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:52,151 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1721.16/1706.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:53,521 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.80/1680.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:54,891 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1701.77/1683.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:53:56,239 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1728.94/1709.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:57,588 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1728.83/1708.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:53:58,957 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1701.57/1681.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:54:00,331 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1695.46/1680.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:54:01,683 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.90/1703.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:54:03,034 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1724.68/1705.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:54:04,405 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.67/1678.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:54:05,777 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1703.44/1673.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:54:07,128 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.07/1706.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:54:08,482 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1719.21/1704.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:54:09,852 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.94/1680.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:54:11,222 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.06/1682.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:54:12,573 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1723.41/1707.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:54:13,921 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1730.37/1709.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:54:15,292 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1698.72/1682.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:54:16,666 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1697.30/1676.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:54:18,018 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1722.39/1705.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:54:19,369 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.64/1707.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:54:20,742 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1694.66/1677.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:54:22,116 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1695.21/1680.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:54:23,465 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1725.59/1707.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:54:24,818 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1725.13/1703.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:54:26,189 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1698.77/1682.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:54:27,558 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1703.64/1684.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:54:29,729 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1724.59/1204.63 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:54:32,222 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1725.00/1203.95 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:54:34,551 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1712.82/1279.59 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:54:36,859 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1707.72/1284.02 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:54:39,356 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1717.99/1204.90 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:54:41,861 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1724.96/1205.24 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:54:44,184 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1690.92/1291.71 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:54:46,496 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1708.19/1281.95 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:54:48,985 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1716.13/1206.34 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:54:51,477 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1727.55/1206.27 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:54:53,795 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1708.11/1285.54 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:54:56,112 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1705.11/1280.61 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:54:58,605 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1720.43/1204.83 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:01,088 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1721.61/1206.05 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:03,421 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1701.69/1285.27 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:55:05,731 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1711.94/1289.80 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:55:08,221 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1726.69/1206.09 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:10,714 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1722.85/1206.03 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:13,039 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1708.12/1274.30 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:55:15,351 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1709.25/1274.28 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:55:17,847 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1725.34/1206.51 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:20,339 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1724.87/1206.09 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:22,656 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1709.65/1276.97 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:55:24,977 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1713.85/1278.72 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:55:27,469 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1728.43/1205.36 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:29,976 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1728.63/1202.56 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:32,287 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1706.38/1293.03 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:55:34,596 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1707.94/1276.08 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:55:37,074 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1724.32/1204.16 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:39,555 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1725.87/1206.04 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:41,858 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1707.56/1288.11 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:55:44,171 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1703.45/1285.00 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:55:46,655 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1726.50/1206.06 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:49,133 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1727.25/1206.48 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:51,459 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1705.24/1278.66 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:55:53,767 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1706.77/1284.25 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:55:56,259 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1725.62/1205.59 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:55:58,749 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1727.79/1204.41 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:01,067 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1710.61/1279.93 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:56:03,369 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1705.32/1275.63 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:56:05,864 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1724.04/1205.84 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:08,355 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1723.47/1204.21 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:10,674 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1712.01/1284.10 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:56:12,992 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1707.24/1285.35 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:56:15,476 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1727.61/1206.53 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:17,961 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1726.00/1205.68 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:20,280 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1711.96/1283.18 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:56:22,579 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1708.07/1285.84 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:56:25,072 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1726.62/1204.05 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:27,571 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1723.09/1206.50 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:29,882 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1708.30/1283.69 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:56:32,177 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1708.12/1285.40 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:56:34,666 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1721.32/1206.83 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:37,162 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1731.46/1205.41 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:39,470 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1716.31/1276.31 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:56:41,781 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1715.54/1286.96 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:56:44,268 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1725.42/1205.48 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:46,757 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1723.34/1206.59 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:49,089 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1709.20/1275.45 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:56:51,397 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1711.73/1267.76 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:56:53,893 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1721.31/1204.31 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:56,371 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1725.04/1204.96 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:56:58,683 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1710.73/1280.75 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:57:00,975 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1708.50/1288.27 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:57:03,463 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1723.55/1205.15 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:57:05,945 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1723.18/1205.68 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:57:08,273 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1705.41/1283.51 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:57:10,582 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1704.91/1282.47 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:57:13,069 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1722.35/1206.88 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:57:15,551 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1726.22/1205.49 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:57:17,859 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1705.54/1286.41 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:57:20,152 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1705.77/1283.29 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:57:22,653 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1722.32/1206.16 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:57:25,147 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1724.87/1205.70 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:57:27,462 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1705.05/1279.82 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:57:29,771 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1710.96/1277.92 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:57:32,270 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1725.54/1205.59 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:57:34,762 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1730.71/1204.31 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:57:37,095 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1704.94/1274.11 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:57:39,397 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1709.58/1279.88 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:57:41,890 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1721.72/1205.04 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:57:44,376 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1728.01/1203.93 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:57:46,694 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1705.71/1288.41 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:57:48,999 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1713.65/1279.01 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:57:51,500 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1719.47/1206.59 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:57:53,996 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1727.86/1206.35 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:57:56,315 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1715.67/1289.35 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:57:58,621 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1706.39/1281.12 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:58:01,128 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1715.02/1205.77 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:58:03,627 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1728.37/1205.82 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:58:05,962 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1716.80/1279.98 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:58:08,271 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1709.50/1279.98 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:58:10,772 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1725.33/1206.08 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:58:13,270 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.16 (max/real adjusted throughput: 1721.03/1205.38 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 00:58:15,603 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1710.26/1282.61 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 00:58:17,924 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.15 (max/real adjusted throughput: 1709.37/1277.30 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 00:58:19,641 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1678.77/1648.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:20,996 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.11/1699.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:22,364 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1710.46/1680.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:23,734 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1705.90/1677.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:58:25,088 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1729.62/1695.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:26,445 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1721.50/1695.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:27,817 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1703.88/1676.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:29,188 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.40/1676.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:58:30,542 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1728.02/1697.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:31,900 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1724.44/1693.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:33,271 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1701.04/1680.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:34,642 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1707.03/1675.15 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:58:36,002 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1717.69/1692.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:37,358 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1720.56/1696.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:38,732 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1703.38/1673.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:40,103 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1703.57/1680.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:58:41,460 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1723.35/1699.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:42,815 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.94/1694.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:44,187 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1702.16/1674.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:45,561 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1702.49/1674.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:58:46,920 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1722.56/1690.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:48,277 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1718.10/1695.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:49,652 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1700.45/1674.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:51,023 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1708.63/1676.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:58:52,381 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1723.89/1687.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:53,734 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1731.30/1695.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:55,106 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1700.50/1677.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:56,480 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1704.97/1673.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:58:57,839 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1723.00/1688.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:58:59,198 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1719.73/1693.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:00,572 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1698.67/1675.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:01,945 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1705.49/1673.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:03,301 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.22/1691.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:04,657 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1724.01/1695.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:06,030 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1701.34/1676.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:07,400 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.84/1683.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:08,760 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1720.05/1688.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:10,118 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1720.72/1692.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:11,489 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.70/1678.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:12,862 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1702.52/1674.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:14,222 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1719.53/1689.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:15,579 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1728.17/1689.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:16,955 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1696.51/1673.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:18,328 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.41/1676.34 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:19,683 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1725.09/1697.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:21,040 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1720.65/1697.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:22,412 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1709.10/1670.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:23,787 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1698.81/1676.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:25,142 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.89/1697.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:26,501 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1723.21/1691.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:27,872 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1705.76/1675.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:29,244 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.17/1676.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:30,600 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1720.21/1699.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:31,959 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1723.76/1689.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:33,332 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1698.07/1678.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:34,708 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1697.21/1676.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:36,062 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.17/1700.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:37,419 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.43/1690.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:38,792 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.42/1676.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:40,164 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1703.46/1678.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:41,519 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1724.68/1695.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:42,872 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1730.49/1697.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:44,247 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1701.21/1673.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:45,616 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1712.11/1673.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:46,971 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1728.40/1697.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:48,327 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1723.45/1694.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:49,700 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1702.96/1676.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:51,076 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1696.64/1672.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:52,434 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.47/1689.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:53,790 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1725.33/1697.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:55,164 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1700.71/1674.74 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:56,536 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1705.68/1674.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 00:59:57,891 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1723.16/1698.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 00:59:59,245 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.72/1697.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:00,614 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1707.30/1679.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:01,987 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1705.12/1675.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:00:03,343 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1724.10/1694.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:04,698 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1725.88/1697.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:06,069 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1707.05/1676.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:07,441 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.09/1677.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:00:08,797 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1724.45/1694.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:10,149 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1728.36/1699.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:11,521 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1704.77/1676.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:12,893 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1703.04/1676.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:00:14,249 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1721.65/1695.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:15,606 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1723.35/1694.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:16,975 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1707.90/1680.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:18,347 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1703.68/1675.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:00:19,703 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1723.44/1699.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:21,058 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.72/1696.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:22,427 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1709.21/1675.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:23,797 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1705.67/1678.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 01:00:25,154 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1725.85/1695.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:26,508 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 5 iterations: 0.11 (max/real adjusted throughput: 1726.12/1697.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:27,881 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1705.14/1672.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:29,254 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 5 iterations: 0.12 (max/real adjusted throughput: 1704.92/1673.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 01:00:29,255 | xffl.distributed.aggregation |     INFO | Dumping benchmarking results to /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs/llama3.1-8b_ns_1_fs_4.csv[0m
[38;20m2025-08-01 01:00:33,045 | xffl.cli.simulate |     INFO | Total simulation execution time: 1021.92 seconds[0m
[38;20m2025-08-01 01:00:33,045 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
