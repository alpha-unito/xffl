[38;20m2025-08-01 16:54:33,372 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
[38;5;39m2025-08-01 16:54:33,374 | xffl.cli.simulate |    DEBUG | Using current virtual environment: /leonardo_scratch/fast/uToID_bench/xffl/.venv[0m
[38;5;39m2025-08-01 16:54:33,374 | xffl.cli.simulate |    DEBUG | New local simulation xFFL environment variables: {'XFFL_VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv', 'XFFL_WORLD_SIZE': '2', 'XFFL_NUM_NODES': '2', 'MASTER_ADDR': 'lrdn1249', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;5;39m2025-08-01 16:54:33,374 | xffl.cli.simulate |    DEBUG | Updated xFFL environment: {'XFFL_VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv', 'XFFL_WORLD_SIZE': '2', 'XFFL_NUM_NODES': '2', 'MASTER_ADDR': 'lrdn1249', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;20m2025-08-01 16:54:33,376 | xffl.cli.simulate |     INFO | Running local simulation...[0m
[38;5;39m2025-08-01 16:54:33,376 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1249: ssh -oStrictHostKeyChecking=no lrdn1249 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=2 XFFL_NUM_NODES=2 MASTER_ADDR=lrdn1249 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_2_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-01 16:54:33,376 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1291: ssh -oStrictHostKeyChecking=no lrdn1291 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=2 XFFL_NUM_NODES=2 MASTER_ADDR=lrdn1249 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_2_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-01 16:54:51,660 | xffl.distributed.distributed_state |    DEBUG | Setting Symmetric Federated Scaling with sizes (1, 1)[0m
[38;5;39m2025-08-01 16:54:51,691 | xffl.distributed.distributed |    DEBUG | [Rank 0]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn1249
                    Master port=29500
                    Rank=0
                    World size=2
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=0
                    Node world size=2
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1)
                    Federated rank=0
                    Federated world size=2
                MESHES:
                    FSDP=DeviceMesh('cuda', [0], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1]
                    Replica group=None
                    Federation=[0]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb6d5030>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb6f2270>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc39550>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc39850>)
                [0m
[38;5;39m2025-08-01 16:54:51,691 |         __main__ |    DEBUG | Rendez-vous time: 7.74 seconds[0m
[38;5;39m2025-08-01 16:54:51,691 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-01 16:54:51,691 | xffl.distributed.distributed |    DEBUG | [Rank 1]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn1249
                    Master port=29500
                    Rank=1
                    World size=2
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=1
                    Node world size=2
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1)
                    Federated rank=1
                    Federated world size=2
                MESHES:
                    FSDP=DeviceMesh('cuda', [1], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1]
                    Replica group=None
                    Federation=[1]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcf8a290>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcfa74d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd4eefd0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd4ef2d0>)
                [0m
[38;5;39m2025-08-01 16:54:51,691 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-01 16:54:52,510 |         __main__ |    DEBUG | Model loading time: 0.82 seconds[0m
[38;5;39m2025-08-01 16:54:52,511 |         __main__ |    DEBUG | Training llama3.1-8b: 8030.26 million trainable parameters[0m
[38;5;39m2025-08-01 16:54:52,511 | xffl.distributed.distributed |    DEBUG | [Rank 0]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-01 16:54:52,530 | xffl.distributed.distributed |    DEBUG | [Rank 1]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1249:1620489:1620489 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.165<0>
lrdn1249:1620489:1620489 [0] NCCL INFO cudaDriverVersion 12020
lrdn1249:1620489:1620489 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1249:1620489:1620489 [0] NCCL INFO Comm config Blocking set to 1
lrdn1291:1644077:1644077 [0] NCCL INFO Bootstrap: Using ib0:10.128.26.77<0>
lrdn1291:1644077:1644077 [0] NCCL INFO cudaDriverVersion 12020
lrdn1291:1644077:1644077 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1291:1644077:1644077 [0] NCCL INFO Comm config Blocking set to 1
lrdn1249:1620489:1620579 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1249:1620489:1620579 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1291:1644077:1644166 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1291:1644077:1644166 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1291:1644077:1644166 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.26.77<0>
lrdn1249:1620489:1620579 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.165<0>
lrdn1291:1644077:1644166 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1291:1644077:1644166 [0] NCCL INFO Using network IB
lrdn1249:1620489:1620579 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1249:1620489:1620579 [0] NCCL INFO Using network IB
lrdn1291:1644077:1644166 [0] NCCL INFO ncclCommInitRankConfig comm 0xe8658c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf446f96731ae5c5 - Init START
lrdn1291:1644077:1644166 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1291:1644077:1644166 [0] NCCL INFO Bootstrap timings total 0.000382 (create 0.000021, send 0.000060, recv 0.000091, ring 0.000001, delay 0.000001)
lrdn1249:1620489:1620579 [0] NCCL INFO ncclCommInitRankConfig comm 0xcfae140 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xa4cf7fc99ed5eb65 - Init START
lrdn1249:1620489:1620579 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1249:1620489:1620579 [0] NCCL INFO Bootstrap timings total 0.000598 (create 0.000027, send 0.000069, recv 0.000262, ring 0.000002, delay 0.000001)
lrdn1291:1644077:1644166 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1291:1644077:1644166 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1291:1644077:1644166 [0] NCCL INFO comm 0xe8658c0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 00/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 01/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 02/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 03/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 04/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 05/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 06/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 07/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 08/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 09/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 10/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 11/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 12/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 13/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 14/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 15/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 16/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 17/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 18/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 19/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 20/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 21/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 22/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 23/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 24/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 25/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 26/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 27/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 28/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 29/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 30/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 31/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 32/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 33/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 34/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 35/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 36/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 37/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 38/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 39/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 40/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 41/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 42/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 43/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 44/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 45/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 46/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 47/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 48/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 49/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 50/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 51/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 52/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 53/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 54/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 55/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 56/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 57/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 58/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 59/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 60/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 61/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 62/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Channel 63/64 : 0
lrdn1291:1644077:1644166 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1291:1644077:1644166 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1291:1644077:1644166 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1291:1644077:1644172 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn1291:1644077:1644173 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn1249:1620489:1620579 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1249:1620489:1620579 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1249:1620489:1620579 [0] NCCL INFO comm 0xcfae140 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 00/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 01/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 02/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 03/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 04/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 05/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 06/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 07/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 08/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 09/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 10/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 11/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 12/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 13/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 14/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 15/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 16/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 17/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 18/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 19/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 20/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 21/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 22/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 23/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 24/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 25/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 26/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 27/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 28/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 29/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 30/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 31/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 32/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 33/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 34/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 35/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 36/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 37/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 38/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 39/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 40/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 41/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 42/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 43/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 44/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 45/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 46/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 47/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 48/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 49/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 50/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 51/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 52/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 53/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 54/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 55/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 56/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 57/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 58/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 59/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 60/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 61/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 62/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Channel 63/64 : 0
lrdn1249:1620489:1620579 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1249:1620489:1620579 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1249:1620489:1620579 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1249:1620489:1620585 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn1249:1620489:1620586 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn1291:1644077:1644166 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1291:1644077:1644166 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1249:1620489:1620579 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1249:1620489:1620579 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1291:1644077:1644166 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1291:1644077:1644166 [0] NCCL INFO ncclCommInitRankConfig comm 0xe8658c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf446f96731ae5c5 - Init COMPLETE
lrdn1291:1644077:1644166 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1249:1620489:1620579 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1249:1620489:1620579 [0] NCCL INFO ncclCommInitRankConfig comm 0xcfae140 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xa4cf7fc99ed5eb65 - Init COMPLETE
lrdn1249:1620489:1620579 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-01 16:54:59,384 |         __main__ |    DEBUG | FSDP wrapping setup time: 6.87 seconds[0m
[38;5;39m2025-08-01 16:54:59,397 |         __main__ |    DEBUG | Total setup time: 15.45 seconds[0m
[38;5;39m2025-08-01 16:54:59,398 |         __main__ |    DEBUG | GPU RAM allocated before training: 16.06 GB[0m


lrdn1249:1620489:1620489 [0] NCCL INFO Comm config Blocking set to 1
lrdn1291:1644077:1644077 [0] NCCL INFO Comm config Blocking set to 1
lrdn1249:1620489:1620593 [0] NCCL INFO Using network IB
lrdn1249:1620489:1620593 [0] NCCL INFO ncclCommInitRankConfig comm 0x2a1aedd0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1b021b4419cfd2b2 - Init START
lrdn1291:1644077:1644179 [0] NCCL INFO Using network IB
lrdn1291:1644077:1644179 [0] NCCL INFO ncclCommInitRankConfig comm 0x122fce90 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1b021b4419cfd2b2 - Init START
lrdn1249:1620489:1620593 [0] NCCL INFO Bootstrap timings total 0.000878 (create 0.000016, send 0.000058, recv 0.000450, ring 0.000018, delay 0.000000)
lrdn1291:1644077:1644179 [0] NCCL INFO Bootstrap timings total 0.000789 (create 0.000023, send 0.000341, recv 0.000266, ring 0.000042, delay 0.000000)
lrdn1249:1620489:1620593 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1249:1620489:1620593 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn1291:1644077:1644179 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1291:1644077:1644179 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn1249:1620489:1620593 [0] NCCL INFO comm 0x2a1aedd0 rank 0 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
lrdn1249:1620489:1620593 [0] NCCL INFO Channel 00/04 : 0 1
lrdn1249:1620489:1620593 [0] NCCL INFO Channel 01/04 : 0 1
lrdn1249:1620489:1620593 [0] NCCL INFO Channel 02/04 : 0 1
lrdn1249:1620489:1620593 [0] NCCL INFO Channel 03/04 : 0 1
lrdn1249:1620489:1620593 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
lrdn1249:1620489:1620593 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1249:1620489:1620593 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1249:1620489:1620594 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn1291:1644077:1644179 [0] NCCL INFO comm 0x122fce90 rank 1 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
lrdn1291:1644077:1644179 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 0/-1/-1->1->-1 [3] 0/-1/-1->1->-1
lrdn1291:1644077:1644179 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1291:1644077:1644180 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1249:1620489:1620595 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn1291:1644077:1644181 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn1249:1620489:1620593 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1249:1620489:1620593 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1249:1620489:1620593 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             0              1              0              0              0              0              0  
       Reduce |        0         0         1   |             0              1              0              0              0              0              0  
    AllGather |        0         0         1   |             0              1              0              0              0              0              0  
ReduceScatter |        0         0         1   |             0              1              0              0              0              0              0  
    AllReduce |        0         0         1   |             0              1              0              0              0              0              0  

lrdn1249:1620489:1620593 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
lrdn1249:1620489:1620593 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1249:1620489:1620593 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1291:1644077:1644179 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1291:1644077:1644179 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1291:1644077:1644179 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
lrdn1291:1644077:1644179 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1249:1620489:1620593 [0] NCCL INFO ncclCommInitRankConfig comm 0x2a1aedd0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1b021b4419cfd2b2 - Init COMPLETE
lrdn1249:1620489:1620593 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 2 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1291:1644077:1644179 [0] NCCL INFO ncclCommInitRankConfig comm 0x122fce90 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1b021b4419cfd2b2 - Init COMPLETE
lrdn1291:1644077:1644179 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1249:1620489:1620597 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 7
lrdn1249:1620489:1620596 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [receive] via NET/IB/5
lrdn1249:1620489:1620596 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [receive] via NET/IB/6
lrdn1249:1620489:1620596 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [receive] via NET/IB/5
lrdn1249:1620489:1620596 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [receive] via NET/IB/6
lrdn1249:1620489:1620596 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1249:1620489:1620596 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1249:1620489:1620596 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1249:1620489:1620596 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1291:1644077:1644183 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
lrdn1291:1644077:1644182 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1291:1644077:1644182 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn1291:1644077:1644182 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1291:1644077:1644182 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn1291:1644077:1644182 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [send] via NET/IB/5
lrdn1291:1644077:1644182 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [send] via NET/IB/6
lrdn1291:1644077:1644182 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [send] via NET/IB/5
lrdn1291:1644077:1644182 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [send] via NET/IB/6
lrdn1249:1620489:1620596 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1291:1644077:1644182 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
[38;20m2025-08-01 16:55:08,658 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.30/112.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:55:17,828 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.59/111.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
lrdn1249:1620489:1620489 [0] NCCL INFO Comm config Blocking set to 1
lrdn1291:1644077:1644077 [0] NCCL INFO Comm config Blocking set to 1
lrdn1249:1620489:1620833 [0] NCCL INFO Using network IB
lrdn1291:1644077:1644419 [0] NCCL INFO Using network IB
lrdn1249:1620489:1620833 [0] NCCL INFO ncclCommInitRankConfig comm 0x2a3b4220 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x52a847e09846dbc4 - Init START
lrdn1291:1644077:1644419 [0] NCCL INFO ncclCommInitRankConfig comm 0x124fe1c0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x52a847e09846dbc4 - Init START
lrdn1249:1620489:1620833 [0] NCCL INFO Bootstrap timings total 0.000879 (create 0.000016, send 0.000058, recv 0.000425, ring 0.000022, delay 0.000000)
lrdn1291:1644077:1644419 [0] NCCL INFO Bootstrap timings total 0.000627 (create 0.000021, send 0.000137, recv 0.000197, ring 0.000062, delay 0.000000)
lrdn1291:1644077:1644419 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1249:1620489:1620833 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1291:1644077:1644419 [0] NCCL INFO comm 0x124fe1c0 rank 1 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
lrdn1291:1644077:1644419 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 0/-1/-1->1->-1 [3] 0/-1/-1->1->-1
lrdn1291:1644077:1644419 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1291:1644077:1644421 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1249:1620489:1620833 [0] NCCL INFO comm 0x2a3b4220 rank 0 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
lrdn1249:1620489:1620833 [0] NCCL INFO Channel 00/04 : 0 1
lrdn1249:1620489:1620833 [0] NCCL INFO Channel 01/04 : 0 1
lrdn1249:1620489:1620833 [0] NCCL INFO Channel 02/04 : 0 1
lrdn1249:1620489:1620833 [0] NCCL INFO Channel 03/04 : 0 1
lrdn1249:1620489:1620833 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
lrdn1249:1620489:1620833 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1249:1620489:1620833 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1249:1620489:1620834 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn1291:1644077:1644420 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn1249:1620489:1620835 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn1249:1620489:1620833 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1249:1620489:1620833 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1249:1620489:1620833 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             0              1              0              0              0              0              0  
       Reduce |        0         0         1   |             0              1              0              0              0              0              0  
    AllGather |        0         0         1   |             0              1              0              0              0              0              0  
ReduceScatter |        0         0         1   |             0              1              0              0              0              0              0  
    AllReduce |        0         0         1   |             0              1              0              0              0              0              0  

lrdn1249:1620489:1620833 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
lrdn1249:1620489:1620833 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1291:1644077:1644419 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1291:1644077:1644419 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1291:1644077:1644419 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
lrdn1291:1644077:1644419 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1249:1620489:1620833 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1249:1620489:1620833 [0] NCCL INFO ncclCommInitRankConfig comm 0x2a3b4220 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x52a847e09846dbc4 - Init COMPLETE
lrdn1249:1620489:1620833 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 2 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1291:1644077:1644419 [0] NCCL INFO ncclCommInitRankConfig comm 0x124fe1c0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x52a847e09846dbc4 - Init COMPLETE
lrdn1291:1644077:1644419 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1249:1620489:1620836 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [receive] via NET/IB/5
lrdn1249:1620489:1620837 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 9
lrdn1291:1644077:1644422 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1291:1644077:1644423 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 6
lrdn1291:1644077:1644422 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn1291:1644077:1644422 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1291:1644077:1644422 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn1291:1644077:1644422 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [send] via NET/IB/5
lrdn1291:1644077:1644422 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [send] via NET/IB/6
lrdn1291:1644077:1644422 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [send] via NET/IB/5
lrdn1291:1644077:1644422 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [send] via NET/IB/6
lrdn1249:1620489:1620836 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [receive] via NET/IB/6
lrdn1249:1620489:1620836 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [receive] via NET/IB/5
lrdn1249:1620489:1620836 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [receive] via NET/IB/6
lrdn1249:1620489:1620836 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1249:1620489:1620836 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1249:1620489:1620836 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1249:1620489:1620836 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1249:1620489:1620836 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1291:1644077:1644422 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1249:1620489:1620489 [0] NCCL INFO Comm config Blocking set to 1
lrdn1249:1620489:1620839 [0] NCCL INFO Using network IB
lrdn1291:1644077:1644077 [0] NCCL INFO Comm config Blocking set to 1
lrdn1249:1620489:1620839 [0] NCCL INFO ncclCommInitRankConfig comm 0x2a42fdb0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x897eb1cc26f7553 - Init START
lrdn1291:1644077:1644424 [0] NCCL INFO Using network IB
lrdn1291:1644077:1644424 [0] NCCL INFO ncclCommInitRankConfig comm 0x12579d50 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x897eb1cc26f7553 - Init START
lrdn1291:1644077:1644424 [0] NCCL INFO Bootstrap timings total 0.000669 (create 0.000015, send 0.000104, recv 0.000272, ring 0.000133, delay 0.000000)
lrdn1249:1620489:1620839 [0] NCCL INFO Bootstrap timings total 0.000966 (create 0.000013, send 0.000056, recv 0.000435, ring 0.000101, delay 0.000000)
lrdn1249:1620489:1620839 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1291:1644077:1644424 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1249:1620489:1620839 [0] NCCL INFO comm 0x2a42fdb0 rank 0 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
lrdn1249:1620489:1620839 [0] NCCL INFO Channel 00/04 : 0 1
lrdn1249:1620489:1620839 [0] NCCL INFO Channel 01/04 : 0 1
lrdn1249:1620489:1620839 [0] NCCL INFO Channel 02/04 : 0 1
lrdn1249:1620489:1620839 [0] NCCL INFO Channel 03/04 : 0 1
lrdn1249:1620489:1620839 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
lrdn1249:1620489:1620839 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1249:1620489:1620839 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1249:1620489:1620840 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn1249:1620489:1620841 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn1291:1644077:1644424 [0] NCCL INFO comm 0x12579d50 rank 1 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
lrdn1291:1644077:1644424 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 0/-1/-1->1->-1 [3] 0/-1/-1->1->-1
lrdn1291:1644077:1644424 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1291:1644077:1644425 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1291:1644077:1644426 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn1291:1644077:1644424 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1291:1644077:1644424 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1291:1644077:1644424 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
lrdn1291:1644077:1644424 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1249:1620489:1620839 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1249:1620489:1620839 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1249:1620489:1620839 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             0              1              0              0              0              0              0  
       Reduce |        0         0         1   |             0              1              0              0              0              0              0  
    AllGather |        0         0         1   |             0              1              0              0              0              0              0  
ReduceScatter |        0         0         1   |             0              1              0              0              0              0              0  
    AllReduce |        0         0         1   |             0              1              0              0              0              0              0  

lrdn1249:1620489:1620839 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
lrdn1249:1620489:1620839 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1249:1620489:1620839 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1291:1644077:1644424 [0] NCCL INFO ncclCommInitRankConfig comm 0x12579d50 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x897eb1cc26f7553 - Init COMPLETE
lrdn1291:1644077:1644424 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1249:1620489:1620839 [0] NCCL INFO ncclCommInitRankConfig comm 0x2a42fdb0 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x897eb1cc26f7553 - Init COMPLETE
lrdn1249:1620489:1620839 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 2 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1291:1644077:1644428 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 9
lrdn1291:1644077:1644427 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1291:1644077:1644427 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn1291:1644077:1644427 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1291:1644077:1644427 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn1291:1644077:1644427 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [send] via NET/IB/5
lrdn1291:1644077:1644427 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [send] via NET/IB/6
lrdn1291:1644077:1644427 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [send] via NET/IB/5
lrdn1291:1644077:1644427 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [send] via NET/IB/6
lrdn1249:1620489:1620843 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 11
lrdn1249:1620489:1620842 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [receive] via NET/IB/5
lrdn1249:1620489:1620842 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [receive] via NET/IB/6
lrdn1249:1620489:1620842 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [receive] via NET/IB/5
lrdn1249:1620489:1620842 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [receive] via NET/IB/6
lrdn1249:1620489:1620842 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1249:1620489:1620842 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1249:1620489:1620842 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1249:1620489:1620842 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1249:1620489:1620842 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1291:1644077:1644427 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1249:1620489:1620489 [0] NCCL INFO Comm config Blocking set to 1
lrdn1249:1620489:1620845 [0] NCCL INFO Using network IB
lrdn1249:1620489:1620845 [0] NCCL INFO ncclCommInitRankConfig comm 0x2a4ab940 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x99cc827d21d9c96b - Init START
lrdn1291:1644077:1644077 [0] NCCL INFO Comm config Blocking set to 1
lrdn1291:1644077:1644429 [0] NCCL INFO Using network IB
lrdn1291:1644077:1644429 [0] NCCL INFO ncclCommInitRankConfig comm 0x125f58e0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x99cc827d21d9c96b - Init START
lrdn1291:1644077:1644429 [0] NCCL INFO Bootstrap timings total 0.000572 (create 0.000014, send 0.000265, recv 0.000141, ring 0.000061, delay 0.000000)
lrdn1249:1620489:1620845 [0] NCCL INFO Bootstrap timings total 0.000751 (create 0.000012, send 0.000053, recv 0.000418, ring 0.000053, delay 0.000000)
lrdn1249:1620489:1620845 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1291:1644077:1644429 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1291:1644077:1644429 [0] NCCL INFO comm 0x125f58e0 rank 1 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
lrdn1291:1644077:1644429 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 0/-1/-1->1->-1 [3] 0/-1/-1->1->-1
lrdn1291:1644077:1644429 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1249:1620489:1620845 [0] NCCL INFO comm 0x2a4ab940 rank 0 nRanks 2 nNodes 2 localRanks 1 localRank 0 MNNVL 0
lrdn1249:1620489:1620845 [0] NCCL INFO Channel 00/04 : 0 1
lrdn1249:1620489:1620845 [0] NCCL INFO Channel 01/04 : 0 1
lrdn1249:1620489:1620845 [0] NCCL INFO Channel 02/04 : 0 1
lrdn1249:1620489:1620845 [0] NCCL INFO Channel 03/04 : 0 1
lrdn1249:1620489:1620845 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
lrdn1249:1620489:1620845 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1249:1620489:1620845 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1249:1620489:1620846 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn1249:1620489:1620847 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 12
lrdn1291:1644077:1644431 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn1291:1644077:1644430 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn1291:1644077:1644429 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1291:1644077:1644429 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1291:1644077:1644429 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
lrdn1291:1644077:1644429 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1249:1620489:1620845 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1249:1620489:1620845 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1249:1620489:1620845 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             0              1              0              0              0              0              0  
       Reduce |        0         0         1   |             0              1              0              0              0              0              0  
    AllGather |        0         0         1   |             0              1              0              0              0              0              0  
ReduceScatter |        0         0         1   |             0              1              0              0              0              0              0  
    AllReduce |        0         0         1   |             0              1              0              0              0              0              0  

lrdn1249:1620489:1620845 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
lrdn1249:1620489:1620845 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1249:1620489:1620845 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1291:1644077:1644429 [0] NCCL INFO ncclCommInitRankConfig comm 0x125f58e0 rank 1 nranks 2 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x99cc827d21d9c96b - Init COMPLETE
lrdn1291:1644077:1644429 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 2 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1249:1620489:1620845 [0] NCCL INFO ncclCommInitRankConfig comm 0x2a4ab940 rank 0 nranks 2 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x99cc827d21d9c96b - Init COMPLETE
lrdn1249:1620489:1620845 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 2 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1291:1644077:1644432 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1291:1644077:1644433 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 12
lrdn1291:1644077:1644432 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn1291:1644077:1644432 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1291:1644077:1644432 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn1249:1620489:1620848 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [receive] via NET/IB/5
lrdn1249:1620489:1620849 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 13
lrdn1249:1620489:1620848 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [receive] via NET/IB/6
lrdn1249:1620489:1620848 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [receive] via NET/IB/5
lrdn1249:1620489:1620848 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [receive] via NET/IB/6
lrdn1249:1620489:1620848 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1249:1620489:1620848 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1249:1620489:1620848 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1291:1644077:1644432 [0] NCCL INFO Channel 00/0 : 1[0] -> 0[0] [send] via NET/IB/5
lrdn1291:1644077:1644432 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[0] [send] via NET/IB/6
lrdn1249:1620489:1620848 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1291:1644077:1644432 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [send] via NET/IB/5
lrdn1291:1644077:1644432 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [send] via NET/IB/6
lrdn1249:1620489:1620848 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1291:1644077:1644432 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
[38;20m2025-08-01 16:55:26,685 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.82/117.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:55:35,453 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.25/116.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 16:55:44,724 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.58/111.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:55:53,965 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 110.88/111.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:56:02,768 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.08/116.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:56:11,559 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.55/116.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 16:56:20,815 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.17 (max/real adjusted throughput: 111.30/110.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:56:30,076 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.38/111.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:56:38,863 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.98/116.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:56:47,652 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.53/117.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 16:56:56,932 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.17 (max/real adjusted throughput: 110.77/110.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:57:06,168 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.21/110.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:57:14,947 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.40/117.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:57:23,908 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 114.97/113.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 16:57:33,405 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.18 (max/real adjusted throughput: 106.72/109.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:57:42,625 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.81/111.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:57:51,435 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.90/116.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:58:00,199 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 116.89/117.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 16:58:09,439 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.38/111.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:58:18,594 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.36/111.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:58:27,385 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.85/117.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:58:36,183 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.46/116.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 16:58:45,415 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 112.65/111.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:58:54,665 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 110.54/111.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:59:03,489 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.28/116.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:59:12,296 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.22/117.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 16:59:21,529 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 110.62/111.34 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:59:30,811 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.12/111.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:59:39,597 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.03/117.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 16:59:48,377 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.17/117.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 16:59:57,638 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.05/110.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:00:06,847 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.36/111.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:00:15,669 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.11/116.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:00:24,497 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.11 (max/real adjusted throughput: 116.38/115.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:00:33,764 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.51/111.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:00:43,032 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.17 (max/real adjusted throughput: 110.43/110.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:00:51,823 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 116.46/117.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:01:00,647 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.84/117.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:01:09,735 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 112.94/113.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:01:18,845 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 111.85/113.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:01:27,613 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 116.87/117.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:01:36,404 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.12/117.15 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:01:45,545 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.89/111.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:01:54,635 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 113.16/113.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:02:03,410 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.00/117.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:02:12,198 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.43/117.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:02:21,280 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 113.40/113.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:02:30,367 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 113.55/112.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:02:39,146 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.82/117.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:02:47,909 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.38/117.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:02:56,998 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 112.88/113.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:03:06,121 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 113.31/112.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:03:14,885 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.11/117.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:03:23,662 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.75/117.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:03:32,765 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 113.12/113.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:03:41,865 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 112.74/113.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:03:50,639 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.27/117.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:03:59,406 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.05/117.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:04:08,498 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 113.32/112.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:04:17,604 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 113.62/113.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:04:26,409 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.68/116.90 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:04:35,238 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.11 (max/real adjusted throughput: 116.70/115.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:04:44,461 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.54/112.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:04:53,715 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.11/111.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:05:02,543 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.11 (max/real adjusted throughput: 116.71/116.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:05:11,355 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.19/116.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:05:20,594 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 110.63/110.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:05:29,851 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 110.32/111.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:05:38,699 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.11 (max/real adjusted throughput: 116.54/116.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:05:47,499 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.83/117.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:05:56,760 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.62/110.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:06:06,024 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.17 (max/real adjusted throughput: 111.21/110.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:06:14,836 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.83/116.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:06:23,657 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.10/116.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:06:32,935 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.85/110.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:06:42,204 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.89/111.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:06:51,026 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.05/116.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:06:59,859 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.39/116.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:07:09,086 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.39/111.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:07:18,322 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.63/111.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:07:27,147 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.11 (max/real adjusted throughput: 117.22/115.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:07:35,956 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.99/116.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:07:45,120 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.29/112.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:07:54,338 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.53/112.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:08:03,154 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.83/116.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:08:11,969 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.43/116.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:08:21,203 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.42/111.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:08:30,414 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 110.07/112.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:08:39,260 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.01/116.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:08:48,076 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.65/116.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:08:57,332 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.84/111.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:09:06,599 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 110.81/110.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:09:15,435 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.11 (max/real adjusted throughput: 116.55/116.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:09:24,233 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.88/116.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:09:33,499 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 112.37/111.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:09:42,744 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.65/110.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:09:51,512 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.18/117.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:10:00,311 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.89/116.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:10:09,483 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.81/112.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:10:18,740 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.64/111.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:10:27,540 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.56/116.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:10:36,333 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.12/116.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:10:45,582 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 110.80/110.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:10:54,825 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.88/111.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:11:03,606 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.94/117.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:11:12,411 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.90/116.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:11:21,684 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.34/110.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:11:30,950 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 110.29/110.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:11:39,767 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.85/116.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:11:48,561 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.98/116.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:11:57,835 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.17 (max/real adjusted throughput: 111.18/110.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:12:07,086 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 109.87/111.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:12:15,876 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.12/116.90 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:12:24,665 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.95/116.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:12:33,892 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.23/111.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:12:43,127 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 110.31/111.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:12:51,915 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.95/117.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:13:00,709 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.11/117.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:13:09,971 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 110.47/111.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:13:19,201 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.53/111.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:13:28,009 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.12/116.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:13:36,792 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.08/116.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:13:46,095 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.17 (max/real adjusted throughput: 110.65/109.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:13:55,314 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.03/111.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:14:04,117 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.00/116.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:14:12,928 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.31/116.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:14:22,194 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.27/111.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:14:31,466 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.17 (max/real adjusted throughput: 110.81/110.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:14:40,249 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.91/117.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:14:49,024 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.03/117.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:14:58,276 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.01/111.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:15:07,508 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 112.24/111.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:15:16,306 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.38/117.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:15:25,102 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.87/117.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:15:34,325 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.32/110.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:15:43,558 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.70/111.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:15:52,338 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.92/117.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:16:01,129 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.14/116.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:16:10,364 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 110.94/111.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:16:19,666 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.63/111.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:16:28,451 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.75/117.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:16:37,257 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.75/116.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:16:46,539 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.17 (max/real adjusted throughput: 111.31/109.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:16:55,757 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.41/111.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:17:04,556 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.95/116.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:17:13,343 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.72/117.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:17:22,405 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 113.25/113.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:17:31,460 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 113.27/113.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:17:40,228 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.40/117.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:17:49,007 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.13/117.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:17:58,203 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.15/111.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:18:07,410 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 110.58/111.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:18:16,214 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.65/116.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:18:24,999 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.91/117.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:18:34,262 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.03/111.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:18:43,511 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 110.56/110.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:18:52,299 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.03/116.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:19:01,100 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.46/116.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:19:10,365 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.77/111.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:19:19,642 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.17 (max/real adjusted throughput: 111.81/109.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:19:28,435 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.78/117.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:19:37,226 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.78/116.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:19:46,554 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 110.99/110.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:19:55,870 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.17 (max/real adjusted throughput: 111.44/109.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:20:04,658 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.85/116.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:20:13,462 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.25/116.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:20:22,668 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.98/111.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:20:31,932 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.62/111.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:20:40,731 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.84/116.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:20:49,540 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.94/116.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:20:58,812 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 109.64/111.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:21:08,077 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.85/111.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:21:16,886 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.44/116.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:21:25,668 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.41/116.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:21:34,920 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.67/111.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:21:44,174 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.77/110.74 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:21:52,988 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.15/116.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:22:01,774 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.03/117.06 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:22:11,017 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.01/111.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:22:20,315 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.31/110.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:22:29,115 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.91/116.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:22:37,909 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.02/117.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:22:47,157 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 109.91/111.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:22:56,383 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.09/112.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:23:05,188 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.95/116.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:23:13,969 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.94/117.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:23:23,208 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.39/110.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:23:32,448 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 110.90/111.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:23:41,259 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 116.95/116.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:23:50,037 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.19/117.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:23:59,632 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 113.07/105.91 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:24:09,396 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.23 (max/real adjusted throughput: 111.57/104.76 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:24:18,520 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 117.49/111.68 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:24:27,625 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 118.11/112.38 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:24:37,341 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 111.43/105.42 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:24:47,043 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 111.15/106.21 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:24:56,142 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 118.11/113.06 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:25:05,229 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.85/112.87 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:25:14,937 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.15/106.20 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:25:24,608 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 113.42/106.48 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:25:33,697 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 118.45/112.95 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:25:42,781 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 118.08/112.89 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:25:52,488 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.14/106.52 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:26:02,219 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.67/106.11 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:26:11,323 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 117.93/111.94 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:26:20,430 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.92/112.31 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:26:30,188 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 111.61/105.94 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:26:39,888 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 112.01/105.24 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:26:48,999 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.37/112.57 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:26:58,100 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.21/112.82 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:27:07,811 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 111.50/105.17 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:27:17,490 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.20 (max/real adjusted throughput: 112.04/106.75 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:27:26,573 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.28/113.20 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:27:35,659 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 118.27/112.68 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:27:45,363 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.98/106.09 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:27:55,072 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 113.13/106.41 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:28:04,153 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.88/113.18 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:28:13,248 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.55/113.19 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:28:22,987 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.33/106.22 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:28:32,695 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 112.79/105.39 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:28:41,761 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 117.02/113.91 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:28:50,862 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.99/112.39 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:29:00,599 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.23 (max/real adjusted throughput: 111.77/104.75 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:29:10,262 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.46/106.59 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:29:19,365 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 118.15/112.39 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:29:28,465 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.03/112.93 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:29:38,170 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 113.05/106.35 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:29:47,876 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.90/106.21 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:29:56,942 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 117.69/113.51 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:30:06,023 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.67/112.84 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:30:15,732 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 111.51/106.11 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:30:25,426 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.20 (max/real adjusted throughput: 113.08/106.63 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:30:34,499 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.22/113.33 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:30:43,581 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.40/113.02 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:30:53,304 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.68/106.19 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:31:03,038 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 111.64/105.87 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:31:12,126 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.98/112.74 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:31:21,216 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 118.02/112.91 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:31:30,955 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.23 (max/real adjusted throughput: 112.73/104.53 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:31:40,684 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 111.63/105.14 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:31:49,776 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.79/112.73 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:31:58,855 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 117.05/113.70 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:32:08,541 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.20 (max/real adjusted throughput: 111.21/106.89 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:32:18,235 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.20 (max/real adjusted throughput: 113.03/106.68 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:32:27,317 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 117.71/113.36 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:32:36,381 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 117.64/113.54 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:32:46,116 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.78/105.99 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:32:55,865 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 111.14/105.27 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:33:04,943 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.55/112.91 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:33:14,020 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 117.75/113.23 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:33:23,707 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 111.43/106.38 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:33:33,462 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 110.99/105.39 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:33:42,542 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.76/113.20 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:33:51,660 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.65/112.48 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:34:01,397 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.19/106.08 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:34:11,101 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.31/106.54 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:34:20,217 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.69/112.50 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:34:29,305 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 118.04/113.18 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:34:39,071 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.23 (max/real adjusted throughput: 110.76/104.56 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:34:48,787 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 111.22/106.39 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:34:57,879 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.74/112.65 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:35:06,975 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.50/112.73 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:35:16,690 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 111.51/106.50 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:35:26,446 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 112.94/105.51 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:35:35,517 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.12/113.28 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:35:44,638 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.49/112.24 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:35:54,377 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 112.11/105.45 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:36:04,091 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 112.17/105.45 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:36:13,173 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.78/113.13 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:36:22,281 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.69/113.19 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:36:31,962 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 113.37/106.32 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:36:41,680 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.66/106.57 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:36:50,772 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.72/112.76 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:36:59,854 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 117.70/113.30 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:37:09,602 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 110.71/105.97 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:37:19,320 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 111.78/105.38 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:37:28,392 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.89/113.11 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:37:37,517 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.56/112.31 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:37:47,203 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.40/106.30 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:37:56,906 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.98/106.59 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:38:06,000 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 117.48/113.26 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:38:15,092 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.88/112.55 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:38:24,829 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 112.58/105.52 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:38:34,538 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.07/106.14 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:38:43,633 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 118.24/112.23 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-01 17:38:52,740 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 117.89/112.27 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-01 17:39:02,078 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.35/111.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:39:11,259 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 110.72/112.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:39:20,012 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.02/117.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:39:28,738 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.45/117.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:39:37,964 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.13/110.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:39:47,129 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.66/112.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:39:55,862 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 118.16/117.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:40:04,594 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.18/117.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:40:13,740 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.52/112.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:40:22,903 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.84/112.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:40:31,645 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.96/117.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:40:40,391 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.14/117.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:40:49,548 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.26/112.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:40:58,768 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.63/111.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:41:07,524 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.68/117.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:41:16,252 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.08/117.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:41:25,421 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.71/112.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:41:34,618 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.71/111.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:41:43,346 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.94/117.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:41:52,078 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.78/117.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:42:01,288 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.86/112.06 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:42:10,506 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 112.39/111.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:42:19,277 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.11 (max/real adjusted throughput: 117.85/116.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:42:28,034 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.72/117.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:42:37,213 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.22/112.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:42:46,411 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.97/111.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:42:55,159 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.82/117.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:43:03,892 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.14/117.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:43:13,068 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.84/111.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:43:22,249 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 110.63/111.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:43:30,995 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 118.03/117.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:43:39,725 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.61/117.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:43:48,921 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.55/111.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:43:58,050 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.48/112.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:44:06,789 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.54/118.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:44:15,524 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 116.81/118.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:44:24,790 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 111.37/110.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:44:34,002 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 110.79/111.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:44:42,731 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.11/117.91 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:44:51,473 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.33/117.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:45:00,621 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.67/112.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:45:09,792 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.93/112.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:45:18,527 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.67/117.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:45:27,274 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.53/117.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:45:36,402 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.45/112.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:45:45,614 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 113.53/110.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:45:54,344 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.36/117.91 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:46:03,080 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.45/117.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:46:12,284 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.27/112.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:46:21,455 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.64/112.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:46:30,197 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.88/117.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:46:38,917 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.53/118.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:46:48,122 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.63/111.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:46:57,266 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.02/112.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:47:06,006 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 118.11/116.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:47:14,745 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.89/117.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:47:23,909 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.69/112.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:47:33,110 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 110.84/111.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:47:41,853 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.89/117.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:47:50,580 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.46/118.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:47:59,768 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.73/111.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:48:08,935 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.87/111.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:48:17,673 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 118.39/117.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:48:26,397 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.49/118.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:48:35,571 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.37/112.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:48:44,768 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.40/112.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:48:53,520 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.06/117.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:49:02,271 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 118.32/116.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:49:11,423 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.89/112.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:49:20,597 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.82/111.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:49:29,339 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.08/117.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:49:38,082 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.90/117.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:49:47,299 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.52/111.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:49:56,480 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.79/112.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:50:05,226 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.58/117.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:50:13,972 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.65/117.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:50:23,131 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.55/112.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:50:32,329 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.63/112.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:50:41,043 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.74/118.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:50:49,801 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.79/117.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:50:59,007 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.59/112.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:51:08,179 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.15/112.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:51:16,919 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.98/117.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:51:25,638 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.55/118.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:51:34,882 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.17 (max/real adjusted throughput: 112.02/110.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:51:44,025 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.84/112.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:51:52,773 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.28/117.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:52:01,528 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.87/117.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:52:10,713 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.46/111.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:52:19,896 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.34/112.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:52:28,623 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.54/117.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:52:37,365 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.10 (max/real adjusted throughput: 117.42/117.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:52:46,561 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.51/111.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:52:55,687 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.43/112.90 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:53:04,410 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.98/117.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 17:53:13,139 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.12/117.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 17:53:22,786 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 110.54/105.22 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:53:32,487 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 111.58/106.07 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:53:41,543 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.10/113.30 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 17:53:50,572 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.82/113.77 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 17:54:00,298 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 112.41/105.59 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:54:10,009 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 111.48/106.05 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:54:19,015 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.12 (max/real adjusted throughput: 118.87/114.29 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 17:54:28,051 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.63/113.44 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 17:54:37,787 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 111.56/105.96 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:54:47,533 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.23 (max/real adjusted throughput: 112.77/104.45 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:54:56,560 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.58/113.73 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 17:55:05,597 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.95/113.41 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 17:55:15,268 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.13/106.54 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:55:24,977 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.87/106.15 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:55:34,012 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 118.62/113.20 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 17:55:43,039 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.61/113.78 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 17:55:52,754 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 111.31/106.33 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:56:02,383 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 113.46/105.95 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:56:11,392 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.51/114.14 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 17:56:20,416 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.77/113.99 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 17:56:30,092 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.20 (max/real adjusted throughput: 111.43/106.86 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:56:39,860 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.43/105.77 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:56:48,882 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.68/113.74 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 17:56:57,944 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 118.91/113.00 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 17:57:07,684 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.23 (max/real adjusted throughput: 111.88/104.73 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:57:17,356 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.20 (max/real adjusted throughput: 112.17/107.17 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:57:26,402 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.69/113.33 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 17:57:35,423 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.55/113.83 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 17:57:45,185 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.23 (max/real adjusted throughput: 112.07/104.72 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:57:54,878 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.20 (max/real adjusted throughput: 111.56/106.87 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:58:03,902 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.80/113.49 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 17:58:12,942 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.06/113.59 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 17:58:22,636 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.17/106.43 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:58:32,403 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 111.33/105.10 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:58:41,450 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.31/113.48 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 17:58:50,504 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 118.58/113.07 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 17:59:00,197 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.20 (max/real adjusted throughput: 112.50/106.71 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:59:09,913 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.71/105.93 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:59:18,952 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.50/113.65 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 17:59:27,994 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.27/113.73 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 17:59:37,682 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.03/106.24 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:59:47,442 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.36/106.13 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 17:59:56,486 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.46/113.28 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 18:00:05,514 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 119.01/113.46 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 18:00:15,264 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.23 (max/real adjusted throughput: 110.71/104.85 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:00:25,025 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 111.80/105.67 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:00:34,066 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.71/113.78 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 18:00:43,113 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.64/113.48 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 18:00:52,829 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 111.27/105.45 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:01:02,535 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 111.97/106.50 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:01:11,564 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.63/114.00 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 18:01:20,608 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 118.51/113.07 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 18:01:30,338 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 111.64/105.23 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:01:40,032 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 111.16/106.49 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:01:49,077 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.54/113.26 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 18:01:58,105 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.09/114.16 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 18:02:07,810 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 112.58/105.54 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:02:17,533 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 110.93/106.32 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:02:26,571 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.83/113.36 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 18:02:35,596 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.41/113.82 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 18:02:45,323 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 113.34/104.94 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:02:55,028 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 111.71/106.22 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:03:04,076 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.25/113.65 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 18:03:13,102 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.40/113.96 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 18:03:22,824 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 112.33/105.60 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:03:32,565 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 111.35/104.94 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:03:41,598 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.73/113.70 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 18:03:50,645 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.12/113.93 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 18:04:00,389 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 112.25/105.64 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:04:10,114 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 111.66/105.16 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:04:19,159 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.50/113.56 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 18:04:28,175 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.68/113.83 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 18:04:37,891 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.09/106.20 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:04:47,595 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.21 (max/real adjusted throughput: 112.21/105.80 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:04:56,591 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.81/114.21 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 18:05:05,618 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.88/113.88 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 18:05:15,382 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 110.67/105.09 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:05:24,977 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.18 (max/real adjusted throughput: 112.45/108.91 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:05:34,012 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.93/113.36 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 18:05:43,011 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 119.08/114.10 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 18:05:52,581 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.20 (max/real adjusted throughput: 113.90/107.15 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:06:02,086 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.18 (max/real adjusted throughput: 113.62/108.79 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:06:11,101 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 119.00/113.70 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 18:06:20,098 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 119.04/114.15 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 18:06:29,589 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.19 (max/real adjusted throughput: 115.23/108.31 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:06:39,058 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.18 (max/real adjusted throughput: 114.80/108.97 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:06:48,057 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.12 (max/real adjusted throughput: 118.76/114.34 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 18:06:57,051 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.12 (max/real adjusted throughput: 119.08/114.23 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 18:07:06,523 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.18 (max/real adjusted throughput: 115.37/108.64 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:07:16,064 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.19 (max/real adjusted throughput: 113.90/107.61 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:07:25,083 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.89/113.91 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 18:07:34,088 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 119.00/113.96 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 18:07:43,706 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 115.32/105.68 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:07:53,433 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.22 (max/real adjusted throughput: 112.09/105.59 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-01 18:08:02,475 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.72/113.95 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-01 18:08:11,487 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.13 (max/real adjusted throughput: 118.73/113.99 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-01 18:08:20,797 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.10/112.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:08:29,956 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.43/112.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:08:38,644 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.47/118.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:08:47,311 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.78/118.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:08:56,469 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 110.95/112.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:09:05,672 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.56/111.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:09:14,349 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.28/118.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:09:23,019 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.42/118.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:09:32,209 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.33/111.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:09:41,358 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.43/112.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:09:50,038 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.41/118.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:09:58,713 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.59/118.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:10:07,864 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.60/112.15 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:10:17,040 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.44/111.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:10:25,716 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.67/118.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:10:34,396 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.51/118.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:10:43,572 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.47/111.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:10:52,736 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 113.33/111.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:11:01,410 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.05/118.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:11:10,102 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.68/117.90 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:11:19,311 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.83/112.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:11:28,521 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 110.93/112.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:11:37,214 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.31/118.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:11:45,885 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.57/118.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:11:55,027 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.32/112.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:12:04,257 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.14/111.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:12:12,930 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.77/118.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:12:21,631 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.64/117.74 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:12:30,795 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.56/112.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:12:39,939 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.78/111.90 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:12:48,609 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.79/118.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:12:57,290 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.77/118.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:13:06,426 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.14/112.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:13:15,587 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.78/112.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:13:24,260 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.54/118.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:13:32,944 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.23/118.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:13:42,137 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.89/112.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:13:51,298 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.48/112.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:13:59,992 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 117.91/118.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:14:08,678 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.35/117.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:14:17,903 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.13/111.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:14:27,072 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.22/111.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:14:35,752 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.45/118.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:14:44,438 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.33/118.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:14:53,602 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.24/112.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:15:02,790 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.75/111.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:15:11,465 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.82/118.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:15:20,137 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.24/118.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:15:29,292 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.60/112.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:15:38,445 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 113.00/112.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:15:47,116 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.24/118.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:15:55,800 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.53/118.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:16:05,017 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 111.89/112.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:16:14,166 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.33/112.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:16:22,833 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.80/118.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:16:31,508 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.76/118.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:16:40,662 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.19/112.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:16:49,887 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.16 (max/real adjusted throughput: 112.15/111.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:16:58,558 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.68/118.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:17:07,224 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.62/118.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:17:16,316 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 113.36/112.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:17:25,500 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.35/112.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:17:34,172 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.73/118.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:17:42,837 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.53/118.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:17:52,001 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 113.43/111.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:18:01,121 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.70/112.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:18:09,792 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.42/118.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:18:18,474 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.69/118.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:18:27,657 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.20/112.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:18:36,830 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.30/112.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:18:45,507 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.68/118.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:18:54,172 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 119.10/118.34 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:19:03,335 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.14/112.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:19:12,532 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.19/111.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:19:21,222 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.36/118.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:19:29,892 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.60/118.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:19:39,048 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.95/111.91 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:19:48,244 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.41/111.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:19:56,917 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.52/118.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:20:05,592 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.26/118.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:20:14,788 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.65/111.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:20:23,921 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.54/112.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:20:32,593 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.35/118.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:20:41,256 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.98/118.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:20:50,404 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 113.18/112.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:20:59,551 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.34/112.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:21:08,219 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.59/118.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:21:16,898 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.65/118.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:21:26,060 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 111.77/112.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:21:35,205 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.14 (max/real adjusted throughput: 112.51/112.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:21:43,895 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.71/118.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:21:52,570 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.72/118.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-01 18:22:01,718 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.15/112.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:22:10,881 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.15 (max/real adjusted throughput: 112.36/112.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:22:19,553 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.08 (max/real adjusted throughput: 118.21/118.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:22:28,245 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.09 (max/real adjusted throughput: 118.44/118.15 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-01 18:22:28,246 | xffl.distributed.aggregation |     INFO | Dumping benchmarking results to /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs/llama3.1-8b_ns_2_fs_1_ppn_1.csv[0m
lrdn1291:1644077:1648650 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1291:1644077:1648650 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1291:1644077:1648650 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1291:1644077:1644430 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1249:1620489:1625069 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1249:1620489:1625069 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1249:1620489:1625069 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1249:1620489:1620846 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1291:1644077:1648650 [0] NCCL INFO comm 0x125f58e0 rank 1 nranks 2 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1249:1620489:1625069 [0] NCCL INFO comm 0x2a4ab940 rank 0 nranks 2 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1291:1644077:1648652 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1291:1644077:1648652 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1291:1644077:1648652 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1291:1644077:1644425 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1249:1620489:1625071 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1249:1620489:1625071 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1249:1620489:1620840 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1249:1620489:1625071 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1291:1644077:1648652 [0] NCCL INFO comm 0x12579d50 rank 1 nranks 2 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1249:1620489:1625071 [0] NCCL INFO comm 0x2a42fdb0 rank 0 nranks 2 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1249:1620489:1625073 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1249:1620489:1625073 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1249:1620489:1625073 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1249:1620489:1620834 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1291:1644077:1648654 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1291:1644077:1648654 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1291:1644077:1648654 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1291:1644077:1644420 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1249:1620489:1625073 [0] NCCL INFO comm 0x2a3b4220 rank 0 nranks 2 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1291:1644077:1648654 [0] NCCL INFO comm 0x124fe1c0 rank 1 nranks 2 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1291:1644077:1648656 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1291:1644077:1648656 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1291:1644077:1648656 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1291:1644077:1644180 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1249:1620489:1625075 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1249:1620489:1625075 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1249:1620489:1625075 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1249:1620489:1620594 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1291:1644077:1648656 [0] NCCL INFO comm 0x122fce90 rank 1 nranks 2 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1249:1620489:1625075 [0] NCCL INFO comm 0x2a1aedd0 rank 0 nranks 2 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1291:1644077:1648659 [0] NCCL INFO comm 0xe8658c0 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1249:1620489:1625078 [0] NCCL INFO comm 0xcfae140 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
[38;20m2025-08-01 18:22:32,266 | xffl.cli.simulate |     INFO | Total simulation execution time: 5278.89 seconds[0m
[38;20m2025-08-01 18:22:32,267 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
