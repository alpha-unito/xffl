[38;20m2025-08-02 00:12:12,935 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
[38;5;39m2025-08-02 00:12:12,937 | xffl.cli.simulate |    DEBUG | Using current virtual environment: /leonardo_scratch/fast/uToID_bench/xffl/.venv[0m
[38;5;39m2025-08-02 00:12:12,937 | xffl.cli.simulate |    DEBUG | New local simulation xFFL environment variables: {'XFFL_VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv', 'XFFL_WORLD_SIZE': '16', 'XFFL_NUM_NODES': '16', 'MASTER_ADDR': 'lrdn0708', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;5;39m2025-08-02 00:12:12,937 | xffl.cli.simulate |    DEBUG | Updated xFFL environment: {'XFFL_VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv', 'XFFL_WORLD_SIZE': '16', 'XFFL_NUM_NODES': '16', 'MASTER_ADDR': 'lrdn0708', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;20m2025-08-02 00:12:12,939 | xffl.cli.simulate |     INFO | Running local simulation...[0m
[38;5;39m2025-08-02 00:12:12,939 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0708: ssh -oStrictHostKeyChecking=no lrdn0708 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=16 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0708 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_16_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:12,940 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0745: ssh -oStrictHostKeyChecking=no lrdn0745 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=16 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0708 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_16_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:12,940 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0757: ssh -oStrictHostKeyChecking=no lrdn0757 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=16 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0708 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=2 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_16_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:12,940 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0780: ssh -oStrictHostKeyChecking=no lrdn0780 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=16 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0708 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=3 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_16_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:12,940 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0805: ssh -oStrictHostKeyChecking=no lrdn0805 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=16 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0708 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=4 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_16_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:12,941 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0824: ssh -oStrictHostKeyChecking=no lrdn0824 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=16 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0708 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=5 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_16_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:12,941 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0836: ssh -oStrictHostKeyChecking=no lrdn0836 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=16 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0708 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=6 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_16_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:12,941 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0853: ssh -oStrictHostKeyChecking=no lrdn0853 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=16 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0708 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=7 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_16_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:12,941 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1028: ssh -oStrictHostKeyChecking=no lrdn1028 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=16 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0708 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=8 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_16_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:12,942 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1093: ssh -oStrictHostKeyChecking=no lrdn1093 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=16 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0708 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=9 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_16_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:12,942 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1124: ssh -oStrictHostKeyChecking=no lrdn1124 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=16 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0708 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=10 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_16_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:12,942 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1138: ssh -oStrictHostKeyChecking=no lrdn1138 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=16 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0708 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=11 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_16_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:12,942 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1139: ssh -oStrictHostKeyChecking=no lrdn1139 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=16 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0708 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=12 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_16_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:12,942 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1206: ssh -oStrictHostKeyChecking=no lrdn1206 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=16 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0708 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=13 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_16_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:12,943 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1218: ssh -oStrictHostKeyChecking=no lrdn1218 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=16 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0708 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=14 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_16_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:12,943 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1226: ssh -oStrictHostKeyChecking=no lrdn1226 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=16 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0708 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=15 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_16_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:32,581 | xffl.distributed.distributed_state |    DEBUG | Setting Symmetric Federated Scaling with sizes (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)[0m
[38;5;39m2025-08-02 00:12:32,639 | xffl.distributed.distributed |    DEBUG | [Rank 0]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0708
                    Master port=29500
                    Rank=0
                    World size=16
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=0
                    Node world size=16
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=0
                    Federated world size=16
                MESHES:
                    FSDP=DeviceMesh('cuda', [0], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    Replica group=None
                    Federation=[0]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcc8bd30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcca8f90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1f05e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1f08e0>)
                [0m
[38;5;39m2025-08-02 00:12:32,639 |         __main__ |    DEBUG | Rendez-vous time: 8.78 seconds[0m
[38;5;39m2025-08-02 00:12:32,639 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:32,642 | xffl.distributed.distributed |    DEBUG | [Rank 14]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0708
                    Master port=29500
                    Rank=14
                    World size=16
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=14
                    Node world size=16
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=14
                    Federated world size=16
                MESHES:
                    FSDP=DeviceMesh('cuda', [14], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    Replica group=None
                    Federation=[14]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc9e6160>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xca033a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf4a540>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf4a840>)
                [0m
[38;5;39m2025-08-02 00:12:32,642 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:32,642 | xffl.distributed.distributed |    DEBUG | [Rank 11]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0708
                    Master port=29500
                    Rank=11
                    World size=16
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=11
                    Node world size=16
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=11
                    Federated world size=16
                MESHES:
                    FSDP=DeviceMesh('cuda', [11], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    Replica group=None
                    Federation=[11]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbe79eb0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe970f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc3de570>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc3de870>)
                [0m
[38;5;39m2025-08-02 00:12:32,642 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:32,642 | xffl.distributed.distributed |    DEBUG | [Rank 15]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0708
                    Master port=29500
                    Rank=15
                    World size=16
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=15
                    Node world size=16
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=15
                    Federated world size=16
                MESHES:
                    FSDP=DeviceMesh('cuda', [15], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    Replica group=None
                    Federation=[15]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbfe4b70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc001d90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc549830>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc549b30>)
                [0m
[38;5;39m2025-08-02 00:12:32,642 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:32,642 | xffl.distributed.distributed |    DEBUG | [Rank 1]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0708
                    Master port=29500
                    Rank=1
                    World size=16
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=1
                    Node world size=16
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=1
                    Federated world size=16
                MESHES:
                    FSDP=DeviceMesh('cuda', [1], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    Replica group=None
                    Federation=[1]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb270770>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb28d9b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb7d53e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb7d56e0>)
                [0m
[38;5;39m2025-08-02 00:12:32,643 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:32,644 | xffl.distributed.distributed |    DEBUG | [Rank 4]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0708
                    Master port=29500
                    Rank=4
                    World size=16
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=4
                    Node world size=16
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=4
                    Federated world size=16
                MESHES:
                    FSDP=DeviceMesh('cuda', [4], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    Replica group=None
                    Federation=[4]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcc80c90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc9ded0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1e57a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1e5aa0>)
                [0m
[38;5;39m2025-08-02 00:12:32,644 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:32,644 | xffl.distributed.distributed |    DEBUG | [Rank 9]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0708
                    Master port=29500
                    Rank=9
                    World size=16
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=9
                    Node world size=16
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=9
                    Federated world size=16
                MESHES:
                    FSDP=DeviceMesh('cuda', [9], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    Replica group=None
                    Federation=[9]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbabdbb0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbadadf0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc0225d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc0228d0>)
                [0m
[38;5;39m2025-08-02 00:12:32,644 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:32,644 | xffl.distributed.distributed |    DEBUG | [Rank 5]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0708
                    Master port=29500
                    Rank=5
                    World size=16
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=5
                    Node world size=16
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=5
                    Federated world size=16
                MESHES:
                    FSDP=DeviceMesh('cuda', [5], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    Replica group=None
                    Federation=[5]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb8575e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb874820>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbdbc260>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbdbc560>)
                [0m
[38;5;39m2025-08-02 00:12:32,644 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:32,644 | xffl.distributed.distributed |    DEBUG | [Rank 6]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0708
                    Master port=29500
                    Rank=6
                    World size=16
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=6
                    Node world size=16
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=6
                    Federated world size=16
                MESHES:
                    FSDP=DeviceMesh('cuda', [6], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    Replica group=None
                    Federation=[6]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc9933c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc9b0600>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcef7ef0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcef81f0>)
                [0m
[38;5;39m2025-08-02 00:12:32,644 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:32,645 | xffl.distributed.distributed |    DEBUG | [Rank 8]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0708
                    Master port=29500
                    Rank=8
                    World size=16
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=8
                    Node world size=16
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=8
                    Federated world size=16
                MESHES:
                    FSDP=DeviceMesh('cuda', [8], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    Replica group=None
                    Federation=[8]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbfe4c40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc001e80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc549640>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc549940>)
                [0m
[38;5;39m2025-08-02 00:12:32,645 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:32,645 | xffl.distributed.distributed |    DEBUG | [Rank 13]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0708
                    Master port=29500
                    Rank=13
                    World size=16
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=13
                    Node world size=16
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=13
                    Federated world size=16
                MESHES:
                    FSDP=DeviceMesh('cuda', [13], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    Replica group=None
                    Federation=[13]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcc93b30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc9d0190>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xccbb370>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1f8ad0>)
                [0m
[38;5;39m2025-08-02 00:12:32,645 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:32,645 | xffl.distributed.distributed |    DEBUG | [Rank 12]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0708
                    Master port=29500
                    Rank=12
                    World size=16
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=12
                    Node world size=16
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=12
                    Federated world size=16
                MESHES:
                    FSDP=DeviceMesh('cuda', [12], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    Replica group=None
                    Federation=[12]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xce81fa0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce9f1e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd3e6960>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd3e6c60>)
                [0m
[38;5;39m2025-08-02 00:12:32,645 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:32,645 | xffl.distributed.distributed |    DEBUG | [Rank 7]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0708
                    Master port=29500
                    Rank=7
                    World size=16
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=7
                    Node world size=16
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=7
                    Federated world size=16
                MESHES:
                    FSDP=DeviceMesh('cuda', [7], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    Replica group=None
                    Federation=[7]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc972350>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc98f590>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xced6c60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xced6f60>)
                [0m
[38;5;39m2025-08-02 00:12:32,645 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:32,645 | xffl.distributed.distributed |    DEBUG | [Rank 3]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0708
                    Master port=29500
                    Rank=3
                    World size=16
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=3
                    Node world size=16
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=3
                    Federated world size=16
                MESHES:
                    FSDP=DeviceMesh('cuda', [3], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    Replica group=None
                    Federation=[3]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc7491c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc766400>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xccadb40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xccade40>)
                [0m
[38;5;39m2025-08-02 00:12:32,645 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:32,645 | xffl.distributed.distributed |    DEBUG | [Rank 10]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0708
                    Master port=29500
                    Rank=10
                    World size=16
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=10
                    Node world size=16
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=10
                    Federated world size=16
                MESHES:
                    FSDP=DeviceMesh('cuda', [10], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    Replica group=None
                    Federation=[10]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xd0b6480>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd0d36c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd61a820>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd61ab20>)
                [0m
[38;5;39m2025-08-02 00:12:32,646 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:32,646 | xffl.distributed.distributed |    DEBUG | [Rank 2]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0708
                    Master port=29500
                    Rank=2
                    World size=16
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=2
                    Node world size=16
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=2
                    Federated world size=16
                MESHES:
                    FSDP=DeviceMesh('cuda', [2], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
                    Replica group=None
                    Federation=[2]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbaef680>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb0c8c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc054020>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc054320>)
                [0m
[38;5;39m2025-08-02 00:12:32,646 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:37,397 | xffl.distributed.distributed |    DEBUG | [Rank 4]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0805:1354481:1354481 [0] NCCL INFO Bootstrap: Using ib0:10.128.18.181<0>
lrdn0805:1354481:1354481 [0] NCCL INFO cudaDriverVersion 12020
lrdn0805:1354481:1354481 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0805:1354481:1354481 [0] NCCL INFO Comm config Blocking set to 1
lrdn0805:1354481:1354571 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0805:1354481:1354571 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0805:1354481:1354571 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.18.181<0>
lrdn0805:1354481:1354571 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0805:1354481:1354571 [0] NCCL INFO Using network IB
lrdn0805:1354481:1354571 [0] NCCL INFO ncclCommInitRankConfig comm 0x10576460 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x593fa8489bb97a23 - Init START
lrdn0805:1354481:1354571 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0805:1354481:1354571 [0] NCCL INFO Bootstrap timings total 0.000390 (create 0.000023, send 0.000061, recv 0.000093, ring 0.000001, delay 0.000000)
lrdn0805:1354481:1354571 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0805:1354481:1354571 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0805:1354481:1354571 [0] NCCL INFO comm 0x10576460 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 00/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 01/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 02/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 03/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 04/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 05/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 06/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 07/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 08/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 09/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 10/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 11/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 12/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 13/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 14/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 15/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 16/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 17/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 18/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 19/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 20/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 21/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 22/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 23/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 24/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 25/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 26/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 27/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 28/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 29/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 30/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 31/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 32/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 33/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 34/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 35/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 36/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 37/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 38/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 39/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 40/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 41/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 42/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 43/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 44/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 45/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 46/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 47/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 48/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 49/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 50/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 51/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 52/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 53/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 54/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 55/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 56/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 57/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 58/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 59/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 60/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 61/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 62/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Channel 63/64 : 0
lrdn0805:1354481:1354571 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0805:1354481:1354571 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0805:1354481:1354571 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0805:1354481:1354577 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0805:1354481:1354578 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0805:1354481:1354571 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0805:1354481:1354571 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0805:1354481:1354571 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0805:1354481:1354571 [0] NCCL INFO ncclCommInitRankConfig comm 0x10576460 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x593fa8489bb97a23 - Init COMPLETE
lrdn0805:1354481:1354571 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 00:12:37,994 |         __main__ |    DEBUG | Model loading time: 5.35 seconds[0m
[38;5;39m2025-08-02 00:12:37,995 |         __main__ |    DEBUG | Training llama3.1-8b: 8030.26 million trainable parameters[0m
[38;5;39m2025-08-02 00:12:37,995 | xffl.distributed.distributed |    DEBUG | [Rank 0]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 00:12:38,258 | xffl.distributed.distributed |    DEBUG | [Rank 1]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0708:3048046:3048046 [0] NCCL INFO Bootstrap: Using ib0:10.128.17.49<0>
lrdn0708:3048046:3048046 [0] NCCL INFO cudaDriverVersion 12020
lrdn0708:3048046:3048046 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0708:3048046:3048046 [0] NCCL INFO Comm config Blocking set to 1
lrdn0708:3048046:3048137 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0708:3048046:3048137 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0708:3048046:3048137 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.17.49<0>
lrdn0708:3048046:3048137 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0708:3048046:3048137 [0] NCCL INFO Using network IB
lrdn0708:3048046:3048137 [0] NCCL INFO ncclCommInitRankConfig comm 0xe57f600 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x448b7474f730868f - Init START
lrdn0708:3048046:3048137 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0708:3048046:3048137 [0] NCCL INFO Bootstrap timings total 0.000413 (create 0.000021, send 0.000062, recv 0.000099, ring 0.000001, delay 0.000001)
[38;5;39m2025-08-02 00:12:38,516 | xffl.distributed.distributed |    DEBUG | [Rank 12]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0708:3048046:3048137 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0708:3048046:3048137 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0708:3048046:3048137 [0] NCCL INFO comm 0xe57f600 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 00/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 01/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 02/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 03/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 04/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 05/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 06/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 07/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 08/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 09/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 10/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 11/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 12/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 13/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 14/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 15/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 16/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 17/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 18/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 19/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 20/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 21/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 22/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 23/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 24/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 25/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 26/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 27/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 28/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 29/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 30/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 31/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 32/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 33/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 34/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 35/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 36/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 37/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 38/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 39/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 40/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 41/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 42/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 43/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 44/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 45/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 46/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 47/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 48/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 49/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 50/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 51/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 52/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 53/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 54/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 55/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 56/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 57/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 58/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 59/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 60/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 61/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 62/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Channel 63/64 : 0
lrdn0708:3048046:3048137 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0708:3048046:3048137 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0708:3048046:3048137 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0708:3048046:3048143 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0708:3048046:3048144 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0708:3048046:3048137 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0708:3048046:3048137 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0708:3048046:3048137 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0708:3048046:3048137 [0] NCCL INFO ncclCommInitRankConfig comm 0xe57f600 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x448b7474f730868f - Init COMPLETE
lrdn0708:3048046:3048137 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0745:1658235:1658235 [0] NCCL INFO Bootstrap: Using ib0:10.128.17.197<0>
lrdn0745:1658235:1658235 [0] NCCL INFO cudaDriverVersion 12020
lrdn0745:1658235:1658235 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0745:1658235:1658235 [0] NCCL INFO Comm config Blocking set to 1
lrdn0745:1658235:1658325 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0745:1658235:1658325 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0745:1658235:1658325 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.17.197<0>
lrdn0745:1658235:1658325 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0745:1658235:1658325 [0] NCCL INFO Using network IB
lrdn0745:1658235:1658325 [0] NCCL INFO ncclCommInitRankConfig comm 0xc366900 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x48953711d04e6068 - Init START
lrdn0745:1658235:1658325 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0745:1658235:1658325 [0] NCCL INFO Bootstrap timings total 0.000403 (create 0.000021, send 0.000065, recv 0.000092, ring 0.000001, delay 0.000000)
lrdn0745:1658235:1658325 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0745:1658235:1658325 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0745:1658235:1658325 [0] NCCL INFO comm 0xc366900 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 00/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 01/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 02/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 03/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 04/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 05/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 06/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 07/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 08/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 09/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 10/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 11/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 12/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 13/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 14/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 15/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 16/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 17/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 18/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 19/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 20/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 21/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 22/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 23/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 24/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 25/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 26/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 27/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 28/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 29/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 30/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 31/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 32/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 33/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 34/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 35/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 36/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 37/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 38/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 39/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 40/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 41/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 42/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 43/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 44/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 45/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 46/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 47/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 48/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 49/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 50/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 51/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 52/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 53/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 54/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 55/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 56/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 57/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 58/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 59/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 60/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 61/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 62/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Channel 63/64 : 0
lrdn0745:1658235:1658325 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0745:1658235:1658325 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0745:1658235:1658325 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0745:1658235:1658331 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn0745:1658235:1658332 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn0745:1658235:1658325 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1139:2620503:2620503 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.237<0>
lrdn0745:1658235:1658325 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1139:2620503:2620503 [0] NCCL INFO cudaDriverVersion 12020
lrdn1139:2620503:2620503 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1139:2620503:2620503 [0] NCCL INFO Comm config Blocking set to 1
lrdn0745:1658235:1658325 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0745:1658235:1658325 [0] NCCL INFO ncclCommInitRankConfig comm 0xc366900 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x48953711d04e6068 - Init COMPLETE
lrdn0745:1658235:1658325 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1139:2620503:2620593 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1139:2620503:2620593 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1139:2620503:2620593 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.237<0>
lrdn1139:2620503:2620593 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1139:2620503:2620593 [0] NCCL INFO Using network IB
lrdn1139:2620503:2620593 [0] NCCL INFO ncclCommInitRankConfig comm 0xe777310 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x711a4ef8feeb8c27 - Init START
lrdn1139:2620503:2620593 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1139:2620503:2620593 [0] NCCL INFO Bootstrap timings total 0.000377 (create 0.000019, send 0.000059, recv 0.000094, ring 0.000001, delay 0.000000)
lrdn1139:2620503:2620593 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1139:2620503:2620593 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1139:2620503:2620593 [0] NCCL INFO comm 0xe777310 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 00/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 01/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 02/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 03/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 04/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 05/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 06/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 07/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 08/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 09/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 10/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 11/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 12/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 13/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 14/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 15/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 16/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 17/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 18/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 19/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 20/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 21/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 22/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 23/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 24/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 25/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 26/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 27/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 28/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 29/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 30/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 31/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 32/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 33/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 34/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 35/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 36/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 37/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 38/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 39/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 40/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 41/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 42/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 43/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 44/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 45/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 46/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 47/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 48/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 49/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 50/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 51/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 52/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 53/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 54/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 55/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 56/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 57/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 58/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 59/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 60/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 61/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 62/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Channel 63/64 : 0
lrdn1139:2620503:2620593 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1139:2620503:2620593 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1139:2620503:2620593 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1139:2620503:2620599 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn1139:2620503:2620600 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn1139:2620503:2620593 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1139:2620503:2620593 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1139:2620503:2620593 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1139:2620503:2620593 [0] NCCL INFO ncclCommInitRankConfig comm 0xe777310 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x711a4ef8feeb8c27 - Init COMPLETE
lrdn1139:2620503:2620593 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 00:12:39,334 | xffl.distributed.distributed |    DEBUG | [Rank 8]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 00:12:39,500 | xffl.distributed.distributed |    DEBUG | [Rank 5]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 00:12:39,502 | xffl.distributed.distributed |    DEBUG | [Rank 15]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 00:12:39,517 | xffl.distributed.distributed |    DEBUG | [Rank 9]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 00:12:39,524 | xffl.distributed.distributed |    DEBUG | [Rank 7]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 00:12:39,552 | xffl.distributed.distributed |    DEBUG | [Rank 6]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 00:12:39,558 | xffl.distributed.distributed |    DEBUG | [Rank 3]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 00:12:39,563 | xffl.distributed.distributed |    DEBUG | [Rank 2]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 00:12:39,563 | xffl.distributed.distributed |    DEBUG | [Rank 11]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 00:12:39,565 | xffl.distributed.distributed |    DEBUG | [Rank 13]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 00:12:39,567 | xffl.distributed.distributed |    DEBUG | [Rank 10]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 00:12:39,573 | xffl.distributed.distributed |    DEBUG | [Rank 14]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1028:895822:895822 [0] NCCL INFO Bootstrap: Using ib0:10.128.22.49<0>
lrdn1028:895822:895822 [0] NCCL INFO cudaDriverVersion 12020
lrdn1028:895822:895822 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1028:895822:895822 [0] NCCL INFO Comm config Blocking set to 1
lrdn1028:895822:895914 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1028:895822:895914 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1028:895822:895914 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.22.49<0>
lrdn1028:895822:895914 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1028:895822:895914 [0] NCCL INFO Using network IB
lrdn1028:895822:895914 [0] NCCL INFO ncclCommInitRankConfig comm 0xd8d8c30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd68c217ab92a94c6 - Init START
lrdn1028:895822:895914 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1028:895822:895914 [0] NCCL INFO Bootstrap timings total 0.000398 (create 0.000020, send 0.000065, recv 0.000089, ring 0.000001, delay 0.000000)
lrdn1028:895822:895914 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1028:895822:895914 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1028:895822:895914 [0] NCCL INFO comm 0xd8d8c30 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 00/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 01/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 02/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 03/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 04/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 05/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 06/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 07/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 08/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 09/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 10/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 11/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 12/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 13/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 14/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 15/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 16/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 17/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 18/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 19/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 20/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 21/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 22/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 23/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 24/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 25/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 26/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 27/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 28/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 29/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 30/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 31/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 32/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 33/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 34/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 35/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 36/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 37/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 38/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 39/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 40/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 41/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 42/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 43/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 44/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 45/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 46/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 47/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 48/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 49/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 50/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 51/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 52/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 53/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 54/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 55/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 56/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 57/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 58/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 59/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 60/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 61/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 62/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Channel 63/64 : 0
lrdn1028:895822:895914 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn1028:895822:895914 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1028:895822:895914 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1028:895822:895922 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn1028:895822:895921 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn1028:895822:895914 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1028:895822:895914 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1028:895822:895914 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1028:895822:895914 [0] NCCL INFO ncclCommInitRankConfig comm 0xd8d8c30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd68c217ab92a94c6 - Init COMPLETE
lrdn1028:895822:895914 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn0824:1054616:1054616 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.1<0>
lrdn0824:1054616:1054616 [0] NCCL INFO cudaDriverVersion 12020
lrdn0824:1054616:1054616 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0824:1054616:1054616 [0] NCCL INFO Comm config Blocking set to 1
lrdn1093:1492163:1492163 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.53<0>
lrdn1093:1492163:1492163 [0] NCCL INFO cudaDriverVersion 12020
lrdn1093:1492163:1492163 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1093:1492163:1492163 [0] NCCL INFO Comm config Blocking set to 1
lrdn0853:1717580:1717580 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.117<0>
lrdn0853:1717580:1717580 [0] NCCL INFO cudaDriverVersion 12020
lrdn0853:1717580:1717580 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0853:1717580:1717580 [0] NCCL INFO Comm config Blocking set to 1
lrdn1226:1654681:1654681 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.73<0>
lrdn1226:1654681:1654681 [0] NCCL INFO cudaDriverVersion 12020
lrdn1226:1654681:1654681 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1226:1654681:1654681 [0] NCCL INFO Comm config Blocking set to 1
lrdn0780:1321542:1321542 [0] NCCL INFO Bootstrap: Using ib0:10.128.18.81<0>
lrdn0780:1321542:1321542 [0] NCCL INFO cudaDriverVersion 12020
lrdn0780:1321542:1321542 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0780:1321542:1321542 [0] NCCL INFO Comm config Blocking set to 1
lrdn0836:1477025:1477025 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.49<0>
lrdn0836:1477025:1477025 [0] NCCL INFO cudaDriverVersion 12020
lrdn0836:1477025:1477025 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0836:1477025:1477025 [0] NCCL INFO Comm config Blocking set to 1
lrdn1124:1626469:1626469 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.177<0>
lrdn1124:1626469:1626469 [0] NCCL INFO cudaDriverVersion 12020
lrdn1124:1626469:1626469 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1206:1547777:1547777 [0] NCCL INFO Bootstrap: Using ib0:10.128.24.249<0>
lrdn1124:1626469:1626469 [0] NCCL INFO Comm config Blocking set to 1
lrdn1206:1547777:1547777 [0] NCCL INFO cudaDriverVersion 12020
lrdn1206:1547777:1547777 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1206:1547777:1547777 [0] NCCL INFO Comm config Blocking set to 1
lrdn1138:684929:684929 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.233<0>
lrdn1138:684929:684929 [0] NCCL INFO cudaDriverVersion 12020
lrdn1138:684929:684929 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1138:684929:684929 [0] NCCL INFO Comm config Blocking set to 1
lrdn1218:1776591:1776591 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.41<0>
lrdn0757:1521334:1521334 [0] NCCL INFO Bootstrap: Using ib0:10.128.17.245<0>
lrdn1218:1776591:1776591 [0] NCCL INFO cudaDriverVersion 12020
lrdn1218:1776591:1776591 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1218:1776591:1776591 [0] NCCL INFO Comm config Blocking set to 1
lrdn0757:1521334:1521334 [0] NCCL INFO cudaDriverVersion 12020
lrdn0757:1521334:1521334 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0757:1521334:1521334 [0] NCCL INFO Comm config Blocking set to 1
lrdn0824:1054616:1054706 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0824:1054616:1054706 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0824:1054616:1054706 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.1<0>
lrdn1093:1492163:1492254 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1093:1492163:1492254 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0853:1717580:1717672 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0853:1717580:1717672 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0824:1054616:1054706 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0824:1054616:1054706 [0] NCCL INFO Using network IB
lrdn0824:1054616:1054706 [0] NCCL INFO ncclCommInitRankConfig comm 0xd14a740 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb2785851637d5147 - Init START
lrdn1226:1654681:1654771 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1226:1654681:1654771 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0824:1054616:1054706 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0824:1054616:1054706 [0] NCCL INFO Bootstrap timings total 0.000403 (create 0.000020, send 0.000062, recv 0.000103, ring 0.000001, delay 0.000001)
lrdn0780:1321542:1321632 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0780:1321542:1321632 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0824:1054616:1054706 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0824:1054616:1054706 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0824:1054616:1054706 [0] NCCL INFO comm 0xd14a740 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 00/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 01/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 02/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 03/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 04/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 05/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 06/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 07/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 08/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 09/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 10/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 11/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 12/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 13/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 14/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 15/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 16/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 17/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 18/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 19/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 20/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 21/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 22/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 23/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 24/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 25/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 26/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 27/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 28/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 29/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 30/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 31/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 32/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 33/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 34/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 35/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 36/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 37/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 38/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 39/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 40/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 41/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 42/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 43/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 44/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 45/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 46/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 47/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 48/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 49/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 50/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 51/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 52/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 53/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 54/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 55/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 56/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 57/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 58/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 59/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 60/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 61/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 62/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Channel 63/64 : 0
lrdn0824:1054616:1054706 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0824:1054616:1054706 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0824:1054616:1054706 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0824:1054616:1054712 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0824:1054616:1054713 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0824:1054616:1054706 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0824:1054616:1054706 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0824:1054616:1054706 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0824:1054616:1054706 [0] NCCL INFO ncclCommInitRankConfig comm 0xd14a740 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb2785851637d5147 - Init COMPLETE
lrdn0824:1054616:1054706 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0853:1717580:1717672 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.117<0>
lrdn1093:1492163:1492254 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.53<0>
lrdn0853:1717580:1717672 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0853:1717580:1717672 [0] NCCL INFO Using network IB
lrdn1093:1492163:1492254 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1093:1492163:1492254 [0] NCCL INFO Using network IB
lrdn1226:1654681:1654771 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.73<0>
lrdn0853:1717580:1717672 [0] NCCL INFO ncclCommInitRankConfig comm 0x17267c50 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7d5111afa360ac0f - Init START
lrdn1093:1492163:1492254 [0] NCCL INFO ncclCommInitRankConfig comm 0xd3b0f50 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xbec5de4e664c0674 - Init START
lrdn0853:1717580:1717672 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0853:1717580:1717672 [0] NCCL INFO Bootstrap timings total 0.000400 (create 0.000021, send 0.000065, recv 0.000100, ring 0.000001, delay 0.000000)
lrdn1093:1492163:1492254 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1093:1492163:1492254 [0] NCCL INFO Bootstrap timings total 0.000388 (create 0.000020, send 0.000062, recv 0.000093, ring 0.000001, delay 0.000000)
lrdn0836:1477025:1477117 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0836:1477025:1477117 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1226:1654681:1654771 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1226:1654681:1654771 [0] NCCL INFO Using network IB
lrdn1226:1654681:1654771 [0] NCCL INFO ncclCommInitRankConfig comm 0x168dbc30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xa7317364094d7774 - Init START
lrdn1226:1654681:1654771 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1226:1654681:1654771 [0] NCCL INFO Bootstrap timings total 0.000409 (create 0.000021, send 0.000063, recv 0.000097, ring 0.000001, delay 0.000000)
lrdn0780:1321542:1321632 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.18.81<0>
lrdn0853:1717580:1717672 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0853:1717580:1717672 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0853:1717580:1717672 [0] NCCL INFO comm 0x17267c50 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 00/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 01/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 02/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 03/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 04/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 05/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 06/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 07/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 08/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 09/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 10/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 11/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 12/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 13/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 14/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 15/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 16/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 17/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 18/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 19/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 20/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 21/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 22/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 23/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 24/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 25/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 26/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 27/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 28/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 29/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 30/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 31/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 32/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 33/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 34/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 35/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 36/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 37/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 38/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 39/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 40/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 41/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 42/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 43/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 44/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 45/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 46/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 47/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 48/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 49/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 50/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 51/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 52/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 53/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 54/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 55/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 56/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 57/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 58/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 59/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 60/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 61/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 62/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Channel 63/64 : 0
lrdn0853:1717580:1717672 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0853:1717580:1717672 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0853:1717580:1717672 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0853:1717580:1717678 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0853:1717580:1717679 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1093:1492163:1492254 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1093:1492163:1492254 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1093:1492163:1492254 [0] NCCL INFO comm 0xd3b0f50 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 00/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 01/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 02/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 03/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 04/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 05/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 06/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 07/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 08/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 09/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 10/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 11/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 12/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 13/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 14/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 15/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 16/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 17/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 18/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 19/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 20/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 21/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 22/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 23/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 24/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 25/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 26/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 27/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 28/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 29/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 30/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 31/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 32/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 33/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 34/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 35/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 36/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 37/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 38/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 39/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 40/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 41/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 42/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 43/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 44/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 45/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 46/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 47/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 48/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 49/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 50/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 51/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 52/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 53/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 54/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 55/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 56/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 57/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 58/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 59/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 60/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 61/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 62/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Channel 63/64 : 0
lrdn1093:1492163:1492254 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1093:1492163:1492254 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1093:1492163:1492254 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1093:1492163:1492260 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn1093:1492163:1492261 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn1124:1626469:1626561 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1206:1547777:1547868 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1206:1547777:1547868 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0780:1321542:1321632 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0780:1321542:1321632 [0] NCCL INFO Using network IB
lrdn1138:684929:685020 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1138:684929:685020 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0780:1321542:1321632 [0] NCCL INFO ncclCommInitRankConfig comm 0xe83cac0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xde3b844ff83340f3 - Init START
lrdn0780:1321542:1321632 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0780:1321542:1321632 [0] NCCL INFO Bootstrap timings total 0.000490 (create 0.000018, send 0.000061, recv 0.000201, ring 0.000001, delay 0.000000)
lrdn1226:1654681:1654771 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1226:1654681:1654771 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1226:1654681:1654771 [0] NCCL INFO comm 0x168dbc30 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 00/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 01/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 02/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 03/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 04/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 05/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 06/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 07/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 08/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 09/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 10/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 11/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 12/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 13/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 14/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 15/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 16/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 17/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 18/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 19/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 20/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 21/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 22/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 23/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 24/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 25/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 26/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 27/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 28/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 29/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 30/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 31/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 32/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 33/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 34/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 35/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 36/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 37/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 38/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 39/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 40/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 41/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 42/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 43/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 44/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 45/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 46/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 47/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 48/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 49/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 50/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 51/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 52/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 53/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 54/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 55/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 56/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 57/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 58/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 59/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 60/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 61/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 62/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Channel 63/64 : 0
lrdn1226:1654681:1654771 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1226:1654681:1654771 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1226:1654681:1654771 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1226:1654681:1654777 [0] NCCL INFO [Proxy Service] Device 0 CPU core 8
lrdn1226:1654681:1654778 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0853:1717580:1717672 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0853:1717580:1717672 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1093:1492163:1492254 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1093:1492163:1492254 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1226:1654681:1654771 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1226:1654681:1654771 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0853:1717580:1717672 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0853:1717580:1717672 [0] NCCL INFO ncclCommInitRankConfig comm 0x17267c50 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7d5111afa360ac0f - Init COMPLETE
lrdn0853:1717580:1717672 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.16, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn1093:1492163:1492254 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1093:1492163:1492254 [0] NCCL INFO ncclCommInitRankConfig comm 0xd3b0f50 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xbec5de4e664c0674 - Init COMPLETE
lrdn1093:1492163:1492254 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.27 (kernels 0.17, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0780:1321542:1321632 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0780:1321542:1321632 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0780:1321542:1321632 [0] NCCL INFO comm 0xe83cac0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 00/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 01/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 02/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 03/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 04/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 05/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 06/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 07/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 08/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 09/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 10/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 11/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 12/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 13/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 14/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 15/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 16/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 17/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 18/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 19/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 20/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 21/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 22/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 23/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 24/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 25/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 26/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 27/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 28/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 29/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 30/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 31/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 32/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 33/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 34/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 35/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 36/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 37/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 38/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 39/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 40/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 41/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 42/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 43/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 44/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 45/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 46/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 47/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 48/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 49/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 50/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 51/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 52/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 53/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 54/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 55/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 56/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 57/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 58/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 59/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 60/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 61/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 62/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Channel 63/64 : 0
lrdn0780:1321542:1321632 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0780:1321542:1321632 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0780:1321542:1321632 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0780:1321542:1321638 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0780:1321542:1321639 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1226:1654681:1654771 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1226:1654681:1654771 [0] NCCL INFO ncclCommInitRankConfig comm 0x168dbc30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xa7317364094d7774 - Init COMPLETE
lrdn1226:1654681:1654771 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0836:1477025:1477117 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.49<0>
lrdn1218:1776591:1776683 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1218:1776591:1776683 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0757:1521334:1521425 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0757:1521334:1521425 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0836:1477025:1477117 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0836:1477025:1477117 [0] NCCL INFO Using network IB
lrdn0836:1477025:1477117 [0] NCCL INFO ncclCommInitRankConfig comm 0x172896d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x4d56cc463b9db4cf - Init START
lrdn0836:1477025:1477117 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0836:1477025:1477117 [0] NCCL INFO Bootstrap timings total 0.000388 (create 0.000017, send 0.000057, recv 0.000093, ring 0.000001, delay 0.000000)
lrdn0780:1321542:1321632 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0780:1321542:1321632 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0780:1321542:1321632 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0780:1321542:1321632 [0] NCCL INFO ncclCommInitRankConfig comm 0xe83cac0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xde3b844ff83340f3 - Init COMPLETE
lrdn0780:1321542:1321632 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn1138:684929:685020 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.233<0>
lrdn1138:684929:685020 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1138:684929:685020 [0] NCCL INFO Using network IB
lrdn0836:1477025:1477117 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0836:1477025:1477117 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0836:1477025:1477117 [0] NCCL INFO comm 0x172896d0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 00/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 01/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 02/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO ncclCommInitRankConfig comm 0xdf6dbd0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x570fcb277391fb33 - Init START
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 03/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 04/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 05/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 06/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 07/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 08/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 09/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 10/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 11/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 12/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 13/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 14/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 15/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 16/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 17/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 18/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 19/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 20/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 21/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 22/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 23/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 24/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 25/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 26/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 27/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 28/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 29/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 30/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 31/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 32/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 33/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 34/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 35/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 36/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 37/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 38/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 39/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 40/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 41/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 42/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 43/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 44/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 45/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 46/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 47/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 48/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 49/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 50/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 51/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 52/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 53/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 54/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 55/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 56/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 57/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 58/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 59/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 60/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 61/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 62/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Channel 63/64 : 0
lrdn0836:1477025:1477117 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0836:1477025:1477117 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0836:1477025:1477117 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0836:1477025:1477124 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn0836:1477025:1477123 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn1138:684929:685020 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1138:684929:685020 [0] NCCL INFO Bootstrap timings total 0.000549 (create 0.000020, send 0.000068, recv 0.000253, ring 0.000001, delay 0.000001)
lrdn1124:1626469:1626561 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.177<0>
lrdn1206:1547777:1547868 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.24.249<0>
lrdn1124:1626469:1626561 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1124:1626469:1626561 [0] NCCL INFO Using network IB
lrdn1124:1626469:1626561 [0] NCCL INFO ncclCommInitRankConfig comm 0xe9ab100 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x4df7fafa2aa3059e - Init START
lrdn1124:1626469:1626561 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1206:1547777:1547868 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1206:1547777:1547868 [0] NCCL INFO Using network IB
lrdn1124:1626469:1626561 [0] NCCL INFO Bootstrap timings total 0.000408 (create 0.000022, send 0.000062, recv 0.000100, ring 0.000001, delay 0.000000)
lrdn1206:1547777:1547868 [0] NCCL INFO ncclCommInitRankConfig comm 0xdd8d520 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf63826f3083cc5eb - Init START
lrdn1206:1547777:1547868 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1206:1547777:1547868 [0] NCCL INFO Bootstrap timings total 0.000407 (create 0.000021, send 0.000067, recv 0.000090, ring 0.000001, delay 0.000000)
lrdn0836:1477025:1477117 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0836:1477025:1477117 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1138:684929:685020 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1138:684929:685020 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1138:684929:685020 [0] NCCL INFO comm 0xdf6dbd0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 00/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 01/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 02/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 03/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 04/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 05/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 06/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 07/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 08/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 09/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 10/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 11/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 12/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 13/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 14/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 15/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 16/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 17/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 18/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 19/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 20/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 21/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 22/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 23/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 24/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 25/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 26/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 27/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 28/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 29/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 30/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 31/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 32/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 33/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 34/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 35/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 36/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 37/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 38/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 39/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 40/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 41/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 42/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 43/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 44/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 45/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 46/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 47/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 48/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 49/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 50/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 51/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 52/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 53/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 54/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 55/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 56/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 57/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 58/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 59/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 60/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 61/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 62/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Channel 63/64 : 0
lrdn1138:684929:685020 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn1138:684929:685020 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1138:684929:685020 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1138:684929:685026 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn1138:684929:685027 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn0836:1477025:1477117 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0836:1477025:1477117 [0] NCCL INFO ncclCommInitRankConfig comm 0x172896d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x4d56cc463b9db4cf - Init COMPLETE
lrdn0836:1477025:1477117 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0757:1521334:1521425 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.17.245<0>
lrdn0757:1521334:1521425 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0757:1521334:1521425 [0] NCCL INFO Using network IB
lrdn1124:1626469:1626561 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1124:1626469:1626561 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1124:1626469:1626561 [0] NCCL INFO comm 0xe9ab100 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 00/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 01/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 02/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 03/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 04/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 05/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 06/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 07/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 08/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 09/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 10/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 11/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 12/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 13/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 14/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 15/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 16/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 17/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 18/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 19/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 20/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 21/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 22/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 23/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 24/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 25/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 26/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 27/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 28/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 29/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 30/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 31/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 32/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 33/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 34/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 35/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 36/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 37/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 38/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 39/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 40/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 41/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 42/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 43/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 44/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 45/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 46/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 47/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 48/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 49/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 50/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 51/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 52/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 53/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 54/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 55/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 56/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 57/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 58/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 59/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 60/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 61/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 62/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Channel 63/64 : 0
lrdn1124:1626469:1626561 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1124:1626469:1626561 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1124:1626469:1626561 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1124:1626469:1626567 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn1124:1626469:1626568 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 15
lrdn0757:1521334:1521425 [0] NCCL INFO ncclCommInitRankConfig comm 0xcbe5740 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf999b521a8bf7735 - Init START
lrdn0757:1521334:1521425 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0757:1521334:1521425 [0] NCCL INFO Bootstrap timings total 0.000517 (create 0.000018, send 0.000062, recv 0.000223, ring 0.000001, delay 0.000000)
lrdn1206:1547777:1547868 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1206:1547777:1547868 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1206:1547777:1547868 [0] NCCL INFO comm 0xdd8d520 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 00/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 01/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 02/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 03/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 04/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 05/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 06/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 07/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 08/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 09/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 10/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 11/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 12/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 13/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 14/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 15/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 16/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 17/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 18/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 19/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 20/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 21/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 22/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 23/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 24/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 25/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 26/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 27/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 28/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 29/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 30/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 31/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 32/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 33/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 34/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 35/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 36/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 37/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 38/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 39/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 40/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 41/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 42/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 43/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 44/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 45/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 46/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 47/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 48/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 49/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 50/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 51/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 52/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 53/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 54/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 55/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 56/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 57/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 58/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 59/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 60/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 61/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 62/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Channel 63/64 : 0
lrdn1206:1547777:1547868 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1206:1547777:1547868 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1206:1547777:1547868 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1206:1547777:1547874 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn1206:1547777:1547875 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn1138:684929:685020 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1218:1776591:1776683 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.41<0>
lrdn1138:684929:685020 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1218:1776591:1776683 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1218:1776591:1776683 [0] NCCL INFO Using network IB
lrdn1138:684929:685020 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1138:684929:685020 [0] NCCL INFO ncclCommInitRankConfig comm 0xdf6dbd0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x570fcb277391fb33 - Init COMPLETE
lrdn1138:684929:685020 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1218:1776591:1776683 [0] NCCL INFO ncclCommInitRankConfig comm 0xdade2b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x94d40ff301030077 - Init START
lrdn1218:1776591:1776683 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1218:1776591:1776683 [0] NCCL INFO Bootstrap timings total 0.000406 (create 0.000021, send 0.000069, recv 0.000096, ring 0.000001, delay 0.000001)
lrdn1124:1626469:1626561 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1124:1626469:1626561 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1206:1547777:1547868 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1206:1547777:1547868 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0757:1521334:1521425 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0757:1521334:1521425 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0757:1521334:1521425 [0] NCCL INFO comm 0xcbe5740 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 00/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 01/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 02/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 03/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 04/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 05/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 06/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 07/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 08/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 09/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 10/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 11/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 12/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 13/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 14/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 15/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 16/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 17/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 18/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 19/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 20/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 21/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 22/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 23/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 24/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 25/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 26/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 27/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 28/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 29/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 30/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 31/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 32/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 33/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 34/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 35/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 36/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 37/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 38/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 39/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 40/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 41/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 42/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 43/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 44/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 45/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 46/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 47/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 48/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 49/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 50/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 51/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 52/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 53/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 54/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 55/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 56/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 57/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 58/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 59/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 60/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 61/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 62/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Channel 63/64 : 0
lrdn0757:1521334:1521425 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0757:1521334:1521425 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0757:1521334:1521425 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0757:1521334:1521432 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0757:1521334:1521431 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn1124:1626469:1626561 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1124:1626469:1626561 [0] NCCL INFO ncclCommInitRankConfig comm 0xe9ab100 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x4df7fafa2aa3059e - Init COMPLETE
lrdn1124:1626469:1626561 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1206:1547777:1547868 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1206:1547777:1547868 [0] NCCL INFO ncclCommInitRankConfig comm 0xdd8d520 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf63826f3083cc5eb - Init COMPLETE
lrdn1206:1547777:1547868 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn1218:1776591:1776683 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1218:1776591:1776683 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1218:1776591:1776683 [0] NCCL INFO comm 0xdade2b0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 00/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 01/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 02/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 03/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 04/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 05/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 06/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 07/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 08/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 09/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 10/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 11/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 12/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 13/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 14/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 15/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 16/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 17/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 18/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 19/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 20/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 21/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 22/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 23/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 24/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 25/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 26/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 27/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 28/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 29/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 30/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 31/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 32/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 33/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 34/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 35/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 36/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 37/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 38/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 39/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 40/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 41/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 42/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 43/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 44/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 45/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 46/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 47/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 48/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 49/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 50/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 51/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 52/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 53/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 54/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 55/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 56/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 57/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 58/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 59/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 60/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 61/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 62/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Channel 63/64 : 0
lrdn1218:1776591:1776683 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1218:1776591:1776683 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1218:1776591:1776683 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1218:1776591:1776690 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1218:1776591:1776689 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0757:1521334:1521425 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0757:1521334:1521425 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0757:1521334:1521425 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0757:1521334:1521425 [0] NCCL INFO ncclCommInitRankConfig comm 0xcbe5740 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf999b521a8bf7735 - Init COMPLETE
lrdn0757:1521334:1521425 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1218:1776591:1776683 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1218:1776591:1776683 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1218:1776591:1776683 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1218:1776591:1776683 [0] NCCL INFO ncclCommInitRankConfig comm 0xdade2b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x94d40ff301030077 - Init COMPLETE
lrdn1218:1776591:1776683 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
[38;5;39m2025-08-02 00:12:45,647 |         __main__ |    DEBUG | FSDP wrapping setup time: 7.65 seconds[0m
[38;5;39m2025-08-02 00:12:45,660 |         __main__ |    DEBUG | Total setup time: 21.80 seconds[0m
[38;5;39m2025-08-02 00:12:45,660 |         __main__ |    DEBUG | GPU RAM allocated before training: 16.06 GB[0m


lrdn0708:3048046:3048046 [0] NCCL INFO Comm config Blocking set to 1
lrdn0708:3048046:3048149 [0] NCCL INFO Using network IB
lrdn0708:3048046:3048149 [0] NCCL INFO ncclCommInitRankConfig comm 0x229ad010 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init START
lrdn0836:1477025:1477025 [0] NCCL INFO Comm config Blocking set to 1
lrdn0805:1354481:1354481 [0] NCCL INFO Comm config Blocking set to 1
lrdn0780:1321542:1321542 [0] NCCL INFO Comm config Blocking set to 1
lrdn0780:1321542:1321640 [0] NCCL INFO Using network IB
lrdn1138:684929:684929 [0] NCCL INFO Comm config Blocking set to 1
lrdn0757:1521334:1521334 [0] NCCL INFO Comm config Blocking set to 1
lrdn0824:1054616:1054616 [0] NCCL INFO Comm config Blocking set to 1
lrdn0836:1477025:1477125 [0] NCCL INFO Using network IB
lrdn0836:1477025:1477125 [0] NCCL INFO ncclCommInitRankConfig comm 0x21ed7e70 rank 6 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init START
lrdn1218:1776591:1776591 [0] NCCL INFO Comm config Blocking set to 1
lrdn1226:1654681:1654681 [0] NCCL INFO Comm config Blocking set to 1
lrdn0853:1717580:1717580 [0] NCCL INFO Comm config Blocking set to 1
lrdn0805:1354481:1354583 [0] NCCL INFO Using network IB
lrdn0780:1321542:1321640 [0] NCCL INFO ncclCommInitRankConfig comm 0x224699f0 rank 3 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init START
lrdn1124:1626469:1626469 [0] NCCL INFO Comm config Blocking set to 1
lrdn1206:1547777:1547777 [0] NCCL INFO Comm config Blocking set to 1
lrdn1206:1547777:1547876 [0] NCCL INFO Using network IB
lrdn1206:1547777:1547876 [0] NCCL INFO ncclCommInitRankConfig comm 0xe1d91a0 rank 13 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init START
lrdn1139:2620503:2620503 [0] NCCL INFO Comm config Blocking set to 1
lrdn1139:2620503:2620606 [0] NCCL INFO Using network IB
lrdn1139:2620503:2620606 [0] NCCL INFO ncclCommInitRankConfig comm 0x174dbf50 rank 12 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init START
lrdn1138:684929:685029 [0] NCCL INFO Using network IB
lrdn1138:684929:685029 [0] NCCL INFO ncclCommInitRankConfig comm 0x21b9cd90 rank 11 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init START
lrdn0757:1521334:1521433 [0] NCCL INFO Using network IB
lrdn0757:1521334:1521433 [0] NCCL INFO ncclCommInitRankConfig comm 0xd030d50 rank 2 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init START
lrdn0824:1054616:1054715 [0] NCCL INFO Using network IB
lrdn0824:1054616:1054715 [0] NCCL INFO ncclCommInitRankConfig comm 0x2a32d0b0 rank 5 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init START
lrdn1218:1776591:1776692 [0] NCCL INFO Using network IB
lrdn1218:1776591:1776692 [0] NCCL INFO ncclCommInitRankConfig comm 0xdf44c60 rank 14 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init START
lrdn1226:1654681:1654779 [0] NCCL INFO Using network IB
lrdn1226:1654681:1654779 [0] NCCL INFO ncclCommInitRankConfig comm 0x2152b670 rank 15 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init START
lrdn1028:895822:895822 [0] NCCL INFO Comm config Blocking set to 1
lrdn1028:895822:895923 [0] NCCL INFO Using network IB
lrdn1028:895822:895923 [0] NCCL INFO ncclCommInitRankConfig comm 0x23bae210 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init START
lrdn1093:1492163:1492163 [0] NCCL INFO Comm config Blocking set to 1
lrdn1093:1492163:1492262 [0] NCCL INFO Using network IB
lrdn1093:1492163:1492262 [0] NCCL INFO ncclCommInitRankConfig comm 0x20fe09a0 rank 9 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init START
lrdn0853:1717580:1717680 [0] NCCL INFO Using network IB
lrdn0853:1717580:1717680 [0] NCCL INFO ncclCommInitRankConfig comm 0x11d3c000 rank 7 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init START
lrdn0805:1354481:1354583 [0] NCCL INFO ncclCommInitRankConfig comm 0x229a1720 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init START
lrdn0745:1658235:1658235 [0] NCCL INFO Comm config Blocking set to 1
lrdn0745:1658235:1658336 [0] NCCL INFO Using network IB
lrdn0745:1658235:1658336 [0] NCCL INFO ncclCommInitRankConfig comm 0xc7b2930 rank 1 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init START
lrdn1124:1626469:1626570 [0] NCCL INFO Using network IB
lrdn1124:1626469:1626570 [0] NCCL INFO ncclCommInitRankConfig comm 0x115f8ed0 rank 10 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init START
lrdn1093:1492163:1492262 [0] NCCL INFO Bootstrap timings total 0.002262 (create 0.000020, send 0.000255, recv 0.000922, ring 0.000852, delay 0.000000)
lrdn1124:1626469:1626570 [0] NCCL INFO Bootstrap timings total 0.002292 (create 0.000023, send 0.000234, recv 0.000950, ring 0.000953, delay 0.000000)
lrdn1138:684929:685029 [0] NCCL INFO Bootstrap timings total 0.002393 (create 0.000022, send 0.000347, recv 0.000960, ring 0.000823, delay 0.000000)
lrdn1139:2620503:2620606 [0] NCCL INFO Bootstrap timings total 0.002440 (create 0.000022, send 0.000417, recv 0.000934, ring 0.000819, delay 0.000000)
lrdn1028:895822:895923 [0] NCCL INFO Bootstrap timings total 0.002518 (create 0.000019, send 0.000372, recv 0.000924, ring 0.001097, delay 0.000000)
lrdn1206:1547777:1547876 [0] NCCL INFO Bootstrap timings total 0.002484 (create 0.000021, send 0.000284, recv 0.000524, ring 0.000874, delay 0.000000)
lrdn0708:3048046:3048149 [0] NCCL INFO Bootstrap timings total 0.002784 (create 0.000015, send 0.000056, recv 0.000597, ring 0.001331, delay 0.000000)
lrdn1218:1776591:1776692 [0] NCCL INFO Bootstrap timings total 0.002573 (create 0.000023, send 0.000403, recv 0.000710, ring 0.001323, delay 0.000000)
lrdn0745:1658235:1658336 [0] NCCL INFO Bootstrap timings total 0.002562 (create 0.000021, send 0.000253, recv 0.000232, ring 0.001853, delay 0.000000)
lrdn0780:1321542:1321640 [0] NCCL INFO Bootstrap timings total 0.002622 (create 0.000022, send 0.000243, recv 0.000410, ring 0.001739, delay 0.000000)
lrdn0757:1521334:1521433 [0] NCCL INFO Bootstrap timings total 0.002620 (create 0.000020, send 0.000284, recv 0.000257, ring 0.001832, delay 0.000000)
lrdn1226:1654681:1654779 [0] NCCL INFO Bootstrap timings total 0.002600 (create 0.000022, send 0.000416, recv 0.000721, ring 0.001327, delay 0.000000)
lrdn0853:1717580:1717680 [0] NCCL INFO Bootstrap timings total 0.002561 (create 0.000020, send 0.000210, recv 0.000485, ring 0.001715, delay 0.000000)
lrdn0805:1354481:1354583 [0] NCCL INFO Bootstrap timings total 0.002629 (create 0.000023, send 0.000087, recv 0.000190, ring 0.001631, delay 0.000000)
lrdn0824:1054616:1054715 [0] NCCL INFO Bootstrap timings total 0.002732 (create 0.000020, send 0.000191, recv 0.000175, ring 0.002218, delay 0.000000)
lrdn0836:1477025:1477125 [0] NCCL INFO Bootstrap timings total 0.002871 (create 0.000015, send 0.000087, recv 0.000820, ring 0.001839, delay 0.000000)
lrdn1028:895822:895923 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0757:1521334:1521433 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0757:1521334:1521433 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn1028:895822:895923 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn0708:3048046:3048149 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0708:3048046:3048149 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn0805:1354481:1354583 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0836:1477025:1477125 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0853:1717580:1717680 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0853:1717580:1717680 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn0805:1354481:1354583 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn0836:1477025:1477125 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn1218:1776591:1776692 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1218:1776591:1776692 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn0745:1658235:1658336 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0745:1658235:1658336 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn0824:1054616:1054715 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0780:1321542:1321640 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0824:1054616:1054715 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn1093:1492163:1492262 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1093:1492163:1492262 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn1226:1654681:1654779 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1226:1654681:1654779 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn0780:1321542:1321640 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn1139:2620503:2620606 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1206:1547777:1547876 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1206:1547777:1547876 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn1138:684929:685029 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1138:684929:685029 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn1124:1626469:1626570 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1124:1626469:1626570 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn1139:2620503:2620606 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn0836:1477025:1477125 [0] NCCL INFO comm 0x21ed7e70 rank 6 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0853:1717580:1717680 [0] NCCL INFO comm 0x11d3c000 rank 7 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0853:1717580:1717680 [0] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 11/3/-1->7->15 [3] 11/3/-1->7->15
lrdn0853:1717580:1717680 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0836:1477025:1477125 [0] NCCL INFO Trees [0] 5/7/-1->6->4 [1] 5/7/-1->6->4 [2] -1/-1/-1->6->5 [3] -1/-1/-1->6->5
lrdn0836:1477025:1477125 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0824:1054616:1054715 [0] NCCL INFO comm 0x2a32d0b0 rank 5 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0824:1054616:1054715 [0] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] -1/-1/-1->5->6 [2] 6/4/-1->5->3 [3] 6/4/-1->5->3
lrdn0824:1054616:1054715 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0853:1717580:1717682 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1138:684929:685029 [0] NCCL INFO comm 0x21b9cd90 rank 11 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1138:684929:685029 [0] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10 [2] 13/9/-1->11->7 [3] 13/9/-1->11->7
lrdn1138:684929:685029 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0853:1717580:1717683 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0836:1477025:1477126 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0836:1477025:1477127 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn1124:1626469:1626570 [0] NCCL INFO comm 0x115f8ed0 rank 10 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1124:1626469:1626570 [0] NCCL INFO Trees [0] 9/11/-1->10->12 [1] 9/11/-1->10->12 [2] -1/-1/-1->10->9 [3] -1/-1/-1->10->9
lrdn1124:1626469:1626570 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1124:1626469:1626572 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0824:1054616:1054717 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0824:1054616:1054716 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1206:1547777:1547876 [0] NCCL INFO comm 0xe1d91a0 rank 13 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1206:1547777:1547876 [0] NCCL INFO Trees [0] -1/-1/-1->13->14 [1] -1/-1/-1->13->14 [2] 14/12/-1->13->11 [3] 14/12/-1->13->11
lrdn1206:1547777:1547876 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1206:1547777:1547877 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0757:1521334:1521433 [0] NCCL INFO comm 0xd030d50 rank 2 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0757:1521334:1521433 [0] NCCL INFO Trees [0] 1/3/-1->2->4 [1] 1/3/-1->2->4 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1
lrdn0757:1521334:1521433 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0757:1521334:1521434 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1218:1776591:1776692 [0] NCCL INFO comm 0xdf44c60 rank 14 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1218:1776591:1776692 [0] NCCL INFO Trees [0] 13/15/-1->14->12 [1] 13/15/-1->14->12 [2] -1/-1/-1->14->13 [3] -1/-1/-1->14->13
lrdn1218:1776591:1776692 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1028:895822:895923 [0] NCCL INFO comm 0x23bae210 rank 8 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1028:895822:895923 [0] NCCL INFO Trees [0] 4/12/-1->8->0 [1] 4/12/-1->8->0 [2] -1/-1/-1->8->9 [3] -1/-1/-1->8->9
lrdn1028:895822:895923 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0708:3048046:3048149 [0] NCCL INFO comm 0x229ad010 rank 0 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0708:3048046:3048149 [0] NCCL INFO Channel 00/04 :  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
lrdn0708:3048046:3048149 [0] NCCL INFO Channel 01/04 :  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
lrdn0708:3048046:3048149 [0] NCCL INFO Channel 02/04 :  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
lrdn0708:3048046:3048149 [0] NCCL INFO Channel 03/04 :  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
lrdn0708:3048046:3048149 [0] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] 8/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
lrdn0708:3048046:3048149 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0708:3048046:3048149 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1206:1547777:1547878 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn1124:1626469:1626571 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1138:684929:685030 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0745:1658235:1658336 [0] NCCL INFO comm 0xc7b2930 rank 1 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0745:1658235:1658336 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] -1/-1/-1->1->2 [2] 2/0/-1->1->3 [3] 2/0/-1->1->3
lrdn0745:1658235:1658336 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0745:1658235:1658338 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn0745:1658235:1658337 [0] NCCL INFO [Proxy Service] Device 0 CPU core 8
lrdn1093:1492163:1492262 [0] NCCL INFO comm 0x20fe09a0 rank 9 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1093:1492163:1492262 [0] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] -1/-1/-1->9->10 [2] 10/8/-1->9->11 [3] 10/8/-1->9->11
lrdn1093:1492163:1492262 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1093:1492163:1492263 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn1093:1492163:1492264 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1138:684929:685031 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0805:1354481:1354583 [0] NCCL INFO comm 0x229a1720 rank 4 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0805:1354481:1354583 [0] NCCL INFO Trees [0] 2/6/-1->4->8 [1] 2/6/-1->4->8 [2] -1/-1/-1->4->5 [3] -1/-1/-1->4->5
lrdn0805:1354481:1354583 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0780:1321542:1321640 [0] NCCL INFO comm 0x224699f0 rank 3 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0780:1321542:1321640 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 5/1/-1->3->7 [3] 5/1/-1->3->7
lrdn0780:1321542:1321640 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0780:1321542:1321641 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0780:1321542:1321642 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0708:3048046:3048150 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0708:3048046:3048151 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn0805:1354481:1354584 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0805:1354481:1354585 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0757:1521334:1521435 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1218:1776591:1776693 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1218:1776591:1776694 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1028:895822:895924 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn1028:895822:895925 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn1226:1654681:1654779 [0] NCCL INFO comm 0x2152b670 rank 15 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1226:1654681:1654779 [0] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14 [2] 7/-1/-1->15->-1 [3] 7/-1/-1->15->-1
lrdn1226:1654681:1654779 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1226:1654681:1654780 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1226:1654681:1654781 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1139:2620503:2620606 [0] NCCL INFO comm 0x174dbf50 rank 12 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1139:2620503:2620606 [0] NCCL INFO Trees [0] 10/14/-1->12->8 [1] 10/14/-1->12->8 [2] -1/-1/-1->12->13 [3] -1/-1/-1->12->13
lrdn1139:2620503:2620606 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1139:2620503:2620608 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1139:2620503:2620607 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn1138:684929:685029 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1138:684929:685029 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1138:684929:685029 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1138:684929:685029 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1028:895822:895923 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1028:895822:895923 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1028:895822:895923 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1028:895822:895923 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0853:1717580:1717680 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0853:1717580:1717680 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0853:1717580:1717680 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0853:1717580:1717680 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1206:1547777:1547876 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1206:1547777:1547876 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1206:1547777:1547876 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1206:1547777:1547876 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1093:1492163:1492262 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0836:1477025:1477125 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0836:1477025:1477125 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0836:1477025:1477125 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0836:1477025:1477125 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1093:1492163:1492262 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1093:1492163:1492262 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1093:1492163:1492262 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1124:1626469:1626570 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1124:1626469:1626570 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1124:1626469:1626570 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1124:1626469:1626570 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0824:1054616:1054715 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0824:1054616:1054715 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0824:1054616:1054715 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0824:1054616:1054715 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1139:2620503:2620606 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1226:1654681:1654779 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1226:1654681:1654779 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1139:2620503:2620606 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1139:2620503:2620606 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1139:2620503:2620606 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0745:1658235:1658336 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1226:1654681:1654779 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1226:1654681:1654779 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0805:1354481:1354583 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0805:1354481:1354583 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0805:1354481:1354583 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0805:1354481:1354583 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0745:1658235:1658336 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0745:1658235:1658336 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0745:1658235:1658336 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0780:1321542:1321640 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1218:1776591:1776692 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0708:3048046:3048149 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0708:3048046:3048149 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0708:3048046:3048149 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             0              1              0              0              0              0              0  
       Reduce |        0         0         1   |             0              1              0              0              0              0              0  
    AllGather |        0         0         1   |             0              1              0              0              0              0              0  
ReduceScatter |        0         0         1   |             0              1              0              0              0              0              0  
    AllReduce |        0         0         1   |             0              1              0              0              0              0              0  

lrdn0780:1321542:1321640 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0780:1321542:1321640 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0780:1321542:1321640 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1218:1776591:1776692 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1218:1776591:1776692 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1218:1776591:1776692 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0757:1521334:1521433 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0757:1521334:1521433 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0757:1521334:1521433 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0757:1521334:1521433 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0708:3048046:3048149 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0708:3048046:3048149 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1138:684929:685029 [0] NCCL INFO ncclCommInitRankConfig comm 0x21b9cd90 rank 11 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init COMPLETE
lrdn1138:684929:685029 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 11 nranks 16 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0708:3048046:3048149 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1028:895822:895923 [0] NCCL INFO ncclCommInitRankConfig comm 0x23bae210 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init COMPLETE
lrdn1028:895822:895923 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 8 nranks 16 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.02, connections 0.00, rest 0.00)
lrdn1206:1547777:1547876 [0] NCCL INFO ncclCommInitRankConfig comm 0xe1d91a0 rank 13 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init COMPLETE
lrdn1206:1547777:1547876 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 13 nranks 16 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0853:1717580:1717680 [0] NCCL INFO ncclCommInitRankConfig comm 0x11d3c000 rank 7 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init COMPLETE
lrdn0853:1717580:1717680 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 7 nranks 16 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.02, connections 0.00, rest 0.00)
lrdn0836:1477025:1477125 [0] NCCL INFO ncclCommInitRankConfig comm 0x21ed7e70 rank 6 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init COMPLETE
lrdn0836:1477025:1477125 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 6 nranks 16 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.02, connections 0.00, rest 0.00)
lrdn1093:1492163:1492262 [0] NCCL INFO ncclCommInitRankConfig comm 0x20fe09a0 rank 9 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init COMPLETE
lrdn1093:1492163:1492262 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 9 nranks 16 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1124:1626469:1626570 [0] NCCL INFO ncclCommInitRankConfig comm 0x115f8ed0 rank 10 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init COMPLETE
lrdn1124:1626469:1626570 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 10 nranks 16 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0824:1054616:1054715 [0] NCCL INFO ncclCommInitRankConfig comm 0x2a32d0b0 rank 5 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init COMPLETE
lrdn0824:1054616:1054715 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 5 nranks 16 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1139:2620503:2620606 [0] NCCL INFO ncclCommInitRankConfig comm 0x174dbf50 rank 12 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init COMPLETE
lrdn1139:2620503:2620606 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 12 nranks 16 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0745:1658235:1658336 [0] NCCL INFO ncclCommInitRankConfig comm 0xc7b2930 rank 1 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init COMPLETE
lrdn0745:1658235:1658336 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 16 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1226:1654681:1654779 [0] NCCL INFO ncclCommInitRankConfig comm 0x2152b670 rank 15 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init COMPLETE
lrdn1226:1654681:1654779 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 15 nranks 16 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0805:1354481:1354583 [0] NCCL INFO ncclCommInitRankConfig comm 0x229a1720 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init COMPLETE
lrdn0805:1354481:1354583 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 4 nranks 16 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1218:1776591:1776692 [0] NCCL INFO ncclCommInitRankConfig comm 0xdf44c60 rank 14 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init COMPLETE
lrdn1218:1776591:1776692 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 14 nranks 16 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.02, connections 0.00, rest 0.00)
lrdn0757:1521334:1521433 [0] NCCL INFO ncclCommInitRankConfig comm 0xd030d50 rank 2 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init COMPLETE
lrdn0757:1521334:1521433 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 16 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.02, connections 0.00, rest 0.00)
lrdn0708:3048046:3048149 [0] NCCL INFO ncclCommInitRankConfig comm 0x229ad010 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init COMPLETE
lrdn0708:3048046:3048149 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 16 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.02, connections 0.00, rest 0.00)
lrdn0780:1321542:1321640 [0] NCCL INFO ncclCommInitRankConfig comm 0x224699f0 rank 3 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb95372ae918cf2bf - Init COMPLETE
lrdn0780:1321542:1321640 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 16 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1028:895822:895927 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 0
lrdn1028:895822:895926 [0] NCCL INFO Channel 00/0 : 7[0] -> 8[0] [receive] via NET/IB/5
lrdn1028:895822:895926 [0] NCCL INFO Channel 01/0 : 7[0] -> 8[0] [receive] via NET/IB/6
lrdn1028:895822:895926 [0] NCCL INFO Channel 02/0 : 7[0] -> 8[0] [receive] via NET/IB/5
lrdn1028:895822:895926 [0] NCCL INFO Channel 03/0 : 7[0] -> 8[0] [receive] via NET/IB/6
lrdn1028:895822:895926 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[0] [send] via NET/IB/5
lrdn1028:895822:895926 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[0] [send] via NET/IB/6
lrdn1028:895822:895926 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[0] [send] via NET/IB/5
lrdn1028:895822:895926 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[0] [send] via NET/IB/6
lrdn1138:684929:685032 [0] NCCL INFO Channel 00/0 : 10[0] -> 11[0] [receive] via NET/IB/5
lrdn1138:684929:685032 [0] NCCL INFO Channel 01/0 : 10[0] -> 11[0] [receive] via NET/IB/6
lrdn0836:1477025:1477129 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
lrdn0836:1477025:1477128 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [receive] via NET/IB/5
lrdn0836:1477025:1477128 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [receive] via NET/IB/6
lrdn0836:1477025:1477128 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [receive] via NET/IB/5
lrdn1206:1547777:1547880 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 5
lrdn1138:684929:685032 [0] NCCL INFO Channel 02/0 : 10[0] -> 11[0] [receive] via NET/IB/5
lrdn1138:684929:685033 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
lrdn1138:684929:685032 [0] NCCL INFO Channel 03/0 : 10[0] -> 11[0] [receive] via NET/IB/6
lrdn0836:1477025:1477128 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [receive] via NET/IB/6
lrdn0836:1477025:1477128 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [send] via NET/IB/5
lrdn0836:1477025:1477128 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [send] via NET/IB/6
lrdn0836:1477025:1477128 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [send] via NET/IB/5
lrdn0853:1717580:1717685 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
lrdn0853:1717580:1717684 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [receive] via NET/IB/5
lrdn0836:1477025:1477128 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [send] via NET/IB/6
lrdn1138:684929:685032 [0] NCCL INFO Channel 00/0 : 11[0] -> 12[0] [send] via NET/IB/5
lrdn1138:684929:685032 [0] NCCL INFO Channel 01/0 : 11[0] -> 12[0] [send] via NET/IB/6
lrdn0853:1717580:1717684 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [receive] via NET/IB/6
lrdn1093:1492163:1492266 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 5
lrdn1093:1492163:1492265 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[0] [receive] via NET/IB/5
lrdn1093:1492163:1492265 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[0] [receive] via NET/IB/6
lrdn1093:1492163:1492265 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[0] [receive] via NET/IB/5
lrdn1093:1492163:1492265 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[0] [receive] via NET/IB/6
lrdn1093:1492163:1492265 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[0] [send] via NET/IB/5
lrdn1093:1492163:1492265 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[0] [send] via NET/IB/6
lrdn1138:684929:685032 [0] NCCL INFO Channel 02/0 : 11[0] -> 12[0] [send] via NET/IB/5
lrdn1093:1492163:1492265 [0] NCCL INFO Channel 02/0 : 9[0] -> 10[0] [send] via NET/IB/5
lrdn1093:1492163:1492265 [0] NCCL INFO Channel 03/0 : 9[0] -> 10[0] [send] via NET/IB/6
lrdn0853:1717580:1717684 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [receive] via NET/IB/5
lrdn0745:1658235:1658339 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1139:2620503:2620610 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 5
lrdn1139:2620503:2620609 [0] NCCL INFO Channel 00/0 : 11[0] -> 12[0] [receive] via NET/IB/5
lrdn0805:1354481:1354587 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 3
lrdn0805:1354481:1354586 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [receive] via NET/IB/5
lrdn0805:1354481:1354586 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [receive] via NET/IB/6
lrdn0805:1354481:1354586 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [receive] via NET/IB/5
lrdn1138:684929:685032 [0] NCCL INFO Channel 03/0 : 11[0] -> 12[0] [send] via NET/IB/6
lrdn0853:1717580:1717684 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [receive] via NET/IB/6
lrdn0805:1354481:1354586 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [receive] via NET/IB/6
lrdn0708:3048046:3048153 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
lrdn0745:1658235:1658339 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn0745:1658235:1658340 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 11
lrdn0745:1658235:1658339 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn0745:1658235:1658339 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn0745:1658235:1658339 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn0745:1658235:1658339 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn0824:1054616:1054718 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [receive] via NET/IB/5
lrdn1206:1547777:1547879 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[0] [receive] via NET/IB/5
lrdn1206:1547777:1547879 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[0] [receive] via NET/IB/6
lrdn1206:1547777:1547879 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[0] [receive] via NET/IB/5
lrdn1206:1547777:1547879 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[0] [receive] via NET/IB/6
lrdn1206:1547777:1547879 [0] NCCL INFO Channel 00/0 : 13[0] -> 14[0] [send] via NET/IB/5
lrdn1206:1547777:1547879 [0] NCCL INFO Channel 01/0 : 13[0] -> 14[0] [send] via NET/IB/6
lrdn1206:1547777:1547879 [0] NCCL INFO Channel 02/0 : 13[0] -> 14[0] [send] via NET/IB/5
lrdn1206:1547777:1547879 [0] NCCL INFO Channel 03/0 : 13[0] -> 14[0] [send] via NET/IB/6
lrdn0853:1717580:1717684 [0] NCCL INFO Channel 00/0 : 7[0] -> 8[0] [send] via NET/IB/5
lrdn0805:1354481:1354586 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [send] via NET/IB/5
lrdn0745:1658235:1658339 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn0745:1658235:1658339 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn0824:1054616:1054719 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 5
lrdn0824:1054616:1054718 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [receive] via NET/IB/6
lrdn1124:1626469:1626574 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
lrdn1124:1626469:1626573 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[0] [receive] via NET/IB/5
lrdn1124:1626469:1626573 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[0] [receive] via NET/IB/6
lrdn1124:1626469:1626573 [0] NCCL INFO Channel 02/0 : 9[0] -> 10[0] [receive] via NET/IB/5
lrdn1124:1626469:1626573 [0] NCCL INFO Channel 03/0 : 9[0] -> 10[0] [receive] via NET/IB/6
lrdn1124:1626469:1626573 [0] NCCL INFO Channel 00/0 : 10[0] -> 11[0] [send] via NET/IB/5
lrdn1124:1626469:1626573 [0] NCCL INFO Channel 01/0 : 10[0] -> 11[0] [send] via NET/IB/6
lrdn0853:1717580:1717684 [0] NCCL INFO Channel 01/0 : 7[0] -> 8[0] [send] via NET/IB/6
lrdn0805:1354481:1354586 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [send] via NET/IB/6
lrdn0805:1354481:1354586 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [send] via NET/IB/5
lrdn0780:1321542:1321644 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
lrdn0757:1521334:1521437 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 3
lrdn0824:1054616:1054718 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [receive] via NET/IB/5
lrdn1124:1626469:1626573 [0] NCCL INFO Channel 02/0 : 10[0] -> 11[0] [send] via NET/IB/5
lrdn0853:1717580:1717684 [0] NCCL INFO Channel 02/0 : 7[0] -> 8[0] [send] via NET/IB/5
lrdn0805:1354481:1354586 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [send] via NET/IB/6
lrdn0780:1321542:1321643 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn0824:1054616:1054718 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [receive] via NET/IB/6
lrdn1124:1626469:1626573 [0] NCCL INFO Channel 03/0 : 10[0] -> 11[0] [send] via NET/IB/6
lrdn0853:1717580:1717684 [0] NCCL INFO Channel 03/0 : 7[0] -> 8[0] [send] via NET/IB/6
lrdn0780:1321542:1321643 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn0824:1054616:1054718 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [send] via NET/IB/5
lrdn0780:1321542:1321643 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn1139:2620503:2620609 [0] NCCL INFO Channel 01/0 : 11[0] -> 12[0] [receive] via NET/IB/6
lrdn0824:1054616:1054718 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [send] via NET/IB/6
lrdn0824:1054616:1054718 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [send] via NET/IB/5
lrdn1226:1654681:1654783 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 7
lrdn1226:1654681:1654782 [0] NCCL INFO Channel 00/0 : 14[0] -> 15[0] [receive] via NET/IB/5
lrdn1226:1654681:1654782 [0] NCCL INFO Channel 01/0 : 14[0] -> 15[0] [receive] via NET/IB/6
lrdn1226:1654681:1654782 [0] NCCL INFO Channel 02/0 : 14[0] -> 15[0] [receive] via NET/IB/5
lrdn1226:1654681:1654782 [0] NCCL INFO Channel 03/0 : 14[0] -> 15[0] [receive] via NET/IB/6
lrdn1226:1654681:1654782 [0] NCCL INFO Channel 00/0 : 15[0] -> 0[0] [send] via NET/IB/5
lrdn1226:1654681:1654782 [0] NCCL INFO Channel 01/0 : 15[0] -> 0[0] [send] via NET/IB/6
lrdn1226:1654681:1654782 [0] NCCL INFO Channel 02/0 : 15[0] -> 0[0] [send] via NET/IB/5
lrdn1226:1654681:1654782 [0] NCCL INFO Channel 03/0 : 15[0] -> 0[0] [send] via NET/IB/6
lrdn1139:2620503:2620609 [0] NCCL INFO Channel 02/0 : 11[0] -> 12[0] [receive] via NET/IB/5
lrdn0780:1321542:1321643 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn0824:1054616:1054718 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [send] via NET/IB/6
lrdn1139:2620503:2620609 [0] NCCL INFO Channel 03/0 : 11[0] -> 12[0] [receive] via NET/IB/6
lrdn0757:1521334:1521436 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn0757:1521334:1521436 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn0757:1521334:1521436 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn0757:1521334:1521436 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn0757:1521334:1521436 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn0757:1521334:1521436 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn0780:1321542:1321643 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [send] via NET/IB/5
lrdn1139:2620503:2620609 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[0] [send] via NET/IB/5
lrdn0757:1521334:1521436 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn0780:1321542:1321643 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [send] via NET/IB/6
lrdn0780:1321542:1321643 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [send] via NET/IB/5
lrdn0757:1521334:1521436 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn0708:3048046:3048152 [0] NCCL INFO Channel 00/0 : 15[0] -> 0[0] [receive] via NET/IB/5
lrdn0780:1321542:1321643 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [send] via NET/IB/6
lrdn0708:3048046:3048152 [0] NCCL INFO Channel 01/0 : 15[0] -> 0[0] [receive] via NET/IB/6
lrdn0708:3048046:3048152 [0] NCCL INFO Channel 02/0 : 15[0] -> 0[0] [receive] via NET/IB/5
lrdn0708:3048046:3048152 [0] NCCL INFO Channel 03/0 : 15[0] -> 0[0] [receive] via NET/IB/6
lrdn0708:3048046:3048152 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn0708:3048046:3048152 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn0708:3048046:3048152 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn0708:3048046:3048152 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1218:1776591:1776695 [0] NCCL INFO Channel 00/0 : 13[0] -> 14[0] [receive] via NET/IB/5
lrdn1218:1776591:1776696 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 3
lrdn1139:2620503:2620609 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[0] [send] via NET/IB/6
lrdn1139:2620503:2620609 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[0] [send] via NET/IB/5
lrdn1139:2620503:2620609 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[0] [send] via NET/IB/6
lrdn1218:1776591:1776695 [0] NCCL INFO Channel 01/0 : 13[0] -> 14[0] [receive] via NET/IB/6
lrdn1218:1776591:1776695 [0] NCCL INFO Channel 02/0 : 13[0] -> 14[0] [receive] via NET/IB/5
lrdn1218:1776591:1776695 [0] NCCL INFO Channel 03/0 : 13[0] -> 14[0] [receive] via NET/IB/6
lrdn1218:1776591:1776695 [0] NCCL INFO Channel 00/0 : 14[0] -> 15[0] [send] via NET/IB/5
lrdn1218:1776591:1776695 [0] NCCL INFO Channel 01/0 : 14[0] -> 15[0] [send] via NET/IB/6
lrdn1218:1776591:1776695 [0] NCCL INFO Channel 02/0 : 14[0] -> 15[0] [send] via NET/IB/5
lrdn1218:1776591:1776695 [0] NCCL INFO Channel 03/0 : 14[0] -> 15[0] [send] via NET/IB/6
lrdn1206:1547777:1547879 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1138:684929:685032 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1124:1626469:1626573 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0745:1658235:1658339 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1218:1776591:1776695 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0780:1321542:1321643 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0805:1354481:1354586 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0708:3048046:3048152 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1093:1492163:1492265 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0757:1521334:1521436 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0824:1054616:1054718 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0853:1717580:1717684 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1226:1654681:1654782 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1139:2620503:2620609 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1028:895822:895926 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0836:1477025:1477128 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
[38;20m2025-08-02 00:13:05,614 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.26 (max/real adjusted throughput: 107.33/106.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:13:23,801 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.29 (max/real adjusted throughput: 106.35/105.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
lrdn0708:3048046:3048046 [0] NCCL INFO Comm config Blocking set to 1
lrdn0757:1521334:1521334 [0] NCCL INFO Comm config Blocking set to 1
lrdn0757:1521334:1521439 [0] NCCL INFO Using network IB
lrdn1206:1547777:1547777 [0] NCCL INFO Comm config Blocking set to 1
lrdn1218:1776591:1776591 [0] NCCL INFO Comm config Blocking set to 1
lrdn1206:1547777:1547882 [0] NCCL INFO Using network IB
lrdn0836:1477025:1477025 [0] NCCL INFO Comm config Blocking set to 1
lrdn0836:1477025:1477132 [0] NCCL INFO Using network IB
lrdn0853:1717580:1717580 [0] NCCL INFO Comm config Blocking set to 1
lrdn0780:1321542:1321542 [0] NCCL INFO Comm config Blocking set to 1
lrdn0745:1658235:1658235 [0] NCCL INFO Comm config Blocking set to 1
lrdn0745:1658235:1658342 [0] NCCL INFO Using network IB
lrdn0824:1054616:1054616 [0] NCCL INFO Comm config Blocking set to 1
lrdn1138:684929:684929 [0] NCCL INFO Comm config Blocking set to 1
lrdn1139:2620503:2620503 [0] NCCL INFO Comm config Blocking set to 1
lrdn1139:2620503:2620614 [0] NCCL INFO Using network IB
lrdn0853:1717580:1717689 [0] NCCL INFO Using network IB
lrdn0780:1321542:1321646 [0] NCCL INFO Using network IB
lrdn1226:1654681:1654681 [0] NCCL INFO Comm config Blocking set to 1
lrdn1028:895822:895822 [0] NCCL INFO Comm config Blocking set to 1
lrdn1028:895822:895931 [0] NCCL INFO Using network IB
lrdn1124:1626469:1626469 [0] NCCL INFO Comm config Blocking set to 1
lrdn1124:1626469:1626578 [0] NCCL INFO Using network IB
lrdn1218:1776591:1776702 [0] NCCL INFO Using network IB
lrdn0824:1054616:1054721 [0] NCCL INFO Using network IB
lrdn0757:1521334:1521439 [0] NCCL INFO ncclCommInitRankConfig comm 0xd23d980 rank 2 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init START
lrdn1138:684929:685035 [0] NCCL INFO Using network IB
lrdn1226:1654681:1654785 [0] NCCL INFO Using network IB
lrdn0708:3048046:3048156 [0] NCCL INFO Using network IB
lrdn0805:1354481:1354481 [0] NCCL INFO Comm config Blocking set to 1
lrdn0805:1354481:1354589 [0] NCCL INFO Using network IB
lrdn1093:1492163:1492163 [0] NCCL INFO Comm config Blocking set to 1
lrdn1093:1492163:1492268 [0] NCCL INFO Using network IB
lrdn1206:1547777:1547882 [0] NCCL INFO ncclCommInitRankConfig comm 0xe3e5730 rank 13 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init START
lrdn0708:3048046:3048156 [0] NCCL INFO ncclCommInitRankConfig comm 0x2c04c030 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init START
lrdn1028:895822:895931 [0] NCCL INFO ncclCommInitRankConfig comm 0x2b375240 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init START
lrdn1139:2620503:2620614 [0] NCCL INFO ncclCommInitRankConfig comm 0x2da12e30 rank 12 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init START
lrdn0824:1054616:1054721 [0] NCCL INFO ncclCommInitRankConfig comm 0x2a536e30 rank 5 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init START
lrdn0745:1658235:1658342 [0] NCCL INFO ncclCommInitRankConfig comm 0xc9bc7c0 rank 1 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init START
lrdn1218:1776591:1776702 [0] NCCL INFO ncclCommInitRankConfig comm 0xe142ee0 rank 14 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init START
lrdn1226:1654681:1654785 [0] NCCL INFO ncclCommInitRankConfig comm 0x11371260 rank 15 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init START
lrdn0805:1354481:1354589 [0] NCCL INFO ncclCommInitRankConfig comm 0x2e0123c0 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init START
lrdn0836:1477025:1477132 [0] NCCL INFO ncclCommInitRankConfig comm 0x11d1fee0 rank 6 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init START
lrdn1124:1626469:1626578 [0] NCCL INFO ncclCommInitRankConfig comm 0x118019d0 rank 10 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init START
lrdn0780:1321542:1321646 [0] NCCL INFO ncclCommInitRankConfig comm 0x2bad7660 rank 3 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init START
lrdn0853:1717580:1717689 [0] NCCL INFO ncclCommInitRankConfig comm 0x11f42e10 rank 7 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init START
lrdn1093:1492163:1492268 [0] NCCL INFO ncclCommInitRankConfig comm 0x12e4c2a0 rank 9 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init START
lrdn1138:684929:685035 [0] NCCL INFO ncclCommInitRankConfig comm 0x21da4e90 rank 11 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init START
lrdn0780:1321542:1321646 [0] NCCL INFO Bootstrap timings total 0.002273 (create 0.000016, send 0.000077, recv 0.001005, ring 0.001075, delay 0.000000)
lrdn0805:1354481:1354589 [0] NCCL INFO Bootstrap timings total 0.002465 (create 0.000016, send 0.000277, recv 0.000997, ring 0.001058, delay 0.000000)
lrdn0757:1521334:1521439 [0] NCCL INFO Bootstrap timings total 0.002656 (create 0.000014, send 0.000409, recv 0.000387, ring 0.001473, delay 0.000000)
lrdn0708:3048046:3048156 [0] NCCL INFO Bootstrap timings total 0.002618 (create 0.000013, send 0.000050, recv 0.000649, ring 0.001788, delay 0.000000)
lrdn0824:1054616:1054721 [0] NCCL INFO Bootstrap timings total 0.002473 (create 0.000017, send 0.000065, recv 0.000225, ring 0.001090, delay 0.000000)
lrdn0745:1658235:1658342 [0] NCCL INFO Bootstrap timings total 0.002445 (create 0.000029, send 0.000083, recv 0.000559, ring 0.001523, delay 0.000000)
lrdn0836:1477025:1477132 [0] NCCL INFO Bootstrap timings total 0.002575 (create 0.000016, send 0.000077, recv 0.001537, ring 0.000837, delay 0.000000)
lrdn1218:1776591:1776702 [0] NCCL INFO Bootstrap timings total 0.002539 (create 0.000016, send 0.000155, recv 0.000675, ring 0.001552, delay 0.000000)
lrdn0853:1717580:1717689 [0] NCCL INFO Bootstrap timings total 0.002411 (create 0.000016, send 0.000260, recv 0.001218, ring 0.000806, delay 0.000000)
lrdn1226:1654681:1654785 [0] NCCL INFO Bootstrap timings total 0.002512 (create 0.000015, send 0.000069, recv 0.000255, ring 0.001532, delay 0.000000)
lrdn1028:895822:895931 [0] NCCL INFO Bootstrap timings total 0.002621 (create 0.000024, send 0.000226, recv 0.000738, ring 0.000798, delay 0.000000)
lrdn1093:1492163:1492268 [0] NCCL INFO Bootstrap timings total 0.002514 (create 0.000016, send 0.000063, recv 0.001170, ring 0.001120, delay 0.000000)
lrdn1139:2620503:2620614 [0] NCCL INFO Bootstrap timings total 0.002614 (create 0.000018, send 0.000247, recv 0.001024, ring 0.000928, delay 0.000000)
lrdn1124:1626469:1626578 [0] NCCL INFO Bootstrap timings total 0.002570 (create 0.000022, send 0.000185, recv 0.001153, ring 0.001038, delay 0.000000)
lrdn1138:684929:685035 [0] NCCL INFO Bootstrap timings total 0.002439 (create 0.000018, send 0.000066, recv 0.001242, ring 0.000986, delay 0.000000)
lrdn1206:1547777:1547882 [0] NCCL INFO Bootstrap timings total 0.002671 (create 0.000022, send 0.000202, recv 0.000579, ring 0.001191, delay 0.000000)
lrdn1124:1626469:1626578 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1093:1492163:1492268 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1139:2620503:2620614 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1138:684929:685035 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0853:1717580:1717689 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0836:1477025:1477132 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0824:1054616:1054721 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0805:1354481:1354589 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1028:895822:895931 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0780:1321542:1321646 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1226:1654681:1654785 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1218:1776591:1776702 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1206:1547777:1547882 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0745:1658235:1658342 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0757:1521334:1521439 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0708:3048046:3048156 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1139:2620503:2620614 [0] NCCL INFO comm 0x2da12e30 rank 12 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1139:2620503:2620614 [0] NCCL INFO Trees [0] 10/14/-1->12->8 [1] 10/14/-1->12->8 [2] -1/-1/-1->12->13 [3] -1/-1/-1->12->13
lrdn1139:2620503:2620614 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1206:1547777:1547882 [0] NCCL INFO comm 0xe3e5730 rank 13 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0708:3048046:3048156 [0] NCCL INFO comm 0x2c04c030 rank 0 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0757:1521334:1521439 [0] NCCL INFO comm 0xd23d980 rank 2 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0757:1521334:1521439 [0] NCCL INFO Trees [0] 1/3/-1->2->4 [1] 1/3/-1->2->4 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1
lrdn0757:1521334:1521439 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1206:1547777:1547882 [0] NCCL INFO Trees [0] -1/-1/-1->13->14 [1] -1/-1/-1->13->14 [2] 14/12/-1->13->11 [3] 14/12/-1->13->11
lrdn1206:1547777:1547882 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1218:1776591:1776702 [0] NCCL INFO comm 0xe142ee0 rank 14 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1218:1776591:1776702 [0] NCCL INFO Trees [0] 13/15/-1->14->12 [1] 13/15/-1->14->12 [2] -1/-1/-1->14->13 [3] -1/-1/-1->14->13
lrdn1218:1776591:1776702 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1226:1654681:1654785 [0] NCCL INFO comm 0x11371260 rank 15 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0708:3048046:3048156 [0] NCCL INFO Channel 00/04 :  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
lrdn0708:3048046:3048156 [0] NCCL INFO Channel 01/04 :  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
lrdn0708:3048046:3048156 [0] NCCL INFO Channel 02/04 :  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
lrdn0708:3048046:3048156 [0] NCCL INFO Channel 03/04 :  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
lrdn0708:3048046:3048156 [0] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] 8/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
lrdn0708:3048046:3048156 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0708:3048046:3048156 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1206:1547777:1547883 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1226:1654681:1654785 [0] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14 [2] 7/-1/-1->15->-1 [3] 7/-1/-1->15->-1
lrdn1226:1654681:1654785 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0708:3048046:3048157 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn0757:1521334:1521440 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1138:684929:685035 [0] NCCL INFO comm 0x21da4e90 rank 11 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1138:684929:685035 [0] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10 [2] 13/9/-1->11->7 [3] 13/9/-1->11->7
lrdn1138:684929:685035 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0805:1354481:1354589 [0] NCCL INFO comm 0x2e0123c0 rank 4 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0805:1354481:1354589 [0] NCCL INFO Trees [0] 2/6/-1->4->8 [1] 2/6/-1->4->8 [2] -1/-1/-1->4->5 [3] -1/-1/-1->4->5
lrdn0805:1354481:1354589 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1226:1654681:1654786 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0708:3048046:3048158 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn1218:1776591:1776704 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0780:1321542:1321646 [0] NCCL INFO comm 0x2bad7660 rank 3 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0780:1321542:1321646 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 5/1/-1->3->7 [3] 5/1/-1->3->7
lrdn0780:1321542:1321646 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1124:1626469:1626578 [0] NCCL INFO comm 0x118019d0 rank 10 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0824:1054616:1054721 [0] NCCL INFO comm 0x2a536e30 rank 5 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0824:1054616:1054721 [0] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] -1/-1/-1->5->6 [2] 6/4/-1->5->3 [3] 6/4/-1->5->3
lrdn0824:1054616:1054721 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1206:1547777:1547884 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn1139:2620503:2620615 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn1139:2620503:2620616 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1138:684929:685036 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1138:684929:685037 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1124:1626469:1626578 [0] NCCL INFO Trees [0] 9/11/-1->10->12 [1] 9/11/-1->10->12 [2] -1/-1/-1->10->9 [3] -1/-1/-1->10->9
lrdn1124:1626469:1626578 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0745:1658235:1658342 [0] NCCL INFO comm 0xc9bc7c0 rank 1 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0745:1658235:1658342 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] -1/-1/-1->1->2 [2] 2/0/-1->1->3 [3] 2/0/-1->1->3
lrdn0745:1658235:1658342 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0745:1658235:1658343 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn0745:1658235:1658344 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn1218:1776591:1776703 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0805:1354481:1354590 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0805:1354481:1354591 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0780:1321542:1321647 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0824:1054616:1054722 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0757:1521334:1521441 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1124:1626469:1626579 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn1124:1626469:1626580 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0780:1321542:1321648 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0824:1054616:1054723 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1226:1654681:1654787 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn1028:895822:895931 [0] NCCL INFO comm 0x2b375240 rank 8 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1028:895822:895931 [0] NCCL INFO Trees [0] 4/12/-1->8->0 [1] 4/12/-1->8->0 [2] -1/-1/-1->8->9 [3] -1/-1/-1->8->9
lrdn1028:895822:895931 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0853:1717580:1717689 [0] NCCL INFO comm 0x11f42e10 rank 7 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0853:1717580:1717689 [0] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 11/3/-1->7->15 [3] 11/3/-1->7->15
lrdn0853:1717580:1717689 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1093:1492163:1492268 [0] NCCL INFO comm 0x12e4c2a0 rank 9 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1093:1492163:1492268 [0] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] -1/-1/-1->9->10 [2] 10/8/-1->9->11 [3] 10/8/-1->9->11
lrdn1093:1492163:1492268 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1093:1492163:1492269 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0853:1717580:1717691 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0836:1477025:1477132 [0] NCCL INFO comm 0x11d1fee0 rank 6 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0836:1477025:1477132 [0] NCCL INFO Trees [0] 5/7/-1->6->4 [1] 5/7/-1->6->4 [2] -1/-1/-1->6->5 [3] -1/-1/-1->6->5
lrdn0836:1477025:1477132 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0836:1477025:1477133 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1093:1492163:1492270 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1028:895822:895932 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0836:1477025:1477134 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn1028:895822:895933 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0853:1717580:1717690 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0824:1054616:1054721 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0824:1054616:1054721 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0824:1054616:1054721 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0824:1054616:1054721 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0805:1354481:1354589 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0805:1354481:1354589 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0805:1354481:1354589 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0805:1354481:1354589 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1138:684929:685035 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1138:684929:685035 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1138:684929:685035 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1138:684929:685035 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0745:1658235:1658342 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0745:1658235:1658342 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0745:1658235:1658342 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0745:1658235:1658342 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0836:1477025:1477132 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0836:1477025:1477132 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0836:1477025:1477132 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0836:1477025:1477132 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0853:1717580:1717689 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0853:1717580:1717689 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0853:1717580:1717689 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0853:1717580:1717689 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0824:1054616:1054721 [0] NCCL INFO ncclCommInitRankConfig comm 0x2a536e30 rank 5 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init COMPLETE
lrdn0824:1054616:1054721 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 5 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0708:3048046:3048156 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0708:3048046:3048156 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1124:1626469:1626578 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1124:1626469:1626578 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0708:3048046:3048156 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             0              1              0              0              0              0              0  
       Reduce |        0         0         1   |             0              1              0              0              0              0              0  
    AllGather |        0         0         1   |             0              1              0              0              0              0              0  
ReduceScatter |        0         0         1   |             0              1              0              0              0              0              0  
    AllReduce |        0         0         1   |             0              1              0              0              0              0              0  

lrdn0757:1521334:1521439 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0805:1354481:1354589 [0] NCCL INFO ncclCommInitRankConfig comm 0x2e0123c0 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init COMPLETE
lrdn0805:1354481:1354589 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 4 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1124:1626469:1626578 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1124:1626469:1626578 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0745:1658235:1658342 [0] NCCL INFO ncclCommInitRankConfig comm 0xc9bc7c0 rank 1 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init COMPLETE
lrdn0745:1658235:1658342 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0708:3048046:3048156 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0708:3048046:3048156 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0757:1521334:1521439 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0757:1521334:1521439 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0757:1521334:1521439 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1138:684929:685035 [0] NCCL INFO ncclCommInitRankConfig comm 0x21da4e90 rank 11 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init COMPLETE
lrdn1138:684929:685035 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 11 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1226:1654681:1654785 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1226:1654681:1654785 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1226:1654681:1654785 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1226:1654681:1654785 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1218:1776591:1776702 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1218:1776591:1776702 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1218:1776591:1776702 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1218:1776591:1776702 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0853:1717580:1717689 [0] NCCL INFO ncclCommInitRankConfig comm 0x11f42e10 rank 7 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init COMPLETE
lrdn0853:1717580:1717689 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 7 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0836:1477025:1477132 [0] NCCL INFO ncclCommInitRankConfig comm 0x11d1fee0 rank 6 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init COMPLETE
lrdn0836:1477025:1477132 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 6 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0708:3048046:3048156 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1139:2620503:2620614 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1139:2620503:2620614 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1139:2620503:2620614 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1139:2620503:2620614 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1028:895822:895931 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1028:895822:895931 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1028:895822:895931 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1028:895822:895931 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1124:1626469:1626578 [0] NCCL INFO ncclCommInitRankConfig comm 0x118019d0 rank 10 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init COMPLETE
lrdn1206:1547777:1547882 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1124:1626469:1626578 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 10 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0757:1521334:1521439 [0] NCCL INFO ncclCommInitRankConfig comm 0xd23d980 rank 2 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init COMPLETE
lrdn1206:1547777:1547882 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1206:1547777:1547882 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1206:1547777:1547882 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0708:3048046:3048156 [0] NCCL INFO ncclCommInitRankConfig comm 0x2c04c030 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init COMPLETE
lrdn0757:1521334:1521439 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0708:3048046:3048156 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1139:2620503:2620614 [0] NCCL INFO ncclCommInitRankConfig comm 0x2da12e30 rank 12 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init COMPLETE
lrdn1139:2620503:2620614 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 12 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1093:1492163:1492268 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1226:1654681:1654785 [0] NCCL INFO ncclCommInitRankConfig comm 0x11371260 rank 15 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init COMPLETE
lrdn1093:1492163:1492268 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1093:1492163:1492268 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1093:1492163:1492268 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1218:1776591:1776702 [0] NCCL INFO ncclCommInitRankConfig comm 0xe142ee0 rank 14 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init COMPLETE
lrdn1218:1776591:1776702 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 14 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1226:1654681:1654785 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 15 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0780:1321542:1321646 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0780:1321542:1321646 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0780:1321542:1321646 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0780:1321542:1321646 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1028:895822:895931 [0] NCCL INFO ncclCommInitRankConfig comm 0x2b375240 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init COMPLETE
lrdn1028:895822:895931 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 8 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1206:1547777:1547882 [0] NCCL INFO ncclCommInitRankConfig comm 0xe3e5730 rank 13 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init COMPLETE
lrdn1206:1547777:1547882 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 13 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1093:1492163:1492268 [0] NCCL INFO ncclCommInitRankConfig comm 0x12e4c2a0 rank 9 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init COMPLETE
lrdn1093:1492163:1492268 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 9 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0780:1321542:1321646 [0] NCCL INFO ncclCommInitRankConfig comm 0x2bad7660 rank 3 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x189fee645dd0e2c1 - Init COMPLETE
lrdn0780:1321542:1321646 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0805:1354481:1354592 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [receive] via NET/IB/5
lrdn0805:1354481:1354592 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [receive] via NET/IB/6
lrdn0805:1354481:1354593 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 7
lrdn0805:1354481:1354592 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [receive] via NET/IB/5
lrdn0805:1354481:1354592 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [receive] via NET/IB/6
lrdn0805:1354481:1354592 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [send] via NET/IB/5
lrdn0805:1354481:1354592 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [send] via NET/IB/6
lrdn0805:1354481:1354592 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [send] via NET/IB/5
lrdn0805:1354481:1354592 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [send] via NET/IB/6
lrdn1139:2620503:2620617 [0] NCCL INFO Channel 00/0 : 11[0] -> 12[0] [receive] via NET/IB/5
lrdn1139:2620503:2620618 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 7
lrdn0836:1477025:1477136 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 8
lrdn0836:1477025:1477135 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [receive] via NET/IB/5
lrdn0836:1477025:1477135 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [receive] via NET/IB/6
lrdn0836:1477025:1477135 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [receive] via NET/IB/5
lrdn1138:684929:685039 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 5
lrdn1138:684929:685038 [0] NCCL INFO Channel 00/0 : 10[0] -> 11[0] [receive] via NET/IB/5
lrdn1138:684929:685038 [0] NCCL INFO Channel 01/0 : 10[0] -> 11[0] [receive] via NET/IB/6
lrdn1138:684929:685038 [0] NCCL INFO Channel 02/0 : 10[0] -> 11[0] [receive] via NET/IB/5
lrdn0836:1477025:1477135 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [receive] via NET/IB/6
lrdn0853:1717580:1717693 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 5
lrdn0853:1717580:1717692 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [receive] via NET/IB/5
lrdn1138:684929:685038 [0] NCCL INFO Channel 03/0 : 10[0] -> 11[0] [receive] via NET/IB/6
lrdn1138:684929:685038 [0] NCCL INFO Channel 00/0 : 11[0] -> 12[0] [send] via NET/IB/5
lrdn0745:1658235:1658346 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 12
lrdn0745:1658235:1658345 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn0745:1658235:1658345 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn0745:1658235:1658345 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn0745:1658235:1658345 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn0745:1658235:1658345 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn0745:1658235:1658345 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn0745:1658235:1658345 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn1139:2620503:2620617 [0] NCCL INFO Channel 01/0 : 11[0] -> 12[0] [receive] via NET/IB/6
lrdn0853:1717580:1717692 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [receive] via NET/IB/6
lrdn1138:684929:685038 [0] NCCL INFO Channel 01/0 : 11[0] -> 12[0] [send] via NET/IB/6
lrdn1138:684929:685038 [0] NCCL INFO Channel 02/0 : 11[0] -> 12[0] [send] via NET/IB/5
lrdn0745:1658235:1658345 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn0757:1521334:1521443 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 5
lrdn1139:2620503:2620617 [0] NCCL INFO Channel 02/0 : 11[0] -> 12[0] [receive] via NET/IB/5
lrdn0853:1717580:1717692 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [receive] via NET/IB/5
lrdn0836:1477025:1477135 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [send] via NET/IB/5
lrdn0836:1477025:1477135 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [send] via NET/IB/6
lrdn1138:684929:685038 [0] NCCL INFO Channel 03/0 : 11[0] -> 12[0] [send] via NET/IB/6
lrdn0824:1054616:1054724 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [receive] via NET/IB/5
lrdn0757:1521334:1521442 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn0836:1477025:1477135 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [send] via NET/IB/5
lrdn0836:1477025:1477135 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [send] via NET/IB/6
lrdn1139:2620503:2620617 [0] NCCL INFO Channel 03/0 : 11[0] -> 12[0] [receive] via NET/IB/6
lrdn0853:1717580:1717692 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [receive] via NET/IB/6
lrdn1139:2620503:2620617 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[0] [send] via NET/IB/5
lrdn0853:1717580:1717692 [0] NCCL INFO Channel 00/0 : 7[0] -> 8[0] [send] via NET/IB/5
lrdn0824:1054616:1054725 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 6
lrdn1139:2620503:2620617 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[0] [send] via NET/IB/6
lrdn0853:1717580:1717692 [0] NCCL INFO Channel 01/0 : 7[0] -> 8[0] [send] via NET/IB/6
lrdn1124:1626469:1626581 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[0] [receive] via NET/IB/5
lrdn1124:1626469:1626582 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 2
lrdn1139:2620503:2620617 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[0] [send] via NET/IB/5
lrdn0853:1717580:1717692 [0] NCCL INFO Channel 02/0 : 7[0] -> 8[0] [send] via NET/IB/5
lrdn0853:1717580:1717692 [0] NCCL INFO Channel 03/0 : 7[0] -> 8[0] [send] via NET/IB/6
lrdn1139:2620503:2620617 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[0] [send] via NET/IB/6
lrdn0757:1521334:1521442 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn0757:1521334:1521442 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn0757:1521334:1521442 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn1226:1654681:1654789 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 10
lrdn1124:1626469:1626581 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[0] [receive] via NET/IB/6
lrdn0824:1054616:1054724 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [receive] via NET/IB/6
lrdn1226:1654681:1654788 [0] NCCL INFO Channel 00/0 : 14[0] -> 15[0] [receive] via NET/IB/5
lrdn0757:1521334:1521442 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn0757:1521334:1521442 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn1124:1626469:1626581 [0] NCCL INFO Channel 02/0 : 9[0] -> 10[0] [receive] via NET/IB/5
lrdn0824:1054616:1054724 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [receive] via NET/IB/5
lrdn0757:1521334:1521442 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn1124:1626469:1626581 [0] NCCL INFO Channel 03/0 : 9[0] -> 10[0] [receive] via NET/IB/6
lrdn0824:1054616:1054724 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [receive] via NET/IB/6
lrdn0757:1521334:1521442 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn1124:1626469:1626581 [0] NCCL INFO Channel 00/0 : 10[0] -> 11[0] [send] via NET/IB/5
lrdn0824:1054616:1054724 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [send] via NET/IB/5
lrdn1206:1547777:1547885 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[0] [receive] via NET/IB/5
lrdn1206:1547777:1547885 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[0] [receive] via NET/IB/6
lrdn1206:1547777:1547886 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
lrdn1206:1547777:1547885 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[0] [receive] via NET/IB/5
lrdn0780:1321542:1321649 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn1218:1776591:1776706 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 5
lrdn1124:1626469:1626581 [0] NCCL INFO Channel 01/0 : 10[0] -> 11[0] [send] via NET/IB/6
lrdn1206:1547777:1547885 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[0] [receive] via NET/IB/6
lrdn0824:1054616:1054724 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [send] via NET/IB/6
lrdn1093:1492163:1492272 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
lrdn1093:1492163:1492271 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[0] [receive] via NET/IB/5
lrdn1093:1492163:1492271 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[0] [receive] via NET/IB/6
lrdn0780:1321542:1321649 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn0708:3048046:3048160 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 6
lrdn1226:1654681:1654788 [0] NCCL INFO Channel 01/0 : 14[0] -> 15[0] [receive] via NET/IB/6
lrdn1218:1776591:1776705 [0] NCCL INFO Channel 00/0 : 13[0] -> 14[0] [receive] via NET/IB/5
lrdn1124:1626469:1626581 [0] NCCL INFO Channel 02/0 : 10[0] -> 11[0] [send] via NET/IB/5
lrdn1124:1626469:1626581 [0] NCCL INFO Channel 03/0 : 10[0] -> 11[0] [send] via NET/IB/6
lrdn0824:1054616:1054724 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [send] via NET/IB/5
lrdn0780:1321542:1321650 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 5
lrdn0708:3048046:3048159 [0] NCCL INFO Channel 00/0 : 15[0] -> 0[0] [receive] via NET/IB/5
lrdn1226:1654681:1654788 [0] NCCL INFO Channel 02/0 : 14[0] -> 15[0] [receive] via NET/IB/5
lrdn1218:1776591:1776705 [0] NCCL INFO Channel 01/0 : 13[0] -> 14[0] [receive] via NET/IB/6
lrdn1206:1547777:1547885 [0] NCCL INFO Channel 00/0 : 13[0] -> 14[0] [send] via NET/IB/5
lrdn1206:1547777:1547885 [0] NCCL INFO Channel 01/0 : 13[0] -> 14[0] [send] via NET/IB/6
lrdn1093:1492163:1492271 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[0] [receive] via NET/IB/5
lrdn1093:1492163:1492271 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[0] [receive] via NET/IB/6
lrdn1093:1492163:1492271 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[0] [send] via NET/IB/5
lrdn0824:1054616:1054724 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [send] via NET/IB/6
lrdn0780:1321542:1321649 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn0708:3048046:3048159 [0] NCCL INFO Channel 01/0 : 15[0] -> 0[0] [receive] via NET/IB/6
lrdn1226:1654681:1654788 [0] NCCL INFO Channel 03/0 : 14[0] -> 15[0] [receive] via NET/IB/6
lrdn1218:1776591:1776705 [0] NCCL INFO Channel 02/0 : 13[0] -> 14[0] [receive] via NET/IB/5
lrdn1093:1492163:1492271 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[0] [send] via NET/IB/6
lrdn0780:1321542:1321649 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn0708:3048046:3048159 [0] NCCL INFO Channel 02/0 : 15[0] -> 0[0] [receive] via NET/IB/5
lrdn1028:895822:895934 [0] NCCL INFO Channel 00/0 : 7[0] -> 8[0] [receive] via NET/IB/5
lrdn1226:1654681:1654788 [0] NCCL INFO Channel 00/0 : 15[0] -> 0[0] [send] via NET/IB/5
lrdn1206:1547777:1547885 [0] NCCL INFO Channel 02/0 : 13[0] -> 14[0] [send] via NET/IB/5
lrdn1093:1492163:1492271 [0] NCCL INFO Channel 02/0 : 9[0] -> 10[0] [send] via NET/IB/5
lrdn1093:1492163:1492271 [0] NCCL INFO Channel 03/0 : 9[0] -> 10[0] [send] via NET/IB/6
lrdn1218:1776591:1776705 [0] NCCL INFO Channel 03/0 : 13[0] -> 14[0] [receive] via NET/IB/6
lrdn1218:1776591:1776705 [0] NCCL INFO Channel 00/0 : 14[0] -> 15[0] [send] via NET/IB/5
lrdn0780:1321542:1321649 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [send] via NET/IB/5
lrdn0708:3048046:3048159 [0] NCCL INFO Channel 03/0 : 15[0] -> 0[0] [receive] via NET/IB/6
lrdn1226:1654681:1654788 [0] NCCL INFO Channel 01/0 : 15[0] -> 0[0] [send] via NET/IB/6
lrdn1226:1654681:1654788 [0] NCCL INFO Channel 02/0 : 15[0] -> 0[0] [send] via NET/IB/5
lrdn1218:1776591:1776705 [0] NCCL INFO Channel 01/0 : 14[0] -> 15[0] [send] via NET/IB/6
lrdn1206:1547777:1547885 [0] NCCL INFO Channel 03/0 : 13[0] -> 14[0] [send] via NET/IB/6
lrdn0780:1321542:1321649 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [send] via NET/IB/6
lrdn0708:3048046:3048159 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1226:1654681:1654788 [0] NCCL INFO Channel 03/0 : 15[0] -> 0[0] [send] via NET/IB/6
lrdn0708:3048046:3048159 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn0780:1321542:1321649 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [send] via NET/IB/5
lrdn0708:3048046:3048159 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1218:1776591:1776705 [0] NCCL INFO Channel 02/0 : 14[0] -> 15[0] [send] via NET/IB/5
lrdn0780:1321542:1321649 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [send] via NET/IB/6
lrdn0708:3048046:3048159 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1218:1776591:1776705 [0] NCCL INFO Channel 03/0 : 14[0] -> 15[0] [send] via NET/IB/6
lrdn1028:895822:895935 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
lrdn1028:895822:895934 [0] NCCL INFO Channel 01/0 : 7[0] -> 8[0] [receive] via NET/IB/6
lrdn1028:895822:895934 [0] NCCL INFO Channel 02/0 : 7[0] -> 8[0] [receive] via NET/IB/5
lrdn1028:895822:895934 [0] NCCL INFO Channel 03/0 : 7[0] -> 8[0] [receive] via NET/IB/6
lrdn1028:895822:895934 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[0] [send] via NET/IB/5
lrdn1028:895822:895934 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[0] [send] via NET/IB/6
lrdn1028:895822:895934 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[0] [send] via NET/IB/5
lrdn1028:895822:895934 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[0] [send] via NET/IB/6
lrdn1138:684929:685038 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1206:1547777:1547885 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1218:1776591:1776705 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1124:1626469:1626581 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1226:1654681:1654788 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0708:3048046:3048159 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1139:2620503:2620617 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0780:1321542:1321649 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0745:1658235:1658345 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0805:1354481:1354592 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0757:1521334:1521442 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0824:1054616:1054724 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0853:1717580:1717692 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1093:1492163:1492271 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1028:895822:895934 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0836:1477025:1477135 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0708:3048046:3048046 [0] NCCL INFO Comm config Blocking set to 1
lrdn1206:1547777:1547777 [0] NCCL INFO Comm config Blocking set to 1
lrdn0757:1521334:1521334 [0] NCCL INFO Comm config Blocking set to 1
lrdn1206:1547777:1547887 [0] NCCL INFO Using network IB
lrdn0757:1521334:1521444 [0] NCCL INFO Using network IB
lrdn1139:2620503:2620503 [0] NCCL INFO Comm config Blocking set to 1
lrdn0805:1354481:1354481 [0] NCCL INFO Comm config Blocking set to 1
lrdn1093:1492163:1492163 [0] NCCL INFO Comm config Blocking set to 1
lrdn0824:1054616:1054616 [0] NCCL INFO Comm config Blocking set to 1
lrdn1226:1654681:1654681 [0] NCCL INFO Comm config Blocking set to 1
lrdn0745:1658235:1658235 [0] NCCL INFO Comm config Blocking set to 1
lrdn0780:1321542:1321542 [0] NCCL INFO Comm config Blocking set to 1
lrdn1138:684929:684929 [0] NCCL INFO Comm config Blocking set to 1
lrdn1206:1547777:1547887 [0] NCCL INFO ncclCommInitRankConfig comm 0xe4612c0 rank 13 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init START
lrdn1218:1776591:1776591 [0] NCCL INFO Comm config Blocking set to 1
lrdn1028:895822:895822 [0] NCCL INFO Comm config Blocking set to 1
lrdn0757:1521334:1521444 [0] NCCL INFO ncclCommInitRankConfig comm 0xd2b9510 rank 2 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init START
lrdn0853:1717580:1717580 [0] NCCL INFO Comm config Blocking set to 1
lrdn0853:1717580:1717694 [0] NCCL INFO Using network IB
lrdn1138:684929:685040 [0] NCCL INFO Using network IB
lrdn1093:1492163:1492273 [0] NCCL INFO Using network IB
lrdn1124:1626469:1626469 [0] NCCL INFO Comm config Blocking set to 1
lrdn0853:1717580:1717694 [0] NCCL INFO ncclCommInitRankConfig comm 0x11fbe9a0 rank 7 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init START
lrdn0836:1477025:1477025 [0] NCCL INFO Comm config Blocking set to 1
lrdn1138:684929:685040 [0] NCCL INFO ncclCommInitRankConfig comm 0x21e20a20 rank 11 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init START
lrdn1139:2620503:2620619 [0] NCCL INFO Using network IB
lrdn1093:1492163:1492273 [0] NCCL INFO ncclCommInitRankConfig comm 0x12ec7e30 rank 9 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init START
lrdn0836:1477025:1477137 [0] NCCL INFO Using network IB
lrdn0805:1354481:1354594 [0] NCCL INFO Using network IB
lrdn0805:1354481:1354594 [0] NCCL INFO ncclCommInitRankConfig comm 0x2e08df50 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init START
lrdn1218:1776591:1776707 [0] NCCL INFO Using network IB
lrdn1218:1776591:1776707 [0] NCCL INFO ncclCommInitRankConfig comm 0xe1bea70 rank 14 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init START
lrdn1124:1626469:1626583 [0] NCCL INFO Using network IB
lrdn0824:1054616:1054726 [0] NCCL INFO Using network IB
lrdn0824:1054616:1054726 [0] NCCL INFO ncclCommInitRankConfig comm 0x2a5b29c0 rank 5 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init START
lrdn1139:2620503:2620619 [0] NCCL INFO ncclCommInitRankConfig comm 0x2da8e9c0 rank 12 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init START
lrdn0780:1321542:1321651 [0] NCCL INFO Using network IB
lrdn0780:1321542:1321651 [0] NCCL INFO ncclCommInitRankConfig comm 0x2bb531f0 rank 3 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init START
lrdn0708:3048046:3048162 [0] NCCL INFO Using network IB
lrdn0708:3048046:3048162 [0] NCCL INFO ncclCommInitRankConfig comm 0x2c0c7bc0 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init START
lrdn1028:895822:895936 [0] NCCL INFO Using network IB
lrdn1028:895822:895936 [0] NCCL INFO ncclCommInitRankConfig comm 0x2b3f0dd0 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init START
lrdn0745:1658235:1658347 [0] NCCL INFO Using network IB
lrdn0745:1658235:1658347 [0] NCCL INFO ncclCommInitRankConfig comm 0xca38350 rank 1 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init START
lrdn1124:1626469:1626583 [0] NCCL INFO ncclCommInitRankConfig comm 0x1187d560 rank 10 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init START
lrdn0836:1477025:1477137 [0] NCCL INFO ncclCommInitRankConfig comm 0x11d9ba70 rank 6 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init START
lrdn1226:1654681:1654790 [0] NCCL INFO Using network IB
lrdn1226:1654681:1654790 [0] NCCL INFO ncclCommInitRankConfig comm 0x113ecdf0 rank 15 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init START
lrdn1028:895822:895936 [0] NCCL INFO Bootstrap timings total 0.002297 (create 0.000022, send 0.000066, recv 0.000803, ring 0.001287, delay 0.000000)
lrdn0853:1717580:1717694 [0] NCCL INFO Bootstrap timings total 0.002462 (create 0.000021, send 0.000060, recv 0.000891, ring 0.001079, delay 0.000000)
lrdn1093:1492163:1492273 [0] NCCL INFO Bootstrap timings total 0.002434 (create 0.000016, send 0.000059, recv 0.001226, ring 0.001000, delay 0.000000)
lrdn0836:1477025:1477137 [0] NCCL INFO Bootstrap timings total 0.002454 (create 0.000013, send 0.000132, recv 0.001025, ring 0.001193, delay 0.000000)
lrdn0824:1054616:1054726 [0] NCCL INFO Bootstrap timings total 0.002510 (create 0.000016, send 0.000126, recv 0.001003, ring 0.001241, delay 0.000000)
lrdn1124:1626469:1626583 [0] NCCL INFO Bootstrap timings total 0.002403 (create 0.000019, send 0.000084, recv 0.001019, ring 0.000923, delay 0.000000)
lrdn0780:1321542:1321651 [0] NCCL INFO Bootstrap timings total 0.002584 (create 0.000018, send 0.000081, recv 0.000232, ring 0.001831, delay 0.000000)
lrdn0757:1521334:1521444 [0] NCCL INFO Bootstrap timings total 0.002708 (create 0.000016, send 0.000242, recv 0.000461, ring 0.001722, delay 0.000000)
lrdn0805:1354481:1354594 [0] NCCL INFO Bootstrap timings total 0.002601 (create 0.000016, send 0.000050, recv 0.000981, ring 0.001453, delay 0.000000)
lrdn1138:684929:685040 [0] NCCL INFO Bootstrap timings total 0.002608 (create 0.000016, send 0.000059, recv 0.000456, ring 0.001075, delay 0.000000)
lrdn0745:1658235:1658347 [0] NCCL INFO Bootstrap timings total 0.002552 (create 0.000021, send 0.000065, recv 0.000571, ring 0.001776, delay 0.000000)
lrdn0708:3048046:3048162 [0] NCCL INFO Bootstrap timings total 0.002827 (create 0.000015, send 0.000052, recv 0.000765, ring 0.001886, delay 0.000000)
lrdn1139:2620503:2620619 [0] NCCL INFO Bootstrap timings total 0.002554 (create 0.000015, send 0.000058, recv 0.000389, ring 0.001857, delay 0.000000)
lrdn1206:1547777:1547887 [0] NCCL INFO Bootstrap timings total 0.002797 (create 0.000021, send 0.000066, recv 0.001144, ring 0.001418, delay 0.000000)
lrdn1226:1654681:1654790 [0] NCCL INFO Bootstrap timings total 0.002596 (create 0.000016, send 0.000050, recv 0.000473, ring 0.001295, delay 0.000000)
lrdn1218:1776591:1776707 [0] NCCL INFO Bootstrap timings total 0.002588 (create 0.000018, send 0.000053, recv 0.001001, ring 0.001422, delay 0.000000)
lrdn0836:1477025:1477137 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1218:1776591:1776707 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1206:1547777:1547887 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0745:1658235:1658347 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0757:1521334:1521444 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0853:1717580:1717694 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1226:1654681:1654790 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0780:1321542:1321651 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0824:1054616:1054726 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0805:1354481:1354594 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0708:3048046:3048162 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1138:684929:685040 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1124:1626469:1626583 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1093:1492163:1492273 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1028:895822:895936 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1139:2620503:2620619 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0708:3048046:3048162 [0] NCCL INFO comm 0x2c0c7bc0 rank 0 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0708:3048046:3048162 [0] NCCL INFO Channel 00/04 :  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
lrdn0708:3048046:3048162 [0] NCCL INFO Channel 01/04 :  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
lrdn0708:3048046:3048162 [0] NCCL INFO Channel 02/04 :  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
lrdn0708:3048046:3048162 [0] NCCL INFO Channel 03/04 :  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
lrdn0708:3048046:3048162 [0] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] 8/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
lrdn0708:3048046:3048162 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0708:3048046:3048162 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1226:1654681:1654790 [0] NCCL INFO comm 0x113ecdf0 rank 15 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1139:2620503:2620619 [0] NCCL INFO comm 0x2da8e9c0 rank 12 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1139:2620503:2620619 [0] NCCL INFO Trees [0] 10/14/-1->12->8 [1] 10/14/-1->12->8 [2] -1/-1/-1->12->13 [3] -1/-1/-1->12->13
lrdn1139:2620503:2620619 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0745:1658235:1658347 [0] NCCL INFO comm 0xca38350 rank 1 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1206:1547777:1547887 [0] NCCL INFO comm 0xe4612c0 rank 13 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1206:1547777:1547887 [0] NCCL INFO Trees [0] -1/-1/-1->13->14 [1] -1/-1/-1->13->14 [2] 14/12/-1->13->11 [3] 14/12/-1->13->11
lrdn1206:1547777:1547887 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1218:1776591:1776707 [0] NCCL INFO comm 0xe1bea70 rank 14 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1218:1776591:1776707 [0] NCCL INFO Trees [0] 13/15/-1->14->12 [1] 13/15/-1->14->12 [2] -1/-1/-1->14->13 [3] -1/-1/-1->14->13
lrdn1218:1776591:1776707 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0708:3048046:3048163 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn1226:1654681:1654790 [0] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14 [2] 7/-1/-1->15->-1 [3] 7/-1/-1->15->-1
lrdn1226:1654681:1654790 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0745:1658235:1658347 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] -1/-1/-1->1->2 [2] 2/0/-1->1->3 [3] 2/0/-1->1->3
lrdn0745:1658235:1658347 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1093:1492163:1492273 [0] NCCL INFO comm 0x12ec7e30 rank 9 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0708:3048046:3048164 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn1226:1654681:1654791 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0745:1658235:1658348 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn1138:684929:685040 [0] NCCL INFO comm 0x21e20a20 rank 11 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1138:684929:685040 [0] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10 [2] 13/9/-1->11->7 [3] 13/9/-1->11->7
lrdn1138:684929:685040 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1138:684929:685041 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn1138:684929:685042 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn1093:1492163:1492273 [0] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] -1/-1/-1->9->10 [2] 10/8/-1->9->11 [3] 10/8/-1->9->11
lrdn1093:1492163:1492273 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0745:1658235:1658349 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn1028:895822:895936 [0] NCCL INFO comm 0x2b3f0dd0 rank 8 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1028:895822:895936 [0] NCCL INFO Trees [0] 4/12/-1->8->0 [1] 4/12/-1->8->0 [2] -1/-1/-1->8->9 [3] -1/-1/-1->8->9
lrdn1028:895822:895936 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0853:1717580:1717694 [0] NCCL INFO comm 0x11fbe9a0 rank 7 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0853:1717580:1717694 [0] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 11/3/-1->7->15 [3] 11/3/-1->7->15
lrdn0853:1717580:1717694 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0780:1321542:1321651 [0] NCCL INFO comm 0x2bb531f0 rank 3 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0780:1321542:1321651 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 5/1/-1->3->7 [3] 5/1/-1->3->7
lrdn0780:1321542:1321651 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1093:1492163:1492274 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn1124:1626469:1626583 [0] NCCL INFO comm 0x1187d560 rank 10 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1124:1626469:1626583 [0] NCCL INFO Trees [0] 9/11/-1->10->12 [1] 9/11/-1->10->12 [2] -1/-1/-1->10->9 [3] -1/-1/-1->10->9
lrdn1124:1626469:1626583 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1124:1626469:1626584 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn1124:1626469:1626585 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn0824:1054616:1054726 [0] NCCL INFO comm 0x2a5b29c0 rank 5 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1139:2620503:2620620 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1139:2620503:2620621 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn1093:1492163:1492275 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn0824:1054616:1054726 [0] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] -1/-1/-1->5->6 [2] 6/4/-1->5->3 [3] 6/4/-1->5->3
lrdn0824:1054616:1054726 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1226:1654681:1654792 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn1218:1776591:1776708 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1218:1776591:1776709 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0757:1521334:1521444 [0] NCCL INFO comm 0xd2b9510 rank 2 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0757:1521334:1521444 [0] NCCL INFO Trees [0] 1/3/-1->2->4 [1] 1/3/-1->2->4 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1
lrdn0757:1521334:1521444 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0853:1717580:1717695 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0853:1717580:1717696 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn1206:1547777:1547888 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn0757:1521334:1521445 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn1206:1547777:1547889 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn0836:1477025:1477137 [0] NCCL INFO comm 0x11d9ba70 rank 6 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0836:1477025:1477137 [0] NCCL INFO Trees [0] 5/7/-1->6->4 [1] 5/7/-1->6->4 [2] -1/-1/-1->6->5 [3] -1/-1/-1->6->5
lrdn0836:1477025:1477137 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0824:1054616:1054727 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn0824:1054616:1054728 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn0757:1521334:1521446 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn0780:1321542:1321652 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn0780:1321542:1321653 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn0836:1477025:1477139 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn0836:1477025:1477138 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn1028:895822:895937 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1028:895822:895938 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0805:1354481:1354594 [0] NCCL INFO comm 0x2e08df50 rank 4 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0805:1354481:1354594 [0] NCCL INFO Trees [0] 2/6/-1->4->8 [1] 2/6/-1->4->8 [2] -1/-1/-1->4->5 [3] -1/-1/-1->4->5
lrdn0805:1354481:1354594 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0805:1354481:1354595 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0805:1354481:1354596 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1218:1776591:1776707 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1218:1776591:1776707 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1218:1776591:1776707 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1218:1776591:1776707 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1206:1547777:1547887 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1206:1547777:1547887 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1206:1547777:1547887 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1206:1547777:1547887 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1139:2620503:2620619 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1139:2620503:2620619 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1139:2620503:2620619 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1139:2620503:2620619 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1226:1654681:1654790 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1226:1654681:1654790 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1226:1654681:1654790 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1226:1654681:1654790 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1138:684929:685040 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1138:684929:685040 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1138:684929:685040 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1138:684929:685040 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1093:1492163:1492273 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1093:1492163:1492273 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1093:1492163:1492273 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1093:1492163:1492273 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0824:1054616:1054726 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0824:1054616:1054726 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0824:1054616:1054726 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0824:1054616:1054726 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0853:1717580:1717694 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0853:1717580:1717694 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0853:1717580:1717694 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0853:1717580:1717694 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1124:1626469:1626583 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1124:1626469:1626583 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1124:1626469:1626583 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1124:1626469:1626583 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0708:3048046:3048162 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0708:3048046:3048162 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0708:3048046:3048162 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             0              1              0              0              0              0              0  
       Reduce |        0         0         1   |             0              1              0              0              0              0              0  
    AllGather |        0         0         1   |             0              1              0              0              0              0              0  
ReduceScatter |        0         0         1   |             0              1              0              0              0              0              0  
    AllReduce |        0         0         1   |             0              1              0              0              0              0              0  

lrdn0805:1354481:1354594 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0805:1354481:1354594 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0805:1354481:1354594 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0805:1354481:1354594 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0708:3048046:3048162 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0708:3048046:3048162 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0745:1658235:1658347 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0745:1658235:1658347 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0745:1658235:1658347 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0745:1658235:1658347 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0780:1321542:1321651 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0780:1321542:1321651 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0780:1321542:1321651 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0780:1321542:1321651 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0757:1521334:1521444 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0757:1521334:1521444 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0757:1521334:1521444 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0757:1521334:1521444 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0836:1477025:1477137 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0836:1477025:1477137 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0836:1477025:1477137 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0836:1477025:1477137 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1028:895822:895936 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1028:895822:895936 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1028:895822:895936 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1028:895822:895936 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0708:3048046:3048162 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1218:1776591:1776707 [0] NCCL INFO ncclCommInitRankConfig comm 0xe1bea70 rank 14 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init COMPLETE
lrdn1218:1776591:1776707 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 14 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1206:1547777:1547887 [0] NCCL INFO ncclCommInitRankConfig comm 0xe4612c0 rank 13 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init COMPLETE
lrdn1206:1547777:1547887 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 13 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1139:2620503:2620619 [0] NCCL INFO ncclCommInitRankConfig comm 0x2da8e9c0 rank 12 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init COMPLETE
lrdn1139:2620503:2620619 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 12 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1138:684929:685040 [0] NCCL INFO ncclCommInitRankConfig comm 0x21e20a20 rank 11 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init COMPLETE
lrdn1138:684929:685040 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 11 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1093:1492163:1492273 [0] NCCL INFO ncclCommInitRankConfig comm 0x12ec7e30 rank 9 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init COMPLETE
lrdn1093:1492163:1492273 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 9 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1124:1626469:1626583 [0] NCCL INFO ncclCommInitRankConfig comm 0x1187d560 rank 10 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init COMPLETE
lrdn1124:1626469:1626583 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 10 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1226:1654681:1654790 [0] NCCL INFO ncclCommInitRankConfig comm 0x113ecdf0 rank 15 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init COMPLETE
lrdn1226:1654681:1654790 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 15 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0824:1054616:1054726 [0] NCCL INFO ncclCommInitRankConfig comm 0x2a5b29c0 rank 5 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init COMPLETE
lrdn0824:1054616:1054726 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 5 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0853:1717580:1717694 [0] NCCL INFO ncclCommInitRankConfig comm 0x11fbe9a0 rank 7 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init COMPLETE
lrdn0853:1717580:1717694 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 7 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0708:3048046:3048162 [0] NCCL INFO ncclCommInitRankConfig comm 0x2c0c7bc0 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init COMPLETE
lrdn0708:3048046:3048162 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0745:1658235:1658347 [0] NCCL INFO ncclCommInitRankConfig comm 0xca38350 rank 1 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init COMPLETE
lrdn0745:1658235:1658347 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0805:1354481:1354594 [0] NCCL INFO ncclCommInitRankConfig comm 0x2e08df50 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init COMPLETE
lrdn0805:1354481:1354594 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 4 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0757:1521334:1521444 [0] NCCL INFO ncclCommInitRankConfig comm 0xd2b9510 rank 2 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init COMPLETE
lrdn0757:1521334:1521444 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.01, rest 0.00)
lrdn0780:1321542:1321651 [0] NCCL INFO ncclCommInitRankConfig comm 0x2bb531f0 rank 3 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init COMPLETE
lrdn0780:1321542:1321651 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0836:1477025:1477137 [0] NCCL INFO ncclCommInitRankConfig comm 0x11d9ba70 rank 6 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init COMPLETE
lrdn0836:1477025:1477137 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 6 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1028:895822:895936 [0] NCCL INFO ncclCommInitRankConfig comm 0x2b3f0dd0 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x53441e0421cde75 - Init COMPLETE
lrdn1028:895822:895936 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 8 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.01, rest 0.00)
lrdn1218:1776591:1776711 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 9
lrdn1218:1776591:1776710 [0] NCCL INFO Channel 00/0 : 13[0] -> 14[0] [receive] via NET/IB/5
lrdn1218:1776591:1776710 [0] NCCL INFO Channel 01/0 : 13[0] -> 14[0] [receive] via NET/IB/6
lrdn1218:1776591:1776710 [0] NCCL INFO Channel 02/0 : 13[0] -> 14[0] [receive] via NET/IB/5
lrdn1218:1776591:1776710 [0] NCCL INFO Channel 03/0 : 13[0] -> 14[0] [receive] via NET/IB/6
lrdn1218:1776591:1776710 [0] NCCL INFO Channel 00/0 : 14[0] -> 15[0] [send] via NET/IB/5
lrdn1218:1776591:1776710 [0] NCCL INFO Channel 01/0 : 14[0] -> 15[0] [send] via NET/IB/6
lrdn1218:1776591:1776710 [0] NCCL INFO Channel 02/0 : 14[0] -> 15[0] [send] via NET/IB/5
lrdn1218:1776591:1776710 [0] NCCL INFO Channel 03/0 : 14[0] -> 15[0] [send] via NET/IB/6
lrdn1139:2620503:2620623 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 9
lrdn1139:2620503:2620622 [0] NCCL INFO Channel 00/0 : 11[0] -> 12[0] [receive] via NET/IB/5
lrdn1139:2620503:2620622 [0] NCCL INFO Channel 01/0 : 11[0] -> 12[0] [receive] via NET/IB/6
lrdn1139:2620503:2620622 [0] NCCL INFO Channel 02/0 : 11[0] -> 12[0] [receive] via NET/IB/5
lrdn1139:2620503:2620622 [0] NCCL INFO Channel 03/0 : 11[0] -> 12[0] [receive] via NET/IB/6
lrdn1139:2620503:2620622 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[0] [send] via NET/IB/5
lrdn1139:2620503:2620622 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[0] [send] via NET/IB/6
lrdn1139:2620503:2620622 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[0] [send] via NET/IB/5
lrdn1139:2620503:2620622 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[0] [send] via NET/IB/6
lrdn1226:1654681:1654794 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 14
lrdn1226:1654681:1654793 [0] NCCL INFO Channel 00/0 : 14[0] -> 15[0] [receive] via NET/IB/5
lrdn1226:1654681:1654793 [0] NCCL INFO Channel 01/0 : 14[0] -> 15[0] [receive] via NET/IB/6
lrdn1226:1654681:1654793 [0] NCCL INFO Channel 02/0 : 14[0] -> 15[0] [receive] via NET/IB/5
lrdn1226:1654681:1654793 [0] NCCL INFO Channel 03/0 : 14[0] -> 15[0] [receive] via NET/IB/6
lrdn1206:1547777:1547890 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[0] [receive] via NET/IB/5
lrdn1093:1492163:1492276 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[0] [receive] via NET/IB/5
lrdn1093:1492163:1492276 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[0] [receive] via NET/IB/6
lrdn1093:1492163:1492277 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 8
lrdn1093:1492163:1492276 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[0] [receive] via NET/IB/5
lrdn1093:1492163:1492276 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[0] [receive] via NET/IB/6
lrdn1138:684929:685044 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 7
lrdn1138:684929:685043 [0] NCCL INFO Channel 00/0 : 10[0] -> 11[0] [receive] via NET/IB/5
lrdn1226:1654681:1654793 [0] NCCL INFO Channel 00/0 : 15[0] -> 0[0] [send] via NET/IB/5
lrdn1206:1547777:1547890 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[0] [receive] via NET/IB/6
lrdn1226:1654681:1654793 [0] NCCL INFO Channel 01/0 : 15[0] -> 0[0] [send] via NET/IB/6
lrdn1226:1654681:1654793 [0] NCCL INFO Channel 02/0 : 15[0] -> 0[0] [send] via NET/IB/5
lrdn1206:1547777:1547891 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 8
lrdn1206:1547777:1547890 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[0] [receive] via NET/IB/5
lrdn1093:1492163:1492276 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[0] [send] via NET/IB/5
lrdn1093:1492163:1492276 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[0] [send] via NET/IB/6
lrdn1093:1492163:1492276 [0] NCCL INFO Channel 02/0 : 9[0] -> 10[0] [send] via NET/IB/5
lrdn0824:1054616:1054730 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 9
lrdn0824:1054616:1054729 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [receive] via NET/IB/5
lrdn1206:1547777:1547890 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[0] [receive] via NET/IB/6
lrdn1093:1492163:1492276 [0] NCCL INFO Channel 03/0 : 9[0] -> 10[0] [send] via NET/IB/6
lrdn1226:1654681:1654793 [0] NCCL INFO Channel 03/0 : 15[0] -> 0[0] [send] via NET/IB/6
lrdn0824:1054616:1054729 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [receive] via NET/IB/6
lrdn1206:1547777:1547890 [0] NCCL INFO Channel 00/0 : 13[0] -> 14[0] [send] via NET/IB/5
lrdn1206:1547777:1547890 [0] NCCL INFO Channel 01/0 : 13[0] -> 14[0] [send] via NET/IB/6
lrdn0824:1054616:1054729 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [receive] via NET/IB/5
lrdn1206:1547777:1547890 [0] NCCL INFO Channel 02/0 : 13[0] -> 14[0] [send] via NET/IB/5
lrdn1206:1547777:1547890 [0] NCCL INFO Channel 03/0 : 13[0] -> 14[0] [send] via NET/IB/6
lrdn0824:1054616:1054729 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [receive] via NET/IB/6
lrdn0824:1054616:1054729 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [send] via NET/IB/5
lrdn0824:1054616:1054729 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [send] via NET/IB/6
lrdn0824:1054616:1054729 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [send] via NET/IB/5
lrdn1138:684929:685043 [0] NCCL INFO Channel 01/0 : 10[0] -> 11[0] [receive] via NET/IB/6
lrdn1138:684929:685043 [0] NCCL INFO Channel 02/0 : 10[0] -> 11[0] [receive] via NET/IB/5
lrdn1138:684929:685043 [0] NCCL INFO Channel 03/0 : 10[0] -> 11[0] [receive] via NET/IB/6
lrdn1138:684929:685043 [0] NCCL INFO Channel 00/0 : 11[0] -> 12[0] [send] via NET/IB/5
lrdn0824:1054616:1054729 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [send] via NET/IB/6
lrdn1124:1626469:1626586 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[0] [receive] via NET/IB/5
lrdn1124:1626469:1626586 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[0] [receive] via NET/IB/6
lrdn0745:1658235:1658351 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 13
lrdn0745:1658235:1658350 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn0745:1658235:1658350 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn0745:1658235:1658350 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn0745:1658235:1658350 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn0745:1658235:1658350 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn0745:1658235:1658350 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn1138:684929:685043 [0] NCCL INFO Channel 01/0 : 11[0] -> 12[0] [send] via NET/IB/6
lrdn1138:684929:685043 [0] NCCL INFO Channel 02/0 : 11[0] -> 12[0] [send] via NET/IB/5
lrdn1124:1626469:1626587 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 8
lrdn1124:1626469:1626586 [0] NCCL INFO Channel 02/0 : 9[0] -> 10[0] [receive] via NET/IB/5
lrdn0708:3048046:3048166 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 2
lrdn0745:1658235:1658350 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn0745:1658235:1658350 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn1138:684929:685043 [0] NCCL INFO Channel 03/0 : 11[0] -> 12[0] [send] via NET/IB/6
lrdn0836:1477025:1477141 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 10
lrdn0836:1477025:1477140 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [receive] via NET/IB/5
lrdn0836:1477025:1477140 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [receive] via NET/IB/6
lrdn0836:1477025:1477140 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [receive] via NET/IB/5
lrdn1124:1626469:1626586 [0] NCCL INFO Channel 03/0 : 9[0] -> 10[0] [receive] via NET/IB/6
lrdn0708:3048046:3048165 [0] NCCL INFO Channel 00/0 : 15[0] -> 0[0] [receive] via NET/IB/5
lrdn0708:3048046:3048165 [0] NCCL INFO Channel 01/0 : 15[0] -> 0[0] [receive] via NET/IB/6
lrdn0836:1477025:1477140 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [receive] via NET/IB/6
lrdn0836:1477025:1477140 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [send] via NET/IB/5
lrdn1124:1626469:1626586 [0] NCCL INFO Channel 00/0 : 10[0] -> 11[0] [send] via NET/IB/5
lrdn0853:1717580:1717697 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [receive] via NET/IB/5
lrdn0853:1717580:1717698 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 8
lrdn0853:1717580:1717697 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [receive] via NET/IB/6
lrdn0708:3048046:3048165 [0] NCCL INFO Channel 02/0 : 15[0] -> 0[0] [receive] via NET/IB/5
lrdn0836:1477025:1477140 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [send] via NET/IB/6
lrdn1124:1626469:1626586 [0] NCCL INFO Channel 01/0 : 10[0] -> 11[0] [send] via NET/IB/6
lrdn0836:1477025:1477140 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [send] via NET/IB/5
lrdn0836:1477025:1477140 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [send] via NET/IB/6
lrdn1124:1626469:1626586 [0] NCCL INFO Channel 02/0 : 10[0] -> 11[0] [send] via NET/IB/5
lrdn1124:1626469:1626586 [0] NCCL INFO Channel 03/0 : 10[0] -> 11[0] [send] via NET/IB/6
lrdn0780:1321542:1321655 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 9
lrdn0805:1354481:1354597 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [receive] via NET/IB/5
lrdn0805:1354481:1354597 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [receive] via NET/IB/6
lrdn0805:1354481:1354598 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 8
lrdn0805:1354481:1354597 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [receive] via NET/IB/5
lrdn0805:1354481:1354597 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [receive] via NET/IB/6
lrdn0805:1354481:1354597 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [send] via NET/IB/5
lrdn0780:1321542:1321654 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn0805:1354481:1354597 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [send] via NET/IB/6
lrdn0805:1354481:1354597 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [send] via NET/IB/5
lrdn0780:1321542:1321654 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn0805:1354481:1354597 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [send] via NET/IB/6
lrdn0780:1321542:1321654 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn0780:1321542:1321654 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn0853:1717580:1717697 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [receive] via NET/IB/5
lrdn0853:1717580:1717697 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [receive] via NET/IB/6
lrdn0853:1717580:1717697 [0] NCCL INFO Channel 00/0 : 7[0] -> 8[0] [send] via NET/IB/5
lrdn0853:1717580:1717697 [0] NCCL INFO Channel 01/0 : 7[0] -> 8[0] [send] via NET/IB/6
lrdn0757:1521334:1521447 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn0757:1521334:1521448 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 9
lrdn0780:1321542:1321654 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [send] via NET/IB/5
lrdn0780:1321542:1321654 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [send] via NET/IB/6
lrdn0853:1717580:1717697 [0] NCCL INFO Channel 02/0 : 7[0] -> 8[0] [send] via NET/IB/5
lrdn0853:1717580:1717697 [0] NCCL INFO Channel 03/0 : 7[0] -> 8[0] [send] via NET/IB/6
lrdn1028:895822:895940 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 7
lrdn0780:1321542:1321654 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [send] via NET/IB/5
lrdn0780:1321542:1321654 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [send] via NET/IB/6
lrdn0757:1521334:1521447 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn0757:1521334:1521447 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn0757:1521334:1521447 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn1028:895822:895939 [0] NCCL INFO Channel 00/0 : 7[0] -> 8[0] [receive] via NET/IB/5
lrdn1028:895822:895939 [0] NCCL INFO Channel 01/0 : 7[0] -> 8[0] [receive] via NET/IB/6
lrdn0757:1521334:1521447 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn0757:1521334:1521447 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn0757:1521334:1521447 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn1028:895822:895939 [0] NCCL INFO Channel 02/0 : 7[0] -> 8[0] [receive] via NET/IB/5
lrdn0757:1521334:1521447 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn1028:895822:895939 [0] NCCL INFO Channel 03/0 : 7[0] -> 8[0] [receive] via NET/IB/6
lrdn1028:895822:895939 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[0] [send] via NET/IB/5
lrdn1028:895822:895939 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[0] [send] via NET/IB/6
lrdn1028:895822:895939 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[0] [send] via NET/IB/5
lrdn1028:895822:895939 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[0] [send] via NET/IB/6
lrdn0708:3048046:3048165 [0] NCCL INFO Channel 03/0 : 15[0] -> 0[0] [receive] via NET/IB/6
lrdn0708:3048046:3048165 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn0708:3048046:3048165 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn0708:3048046:3048165 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn0708:3048046:3048165 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1028:895822:895939 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1093:1492163:1492276 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0853:1717580:1717697 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1124:1626469:1626586 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1206:1547777:1547890 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1218:1776591:1776710 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1138:684929:685043 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0836:1477025:1477140 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0824:1054616:1054729 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0757:1521334:1521447 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1139:2620503:2620622 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0745:1658235:1658350 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0780:1321542:1321654 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1226:1654681:1654793 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0805:1354481:1354597 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0708:3048046:3048165 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0708:3048046:3048046 [0] NCCL INFO Comm config Blocking set to 1
lrdn1206:1547777:1547777 [0] NCCL INFO Comm config Blocking set to 1
lrdn0757:1521334:1521334 [0] NCCL INFO Comm config Blocking set to 1
lrdn0757:1521334:1521449 [0] NCCL INFO Using network IB
lrdn0708:3048046:3048168 [0] NCCL INFO Using network IB
lrdn1093:1492163:1492163 [0] NCCL INFO Comm config Blocking set to 1
lrdn1028:895822:895822 [0] NCCL INFO Comm config Blocking set to 1
lrdn0757:1521334:1521449 [0] NCCL INFO ncclCommInitRankConfig comm 0xd3350a0 rank 2 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init START
lrdn0836:1477025:1477025 [0] NCCL INFO Comm config Blocking set to 1
lrdn0745:1658235:1658235 [0] NCCL INFO Comm config Blocking set to 1
lrdn1124:1626469:1626469 [0] NCCL INFO Comm config Blocking set to 1
lrdn0708:3048046:3048168 [0] NCCL INFO ncclCommInitRankConfig comm 0x2c143750 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init START
lrdn0853:1717580:1717580 [0] NCCL INFO Comm config Blocking set to 1
lrdn1218:1776591:1776591 [0] NCCL INFO Comm config Blocking set to 1
lrdn0780:1321542:1321542 [0] NCCL INFO Comm config Blocking set to 1
lrdn1226:1654681:1654681 [0] NCCL INFO Comm config Blocking set to 1
lrdn0824:1054616:1054616 [0] NCCL INFO Comm config Blocking set to 1
lrdn1138:684929:684929 [0] NCCL INFO Comm config Blocking set to 1
lrdn1139:2620503:2620503 [0] NCCL INFO Comm config Blocking set to 1
lrdn1139:2620503:2620624 [0] NCCL INFO Using network IB
lrdn1139:2620503:2620624 [0] NCCL INFO ncclCommInitRankConfig comm 0x2db0a550 rank 12 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init START
lrdn1206:1547777:1547892 [0] NCCL INFO Using network IB
lrdn1206:1547777:1547892 [0] NCCL INFO ncclCommInitRankConfig comm 0xe4dce50 rank 13 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init START
lrdn0805:1354481:1354481 [0] NCCL INFO Comm config Blocking set to 1
lrdn1028:895822:895941 [0] NCCL INFO Using network IB
lrdn1028:895822:895941 [0] NCCL INFO ncclCommInitRankConfig comm 0x2b46c960 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init START
lrdn1218:1776591:1776712 [0] NCCL INFO Using network IB
lrdn1218:1776591:1776712 [0] NCCL INFO ncclCommInitRankConfig comm 0xe23a600 rank 14 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init START
lrdn1138:684929:685045 [0] NCCL INFO Using network IB
lrdn0836:1477025:1477142 [0] NCCL INFO Using network IB
lrdn0836:1477025:1477142 [0] NCCL INFO ncclCommInitRankConfig comm 0x11e17600 rank 6 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init START
lrdn1093:1492163:1492278 [0] NCCL INFO Using network IB
lrdn0805:1354481:1354599 [0] NCCL INFO Using network IB
lrdn0780:1321542:1321656 [0] NCCL INFO Using network IB
lrdn0780:1321542:1321656 [0] NCCL INFO ncclCommInitRankConfig comm 0x2bbced80 rank 3 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init START
lrdn0853:1717580:1717699 [0] NCCL INFO Using network IB
lrdn1226:1654681:1654795 [0] NCCL INFO Using network IB
lrdn1226:1654681:1654795 [0] NCCL INFO ncclCommInitRankConfig comm 0x11468980 rank 15 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init START
lrdn0824:1054616:1054731 [0] NCCL INFO Using network IB
lrdn1138:684929:685045 [0] NCCL INFO ncclCommInitRankConfig comm 0x21e9c5b0 rank 11 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init START
lrdn0745:1658235:1658352 [0] NCCL INFO Using network IB
lrdn0745:1658235:1658352 [0] NCCL INFO ncclCommInitRankConfig comm 0xcab3ee0 rank 1 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init START
lrdn1124:1626469:1626588 [0] NCCL INFO Using network IB
lrdn1124:1626469:1626588 [0] NCCL INFO ncclCommInitRankConfig comm 0x118f90f0 rank 10 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init START
lrdn0853:1717580:1717699 [0] NCCL INFO ncclCommInitRankConfig comm 0x1203a530 rank 7 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init START
lrdn0824:1054616:1054731 [0] NCCL INFO ncclCommInitRankConfig comm 0x2a62e550 rank 5 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init START
lrdn0805:1354481:1354599 [0] NCCL INFO ncclCommInitRankConfig comm 0x2e109ae0 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init START
lrdn1093:1492163:1492278 [0] NCCL INFO ncclCommInitRankConfig comm 0x12f439c0 rank 9 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init START
lrdn0805:1354481:1354599 [0] NCCL INFO Bootstrap timings total 0.001602 (create 0.000018, send 0.000060, recv 0.000436, ring 0.000945, delay 0.000000)
lrdn0824:1054616:1054731 [0] NCCL INFO Bootstrap timings total 0.001649 (create 0.000017, send 0.000060, recv 0.000385, ring 0.000930, delay 0.000000)
lrdn0836:1477025:1477142 [0] NCCL INFO Bootstrap timings total 0.001759 (create 0.000017, send 0.000077, recv 0.000684, ring 0.000846, delay 0.000000)
lrdn0780:1321542:1321656 [0] NCCL INFO Bootstrap timings total 0.001788 (create 0.000014, send 0.000050, recv 0.000547, ring 0.001091, delay 0.000000)
lrdn0757:1521334:1521449 [0] NCCL INFO Bootstrap timings total 0.002034 (create 0.000014, send 0.000057, recv 0.000553, ring 0.001308, delay 0.000000)
lrdn0853:1717580:1717699 [0] NCCL INFO Bootstrap timings total 0.001665 (create 0.000013, send 0.000060, recv 0.000613, ring 0.000861, delay 0.000000)
lrdn0708:3048046:3048168 [0] NCCL INFO Bootstrap timings total 0.002061 (create 0.000012, send 0.000046, recv 0.000348, ring 0.001296, delay 0.000000)
lrdn0745:1658235:1658352 [0] NCCL INFO Bootstrap timings total 0.001843 (create 0.000019, send 0.000053, recv 0.000256, ring 0.001393, delay 0.000000)
lrdn1093:1492163:1492278 [0] NCCL INFO Bootstrap timings total 0.001760 (create 0.000022, send 0.000083, recv 0.000747, ring 0.000648, delay 0.000000)
lrdn1138:684929:685045 [0] NCCL INFO Bootstrap timings total 0.001839 (create 0.000016, send 0.000085, recv 0.000605, ring 0.000912, delay 0.000000)
lrdn1226:1654681:1654795 [0] NCCL INFO Bootstrap timings total 0.001895 (create 0.000017, send 0.000071, recv 0.000366, ring 0.001350, delay 0.000000)
lrdn1139:2620503:2620624 [0] NCCL INFO Bootstrap timings total 0.002048 (create 0.000016, send 0.000074, recv 0.000123, ring 0.000877, delay 0.000000)
lrdn1124:1626469:1626588 [0] NCCL INFO Bootstrap timings total 0.001914 (create 0.000044, send 0.000072, recv 0.000592, ring 0.000765, delay 0.000000)
lrdn1206:1547777:1547892 [0] NCCL INFO Bootstrap timings total 0.002098 (create 0.000018, send 0.000073, recv 0.000290, ring 0.001583, delay 0.000000)
lrdn1218:1776591:1776712 [0] NCCL INFO Bootstrap timings total 0.001955 (create 0.000016, send 0.000085, recv 0.000355, ring 0.001401, delay 0.000000)
lrdn1028:895822:895941 [0] NCCL INFO Bootstrap timings total 0.001856 (create 0.000022, send 0.000055, recv 0.000827, ring 0.000716, delay 0.000000)
lrdn1093:1492163:1492278 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0853:1717580:1717699 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0824:1054616:1054731 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1206:1547777:1547892 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0745:1658235:1658352 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0780:1321542:1321656 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1226:1654681:1654795 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1028:895822:895941 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0805:1354481:1354599 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0836:1477025:1477142 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1138:684929:685045 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0757:1521334:1521449 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1218:1776591:1776712 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1139:2620503:2620624 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0708:3048046:3048168 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1124:1626469:1626588 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1093:1492163:1492278 [0] NCCL INFO comm 0x12f439c0 rank 9 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1093:1492163:1492278 [0] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] -1/-1/-1->9->10 [2] 10/8/-1->9->11 [3] 10/8/-1->9->11
lrdn1093:1492163:1492278 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1028:895822:895941 [0] NCCL INFO comm 0x2b46c960 rank 8 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1124:1626469:1626588 [0] NCCL INFO comm 0x118f90f0 rank 10 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1124:1626469:1626588 [0] NCCL INFO Trees [0] 9/11/-1->10->12 [1] 9/11/-1->10->12 [2] -1/-1/-1->10->9 [3] -1/-1/-1->10->9
lrdn1124:1626469:1626588 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1028:895822:895941 [0] NCCL INFO Trees [0] 4/12/-1->8->0 [1] 4/12/-1->8->0 [2] -1/-1/-1->8->9 [3] -1/-1/-1->8->9
lrdn1028:895822:895941 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1139:2620503:2620624 [0] NCCL INFO comm 0x2db0a550 rank 12 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1139:2620503:2620624 [0] NCCL INFO Trees [0] 10/14/-1->12->8 [1] 10/14/-1->12->8 [2] -1/-1/-1->12->13 [3] -1/-1/-1->12->13
lrdn1139:2620503:2620624 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1206:1547777:1547892 [0] NCCL INFO comm 0xe4dce50 rank 13 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1093:1492163:1492279 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn1206:1547777:1547892 [0] NCCL INFO Trees [0] -1/-1/-1->13->14 [1] -1/-1/-1->13->14 [2] 14/12/-1->13->11 [3] 14/12/-1->13->11
lrdn1206:1547777:1547892 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0853:1717580:1717699 [0] NCCL INFO comm 0x1203a530 rank 7 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0853:1717580:1717699 [0] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 11/3/-1->7->15 [3] 11/3/-1->7->15
lrdn0853:1717580:1717699 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0824:1054616:1054731 [0] NCCL INFO comm 0x2a62e550 rank 5 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0824:1054616:1054731 [0] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] -1/-1/-1->5->6 [2] 6/4/-1->5->3 [3] 6/4/-1->5->3
lrdn0824:1054616:1054731 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0780:1321542:1321656 [0] NCCL INFO comm 0x2bbced80 rank 3 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0780:1321542:1321656 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 5/1/-1->3->7 [3] 5/1/-1->3->7
lrdn0780:1321542:1321656 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0757:1521334:1521449 [0] NCCL INFO comm 0xd3350a0 rank 2 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0757:1521334:1521449 [0] NCCL INFO Trees [0] 1/3/-1->2->4 [1] 1/3/-1->2->4 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1
lrdn0757:1521334:1521449 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0805:1354481:1354599 [0] NCCL INFO comm 0x2e109ae0 rank 4 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0805:1354481:1354599 [0] NCCL INFO Trees [0] 2/6/-1->4->8 [1] 2/6/-1->4->8 [2] -1/-1/-1->4->5 [3] -1/-1/-1->4->5
lrdn0805:1354481:1354599 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1028:895822:895943 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn1124:1626469:1626589 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1139:2620503:2620625 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn1226:1654681:1654795 [0] NCCL INFO comm 0x11468980 rank 15 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1206:1547777:1547894 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn0745:1658235:1658352 [0] NCCL INFO comm 0xcab3ee0 rank 1 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0745:1658235:1658352 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] -1/-1/-1->1->2 [2] 2/0/-1->1->3 [3] 2/0/-1->1->3
lrdn0745:1658235:1658352 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0708:3048046:3048168 [0] NCCL INFO comm 0x2c143750 rank 0 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0708:3048046:3048168 [0] NCCL INFO Channel 00/04 :  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
lrdn0708:3048046:3048168 [0] NCCL INFO Channel 01/04 :  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
lrdn0708:3048046:3048168 [0] NCCL INFO Channel 02/04 :  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
lrdn1138:684929:685045 [0] NCCL INFO comm 0x21e9c5b0 rank 11 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1138:684929:685045 [0] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10 [2] 13/9/-1->11->7 [3] 13/9/-1->11->7
lrdn1138:684929:685045 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1138:684929:685046 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1138:684929:685047 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn0780:1321542:1321657 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn1226:1654681:1654795 [0] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14 [2] 7/-1/-1->15->-1 [3] 7/-1/-1->15->-1
lrdn1226:1654681:1654795 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1028:895822:895942 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0805:1354481:1354600 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn1218:1776591:1776712 [0] NCCL INFO comm 0xe23a600 rank 14 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn1218:1776591:1776712 [0] NCCL INFO Trees [0] 13/15/-1->14->12 [1] 13/15/-1->14->12 [2] -1/-1/-1->14->13 [3] -1/-1/-1->14->13
lrdn1218:1776591:1776712 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0853:1717580:1717701 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn0824:1054616:1054732 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn0824:1054616:1054733 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn0757:1521334:1521451 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn0708:3048046:3048168 [0] NCCL INFO Channel 03/04 :  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15
lrdn0708:3048046:3048168 [0] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] 8/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
lrdn0708:3048046:3048168 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0708:3048046:3048168 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0708:3048046:3048169 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn0745:1658235:1658353 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn1093:1492163:1492280 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn1139:2620503:2620626 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn1226:1654681:1654796 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0805:1354481:1354601 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn1206:1547777:1547893 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn0836:1477025:1477142 [0] NCCL INFO comm 0x11e17600 rank 6 nRanks 16 nNodes 16 localRanks 1 localRank 0 MNNVL 0
lrdn0836:1477025:1477142 [0] NCCL INFO Trees [0] 5/7/-1->6->4 [1] 5/7/-1->6->4 [2] -1/-1/-1->6->5 [3] -1/-1/-1->6->5
lrdn0836:1477025:1477142 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0836:1477025:1477143 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn0708:3048046:3048170 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0745:1658235:1658354 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn0780:1321542:1321658 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn1218:1776591:1776713 [0] NCCL INFO [Proxy Service] Device 0 CPU core 8
lrdn0853:1717580:1717700 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn1124:1626469:1626590 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1226:1654681:1654797 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0836:1477025:1477144 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0757:1521334:1521450 [0] NCCL INFO [Proxy Service] Device 0 CPU core 8
lrdn1218:1776591:1776714 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn1139:2620503:2620624 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1139:2620503:2620624 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1139:2620503:2620624 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1139:2620503:2620624 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1138:684929:685045 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1138:684929:685045 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1138:684929:685045 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1138:684929:685045 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1226:1654681:1654795 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1226:1654681:1654795 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1139:2620503:2620624 [0] NCCL INFO ncclCommInitRankConfig comm 0x2db0a550 rank 12 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init COMPLETE
lrdn1226:1654681:1654795 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1226:1654681:1654795 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0757:1521334:1521449 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0757:1521334:1521449 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1139:2620503:2620624 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 12 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1124:1626469:1626588 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1124:1626469:1626588 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0757:1521334:1521449 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0757:1521334:1521449 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1206:1547777:1547892 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1206:1547777:1547892 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1124:1626469:1626588 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1124:1626469:1626588 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1206:1547777:1547892 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1206:1547777:1547892 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0708:3048046:3048168 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0708:3048046:3048168 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0708:3048046:3048168 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             0              1              0              0              0              0              0  
       Reduce |        0         0         1   |             0              1              0              0              0              0              0  
    AllGather |        0         0         1   |             0              1              0              0              0              0              0  
ReduceScatter |        0         0         1   |             0              1              0              0              0              0              0  
    AllReduce |        0         0         1   |             0              1              0              0              0              0              0  

lrdn0708:3048046:3048168 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0708:3048046:3048168 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1138:684929:685045 [0] NCCL INFO ncclCommInitRankConfig comm 0x21e9c5b0 rank 11 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init COMPLETE
lrdn1138:684929:685045 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 11 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1218:1776591:1776712 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1218:1776591:1776712 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1218:1776591:1776712 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1218:1776591:1776712 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1226:1654681:1654795 [0] NCCL INFO ncclCommInitRankConfig comm 0x11468980 rank 15 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init COMPLETE
lrdn1226:1654681:1654795 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 15 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0757:1521334:1521449 [0] NCCL INFO ncclCommInitRankConfig comm 0xd3350a0 rank 2 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init COMPLETE
lrdn0757:1521334:1521449 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0836:1477025:1477142 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0836:1477025:1477142 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0836:1477025:1477142 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0836:1477025:1477142 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1124:1626469:1626588 [0] NCCL INFO ncclCommInitRankConfig comm 0x118f90f0 rank 10 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init COMPLETE
lrdn1124:1626469:1626588 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 10 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0708:3048046:3048168 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0824:1054616:1054731 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0824:1054616:1054731 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0824:1054616:1054731 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0824:1054616:1054731 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1206:1547777:1547892 [0] NCCL INFO ncclCommInitRankConfig comm 0xe4dce50 rank 13 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init COMPLETE
lrdn1206:1547777:1547892 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 13 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0805:1354481:1354599 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0805:1354481:1354599 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0805:1354481:1354599 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0805:1354481:1354599 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1093:1492163:1492278 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1093:1492163:1492278 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1093:1492163:1492278 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1093:1492163:1492278 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0708:3048046:3048168 [0] NCCL INFO ncclCommInitRankConfig comm 0x2c143750 rank 0 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init COMPLETE
lrdn0708:3048046:3048168 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1218:1776591:1776712 [0] NCCL INFO ncclCommInitRankConfig comm 0xe23a600 rank 14 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init COMPLETE
lrdn1218:1776591:1776712 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 14 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0836:1477025:1477142 [0] NCCL INFO ncclCommInitRankConfig comm 0x11e17600 rank 6 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init COMPLETE
lrdn0836:1477025:1477142 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 6 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0824:1054616:1054731 [0] NCCL INFO ncclCommInitRankConfig comm 0x2a62e550 rank 5 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init COMPLETE
lrdn0824:1054616:1054731 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 5 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0780:1321542:1321656 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0780:1321542:1321656 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0780:1321542:1321656 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0780:1321542:1321656 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn1093:1492163:1492278 [0] NCCL INFO ncclCommInitRankConfig comm 0x12f439c0 rank 9 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init COMPLETE
lrdn1093:1492163:1492278 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 9 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0805:1354481:1354599 [0] NCCL INFO ncclCommInitRankConfig comm 0x2e109ae0 rank 4 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init COMPLETE
lrdn0853:1717580:1717699 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0853:1717580:1717699 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0853:1717580:1717699 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0853:1717580:1717699 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0805:1354481:1354599 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 4 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1028:895822:895941 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1028:895822:895941 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1028:895822:895941 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn1028:895822:895941 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0853:1717580:1717699 [0] NCCL INFO ncclCommInitRankConfig comm 0x1203a530 rank 7 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init COMPLETE
lrdn0853:1717580:1717699 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 7 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0780:1321542:1321656 [0] NCCL INFO ncclCommInitRankConfig comm 0x2bbced80 rank 3 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init COMPLETE
lrdn0780:1321542:1321656 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1028:895822:895941 [0] NCCL INFO ncclCommInitRankConfig comm 0x2b46c960 rank 8 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init COMPLETE
lrdn1028:895822:895941 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 8 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0745:1658235:1658352 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0745:1658235:1658352 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0745:1658235:1658352 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 512 | 512
lrdn0745:1658235:1658352 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
lrdn0745:1658235:1658352 [0] NCCL INFO ncclCommInitRankConfig comm 0xcab3ee0 rank 1 nranks 16 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x66c772a5f8c70fb9 - Init COMPLETE
lrdn0745:1658235:1658352 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 16 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.01, rest 0.00)
lrdn0757:1521334:1521452 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn0757:1521334:1521452 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn0757:1521334:1521453 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 11
lrdn0757:1521334:1521452 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn0757:1521334:1521452 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn0757:1521334:1521452 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn0757:1521334:1521452 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn0757:1521334:1521452 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn0757:1521334:1521452 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn1226:1654681:1654798 [0] NCCL INFO Channel 00/0 : 14[0] -> 15[0] [receive] via NET/IB/5
lrdn1226:1654681:1654799 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 12
lrdn1124:1626469:1626591 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[0] [receive] via NET/IB/5
lrdn1139:2620503:2620628 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 13
lrdn1139:2620503:2620627 [0] NCCL INFO Channel 00/0 : 11[0] -> 12[0] [receive] via NET/IB/5
lrdn1124:1626469:1626592 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 10
lrdn1124:1626469:1626591 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[0] [receive] via NET/IB/6
lrdn1226:1654681:1654798 [0] NCCL INFO Channel 01/0 : 14[0] -> 15[0] [receive] via NET/IB/6
lrdn1124:1626469:1626591 [0] NCCL INFO Channel 02/0 : 9[0] -> 10[0] [receive] via NET/IB/5
lrdn1226:1654681:1654798 [0] NCCL INFO Channel 02/0 : 14[0] -> 15[0] [receive] via NET/IB/5
lrdn1124:1626469:1626591 [0] NCCL INFO Channel 03/0 : 9[0] -> 10[0] [receive] via NET/IB/6
lrdn1124:1626469:1626591 [0] NCCL INFO Channel 00/0 : 10[0] -> 11[0] [send] via NET/IB/5
lrdn1226:1654681:1654798 [0] NCCL INFO Channel 03/0 : 14[0] -> 15[0] [receive] via NET/IB/6
lrdn1138:684929:685048 [0] NCCL INFO Channel 00/0 : 10[0] -> 11[0] [receive] via NET/IB/5
lrdn1138:684929:685049 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 10
lrdn1138:684929:685048 [0] NCCL INFO Channel 01/0 : 10[0] -> 11[0] [receive] via NET/IB/6
lrdn1138:684929:685048 [0] NCCL INFO Channel 02/0 : 10[0] -> 11[0] [receive] via NET/IB/5
lrdn1124:1626469:1626591 [0] NCCL INFO Channel 01/0 : 10[0] -> 11[0] [send] via NET/IB/6
lrdn1226:1654681:1654798 [0] NCCL INFO Channel 00/0 : 15[0] -> 0[0] [send] via NET/IB/5
lrdn1226:1654681:1654798 [0] NCCL INFO Channel 01/0 : 15[0] -> 0[0] [send] via NET/IB/6
lrdn1124:1626469:1626591 [0] NCCL INFO Channel 02/0 : 10[0] -> 11[0] [send] via NET/IB/5
lrdn1124:1626469:1626591 [0] NCCL INFO Channel 03/0 : 10[0] -> 11[0] [send] via NET/IB/6
lrdn1139:2620503:2620627 [0] NCCL INFO Channel 01/0 : 11[0] -> 12[0] [receive] via NET/IB/6
lrdn1226:1654681:1654798 [0] NCCL INFO Channel 02/0 : 15[0] -> 0[0] [send] via NET/IB/5
lrdn1138:684929:685048 [0] NCCL INFO Channel 03/0 : 10[0] -> 11[0] [receive] via NET/IB/6
lrdn1138:684929:685048 [0] NCCL INFO Channel 00/0 : 11[0] -> 12[0] [send] via NET/IB/5
lrdn1138:684929:685048 [0] NCCL INFO Channel 01/0 : 11[0] -> 12[0] [send] via NET/IB/6
lrdn1139:2620503:2620627 [0] NCCL INFO Channel 02/0 : 11[0] -> 12[0] [receive] via NET/IB/5
lrdn1226:1654681:1654798 [0] NCCL INFO Channel 03/0 : 15[0] -> 0[0] [send] via NET/IB/6
lrdn1139:2620503:2620627 [0] NCCL INFO Channel 03/0 : 11[0] -> 12[0] [receive] via NET/IB/6
lrdn1138:684929:685048 [0] NCCL INFO Channel 02/0 : 11[0] -> 12[0] [send] via NET/IB/5
lrdn1138:684929:685048 [0] NCCL INFO Channel 03/0 : 11[0] -> 12[0] [send] via NET/IB/6
lrdn1139:2620503:2620627 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[0] [send] via NET/IB/5
lrdn1139:2620503:2620627 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[0] [send] via NET/IB/6
lrdn1139:2620503:2620627 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[0] [send] via NET/IB/5
lrdn1139:2620503:2620627 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[0] [send] via NET/IB/6
lrdn0805:1354481:1354603 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 10
lrdn0805:1354481:1354602 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [receive] via NET/IB/5
lrdn0708:3048046:3048172 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 7
lrdn0708:3048046:3048171 [0] NCCL INFO Channel 00/0 : 15[0] -> 0[0] [receive] via NET/IB/5
lrdn0805:1354481:1354602 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [receive] via NET/IB/6
lrdn0805:1354481:1354602 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [receive] via NET/IB/5
lrdn0824:1054616:1054735 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 12
lrdn0824:1054616:1054734 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [receive] via NET/IB/5
lrdn0708:3048046:3048171 [0] NCCL INFO Channel 01/0 : 15[0] -> 0[0] [receive] via NET/IB/6
lrdn0824:1054616:1054734 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [receive] via NET/IB/6
lrdn0708:3048046:3048171 [0] NCCL INFO Channel 02/0 : 15[0] -> 0[0] [receive] via NET/IB/5
lrdn0805:1354481:1354602 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [receive] via NET/IB/6
lrdn0805:1354481:1354602 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [send] via NET/IB/5
lrdn0824:1054616:1054734 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [receive] via NET/IB/5
lrdn0708:3048046:3048171 [0] NCCL INFO Channel 03/0 : 15[0] -> 0[0] [receive] via NET/IB/6
lrdn0805:1354481:1354602 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [send] via NET/IB/6
lrdn0708:3048046:3048171 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn0824:1054616:1054734 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [receive] via NET/IB/6
lrdn0805:1354481:1354602 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [send] via NET/IB/5
lrdn0708:3048046:3048171 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn0824:1054616:1054734 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [send] via NET/IB/5
lrdn0805:1354481:1354602 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [send] via NET/IB/6
lrdn0708:3048046:3048171 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn0824:1054616:1054734 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [send] via NET/IB/6
lrdn0708:3048046:3048171 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn0836:1477025:1477145 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [receive] via NET/IB/5
lrdn0824:1054616:1054734 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [send] via NET/IB/5
lrdn0780:1321542:1321660 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 12
lrdn0824:1054616:1054734 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [send] via NET/IB/6
lrdn1206:1547777:1547896 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 11
lrdn1206:1547777:1547895 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[0] [receive] via NET/IB/5
lrdn1206:1547777:1547895 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[0] [receive] via NET/IB/6
lrdn1206:1547777:1547895 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[0] [receive] via NET/IB/5
lrdn1206:1547777:1547895 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[0] [receive] via NET/IB/6
lrdn1206:1547777:1547895 [0] NCCL INFO Channel 00/0 : 13[0] -> 14[0] [send] via NET/IB/5
lrdn0836:1477025:1477145 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [receive] via NET/IB/6
lrdn0836:1477025:1477146 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 14
lrdn0836:1477025:1477145 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [receive] via NET/IB/5
lrdn0836:1477025:1477145 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [receive] via NET/IB/6
lrdn0836:1477025:1477145 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [send] via NET/IB/5
lrdn0780:1321542:1321659 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn0780:1321542:1321659 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn0780:1321542:1321659 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn1206:1547777:1547895 [0] NCCL INFO Channel 01/0 : 13[0] -> 14[0] [send] via NET/IB/6
lrdn1206:1547777:1547895 [0] NCCL INFO Channel 02/0 : 13[0] -> 14[0] [send] via NET/IB/5
lrdn0780:1321542:1321659 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn0780:1321542:1321659 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [send] via NET/IB/5
lrdn1206:1547777:1547895 [0] NCCL INFO Channel 03/0 : 13[0] -> 14[0] [send] via NET/IB/6
lrdn0780:1321542:1321659 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [send] via NET/IB/6
lrdn0836:1477025:1477145 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [send] via NET/IB/6
lrdn0836:1477025:1477145 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [send] via NET/IB/5
lrdn0836:1477025:1477145 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [send] via NET/IB/6
lrdn0780:1321542:1321659 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [send] via NET/IB/5
lrdn0780:1321542:1321659 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [send] via NET/IB/6
lrdn1218:1776591:1776715 [0] NCCL INFO Channel 00/0 : 13[0] -> 14[0] [receive] via NET/IB/5
lrdn1218:1776591:1776716 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 12
lrdn1218:1776591:1776715 [0] NCCL INFO Channel 01/0 : 13[0] -> 14[0] [receive] via NET/IB/6
lrdn1218:1776591:1776715 [0] NCCL INFO Channel 02/0 : 13[0] -> 14[0] [receive] via NET/IB/5
lrdn1218:1776591:1776715 [0] NCCL INFO Channel 03/0 : 13[0] -> 14[0] [receive] via NET/IB/6
lrdn0853:1717580:1717702 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [receive] via NET/IB/5
lrdn0853:1717580:1717702 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [receive] via NET/IB/6
lrdn1218:1776591:1776715 [0] NCCL INFO Channel 00/0 : 14[0] -> 15[0] [send] via NET/IB/5
lrdn0853:1717580:1717703 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 10
lrdn0853:1717580:1717702 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [receive] via NET/IB/5
lrdn1218:1776591:1776715 [0] NCCL INFO Channel 01/0 : 14[0] -> 15[0] [send] via NET/IB/6
lrdn0853:1717580:1717702 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [receive] via NET/IB/6
lrdn1218:1776591:1776715 [0] NCCL INFO Channel 02/0 : 14[0] -> 15[0] [send] via NET/IB/5
lrdn0853:1717580:1717702 [0] NCCL INFO Channel 00/0 : 7[0] -> 8[0] [send] via NET/IB/5
lrdn0853:1717580:1717702 [0] NCCL INFO Channel 01/0 : 7[0] -> 8[0] [send] via NET/IB/6
lrdn1218:1776591:1776715 [0] NCCL INFO Channel 03/0 : 14[0] -> 15[0] [send] via NET/IB/6
lrdn0853:1717580:1717702 [0] NCCL INFO Channel 02/0 : 7[0] -> 8[0] [send] via NET/IB/5
lrdn1028:895822:895944 [0] NCCL INFO Channel 00/0 : 7[0] -> 8[0] [receive] via NET/IB/5
lrdn1093:1492163:1492281 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[0] [receive] via NET/IB/5
lrdn1093:1492163:1492281 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[0] [receive] via NET/IB/6
lrdn1093:1492163:1492282 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 11
lrdn1093:1492163:1492281 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[0] [receive] via NET/IB/5
lrdn0853:1717580:1717702 [0] NCCL INFO Channel 03/0 : 7[0] -> 8[0] [send] via NET/IB/6
lrdn1028:895822:895944 [0] NCCL INFO Channel 01/0 : 7[0] -> 8[0] [receive] via NET/IB/6
lrdn1093:1492163:1492281 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[0] [receive] via NET/IB/6
lrdn1093:1492163:1492281 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[0] [send] via NET/IB/5
lrdn1093:1492163:1492281 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[0] [send] via NET/IB/6
lrdn1028:895822:895945 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 9
lrdn1028:895822:895944 [0] NCCL INFO Channel 02/0 : 7[0] -> 8[0] [receive] via NET/IB/5
lrdn1028:895822:895944 [0] NCCL INFO Channel 03/0 : 7[0] -> 8[0] [receive] via NET/IB/6
lrdn1093:1492163:1492281 [0] NCCL INFO Channel 02/0 : 9[0] -> 10[0] [send] via NET/IB/5
lrdn1093:1492163:1492281 [0] NCCL INFO Channel 03/0 : 9[0] -> 10[0] [send] via NET/IB/6
lrdn1028:895822:895944 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[0] [send] via NET/IB/5
lrdn1028:895822:895944 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[0] [send] via NET/IB/6
lrdn1028:895822:895944 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[0] [send] via NET/IB/5
lrdn1028:895822:895944 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[0] [send] via NET/IB/6
lrdn0745:1658235:1658356 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 0
lrdn0745:1658235:1658355 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn0745:1658235:1658355 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn0745:1658235:1658355 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn0745:1658235:1658355 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn0745:1658235:1658355 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn0745:1658235:1658355 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn0745:1658235:1658355 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn0745:1658235:1658355 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn1218:1776591:1776715 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1226:1654681:1654798 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0745:1658235:1658355 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0757:1521334:1521452 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0708:3048046:3048171 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0853:1717580:1717702 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0780:1321542:1321659 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1028:895822:895944 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0805:1354481:1354602 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0836:1477025:1477145 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0824:1054616:1054734 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1093:1492163:1492281 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1124:1626469:1626591 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1138:684929:685048 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1206:1547777:1547895 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1139:2620503:2620627 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
[38;20m2025-08-02 00:13:40,517 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 114.73/115.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:13:57,485 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 114.42/113.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:14:15,849 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 103.32/106.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:14:33,873 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 106.37/107.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:14:50,513 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 115.97/116.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:15:07,437 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 113.85/113.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:15:25,349 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 108.46/108.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:15:43,327 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 107.87/107.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:16:00,029 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 116.23/114.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:16:17,132 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 112.31/113.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:16:34,999 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 108.58/107.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:16:53,024 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 106.09/108.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:17:09,898 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 114.44/113.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:17:26,713 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 114.06/115.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:17:44,296 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 115.46/105.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:18:02,064 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 109.28/108.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:18:18,949 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 115.32/112.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:18:35,760 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 114.88/113.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:18:53,780 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 104.76/107.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:19:11,704 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 107.83/106.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:19:28,626 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 113.01/113.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:19:45,689 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 113.43/111.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:20:03,689 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 106.57/108.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:20:21,678 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 108.48/106.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:20:38,485 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 114.04/115.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:20:55,101 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 116.05/116.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:21:12,943 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 107.91/107.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:21:30,899 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 105.83/108.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:21:47,820 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 113.52/112.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:22:04,706 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 113.20/115.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:22:22,757 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.30 (max/real adjusted throughput: 106.45/104.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:22:40,759 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 106.52/108.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:22:57,436 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 115.84/115.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:23:14,401 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 113.26/113.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:23:32,568 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.32 (max/real adjusted throughput: 106.03/103.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:23:50,628 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 106.03/107.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:24:07,787 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 112.00/112.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:24:24,695 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 112.32/115.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:24:42,809 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 104.64/107.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:25:00,794 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 107.44/105.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:25:17,764 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 113.23/114.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:25:34,497 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 114.93/115.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:25:52,604 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 104.90/107.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:26:10,744 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.29 (max/real adjusted throughput: 106.31/105.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:26:27,745 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 112.77/113.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:26:44,750 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 112.39/113.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:27:02,624 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 107.79/107.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:27:20,638 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 106.42/106.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:27:37,463 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 114.62/114.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:27:54,305 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 114.15/114.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:28:12,198 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 107.37/108.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:28:29,096 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 113.54/116.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:28:45,647 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 116.35/116.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:29:02,294 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 116.46/114.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:29:20,332 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.30 (max/real adjusted throughput: 108.59/104.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:29:38,190 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 107.66/107.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:29:55,144 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 114.14/112.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:30:12,252 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 112.13/112.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:30:30,520 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 104.68/106.06 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:30:48,454 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 106.93/107.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:31:05,463 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 112.18/113.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:31:22,167 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 114.84/116.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:31:40,208 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 106.05/107.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:31:58,132 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.26 (max/real adjusted throughput: 107.69/106.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:32:15,292 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 113.13/110.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:32:32,260 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 113.58/112.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:32:50,206 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 107.64/108.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:33:08,331 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.29 (max/real adjusted throughput: 108.67/105.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:33:25,303 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 114.12/113.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:33:42,147 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 114.52/115.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:34:00,042 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 107.38/108.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:34:18,191 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 107.20/105.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:34:35,213 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 113.24/113.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:34:52,370 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 112.81/112.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:35:10,290 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.26 (max/real adjusted throughput: 107.68/106.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:35:28,455 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 103.27/107.90 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:35:45,217 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 114.29/115.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:36:01,924 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 114.90/115.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:36:19,896 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 108.55/105.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:36:37,966 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 107.10/106.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:36:54,709 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 114.64/114.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:37:11,463 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 115.21/115.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:37:29,544 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 108.08/106.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:37:47,559 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 106.85/106.99 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:38:04,362 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 114.74/114.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:38:21,227 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 112.91/116.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:38:39,331 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.26 (max/real adjusted throughput: 106.17/106.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:38:57,382 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 106.85/107.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:39:14,189 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 115.05/115.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:39:31,245 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 113.95/115.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:39:49,318 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 106.15/107.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:40:07,535 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.32 (max/real adjusted throughput: 108.37/103.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:40:24,458 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 113.31/115.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:40:41,209 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 115.87/115.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:40:59,270 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 107.50/107.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:41:17,304 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 107.02/107.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:41:34,083 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 115.78/114.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:41:50,850 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 115.08/114.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:42:08,782 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 107.94/107.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:42:26,796 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 105.91/107.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:42:43,677 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 114.25/114.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:43:00,363 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 115.47/115.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:43:18,339 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 106.86/106.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:43:36,488 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 105.44/108.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:43:53,265 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 114.96/114.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:44:09,958 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 114.57/116.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:44:27,913 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.26 (max/real adjusted throughput: 107.47/106.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:44:45,677 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 107.65/109.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:45:02,491 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 115.08/114.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:45:19,159 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 115.28/116.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:45:37,097 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 107.74/108.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:45:54,890 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 107.84/108.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:46:11,629 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 114.90/114.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:46:28,339 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 115.14/115.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:46:46,361 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 106.91/106.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:47:04,037 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 109.24/108.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:47:20,748 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 114.78/115.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:47:37,533 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 113.86/115.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:47:55,409 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 109.11/107.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:48:13,271 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 107.98/108.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:48:30,023 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 115.74/114.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:48:46,740 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 115.51/115.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:49:04,695 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 106.42/107.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:49:22,765 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 106.94/106.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:49:39,449 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 115.89/115.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:49:56,232 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 114.96/115.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:50:14,084 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 107.29/108.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:50:32,189 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 104.07/108.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:50:49,005 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 115.47/113.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:51:05,717 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 115.03/115.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:51:24,138 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.37 (max/real adjusted throughput: 106.25/101.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:51:42,170 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 107.18/106.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:51:58,946 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 114.66/114.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:52:15,659 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 115.03/115.15 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:52:34,017 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.34 (max/real adjusted throughput: 107.31/103.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:52:52,113 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 106.21/107.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:53:08,901 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 114.91/114.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:53:25,662 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 115.29/114.74 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:53:44,196 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 99.99/108.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:54:02,408 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.32 (max/real adjusted throughput: 105.88/104.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:54:19,161 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 114.31/116.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:54:35,945 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 115.66/114.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:54:54,229 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 104.11/107.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:55:12,358 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 106.82/106.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:55:29,121 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 115.56/115.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:55:45,982 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 114.81/114.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:56:03,918 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 105.71/109.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:56:21,916 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 106.65/107.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:56:38,682 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 114.23/115.34 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:56:55,399 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 115.66/115.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:57:13,287 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 108.51/108.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:57:31,338 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 108.01/105.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:57:48,188 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 115.17/114.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:58:04,987 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 115.12/114.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:58:23,576 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.30 (max/real adjusted throughput: 101.66/104.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:58:41,922 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 105.45/105.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:58:58,814 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 114.39/113.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:59:15,517 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 115.90/115.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:59:33,818 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.34 (max/real adjusted throughput: 106.22/102.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:59:52,166 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.29 (max/real adjusted throughput: 105.25/105.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:00:09,004 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 113.60/115.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:00:25,851 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 115.73/114.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:00:44,291 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.35 (max/real adjusted throughput: 107.62/102.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:01:02,526 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 105.77/105.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:01:19,418 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 113.91/113.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:01:36,153 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 114.98/114.91 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:01:54,865 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.33 (max/real adjusted throughput: 103.75/103.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:02:13,248 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 104.56/105.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:02:30,119 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 113.32/115.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:02:46,860 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 115.61/114.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:03:05,507 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.29 (max/real adjusted throughput: 102.69/105.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:03:23,952 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 102.96/105.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:03:40,768 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 114.96/114.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:03:57,618 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 115.14/113.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:04:16,211 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.33 (max/real adjusted throughput: 104.85/103.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:04:34,969 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.32 (max/real adjusted throughput: 103.17/103.91 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:04:51,595 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 116.00/115.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:05:08,410 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 114.98/115.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:05:26,939 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.36 (max/real adjusted throughput: 104.99/102.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:05:45,577 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.32 (max/real adjusted throughput: 106.98/103.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:06:02,371 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 115.77/113.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:06:19,197 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 115.05/115.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:06:37,708 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.32 (max/real adjusted throughput: 104.32/103.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:06:56,441 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.45 (max/real adjusted throughput: 105.91/98.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:07:13,207 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 115.26/114.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:07:29,935 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 115.24/114.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:07:48,399 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.33 (max/real adjusted throughput: 106.08/103.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:08:07,050 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.37 (max/real adjusted throughput: 104.29/101.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:08:23,911 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 113.91/114.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:08:40,731 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 114.93/114.06 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:08:59,131 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.31 (max/real adjusted throughput: 107.03/104.19 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:09:17,019 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.26 (max/real adjusted throughput: 112.87/106.71 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:09:34,254 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 114.25/111.69 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:09:51,537 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 114.08/109.61 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:10:09,716 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 106.77/107.29 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:10:27,803 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 109.67/105.94 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:10:45,214 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 112.41/110.33 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:11:02,326 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 115.05/112.20 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:11:20,590 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.29 (max/real adjusted throughput: 108.67/105.06 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:11:38,752 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.29 (max/real adjusted throughput: 110.43/105.30 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:11:55,861 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 116.98/111.21 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:12:13,067 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 113.68/112.07 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:12:31,411 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.32 (max/real adjusted throughput: 107.97/103.83 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:12:49,660 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 108.63/105.82 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:13:06,875 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 113.52/113.06 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:13:23,903 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 116.86/111.81 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:13:42,072 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.26 (max/real adjusted throughput: 109.78/106.56 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:14:00,354 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 107.37/106.18 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:14:17,559 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 116.04/111.32 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:14:34,800 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 114.24/112.98 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:14:53,195 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.33 (max/real adjusted throughput: 110.48/103.61 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:15:11,490 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.31 (max/real adjusted throughput: 109.35/104.18 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:15:28,482 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 116.96/111.43 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:15:45,709 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 115.19/110.11 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:16:04,160 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.36 (max/real adjusted throughput: 107.78/102.25 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:16:22,195 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.26 (max/real adjusted throughput: 109.85/106.47 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:16:39,425 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 115.07/110.38 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:16:56,411 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 116.23/112.68 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:17:14,699 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.32 (max/real adjusted throughput: 109.98/103.87 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:17:33,059 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.30 (max/real adjusted throughput: 107.66/104.75 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:17:50,062 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 116.92/111.75 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:18:07,220 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 114.55/111.60 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:18:25,343 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 109.82/105.54 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:18:43,815 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.26 (max/real adjusted throughput: 103.82/106.68 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:19:01,022 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 115.92/109.95 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:19:18,301 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 115.93/109.47 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:19:35,457 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 117.17/111.22 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:19:53,145 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 112.33/110.65 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:20:10,042 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 117.12/113.10 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:20:26,973 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 116.88/112.88 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:20:44,120 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 117.24/112.19 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:21:01,124 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 116.39/114.18 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:21:18,093 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 117.43/111.33 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:21:35,022 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 116.96/113.19 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:21:53,094 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.31 (max/real adjusted throughput: 111.98/104.47 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:22:11,159 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 111.09/106.92 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:22:28,056 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 117.52/112.98 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:22:45,104 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 116.20/112.21 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:23:03,201 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 111.13/105.52 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:23:21,210 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 110.46/107.65 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:23:38,387 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 115.94/110.90 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:23:55,257 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 116.80/113.95 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:24:13,370 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 110.82/105.91 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:24:31,490 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 107.92/108.18 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:24:48,390 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.60/112.53 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:25:05,600 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 115.78/109.72 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:25:23,732 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.30 (max/real adjusted throughput: 110.79/104.64 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:25:41,908 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 109.41/106.90 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:25:58,996 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 114.88/113.43 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:26:15,965 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 117.19/112.16 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:26:33,932 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 110.66/107.57 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:26:52,234 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.33 (max/real adjusted throughput: 110.07/103.56 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:27:09,137 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 117.30/112.96 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:27:26,172 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 117.07/111.81 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:27:44,338 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 107.25/107.24 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:28:02,364 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 107.64/108.44 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:28:19,287 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.24/112.69 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:28:36,274 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.10/112.50 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:28:54,248 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 110.40/106.99 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:29:12,142 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 111.08/107.65 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:29:29,104 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 116.84/112.86 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:29:46,031 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.09/112.73 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:30:03,940 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 109.97/108.75 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:30:22,148 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.35 (max/real adjusted throughput: 111.15/102.59 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:30:39,105 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 116.31/113.63 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:30:55,977 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 117.16/113.78 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:31:14,098 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 109.22/107.15 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:31:31,704 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 111.46/110.99 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:31:48,691 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 115.43/113.69 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:32:05,642 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.09/112.53 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:32:23,628 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 109.67/107.73 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:32:41,750 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.30 (max/real adjusted throughput: 111.29/104.85 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:32:58,741 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 116.88/112.17 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:33:15,708 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 116.84/112.49 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:33:33,601 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 111.69/107.54 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:33:51,557 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 111.78/107.13 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:34:08,507 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.52/112.39 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:34:25,559 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 116.13/112.23 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:34:43,474 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 110.71/107.86 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:35:01,389 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 111.35/107.42 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:35:18,395 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 116.17/112.48 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:35:35,380 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 117.43/111.77 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:35:53,334 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 110.72/107.16 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:36:11,353 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 111.46/105.86 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:36:28,281 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 116.96/112.70 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:36:45,248 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.17/112.49 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:37:02,914 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 109.75/110.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:37:20,174 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 111.58/111.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:37:36,895 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 115.73/114.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:37:53,405 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 116.60/116.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:38:10,911 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 110.35/109.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:38:28,282 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 111.25/110.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:38:44,837 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 116.55/116.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:39:01,371 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 116.90/115.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:39:18,807 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 110.86/110.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:39:36,143 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 111.35/111.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:39:52,747 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 116.14/116.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:40:09,326 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 117.01/116.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:40:26,834 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 109.54/111.15 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:40:44,163 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 111.22/110.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:41:00,771 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 115.76/116.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:41:17,415 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 115.75/115.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:41:35,085 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 106.98/111.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:41:52,257 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 112.10/112.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:42:08,761 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 117.14/116.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:42:25,545 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 114.55/114.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:42:43,272 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 108.44/109.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:43:00,681 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 111.51/109.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:43:17,141 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 117.25/117.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:43:33,569 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.19/117.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:43:51,208 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 109.74/108.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:44:08,658 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 109.00/111.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:44:25,070 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 117.69/117.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:44:41,468 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.57/117.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:44:59,263 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 106.20/109.90 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:45:16,661 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 111.79/110.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:45:33,329 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 116.13/115.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:45:49,853 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 116.24/118.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:46:07,633 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 108.86/109.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:46:25,073 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 111.05/109.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:46:41,510 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 117.56/117.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:46:58,053 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 116.87/116.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:47:15,474 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 110.25/111.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:47:32,784 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 110.94/112.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:47:49,259 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 117.44/116.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:48:05,806 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 117.58/115.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:48:23,169 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 110.86/111.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:48:40,691 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 110.24/111.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:48:57,313 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 117.22/114.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:49:13,754 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.56/117.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:49:31,162 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 111.24/111.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:49:48,494 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 111.01/110.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:50:04,939 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 116.69/117.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:50:21,350 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.32/117.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:50:38,669 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 112.02/111.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:50:55,980 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 111.35/110.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:51:12,524 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 115.45/118.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:51:28,894 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.79/117.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:51:46,197 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 112.35/111.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:52:03,307 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 111.68/112.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:52:19,824 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 116.74/117.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:52:36,363 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 116.59/116.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:52:53,909 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 107.85/111.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:53:11,324 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 111.15/111.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:53:28,129 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 114.40/115.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:53:44,601 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 116.85/117.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:54:02,123 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 111.87/107.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:54:19,575 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 110.16/110.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:54:36,209 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 115.66/116.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:54:53,006 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 113.86/115.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:55:10,366 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 111.41/110.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:55:28,019 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 107.14/110.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:55:44,548 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 117.23/116.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:56:01,188 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 115.39/116.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:56:18,729 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 108.03/111.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:56:36,244 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 110.92/109.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:56:52,777 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 116.08/117.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:57:09,247 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 116.15/117.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:57:26,822 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 109.57/110.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:57:44,304 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 109.46/111.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:58:00,871 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 117.05/115.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:58:17,599 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 115.50/114.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:58:35,079 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 109.06/111.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:58:52,436 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 111.11/110.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:59:09,277 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 113.87/115.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:59:25,767 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 115.88/117.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:59:43,016 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 112.64/111.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:00:00,347 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 111.58/111.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:00:16,818 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 116.50/117.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:00:33,352 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 117.31/115.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:00:50,602 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 110.63/112.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:01:08,031 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 111.26/109.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:01:24,499 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 116.44/117.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:01:40,881 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.71/117.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:01:58,420 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 109.74/109.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:02:15,603 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 115.44/106.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:02:32,012 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.82/117.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:02:48,654 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 115.13/115.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:03:05,908 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 110.83/111.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:03:23,202 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 111.00/112.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:03:39,801 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 116.90/115.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:03:56,497 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 113.66/117.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:04:14,328 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 109.75/108.90 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:04:32,467 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.31 (max/real adjusted throughput: 111.70/104.16 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:04:49,370 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 117.15/113.27 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:05:06,423 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 117.32/111.88 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:05:24,417 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 112.55/105.71 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:05:42,381 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.29 (max/real adjusted throughput: 112.96/105.07 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:05:59,274 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 117.44/112.92 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:06:16,226 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.47/112.45 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:06:34,109 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.30 (max/real adjusted throughput: 113.55/104.76 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:06:52,047 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 111.06/107.28 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:07:08,968 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 116.73/113.00 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:07:26,007 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 116.94/111.45 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:07:43,907 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 111.97/105.87 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:08:01,859 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 110.55/107.24 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:08:18,820 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 116.54/112.40 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:08:35,837 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 116.89/111.82 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:08:53,955 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.26 (max/real adjusted throughput: 107.94/106.56 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:09:11,694 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 112.04/108.02 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:09:28,614 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 116.60/113.03 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:09:45,534 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 117.04/113.75 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:10:03,418 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 112.82/106.94 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:10:21,282 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 112.81/107.88 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:10:38,295 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 117.71/111.83 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:10:55,261 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.02/112.78 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:11:13,279 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 107.86/109.05 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:11:31,059 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 110.88/109.52 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:11:47,969 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.53/112.60 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:12:04,986 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 115.81/112.99 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:12:22,335 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 114.86/111.83 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:12:39,538 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 116.14/112.93 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:12:56,487 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.19/112.50 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:13:13,376 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 117.54/113.11 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:13:31,068 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 111.52/109.34 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:13:48,743 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 111.30/109.33 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:14:05,828 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 117.17/110.47 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:14:22,805 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 117.84/111.61 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:14:40,577 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 110.75/108.15 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:14:58,250 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 110.76/109.42 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:15:15,156 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 116.40/113.60 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:15:32,100 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.00/112.75 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:15:49,876 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 112.95/106.22 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:16:07,427 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 114.53/109.14 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:16:24,355 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.42/112.56 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:16:41,383 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 117.12/111.18 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:16:58,929 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 113.50/109.11 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:17:16,794 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 112.53/106.88 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:17:33,739 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.74/112.37 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:17:50,705 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.93/112.55 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:18:08,469 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 112.28/107.38 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:18:26,288 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 110.91/108.04 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:18:43,249 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 116.88/112.50 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:19:00,190 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 117.18/113.26 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:19:18,180 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 111.11/105.50 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:19:35,809 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 113.06/110.11 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:19:52,834 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 115.84/112.34 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:20:09,905 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 117.49/110.24 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:20:27,646 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 113.35/107.48 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:20:45,260 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 113.69/108.18 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:21:02,194 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.86/112.69 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:21:19,144 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 116.28/113.07 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:21:36,958 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.26 (max/real adjusted throughput: 113.46/106.66 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:21:54,900 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 110.54/106.17 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:22:11,710 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 117.97/113.64 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:22:28,622 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 116.55/113.67 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:22:46,437 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 111.52/107.89 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:23:04,103 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 113.41/108.55 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:23:21,163 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.47/112.64 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:23:38,108 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.22/112.82 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:23:55,719 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 112.44/110.17 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:24:13,487 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 110.48/109.13 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:24:30,507 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 116.91/112.05 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:24:47,366 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 116.96/113.74 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:25:05,196 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 111.42/108.49 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:25:22,997 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 111.86/109.97 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:25:39,965 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 116.72/112.51 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:25:56,898 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 116.53/112.85 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:26:14,809 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 110.79/107.74 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:26:32,658 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.26 (max/real adjusted throughput: 112.17/106.56 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:26:49,677 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 117.21/110.95 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:27:06,557 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 118.26/112.79 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:27:24,200 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 112.77/109.51 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:27:42,038 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 112.58/107.46 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:27:58,945 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 117.27/113.96 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:28:15,905 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 116.78/113.00 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:28:33,800 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.26 (max/real adjusted throughput: 111.46/106.78 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:28:51,399 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 113.34/108.91 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:29:08,317 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.56/112.51 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:29:25,239 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 117.06/113.40 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:29:43,044 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 110.39/109.86 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:30:00,838 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 112.07/108.81 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:30:17,753 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 116.70/113.52 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:30:34,827 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 116.26/111.79 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:30:52,629 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 113.13/106.83 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:31:10,257 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 112.62/108.98 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:31:27,141 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 117.71/112.62 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:31:44,040 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 117.63/113.16 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:32:01,467 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 111.70/111.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:32:18,783 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 112.40/110.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:32:35,257 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 117.05/116.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:32:51,747 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 116.69/116.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:33:08,738 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 113.55/113.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:33:25,860 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 113.28/112.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:33:42,358 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 116.82/116.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:33:58,805 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 117.06/117.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:34:16,064 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 111.19/112.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:34:33,359 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 111.71/111.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:34:49,776 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.89/117.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:35:06,360 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 115.01/117.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:35:23,683 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 111.48/110.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:35:40,915 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 111.79/112.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:35:57,388 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 117.22/116.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:36:13,947 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 116.25/117.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:36:31,130 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 112.79/111.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:36:48,415 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 112.14/111.06 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:37:04,939 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 116.45/116.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:37:21,353 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.67/117.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:37:38,678 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 111.66/109.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:37:55,907 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 112.72/110.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:38:12,379 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.12/117.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:38:28,809 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.14/117.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:38:46,094 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 111.53/110.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:39:03,338 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 111.93/111.06 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:39:19,778 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 116.94/116.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:39:36,225 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.39/117.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:39:53,515 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 111.46/110.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:40:10,700 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 112.82/112.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:40:27,257 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 117.06/115.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:40:43,723 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 116.62/117.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:41:00,958 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 112.51/112.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:41:18,181 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 111.54/113.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:41:34,660 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 116.56/116.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:41:51,089 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.26/117.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:42:08,354 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 111.34/113.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:42:25,352 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 112.24/116.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:42:41,815 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.01/117.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:42:58,271 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 116.86/117.15 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:43:15,562 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 111.99/111.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:43:32,874 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 111.84/111.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:43:49,341 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 117.64/116.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:44:05,833 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 116.85/117.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:44:23,143 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 112.08/112.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:44:40,498 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 111.85/110.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:44:57,037 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 117.22/116.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:45:13,520 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 116.77/116.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:45:30,672 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 113.05/112.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:45:47,837 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 113.30/111.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:46:04,365 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 117.08/116.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:46:20,914 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 116.37/116.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:46:38,228 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 111.82/112.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:46:55,498 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 111.90/111.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:47:11,919 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.34/117.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:47:28,463 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 116.14/116.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:47:45,708 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 111.34/111.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:48:02,903 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 113.06/111.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:48:19,415 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 116.09/117.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:48:35,922 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 117.33/116.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:48:53,102 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 111.93/111.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:49:10,341 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 112.18/111.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:49:26,777 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 116.96/117.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:49:43,255 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 117.20/116.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:50:00,505 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 112.66/111.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:50:17,955 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 113.37/109.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:50:34,414 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 117.42/117.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:50:50,942 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 117.48/115.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:51:08,290 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 111.21/111.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:51:25,668 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 111.88/111.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:51:42,134 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.09/117.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:51:58,573 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 116.90/117.15 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:52:15,651 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 114.47/111.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:52:33,037 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 111.58/111.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:52:49,554 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 117.48/116.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:53:06,011 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 117.42/116.34 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:53:23,217 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 110.70/112.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:53:40,491 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 112.61/111.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:53:56,960 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 117.10/117.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:54:13,418 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 116.82/117.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:54:30,706 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 111.62/111.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:54:47,981 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 112.49/110.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:55:04,407 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.31/117.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:55:20,997 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 116.12/116.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:55:38,278 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 112.17/110.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:55:55,548 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 112.71/111.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:56:12,010 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 117.09/117.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:56:28,606 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 116.74/115.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:56:45,918 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 111.60/111.06 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:57:03,238 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 110.95/110.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:57:19,751 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 116.94/116.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:57:36,286 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 117.39/117.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:57:53,563 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 111.56/112.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:58:10,847 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 111.41/111.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:58:27,320 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 117.00/117.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:58:43,839 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 116.68/116.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:58:43,839 | xffl.distributed.aggregation |     INFO | Dumping benchmarking results to /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs/llama3.1-8b_ns_16_fs_1_ppn_1.csv[0m
lrdn0708:3048046:3056360 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0708:3048046:3056360 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0708:3048046:3056360 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0708:3048046:3048169 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0745:1658235:1666552 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0745:1658235:1666552 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0745:1658235:1666552 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0745:1658235:1658353 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0757:1521334:1529645 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0757:1521334:1529645 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0757:1521334:1529645 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0780:1321542:1329831 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0780:1321542:1329831 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0780:1321542:1329831 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0780:1321542:1321657 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0757:1521334:1521450 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0824:1054616:1062913 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0824:1054616:1062913 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0824:1054616:1062913 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0824:1054616:1054732 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0805:1354481:1363119 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0805:1354481:1363119 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0805:1354481:1363119 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0805:1354481:1354600 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0836:1477025:1486814 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0836:1477025:1477143 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0836:1477025:1486814 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0836:1477025:1486814 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0853:1717580:1726554 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0853:1717580:1726554 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0853:1717580:1726554 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0853:1717580:1717700 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1028:895822:904773 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1028:895822:904773 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1028:895822:904773 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1028:895822:895942 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1093:1492163:1500467 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1093:1492163:1492279 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1093:1492163:1500467 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1093:1492163:1500467 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1124:1626469:1635495 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1124:1626469:1635495 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1124:1626469:1635495 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1124:1626469:1626589 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1139:2620503:2630944 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1139:2620503:2630944 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1139:2620503:2630944 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1139:2620503:2620625 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1138:684929:694242 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1138:684929:694242 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1138:684929:694242 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1138:684929:685046 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1226:1654681:1662985 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1226:1654681:1662985 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1226:1654681:1662985 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1226:1654681:1654796 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1218:1776591:1785527 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1218:1776591:1785527 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1218:1776591:1785527 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1218:1776591:1776713 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1206:1547777:1556064 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1206:1547777:1556064 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1206:1547777:1547893 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1206:1547777:1556064 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0708:3048046:3056360 [0] NCCL INFO comm 0x2c143750 rank 0 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0745:1658235:1666552 [0] NCCL INFO comm 0xcab3ee0 rank 1 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0805:1354481:1363119 [0] NCCL INFO comm 0x2e109ae0 rank 4 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0757:1521334:1529645 [0] NCCL INFO comm 0xd3350a0 rank 2 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0780:1321542:1329831 [0] NCCL INFO comm 0x2bbced80 rank 3 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0824:1054616:1062913 [0] NCCL INFO comm 0x2a62e550 rank 5 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0836:1477025:1486814 [0] NCCL INFO comm 0x11e17600 rank 6 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1093:1492163:1500467 [0] NCCL INFO comm 0x12f439c0 rank 9 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1028:895822:904773 [0] NCCL INFO comm 0x2b46c960 rank 8 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0853:1717580:1726554 [0] NCCL INFO comm 0x1203a530 rank 7 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1124:1626469:1635495 [0] NCCL INFO comm 0x118f90f0 rank 10 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1139:2620503:2630944 [0] NCCL INFO comm 0x2db0a550 rank 12 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1138:684929:694242 [0] NCCL INFO comm 0x21e9c5b0 rank 11 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1206:1547777:1556064 [0] NCCL INFO comm 0xe4dce50 rank 13 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1226:1654681:1662985 [0] NCCL INFO comm 0x11468980 rank 15 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1218:1776591:1785527 [0] NCCL INFO comm 0xe23a600 rank 14 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0708:3048046:3056362 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0708:3048046:3056362 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0708:3048046:3056362 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0708:3048046:3048163 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0745:1658235:1666554 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0745:1658235:1666554 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0745:1658235:1666554 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0745:1658235:1658348 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0757:1521334:1529647 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0757:1521334:1529647 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0757:1521334:1529647 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0757:1521334:1521445 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0780:1321542:1329833 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0780:1321542:1329833 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0780:1321542:1329833 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0780:1321542:1321652 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0805:1354481:1363121 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0805:1354481:1363121 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0805:1354481:1363121 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0805:1354481:1354595 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0824:1054616:1062915 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0824:1054616:1062915 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0824:1054616:1062915 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0824:1054616:1054727 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0836:1477025:1486816 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0836:1477025:1486816 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0836:1477025:1486816 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0836:1477025:1477138 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1028:895822:904775 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1028:895822:904775 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1028:895822:904775 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1028:895822:895937 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0853:1717580:1726556 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0853:1717580:1726556 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0853:1717580:1726556 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0853:1717580:1717695 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1093:1492163:1500469 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1093:1492163:1500469 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1093:1492163:1500469 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1093:1492163:1492274 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1124:1626469:1635497 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1124:1626469:1635497 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1124:1626469:1635497 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1124:1626469:1626584 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1138:684929:694244 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1138:684929:694244 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1138:684929:694244 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1138:684929:685041 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1139:2620503:2630946 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1139:2620503:2630946 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1139:2620503:2630946 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1139:2620503:2620620 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1206:1547777:1556066 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1206:1547777:1556066 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1206:1547777:1556066 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1206:1547777:1547888 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1218:1776591:1785529 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1218:1776591:1785529 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1218:1776591:1785529 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1218:1776591:1776708 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1226:1654681:1662987 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1226:1654681:1662987 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1226:1654681:1662987 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1226:1654681:1654791 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0708:3048046:3056362 [0] NCCL INFO comm 0x2c0c7bc0 rank 0 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0745:1658235:1666554 [0] NCCL INFO comm 0xca38350 rank 1 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0757:1521334:1529647 [0] NCCL INFO comm 0xd2b9510 rank 2 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0780:1321542:1329833 [0] NCCL INFO comm 0x2bb531f0 rank 3 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0805:1354481:1363121 [0] NCCL INFO comm 0x2e08df50 rank 4 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0824:1054616:1062915 [0] NCCL INFO comm 0x2a5b29c0 rank 5 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0836:1477025:1486816 [0] NCCL INFO comm 0x11d9ba70 rank 6 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1028:895822:904775 [0] NCCL INFO comm 0x2b3f0dd0 rank 8 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0853:1717580:1726556 [0] NCCL INFO comm 0x11fbe9a0 rank 7 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1093:1492163:1500469 [0] NCCL INFO comm 0x12ec7e30 rank 9 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1124:1626469:1635497 [0] NCCL INFO comm 0x1187d560 rank 10 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1139:2620503:2630946 [0] NCCL INFO comm 0x2da8e9c0 rank 12 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1138:684929:694244 [0] NCCL INFO comm 0x21e20a20 rank 11 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1206:1547777:1556066 [0] NCCL INFO comm 0xe4612c0 rank 13 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1218:1776591:1785529 [0] NCCL INFO comm 0xe1bea70 rank 14 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1226:1654681:1662987 [0] NCCL INFO comm 0x113ecdf0 rank 15 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0708:3048046:3056364 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0708:3048046:3056364 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0708:3048046:3056364 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0708:3048046:3048157 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0780:1321542:1329835 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0780:1321542:1329835 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0780:1321542:1329835 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0780:1321542:1321647 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0757:1521334:1529649 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0757:1521334:1529649 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0757:1521334:1529649 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0757:1521334:1521440 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0745:1658235:1666556 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0745:1658235:1666556 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0745:1658235:1666556 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0745:1658235:1658343 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0805:1354481:1363123 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0805:1354481:1363123 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0805:1354481:1363123 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0805:1354481:1354590 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0824:1054616:1062917 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0824:1054616:1062917 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0824:1054616:1062917 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0824:1054616:1054722 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0853:1717580:1726558 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0853:1717580:1726558 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0853:1717580:1726558 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0836:1477025:1486818 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0836:1477025:1486818 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0836:1477025:1486818 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0836:1477025:1477133 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1028:895822:904777 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1028:895822:904777 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1028:895822:904777 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1028:895822:895932 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0853:1717580:1717690 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1093:1492163:1500471 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1093:1492163:1500471 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1093:1492163:1500471 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1124:1626469:1635499 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1124:1626469:1635499 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1124:1626469:1635499 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1124:1626469:1626579 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1093:1492163:1492269 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1138:684929:694246 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1138:684929:694246 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1138:684929:694246 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1138:684929:685036 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1139:2620503:2630948 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1139:2620503:2630948 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1139:2620503:2630948 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1139:2620503:2620615 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1206:1547777:1556068 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1206:1547777:1556068 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1206:1547777:1556068 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1206:1547777:1547883 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1218:1776591:1785531 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1218:1776591:1785531 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1218:1776591:1785531 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1218:1776591:1776703 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1226:1654681:1662989 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1226:1654681:1662989 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1226:1654681:1662989 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1226:1654681:1654786 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0708:3048046:3056364 [0] NCCL INFO comm 0x2c04c030 rank 0 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0780:1321542:1329835 [0] NCCL INFO comm 0x2bad7660 rank 3 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0757:1521334:1529649 [0] NCCL INFO comm 0xd23d980 rank 2 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0745:1658235:1666556 [0] NCCL INFO comm 0xc9bc7c0 rank 1 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0805:1354481:1363123 [0] NCCL INFO comm 0x2e0123c0 rank 4 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1028:895822:904777 [0] NCCL INFO comm 0x2b375240 rank 8 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0836:1477025:1486818 [0] NCCL INFO comm 0x11d1fee0 rank 6 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0824:1054616:1062917 [0] NCCL INFO comm 0x2a536e30 rank 5 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0853:1717580:1726558 [0] NCCL INFO comm 0x11f42e10 rank 7 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1124:1626469:1635499 [0] NCCL INFO comm 0x118019d0 rank 10 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1093:1492163:1500471 [0] NCCL INFO comm 0x12e4c2a0 rank 9 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1138:684929:694246 [0] NCCL INFO comm 0x21da4e90 rank 11 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1206:1547777:1556068 [0] NCCL INFO comm 0xe3e5730 rank 13 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1139:2620503:2630948 [0] NCCL INFO comm 0x2da12e30 rank 12 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1218:1776591:1785531 [0] NCCL INFO comm 0xe142ee0 rank 14 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1226:1654681:1662989 [0] NCCL INFO comm 0x11371260 rank 15 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0708:3048046:3056366 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0708:3048046:3056366 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0708:3048046:3056366 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0708:3048046:3048150 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0745:1658235:1666558 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0745:1658235:1666558 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0745:1658235:1666558 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0745:1658235:1658337 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0757:1521334:1529651 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0757:1521334:1529651 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0757:1521334:1529651 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0757:1521334:1521434 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0780:1321542:1329837 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0780:1321542:1329837 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0780:1321542:1329837 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0780:1321542:1321641 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0805:1354481:1363125 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0805:1354481:1363125 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0805:1354481:1363125 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0805:1354481:1354584 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0824:1054616:1062919 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0824:1054616:1062919 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0824:1054616:1062919 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0824:1054616:1054716 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0836:1477025:1486820 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0836:1477025:1486820 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0836:1477025:1486820 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0836:1477025:1477126 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0853:1717580:1726560 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0853:1717580:1726560 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0853:1717580:1726560 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0853:1717580:1717682 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1028:895822:904779 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1028:895822:904779 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1028:895822:904779 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1028:895822:895924 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1093:1492163:1500473 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1093:1492163:1500473 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1093:1492163:1500473 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1093:1492163:1492263 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1124:1626469:1635501 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1124:1626469:1635501 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1124:1626469:1635501 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1124:1626469:1626571 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1138:684929:694248 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1138:684929:694248 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1138:684929:694248 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1138:684929:685030 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1139:2620503:2630950 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1139:2620503:2630950 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1139:2620503:2630950 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1139:2620503:2620607 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1206:1547777:1556070 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1206:1547777:1556070 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1206:1547777:1556070 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1206:1547777:1547877 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1218:1776591:1785533 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1218:1776591:1785533 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1218:1776591:1785533 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1218:1776591:1776693 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1226:1654681:1662991 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1226:1654681:1662991 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1226:1654681:1662991 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1226:1654681:1654780 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0708:3048046:3056366 [0] NCCL INFO comm 0x229ad010 rank 0 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0745:1658235:1666558 [0] NCCL INFO comm 0xc7b2930 rank 1 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0757:1521334:1529651 [0] NCCL INFO comm 0xd030d50 rank 2 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0780:1321542:1329837 [0] NCCL INFO comm 0x224699f0 rank 3 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0805:1354481:1363125 [0] NCCL INFO comm 0x229a1720 rank 4 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0824:1054616:1062919 [0] NCCL INFO comm 0x2a32d0b0 rank 5 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0836:1477025:1486820 [0] NCCL INFO comm 0x21ed7e70 rank 6 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0853:1717580:1726560 [0] NCCL INFO comm 0x11d3c000 rank 7 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1028:895822:904779 [0] NCCL INFO comm 0x23bae210 rank 8 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1093:1492163:1500473 [0] NCCL INFO comm 0x20fe09a0 rank 9 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1124:1626469:1635501 [0] NCCL INFO comm 0x115f8ed0 rank 10 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1138:684929:694248 [0] NCCL INFO comm 0x21b9cd90 rank 11 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1139:2620503:2630950 [0] NCCL INFO comm 0x174dbf50 rank 12 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1206:1547777:1556070 [0] NCCL INFO comm 0xe1d91a0 rank 13 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1218:1776591:1785533 [0] NCCL INFO comm 0xdf44c60 rank 14 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1226:1654681:1662991 [0] NCCL INFO comm 0x2152b670 rank 15 nranks 16 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1028:895822:904782 [0] NCCL INFO comm 0xd8d8c30 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1093:1492163:1500476 [0] NCCL INFO comm 0xd3b0f50 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1206:1547777:1556073 [0] NCCL INFO comm 0xdd8d520 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1226:1654681:1662994 [0] NCCL INFO comm 0x168dbc30 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1139:2620503:2630953 [0] NCCL INFO comm 0xe777310 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0745:1658235:1666561 [0] NCCL INFO comm 0xc366900 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0780:1321542:1329840 [0] NCCL INFO comm 0xe83cac0 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0853:1717580:1726563 [0] NCCL INFO comm 0x17267c50 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0836:1477025:1486823 [0] NCCL INFO comm 0x172896d0 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0708:3048046:3056369 [0] NCCL INFO comm 0xe57f600 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1218:1776591:1785536 [0] NCCL INFO comm 0xdade2b0 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0805:1354481:1363128 [0] NCCL INFO comm 0x10576460 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1138:684929:694251 [0] NCCL INFO comm 0xdf6dbd0 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1124:1626469:1635504 [0] NCCL INFO comm 0xe9ab100 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0824:1054616:1062922 [0] NCCL INFO comm 0xd14a740 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0757:1521334:1529654 [0] NCCL INFO comm 0xcbe5740 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
[38;20m2025-08-02 02:58:49,039 | xffl.cli.simulate |     INFO | Total simulation execution time: 9996.10 seconds[0m
[38;20m2025-08-02 02:58:49,039 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
