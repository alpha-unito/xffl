[38;20m2025-08-02 04:26:35,459 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
[38;5;39m2025-08-02 04:26:35,461 | xffl.cli.simulate |    DEBUG | Using current virtual environment: /leonardo_scratch/fast/uToID_bench/xffl/.venv[0m
[38;5;39m2025-08-02 04:26:35,461 | xffl.cli.simulate |    DEBUG | New local simulation xFFL environment variables: {'XFFL_VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv', 'XFFL_WORLD_SIZE': '8', 'XFFL_NUM_NODES': '8', 'MASTER_ADDR': 'lrdn0120', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;5;39m2025-08-02 04:26:35,461 | xffl.cli.simulate |    DEBUG | Updated xFFL environment: {'XFFL_VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv', 'XFFL_WORLD_SIZE': '8', 'XFFL_NUM_NODES': '8', 'MASTER_ADDR': 'lrdn0120', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;20m2025-08-02 04:26:35,463 | xffl.cli.simulate |     INFO | Running local simulation...[0m
[38;5;39m2025-08-02 04:26:35,463 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0120: ssh -oStrictHostKeyChecking=no lrdn0120 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=8 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0120 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_8_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 04:26:35,463 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0378: ssh -oStrictHostKeyChecking=no lrdn0378 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=8 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0120 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_8_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 04:26:35,463 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0398: ssh -oStrictHostKeyChecking=no lrdn0398 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=8 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0120 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=2 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_8_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 04:26:35,464 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0786: ssh -oStrictHostKeyChecking=no lrdn0786 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=8 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0120 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=3 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_8_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 04:26:35,464 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0872: ssh -oStrictHostKeyChecking=no lrdn0872 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=8 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0120 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=4 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_8_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 04:26:35,464 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0905: ssh -oStrictHostKeyChecking=no lrdn0905 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=8 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0120 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=5 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_8_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 04:26:35,464 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1057: ssh -oStrictHostKeyChecking=no lrdn1057 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=8 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0120 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=6 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_8_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 04:26:35,465 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1067: ssh -oStrictHostKeyChecking=no lrdn1067 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=8 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0120 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=7 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_8_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 04:26:53,933 | xffl.distributed.distributed_state |    DEBUG | Setting Symmetric Federated Scaling with sizes (1, 1, 1, 1, 1, 1, 1, 1)[0m
[38;5;39m2025-08-02 04:26:53,966 | xffl.distributed.distributed |    DEBUG | [Rank 0]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0120
                    Master port=29500
                    Rank=0
                    World size=8
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=0
                    Node world size=8
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=0
                    Federated world size=8
                MESHES:
                    FSDP=DeviceMesh('cuda', [0], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7]
                    Replica group=None
                    Federation=[0]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb934480>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9516c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe98da0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe990a0>)
                [0m
[38;5;39m2025-08-02 04:26:53,966 |         __main__ |    DEBUG | Rendez-vous time: 7.87 seconds[0m
[38;5;39m2025-08-02 04:26:53,966 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 04:26:53,967 | xffl.distributed.distributed |    DEBUG | [Rank 7]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0120
                    Master port=29500
                    Rank=7
                    World size=8
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=7
                    Node world size=8
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=7
                    Federated world size=8
                MESHES:
                    FSDP=DeviceMesh('cuda', [7], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7]
                    Replica group=None
                    Federation=[7]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc3b80f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc3d5330>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc91cd50>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc91d050>)
                [0m
[38;5;39m2025-08-02 04:26:53,967 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 04:26:53,968 | xffl.distributed.distributed |    DEBUG | [Rank 3]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0120
                    Master port=29500
                    Rank=3
                    World size=8
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=3
                    Node world size=8
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=3
                    Federated world size=8
                MESHES:
                    FSDP=DeviceMesh('cuda', [3], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7]
                    Replica group=None
                    Federation=[3]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbb2d4d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb4a710>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc0920a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc0923a0>)
                [0m
[38;5;39m2025-08-02 04:26:53,968 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 04:26:53,968 | xffl.distributed.distributed |    DEBUG | [Rank 1]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0120
                    Master port=29500
                    Rank=1
                    World size=8
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=1
                    Node world size=8
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=1
                    Federated world size=8
                MESHES:
                    FSDP=DeviceMesh('cuda', [1], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7]
                    Replica group=None
                    Federation=[1]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcc04ab0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc21cf0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd169980>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd169c80>)
                [0m
[38;5;39m2025-08-02 04:26:53,968 | xffl.distributed.distributed |    DEBUG | [Rank 2]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0120
                    Master port=29500
                    Rank=2
                    World size=8
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=2
                    Node world size=8
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=2
                    Federated world size=8
                MESHES:
                    FSDP=DeviceMesh('cuda', [2], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7]
                    Replica group=None
                    Federation=[2]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbaf08d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb0db10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc0554c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc0557c0>)
                [0m
[38;5;39m2025-08-02 04:26:53,968 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 04:26:53,968 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 04:26:53,968 | xffl.distributed.distributed |    DEBUG | [Rank 4]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0120
                    Master port=29500
                    Rank=4
                    World size=8
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=4
                    Node world size=8
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=4
                    Federated world size=8
                MESHES:
                    FSDP=DeviceMesh('cuda', [4], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7]
                    Replica group=None
                    Federation=[4]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbf51610>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf6e850>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc4b5f90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc4b6290>)
                [0m
[38;5;39m2025-08-02 04:26:53,968 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 04:26:53,968 | xffl.distributed.distributed |    DEBUG | [Rank 6]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0120
                    Master port=29500
                    Rank=6
                    World size=8
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=6
                    Node world size=8
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=6
                    Federated world size=8
                MESHES:
                    FSDP=DeviceMesh('cuda', [6], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7]
                    Replica group=None
                    Federation=[6]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb4329e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb44fc20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb997600>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb997900>)
                [0m
[38;5;39m2025-08-02 04:26:53,968 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 04:26:53,968 | xffl.distributed.distributed |    DEBUG | [Rank 5]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0120
                    Master port=29500
                    Rank=5
                    World size=8
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=5
                    Node world size=8
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=5
                    Federated world size=8
                MESHES:
                    FSDP=DeviceMesh('cuda', [5], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7]
                    Replica group=None
                    Federation=[5]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbe1d5b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe3a7f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc382290>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc382590>)
                [0m
[38;5;39m2025-08-02 04:26:53,968 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 04:26:55,949 | xffl.distributed.distributed |    DEBUG | [Rank 3]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0786:1856354:1856354 [0] NCCL INFO Bootstrap: Using ib0:10.128.18.105<0>
lrdn0786:1856354:1856354 [0] NCCL INFO cudaDriverVersion 12020
lrdn0786:1856354:1856354 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0786:1856354:1856354 [0] NCCL INFO Comm config Blocking set to 1
lrdn0786:1856354:1856444 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0786:1856354:1856444 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0786:1856354:1856444 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.18.105<0>
lrdn0786:1856354:1856444 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0786:1856354:1856444 [0] NCCL INFO Using network IB
lrdn0786:1856354:1856444 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc12d70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc697b1de2d6fc4ff - Init START
lrdn0786:1856354:1856444 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0786:1856354:1856444 [0] NCCL INFO Bootstrap timings total 0.000410 (create 0.000022, send 0.000066, recv 0.000093, ring 0.000002, delay 0.000000)
lrdn0786:1856354:1856444 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0786:1856354:1856444 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0786:1856354:1856444 [0] NCCL INFO comm 0xdc12d70 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 00/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 01/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 02/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 03/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 04/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 05/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 06/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 07/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 08/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 09/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 10/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 11/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 12/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 13/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 14/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 15/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 16/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 17/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 18/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 19/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 20/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 21/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 22/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 23/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 24/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 25/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 26/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 27/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 28/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 29/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 30/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 31/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 32/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 33/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 34/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 35/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 36/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 37/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 38/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 39/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 40/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 41/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 42/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 43/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 44/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 45/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 46/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 47/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 48/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 49/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 50/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 51/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 52/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 53/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 54/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 55/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 56/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 57/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 58/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 59/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 60/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 61/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 62/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Channel 63/64 : 0
lrdn0786:1856354:1856444 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0786:1856354:1856444 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0786:1856354:1856444 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0786:1856354:1856450 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn0786:1856354:1856451 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn0786:1856354:1856444 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0786:1856354:1856444 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0786:1856354:1856444 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0786:1856354:1856444 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc12d70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc697b1de2d6fc4ff - Init COMPLETE
lrdn0786:1856354:1856444 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 04:26:56,724 | xffl.distributed.distributed |    DEBUG | [Rank 1]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 04:26:56,748 | xffl.distributed.distributed |    DEBUG | [Rank 2]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 04:26:56,936 | xffl.distributed.distributed |    DEBUG | [Rank 5]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 04:26:56,985 | xffl.distributed.distributed |    DEBUG | [Rank 4]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0378:1638354:1638354 [0] NCCL INFO Bootstrap: Using ib0:10.128.12.9<0>
lrdn0378:1638354:1638354 [0] NCCL INFO cudaDriverVersion 12020
lrdn0378:1638354:1638354 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0378:1638354:1638354 [0] NCCL INFO Comm config Blocking set to 1
lrdn0398:3637919:3637919 [0] NCCL INFO Bootstrap: Using ib0:10.128.12.89<0>
lrdn0398:3637919:3637919 [0] NCCL INFO cudaDriverVersion 12020
lrdn0398:3637919:3637919 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0398:3637919:3637919 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 04:26:57,159 | xffl.distributed.distributed |    DEBUG | [Rank 7]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0378:1638354:1638445 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0378:1638354:1638445 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0398:3637919:3638009 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0398:3637919:3638009 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 04:26:57,217 |         __main__ |    DEBUG | Model loading time: 3.25 seconds[0m
[38;5;39m2025-08-02 04:26:57,218 |         __main__ |    DEBUG | Training llama3.1-8b: 8030.26 million trainable parameters[0m
[38;5;39m2025-08-02 04:26:57,218 | xffl.distributed.distributed |    DEBUG | [Rank 0]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0378:1638354:1638445 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.12.9<0>
lrdn0378:1638354:1638445 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0378:1638354:1638445 [0] NCCL INFO Using network IB
lrdn0378:1638354:1638445 [0] NCCL INFO ncclCommInitRankConfig comm 0xdcec4d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb0fa2c32d390c939 - Init START
lrdn0378:1638354:1638445 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0378:1638354:1638445 [0] NCCL INFO Bootstrap timings total 0.000384 (create 0.000020, send 0.000057, recv 0.000086, ring 0.000001, delay 0.000000)
lrdn0905:2120263:2120263 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.69<0>
lrdn0905:2120263:2120263 [0] NCCL INFO cudaDriverVersion 12020
lrdn0905:2120263:2120263 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0905:2120263:2120263 [0] NCCL INFO Comm config Blocking set to 1
lrdn0378:1638354:1638445 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0378:1638354:1638445 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0378:1638354:1638445 [0] NCCL INFO comm 0xdcec4d0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 00/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 01/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 02/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 03/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 04/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 05/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 06/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 07/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 08/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 09/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 10/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 11/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 12/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 13/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 14/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 15/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 16/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 17/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 18/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 19/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 20/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 21/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 22/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 23/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 24/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 25/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 26/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 27/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 28/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 29/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 30/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 31/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 32/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 33/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 34/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 35/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 36/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 37/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 38/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 39/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 40/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 41/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 42/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 43/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 44/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 45/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 46/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 47/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 48/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 49/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 50/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 51/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 52/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 53/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 54/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 55/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 56/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 57/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 58/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 59/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 60/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 61/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 62/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Channel 63/64 : 0
lrdn0378:1638354:1638445 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0378:1638354:1638445 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0378:1638354:1638445 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0378:1638354:1638451 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0378:1638354:1638452 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0398:3637919:3638009 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.12.89<0>
lrdn0378:1638354:1638445 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0378:1638354:1638445 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0378:1638354:1638445 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0378:1638354:1638445 [0] NCCL INFO ncclCommInitRankConfig comm 0xdcec4d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb0fa2c32d390c939 - Init COMPLETE
lrdn0378:1638354:1638445 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0398:3637919:3638009 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0398:3637919:3638009 [0] NCCL INFO Using network IB
[38;5;39m2025-08-02 04:26:57,274 | xffl.distributed.distributed |    DEBUG | [Rank 6]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0398:3637919:3638009 [0] NCCL INFO ncclCommInitRankConfig comm 0xebd6540 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8fa18b0412160ce2 - Init START
lrdn0398:3637919:3638009 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0398:3637919:3638009 [0] NCCL INFO Bootstrap timings total 0.000404 (create 0.000022, send 0.000064, recv 0.000095, ring 0.000001, delay 0.000001)
lrdn0398:3637919:3638009 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0398:3637919:3638009 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0398:3637919:3638009 [0] NCCL INFO comm 0xebd6540 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 00/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 01/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 02/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 03/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 04/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 05/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 06/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 07/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 08/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 09/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 10/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 11/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 12/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 13/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 14/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 15/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 16/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 17/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 18/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 19/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 20/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 21/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 22/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 23/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 24/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 25/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 26/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 27/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 28/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 29/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 30/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 31/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 32/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 33/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 34/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 35/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 36/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 37/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 38/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 39/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 40/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 41/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 42/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 43/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 44/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 45/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 46/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 47/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 48/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 49/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 50/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 51/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 52/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 53/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 54/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 55/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 56/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 57/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 58/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 59/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 60/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 61/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 62/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Channel 63/64 : 0
lrdn0398:3637919:3638009 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0398:3637919:3638009 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0398:3637919:3638009 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0398:3637919:3638016 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn0398:3637919:3638015 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn0872:519656:519656 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.193<0>
lrdn0872:519656:519656 [0] NCCL INFO cudaDriverVersion 12020
lrdn0872:519656:519656 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0872:519656:519656 [0] NCCL INFO Comm config Blocking set to 1
lrdn0398:3637919:3638009 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0398:3637919:3638009 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0398:3637919:3638009 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0398:3637919:3638009 [0] NCCL INFO ncclCommInitRankConfig comm 0xebd6540 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8fa18b0412160ce2 - Init COMPLETE
lrdn0398:3637919:3638009 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn0905:2120263:2120353 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0905:2120263:2120353 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0905:2120263:2120353 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.69<0>
lrdn0905:2120263:2120353 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0905:2120263:2120353 [0] NCCL INFO Using network IB
lrdn0905:2120263:2120353 [0] NCCL INFO ncclCommInitRankConfig comm 0xd7043f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x2e190fc61fcfd89b - Init START
lrdn0905:2120263:2120353 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0905:2120263:2120353 [0] NCCL INFO Bootstrap timings total 0.000394 (create 0.000020, send 0.000061, recv 0.000095, ring 0.000001, delay 0.000000)
lrdn0872:519656:519747 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0872:519656:519747 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0905:2120263:2120353 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0905:2120263:2120353 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0905:2120263:2120353 [0] NCCL INFO comm 0xd7043f0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 00/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 01/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 02/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 03/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 04/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 05/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 06/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 07/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 08/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 09/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 10/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 11/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 12/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 13/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 14/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 15/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 16/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 17/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 18/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 19/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 20/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 21/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 22/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 23/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 24/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 25/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 26/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 27/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 28/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 29/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 30/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 31/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 32/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 33/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 34/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 35/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 36/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 37/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 38/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 39/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 40/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 41/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 42/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 43/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 44/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 45/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 46/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 47/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 48/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 49/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 50/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 51/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 52/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 53/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 54/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 55/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 56/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 57/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 58/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 59/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 60/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 61/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 62/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Channel 63/64 : 0
lrdn0905:2120263:2120353 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0905:2120263:2120353 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0905:2120263:2120353 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0905:2120263:2120359 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn0905:2120263:2120360 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn0905:2120263:2120353 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0905:2120263:2120353 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1067:2554046:2554046 [0] NCCL INFO Bootstrap: Using ib0:10.128.22.205<0>
lrdn0905:2120263:2120353 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0905:2120263:2120353 [0] NCCL INFO ncclCommInitRankConfig comm 0xd7043f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x2e190fc61fcfd89b - Init COMPLETE
lrdn0905:2120263:2120353 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn1067:2554046:2554046 [0] NCCL INFO cudaDriverVersion 12020
lrdn1067:2554046:2554046 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1067:2554046:2554046 [0] NCCL INFO Comm config Blocking set to 1
lrdn0872:519656:519747 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.193<0>
lrdn0872:519656:519747 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0872:519656:519747 [0] NCCL INFO Using network IB
lrdn0872:519656:519747 [0] NCCL INFO ncclCommInitRankConfig comm 0xd039110 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd23f522885202f99 - Init START
lrdn0872:519656:519747 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0872:519656:519747 [0] NCCL INFO Bootstrap timings total 0.000387 (create 0.000019, send 0.000063, recv 0.000085, ring 0.000001, delay 0.000000)
lrdn0872:519656:519747 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0872:519656:519747 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0872:519656:519747 [0] NCCL INFO comm 0xd039110 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 00/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 01/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 02/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 03/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 04/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 05/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 06/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 07/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 08/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 09/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 10/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 11/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 12/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 13/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 14/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 15/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 16/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 17/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 18/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 19/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 20/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 21/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 22/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 23/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 24/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 25/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 26/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 27/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 28/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 29/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 30/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 31/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 32/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 33/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 34/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 35/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 36/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 37/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 38/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 39/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 40/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 41/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 42/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 43/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 44/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 45/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 46/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 47/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 48/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 49/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 50/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 51/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 52/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 53/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 54/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 55/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 56/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 57/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 58/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 59/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 60/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 61/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 62/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Channel 63/64 : 0
lrdn0872:519656:519747 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0872:519656:519747 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0872:519656:519747 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0872:519656:519753 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0872:519656:519754 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0120:2076463:2076463 [0] NCCL INFO Bootstrap: Using ib0:10.128.8.1<0>
lrdn0120:2076463:2076463 [0] NCCL INFO cudaDriverVersion 12020
lrdn0120:2076463:2076463 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0120:2076463:2076463 [0] NCCL INFO Comm config Blocking set to 1
lrdn0872:519656:519747 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0872:519656:519747 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0872:519656:519747 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0872:519656:519747 [0] NCCL INFO ncclCommInitRankConfig comm 0xd039110 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd23f522885202f99 - Init COMPLETE
lrdn0872:519656:519747 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn1067:2554046:2554152 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1067:2554046:2554152 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1067:2554046:2554152 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.22.205<0>
lrdn1067:2554046:2554152 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1067:2554046:2554152 [0] NCCL INFO Using network IB
lrdn1067:2554046:2554152 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc9e8e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x71fa653eb7d3747e - Init START
lrdn1067:2554046:2554152 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1067:2554046:2554152 [0] NCCL INFO Bootstrap timings total 0.000412 (create 0.000022, send 0.000079, recv 0.000092, ring 0.000001, delay 0.000001)
lrdn0120:2076463:2076556 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0120:2076463:2076556 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1067:2554046:2554152 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1067:2554046:2554152 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1067:2554046:2554152 [0] NCCL INFO comm 0xdc9e8e0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 00/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 01/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 02/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 03/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 04/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 05/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 06/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 07/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 08/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 09/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 10/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 11/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 12/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 13/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 14/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 15/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 16/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 17/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 18/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 19/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 20/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 21/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 22/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 23/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 24/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 25/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 26/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 27/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 28/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 29/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 30/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 31/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 32/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 33/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 34/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 35/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 36/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 37/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 38/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 39/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 40/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 41/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 42/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 43/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 44/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 45/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 46/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 47/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 48/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 49/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 50/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 51/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 52/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 53/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 54/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 55/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 56/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 57/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 58/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 59/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 60/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 61/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 62/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Channel 63/64 : 0
lrdn1067:2554046:2554152 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1067:2554046:2554152 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1067:2554046:2554152 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1067:2554046:2554158 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn1067:2554046:2554159 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn1067:2554046:2554152 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1067:2554046:2554152 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1067:2554046:2554152 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1067:2554046:2554152 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc9e8e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x71fa653eb7d3747e - Init COMPLETE
lrdn1067:2554046:2554152 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0120:2076463:2076556 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.8.1<0>
lrdn0120:2076463:2076556 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0120:2076463:2076556 [0] NCCL INFO Using network IB
lrdn0120:2076463:2076556 [0] NCCL INFO ncclCommInitRankConfig comm 0xd21ba40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5c679f9865f36b05 - Init START
lrdn0120:2076463:2076556 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0120:2076463:2076556 [0] NCCL INFO Bootstrap timings total 0.000408 (create 0.000021, send 0.000067, recv 0.000089, ring 0.000002, delay 0.000000)
lrdn1057:3554428:3554428 [0] NCCL INFO Bootstrap: Using ib0:10.128.22.165<0>
lrdn1057:3554428:3554428 [0] NCCL INFO cudaDriverVersion 12020
lrdn1057:3554428:3554428 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1057:3554428:3554428 [0] NCCL INFO Comm config Blocking set to 1
lrdn0120:2076463:2076556 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0120:2076463:2076556 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0120:2076463:2076556 [0] NCCL INFO comm 0xd21ba40 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 00/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 01/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 02/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 03/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 04/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 05/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 06/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 07/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 08/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 09/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 10/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 11/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 12/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 13/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 14/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 15/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 16/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 17/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 18/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 19/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 20/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 21/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 22/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 23/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 24/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 25/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 26/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 27/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 28/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 29/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 30/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 31/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 32/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 33/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 34/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 35/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 36/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 37/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 38/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 39/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 40/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 41/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 42/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 43/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 44/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 45/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 46/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 47/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 48/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 49/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 50/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 51/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 52/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 53/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 54/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 55/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 56/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 57/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 58/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 59/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 60/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 61/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 62/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Channel 63/64 : 0
lrdn0120:2076463:2076556 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0120:2076463:2076556 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0120:2076463:2076556 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0120:2076463:2076563 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0120:2076463:2076562 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn0120:2076463:2076556 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0120:2076463:2076556 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0120:2076463:2076556 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0120:2076463:2076556 [0] NCCL INFO ncclCommInitRankConfig comm 0xd21ba40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5c679f9865f36b05 - Init COMPLETE
lrdn0120:2076463:2076556 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1057:3554428:3554518 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1057:3554428:3554518 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1057:3554428:3554518 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.22.165<0>
lrdn1057:3554428:3554518 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1057:3554428:3554518 [0] NCCL INFO Using network IB
lrdn1057:3554428:3554518 [0] NCCL INFO ncclCommInitRankConfig comm 0xcd1b400 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc7615452bc92c75b - Init START
lrdn1057:3554428:3554518 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1057:3554428:3554518 [0] NCCL INFO Bootstrap timings total 0.000444 (create 0.000024, send 0.000072, recv 0.000110, ring 0.000001, delay 0.000001)
lrdn1057:3554428:3554518 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1057:3554428:3554518 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1057:3554428:3554518 [0] NCCL INFO comm 0xcd1b400 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 00/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 01/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 02/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 03/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 04/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 05/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 06/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 07/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 08/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 09/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 10/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 11/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 12/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 13/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 14/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 15/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 16/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 17/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 18/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 19/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 20/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 21/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 22/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 23/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 24/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 25/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 26/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 27/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 28/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 29/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 30/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 31/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 32/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 33/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 34/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 35/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 36/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 37/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 38/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 39/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 40/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 41/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 42/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 43/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 44/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 45/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 46/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 47/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 48/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 49/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 50/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 51/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 52/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 53/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 54/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 55/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 56/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 57/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 58/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 59/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 60/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 61/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 62/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Channel 63/64 : 0
lrdn1057:3554428:3554518 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1057:3554428:3554518 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1057:3554428:3554518 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1057:3554428:3554528 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn1057:3554428:3554529 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn1057:3554428:3554518 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1057:3554428:3554518 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1057:3554428:3554518 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1057:3554428:3554518 [0] NCCL INFO ncclCommInitRankConfig comm 0xcd1b400 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc7615452bc92c75b - Init COMPLETE
lrdn1057:3554428:3554518 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 04:27:03,728 |         __main__ |    DEBUG | FSDP wrapping setup time: 6.51 seconds[0m
[38;5;39m2025-08-02 04:27:03,741 |         __main__ |    DEBUG | Total setup time: 17.64 seconds[0m
[38;5;39m2025-08-02 04:27:03,741 |         __main__ |    DEBUG | GPU RAM allocated before training: 16.06 GB[0m


lrdn0120:2076463:2076463 [0] NCCL INFO Comm config Blocking set to 1
lrdn0120:2076463:2076567 [0] NCCL INFO Using network IB
lrdn0120:2076463:2076567 [0] NCCL INFO ncclCommInitRankConfig comm 0x15f81490 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x72de173ce71fd1fb - Init START
lrdn0786:1856354:1856354 [0] NCCL INFO Comm config Blocking set to 1
lrdn0786:1856354:1856454 [0] NCCL INFO Using network IB
lrdn0378:1638354:1638354 [0] NCCL INFO Comm config Blocking set to 1
lrdn0905:2120263:2120263 [0] NCCL INFO Comm config Blocking set to 1
lrdn0872:519656:519656 [0] NCCL INFO Comm config Blocking set to 1
lrdn0872:519656:519760 [0] NCCL INFO Using network IB
lrdn0786:1856354:1856454 [0] NCCL INFO ncclCommInitRankConfig comm 0x2185ed00 rank 3 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x72de173ce71fd1fb - Init START
lrdn0378:1638354:1638460 [0] NCCL INFO Using network IB
lrdn0872:519656:519760 [0] NCCL INFO ncclCommInitRankConfig comm 0xd486250 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x72de173ce71fd1fb - Init START
lrdn0905:2120263:2120364 [0] NCCL INFO Using network IB
lrdn1067:2554046:2554046 [0] NCCL INFO Comm config Blocking set to 1
lrdn0378:1638354:1638460 [0] NCCL INFO ncclCommInitRankConfig comm 0xe138980 rank 1 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x72de173ce71fd1fb - Init START
lrdn0398:3637919:3637919 [0] NCCL INFO Comm config Blocking set to 1
lrdn0398:3637919:3638020 [0] NCCL INFO Using network IB
lrdn0398:3637919:3638020 [0] NCCL INFO ncclCommInitRankConfig comm 0x1691af00 rank 2 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x72de173ce71fd1fb - Init START
lrdn1067:2554046:2554162 [0] NCCL INFO Using network IB
lrdn0905:2120263:2120364 [0] NCCL INFO ncclCommInitRankConfig comm 0x16467be0 rank 5 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x72de173ce71fd1fb - Init START
lrdn1067:2554046:2554162 [0] NCCL INFO ncclCommInitRankConfig comm 0x2ae7f150 rank 7 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x72de173ce71fd1fb - Init START
lrdn1057:3554428:3554428 [0] NCCL INFO Comm config Blocking set to 1
lrdn1057:3554428:3554532 [0] NCCL INFO Using network IB
lrdn1057:3554428:3554532 [0] NCCL INFO ncclCommInitRankConfig comm 0x296fa700 rank 6 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x72de173ce71fd1fb - Init START
lrdn0120:2076463:2076567 [0] NCCL INFO Bootstrap timings total 0.002163 (create 0.000017, send 0.000055, recv 0.001145, ring 0.000338, delay 0.000000)
lrdn0786:1856354:1856454 [0] NCCL INFO Bootstrap timings total 0.002011 (create 0.000021, send 0.000504, recv 0.000460, ring 0.000827, delay 0.000000)
lrdn0398:3637919:3638020 [0] NCCL INFO Bootstrap timings total 0.001911 (create 0.000023, send 0.000405, recv 0.000404, ring 0.000629, delay 0.000000)
lrdn0378:1638354:1638460 [0] NCCL INFO Bootstrap timings total 0.001899 (create 0.000018, send 0.000679, recv 0.000405, ring 0.000696, delay 0.000000)
lrdn1067:2554046:2554162 [0] NCCL INFO Bootstrap timings total 0.001804 (create 0.000025, send 0.000749, recv 0.000539, ring 0.000372, delay 0.000000)
lrdn0905:2120263:2120364 [0] NCCL INFO Bootstrap timings total 0.001851 (create 0.000022, send 0.000390, recv 0.000748, ring 0.000226, delay 0.000000)
lrdn0872:519656:519760 [0] NCCL INFO Bootstrap timings total 0.002019 (create 0.000022, send 0.000556, recv 0.000442, ring 0.000777, delay 0.000000)
lrdn1057:3554428:3554532 [0] NCCL INFO Bootstrap timings total 0.002009 (create 0.000023, send 0.000866, recv 0.000506, ring 0.000209, delay 0.000000)
lrdn0378:1638354:1638460 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0378:1638354:1638460 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn0786:1856354:1856454 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0786:1856354:1856454 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn0872:519656:519760 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0398:3637919:3638020 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0398:3637919:3638020 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn0872:519656:519760 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn0905:2120263:2120364 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0905:2120263:2120364 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn1057:3554428:3554532 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1057:3554428:3554532 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn0120:2076463:2076567 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0120:2076463:2076567 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn1067:2554046:2554162 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1067:2554046:2554162 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn0905:2120263:2120364 [0] NCCL INFO comm 0x16467be0 rank 5 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0905:2120263:2120364 [0] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] -1/-1/-1->5->6 [2] 6/4/-1->5->3 [3] 6/4/-1->5->3
lrdn0905:2120263:2120364 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0398:3637919:3638020 [0] NCCL INFO comm 0x1691af00 rank 2 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0398:3637919:3638020 [0] NCCL INFO Trees [0] 1/3/-1->2->4 [1] 1/3/-1->2->4 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1
lrdn0398:3637919:3638020 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1057:3554428:3554532 [0] NCCL INFO comm 0x296fa700 rank 6 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn1057:3554428:3554532 [0] NCCL INFO Trees [0] 5/7/-1->6->4 [1] 5/7/-1->6->4 [2] -1/-1/-1->6->5 [3] -1/-1/-1->6->5
lrdn0872:519656:519760 [0] NCCL INFO comm 0xd486250 rank 4 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0872:519656:519760 [0] NCCL INFO Trees [0] 2/6/-1->4->0 [1] 2/6/-1->4->0 [2] -1/-1/-1->4->5 [3] -1/-1/-1->4->5
lrdn0872:519656:519760 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0905:2120263:2120365 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0872:519656:519761 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1067:2554046:2554162 [0] NCCL INFO comm 0x2ae7f150 rank 7 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn1067:2554046:2554162 [0] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 3/-1/-1->7->-1 [3] 3/-1/-1->7->-1
lrdn1067:2554046:2554162 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1057:3554428:3554532 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1057:3554428:3554534 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0398:3637919:3638021 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0398:3637919:3638022 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0378:1638354:1638460 [0] NCCL INFO comm 0xe138980 rank 1 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0378:1638354:1638460 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] -1/-1/-1->1->2 [2] 2/0/-1->1->3 [3] 2/0/-1->1->3
lrdn0378:1638354:1638460 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0378:1638354:1638461 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0378:1638354:1638462 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0786:1856354:1856454 [0] NCCL INFO comm 0x2185ed00 rank 3 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0786:1856354:1856454 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 5/1/-1->3->7 [3] 5/1/-1->3->7
lrdn0786:1856354:1856454 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0786:1856354:1856455 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0786:1856354:1856456 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn1067:2554046:2554164 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn1067:2554046:2554163 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1057:3554428:3554533 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0872:519656:519762 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0120:2076463:2076567 [0] NCCL INFO comm 0x15f81490 rank 0 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0120:2076463:2076567 [0] NCCL INFO Channel 00/04 : 0 1 2 3 4 5 6 7
lrdn0120:2076463:2076567 [0] NCCL INFO Channel 01/04 : 0 1 2 3 4 5 6 7
lrdn0120:2076463:2076567 [0] NCCL INFO Channel 02/04 : 0 1 2 3 4 5 6 7
lrdn0120:2076463:2076567 [0] NCCL INFO Channel 03/04 : 0 1 2 3 4 5 6 7
lrdn0120:2076463:2076567 [0] NCCL INFO Trees [0] 4/-1/-1->0->-1 [1] 4/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
lrdn0120:2076463:2076567 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0120:2076463:2076567 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0120:2076463:2076568 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn0120:2076463:2076569 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn0905:2120263:2120366 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0120:2076463:2076567 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0120:2076463:2076567 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0120:2076463:2076567 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             0              1              0              0              0              0              0  
       Reduce |        0         0         1   |             0              1              0              0              0              0              0  
    AllGather |        0         0         1   |             0              1              0              0              0              0              0  
ReduceScatter |        0         0         1   |             0              1              0              0              0              0              0  
    AllReduce |        0         0         1   |             0              1              0              0              0              0              0  

lrdn0120:2076463:2076567 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0120:2076463:2076567 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0120:2076463:2076567 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1067:2554046:2554162 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1067:2554046:2554162 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1067:2554046:2554162 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn1067:2554046:2554162 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0786:1856354:1856454 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0786:1856354:1856454 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0786:1856354:1856454 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0786:1856354:1856454 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0872:519656:519760 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0872:519656:519760 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0872:519656:519760 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0872:519656:519760 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0378:1638354:1638460 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0378:1638354:1638460 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0378:1638354:1638460 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0378:1638354:1638460 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1057:3554428:3554532 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1057:3554428:3554532 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1057:3554428:3554532 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn1057:3554428:3554532 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0905:2120263:2120364 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0905:2120263:2120364 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0905:2120263:2120364 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0905:2120263:2120364 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0120:2076463:2076567 [0] NCCL INFO ncclCommInitRankConfig comm 0x15f81490 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x72de173ce71fd1fb - Init COMPLETE
lrdn0120:2076463:2076567 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0398:3637919:3638020 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0398:3637919:3638020 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0398:3637919:3638020 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0398:3637919:3638020 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1067:2554046:2554162 [0] NCCL INFO ncclCommInitRankConfig comm 0x2ae7f150 rank 7 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x72de173ce71fd1fb - Init COMPLETE
lrdn1067:2554046:2554162 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 7 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0786:1856354:1856454 [0] NCCL INFO ncclCommInitRankConfig comm 0x2185ed00 rank 3 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x72de173ce71fd1fb - Init COMPLETE
lrdn0786:1856354:1856454 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.02, connections 0.00, rest 0.00)
lrdn0872:519656:519760 [0] NCCL INFO ncclCommInitRankConfig comm 0xd486250 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x72de173ce71fd1fb - Init COMPLETE
lrdn0872:519656:519760 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 4 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.02, connections 0.00, rest 0.00)
lrdn0378:1638354:1638460 [0] NCCL INFO ncclCommInitRankConfig comm 0xe138980 rank 1 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x72de173ce71fd1fb - Init COMPLETE
lrdn0378:1638354:1638460 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.02, connections 0.00, rest 0.00)
lrdn0905:2120263:2120364 [0] NCCL INFO ncclCommInitRankConfig comm 0x16467be0 rank 5 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x72de173ce71fd1fb - Init COMPLETE
lrdn0905:2120263:2120364 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 5 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1057:3554428:3554532 [0] NCCL INFO ncclCommInitRankConfig comm 0x296fa700 rank 6 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x72de173ce71fd1fb - Init COMPLETE
lrdn1057:3554428:3554532 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 6 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0398:3637919:3638020 [0] NCCL INFO ncclCommInitRankConfig comm 0x1691af00 rank 2 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x72de173ce71fd1fb - Init COMPLETE
lrdn0398:3637919:3638020 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.02, connections 0.00, rest 0.00)
lrdn0786:1856354:1856458 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 10
lrdn0786:1856354:1856457 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn0786:1856354:1856457 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn0872:519656:519764 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
lrdn0872:519656:519763 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [receive] via NET/IB/5
lrdn0786:1856354:1856457 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn0786:1856354:1856457 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn0786:1856354:1856457 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [send] via NET/IB/5
lrdn0872:519656:519763 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [receive] via NET/IB/6
lrdn0872:519656:519763 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [receive] via NET/IB/5
lrdn0786:1856354:1856457 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [send] via NET/IB/6
lrdn0872:519656:519763 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [receive] via NET/IB/6
lrdn0872:519656:519763 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [send] via NET/IB/5
lrdn0786:1856354:1856457 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [send] via NET/IB/5
lrdn0120:2076463:2076570 [0] NCCL INFO Channel 00/0 : 7[0] -> 0[0] [receive] via NET/IB/5
lrdn0872:519656:519763 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [send] via NET/IB/6
lrdn0786:1856354:1856457 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [send] via NET/IB/6
lrdn0872:519656:519763 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [send] via NET/IB/5
lrdn0120:2076463:2076571 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 10
lrdn0905:2120263:2120367 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [receive] via NET/IB/5
lrdn0872:519656:519763 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [send] via NET/IB/6
lrdn0398:3637919:3638024 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 3
lrdn0398:3637919:3638023 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn0398:3637919:3638023 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn1067:2554046:2554166 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 2
lrdn0398:3637919:3638023 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn1067:2554046:2554165 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [receive] via NET/IB/5
lrdn0120:2076463:2076570 [0] NCCL INFO Channel 01/0 : 7[0] -> 0[0] [receive] via NET/IB/6
lrdn0398:3637919:3638023 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn1067:2554046:2554165 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [receive] via NET/IB/6
lrdn0120:2076463:2076570 [0] NCCL INFO Channel 02/0 : 7[0] -> 0[0] [receive] via NET/IB/5
lrdn0398:3637919:3638023 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn0905:2120263:2120368 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
lrdn0905:2120263:2120367 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [receive] via NET/IB/6
lrdn0905:2120263:2120367 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [receive] via NET/IB/5
lrdn0905:2120263:2120367 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [receive] via NET/IB/6
lrdn0905:2120263:2120367 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [send] via NET/IB/5
lrdn0905:2120263:2120367 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [send] via NET/IB/6
lrdn0905:2120263:2120367 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [send] via NET/IB/5
lrdn0905:2120263:2120367 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [send] via NET/IB/6
lrdn1057:3554428:3554535 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [receive] via NET/IB/5
lrdn1057:3554428:3554536 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
lrdn1057:3554428:3554535 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [receive] via NET/IB/6
lrdn1057:3554428:3554535 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [receive] via NET/IB/5
lrdn1067:2554046:2554165 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [receive] via NET/IB/5
lrdn0120:2076463:2076570 [0] NCCL INFO Channel 03/0 : 7[0] -> 0[0] [receive] via NET/IB/6
lrdn1057:3554428:3554535 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [receive] via NET/IB/6
lrdn0398:3637919:3638023 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn1057:3554428:3554535 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [send] via NET/IB/5
lrdn1057:3554428:3554535 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [send] via NET/IB/6
lrdn1067:2554046:2554165 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [receive] via NET/IB/6
lrdn1067:2554046:2554165 [0] NCCL INFO Channel 00/0 : 7[0] -> 0[0] [send] via NET/IB/5
lrdn0120:2076463:2076570 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn0398:3637919:3638023 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn1057:3554428:3554535 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [send] via NET/IB/5
lrdn0120:2076463:2076570 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1067:2554046:2554165 [0] NCCL INFO Channel 01/0 : 7[0] -> 0[0] [send] via NET/IB/6
lrdn0398:3637919:3638023 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn0120:2076463:2076570 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1057:3554428:3554535 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [send] via NET/IB/6
lrdn1067:2554046:2554165 [0] NCCL INFO Channel 02/0 : 7[0] -> 0[0] [send] via NET/IB/5
lrdn0120:2076463:2076570 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn0378:1638354:1638463 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1067:2554046:2554165 [0] NCCL INFO Channel 03/0 : 7[0] -> 0[0] [send] via NET/IB/6
lrdn0378:1638354:1638464 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 6
lrdn0378:1638354:1638463 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn0378:1638354:1638463 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn0378:1638354:1638463 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn0378:1638354:1638463 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn0378:1638354:1638463 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn0378:1638354:1638463 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn0378:1638354:1638463 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn0786:1856354:1856457 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0905:2120263:2120367 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0398:3637919:3638023 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1057:3554428:3554535 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0872:519656:519763 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0120:2076463:2076570 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1067:2554046:2554165 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0378:1638354:1638463 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
[38;20m2025-08-02 04:27:20,722 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 105.05/108.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:27:37,953 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 103.05/107.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
lrdn0120:2076463:2076463 [0] NCCL INFO Comm config Blocking set to 1
lrdn1057:3554428:3554428 [0] NCCL INFO Comm config Blocking set to 1
lrdn0398:3637919:3637919 [0] NCCL INFO Comm config Blocking set to 1
lrdn0120:2076463:2076575 [0] NCCL INFO Using network IB
lrdn0786:1856354:1856354 [0] NCCL INFO Comm config Blocking set to 1
lrdn1057:3554428:3554542 [0] NCCL INFO Using network IB
lrdn0872:519656:519656 [0] NCCL INFO Comm config Blocking set to 1
lrdn0872:519656:519770 [0] NCCL INFO Using network IB
lrdn0905:2120263:2120263 [0] NCCL INFO Comm config Blocking set to 1
lrdn0786:1856354:1856462 [0] NCCL INFO Using network IB
lrdn0398:3637919:3638029 [0] NCCL INFO Using network IB
lrdn0378:1638354:1638354 [0] NCCL INFO Comm config Blocking set to 1
lrdn0378:1638354:1638470 [0] NCCL INFO Using network IB
lrdn0905:2120263:2120372 [0] NCCL INFO Using network IB
lrdn1067:2554046:2554046 [0] NCCL INFO Comm config Blocking set to 1
lrdn1067:2554046:2554202 [0] NCCL INFO Using network IB
lrdn0120:2076463:2076575 [0] NCCL INFO ncclCommInitRankConfig comm 0x114b24c0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd743084ebb58b493 - Init START
lrdn0872:519656:519770 [0] NCCL INFO ncclCommInitRankConfig comm 0xd68d7b0 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd743084ebb58b493 - Init START
lrdn1057:3554428:3554542 [0] NCCL INFO ncclCommInitRankConfig comm 0xc033cb0 rank 6 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd743084ebb58b493 - Init START
lrdn0398:3637919:3638029 [0] NCCL INFO ncclCommInitRankConfig comm 0x16b22b40 rank 2 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd743084ebb58b493 - Init START
lrdn0786:1856354:1856462 [0] NCCL INFO ncclCommInitRankConfig comm 0x2aead710 rank 3 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd743084ebb58b493 - Init START
lrdn0378:1638354:1638470 [0] NCCL INFO ncclCommInitRankConfig comm 0xe340630 rank 1 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd743084ebb58b493 - Init START
lrdn0905:2120263:2120372 [0] NCCL INFO ncclCommInitRankConfig comm 0xc99b100 rank 5 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd743084ebb58b493 - Init START
lrdn1067:2554046:2554202 [0] NCCL INFO ncclCommInitRankConfig comm 0xcfb8ba0 rank 7 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd743084ebb58b493 - Init START
lrdn0120:2076463:2076575 [0] NCCL INFO Bootstrap timings total 0.001825 (create 0.000012, send 0.000046, recv 0.000944, ring 0.000727, delay 0.000000)
lrdn1067:2554046:2554202 [0] NCCL INFO Bootstrap timings total 0.001465 (create 0.000013, send 0.000082, recv 0.000408, ring 0.000811, delay 0.000000)
lrdn0378:1638354:1638470 [0] NCCL INFO Bootstrap timings total 0.001633 (create 0.000018, send 0.000402, recv 0.000436, ring 0.000353, delay 0.000000)
lrdn1057:3554428:3554542 [0] NCCL INFO Bootstrap timings total 0.001743 (create 0.000020, send 0.000097, recv 0.000563, ring 0.000745, delay 0.000000)
lrdn0398:3637919:3638029 [0] NCCL INFO Bootstrap timings total 0.001800 (create 0.000021, send 0.000068, recv 0.000203, ring 0.000487, delay 0.000000)
lrdn0786:1856354:1856462 [0] NCCL INFO Bootstrap timings total 0.001904 (create 0.000014, send 0.000067, recv 0.000361, ring 0.001210, delay 0.000000)
lrdn0872:519656:519770 [0] NCCL INFO Bootstrap timings total 0.001949 (create 0.000016, send 0.000257, recv 0.000522, ring 0.000913, delay 0.000000)
lrdn0905:2120263:2120372 [0] NCCL INFO Bootstrap timings total 0.001723 (create 0.000016, send 0.000204, recv 0.000470, ring 0.000894, delay 0.000000)
lrdn0120:2076463:2076575 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1067:2554046:2554202 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0398:3637919:3638029 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0786:1856354:1856462 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1057:3554428:3554542 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0905:2120263:2120372 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0872:519656:519770 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0378:1638354:1638470 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0120:2076463:2076575 [0] NCCL INFO comm 0x114b24c0 rank 0 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0120:2076463:2076575 [0] NCCL INFO Channel 00/04 : 0 1 2 3 4 5 6 7
lrdn0120:2076463:2076575 [0] NCCL INFO Channel 01/04 : 0 1 2 3 4 5 6 7
lrdn0120:2076463:2076575 [0] NCCL INFO Channel 02/04 : 0 1 2 3 4 5 6 7
lrdn0120:2076463:2076575 [0] NCCL INFO Channel 03/04 : 0 1 2 3 4 5 6 7
lrdn0120:2076463:2076575 [0] NCCL INFO Trees [0] 4/-1/-1->0->-1 [1] 4/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
lrdn0120:2076463:2076575 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0120:2076463:2076575 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1067:2554046:2554202 [0] NCCL INFO comm 0xcfb8ba0 rank 7 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn1067:2554046:2554202 [0] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 3/-1/-1->7->-1 [3] 3/-1/-1->7->-1
lrdn1067:2554046:2554202 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1057:3554428:3554542 [0] NCCL INFO comm 0xc033cb0 rank 6 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn1057:3554428:3554542 [0] NCCL INFO Trees [0] 5/7/-1->6->4 [1] 5/7/-1->6->4 [2] -1/-1/-1->6->5 [3] -1/-1/-1->6->5
lrdn1057:3554428:3554542 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0120:2076463:2076577 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn1067:2554046:2554203 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1067:2554046:2554204 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn1057:3554428:3554543 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0120:2076463:2076576 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn1057:3554428:3554544 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0398:3637919:3638029 [0] NCCL INFO comm 0x16b22b40 rank 2 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0398:3637919:3638029 [0] NCCL INFO Trees [0] 1/3/-1->2->4 [1] 1/3/-1->2->4 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1
lrdn0786:1856354:1856462 [0] NCCL INFO comm 0x2aead710 rank 3 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0786:1856354:1856462 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 5/1/-1->3->7 [3] 5/1/-1->3->7
lrdn0786:1856354:1856462 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0786:1856354:1856463 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0786:1856354:1856464 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0398:3637919:3638029 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0398:3637919:3638030 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0398:3637919:3638031 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0872:519656:519770 [0] NCCL INFO comm 0xd68d7b0 rank 4 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0872:519656:519770 [0] NCCL INFO Trees [0] 2/6/-1->4->0 [1] 2/6/-1->4->0 [2] -1/-1/-1->4->5 [3] -1/-1/-1->4->5
lrdn0872:519656:519770 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0872:519656:519772 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0378:1638354:1638470 [0] NCCL INFO comm 0xe340630 rank 1 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0378:1638354:1638470 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] -1/-1/-1->1->2 [2] 2/0/-1->1->3 [3] 2/0/-1->1->3
lrdn0378:1638354:1638470 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0378:1638354:1638471 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0378:1638354:1638472 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0872:519656:519771 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0905:2120263:2120372 [0] NCCL INFO comm 0xc99b100 rank 5 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0905:2120263:2120372 [0] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] -1/-1/-1->5->6 [2] 6/4/-1->5->3 [3] 6/4/-1->5->3
lrdn0905:2120263:2120372 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0905:2120263:2120373 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0905:2120263:2120374 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0120:2076463:2076575 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0120:2076463:2076575 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0120:2076463:2076575 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             0              1              0              0              0              0              0  
       Reduce |        0         0         1   |             0              1              0              0              0              0              0  
    AllGather |        0         0         1   |             0              1              0              0              0              0              0  
ReduceScatter |        0         0         1   |             0              1              0              0              0              0              0  
    AllReduce |        0         0         1   |             0              1              0              0              0              0              0  

lrdn0378:1638354:1638470 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0378:1638354:1638470 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0378:1638354:1638470 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0378:1638354:1638470 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0120:2076463:2076575 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0120:2076463:2076575 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0398:3637919:3638029 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0398:3637919:3638029 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0398:3637919:3638029 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0398:3637919:3638029 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0120:2076463:2076575 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0786:1856354:1856462 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0786:1856354:1856462 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0786:1856354:1856462 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0786:1856354:1856462 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0378:1638354:1638470 [0] NCCL INFO ncclCommInitRankConfig comm 0xe340630 rank 1 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd743084ebb58b493 - Init COMPLETE
lrdn0378:1638354:1638470 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0398:3637919:3638029 [0] NCCL INFO ncclCommInitRankConfig comm 0x16b22b40 rank 2 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd743084ebb58b493 - Init COMPLETE
lrdn0398:3637919:3638029 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0120:2076463:2076575 [0] NCCL INFO ncclCommInitRankConfig comm 0x114b24c0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd743084ebb58b493 - Init COMPLETE
lrdn0120:2076463:2076575 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0786:1856354:1856462 [0] NCCL INFO ncclCommInitRankConfig comm 0x2aead710 rank 3 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd743084ebb58b493 - Init COMPLETE
lrdn0786:1856354:1856462 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1067:2554046:2554202 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1067:2554046:2554202 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1067:2554046:2554202 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn1067:2554046:2554202 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1057:3554428:3554542 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1057:3554428:3554542 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1057:3554428:3554542 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn1057:3554428:3554542 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0905:2120263:2120372 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0905:2120263:2120372 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0905:2120263:2120372 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0905:2120263:2120372 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1067:2554046:2554202 [0] NCCL INFO ncclCommInitRankConfig comm 0xcfb8ba0 rank 7 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd743084ebb58b493 - Init COMPLETE
lrdn1067:2554046:2554202 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 7 nranks 8 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0872:519656:519770 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0872:519656:519770 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0872:519656:519770 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0872:519656:519770 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1057:3554428:3554542 [0] NCCL INFO ncclCommInitRankConfig comm 0xc033cb0 rank 6 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd743084ebb58b493 - Init COMPLETE
lrdn1057:3554428:3554542 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 6 nranks 8 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0905:2120263:2120372 [0] NCCL INFO ncclCommInitRankConfig comm 0xc99b100 rank 5 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd743084ebb58b493 - Init COMPLETE
lrdn0905:2120263:2120372 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 5 nranks 8 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0872:519656:519770 [0] NCCL INFO ncclCommInitRankConfig comm 0xd68d7b0 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd743084ebb58b493 - Init COMPLETE
lrdn0872:519656:519770 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 4 nranks 8 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0786:1856354:1856466 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 7
lrdn0786:1856354:1856465 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn0786:1856354:1856465 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn0786:1856354:1856465 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn0786:1856354:1856465 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn0786:1856354:1856465 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [send] via NET/IB/5
lrdn0786:1856354:1856465 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [send] via NET/IB/6
lrdn0786:1856354:1856465 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [send] via NET/IB/5
lrdn0398:3637919:3638032 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn0398:3637919:3638032 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn0398:3637919:3638033 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
lrdn0398:3637919:3638032 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn0398:3637919:3638032 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn0398:3637919:3638032 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn0398:3637919:3638032 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn0398:3637919:3638032 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn0398:3637919:3638032 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn0786:1856354:1856465 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [send] via NET/IB/6
lrdn0378:1638354:1638473 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn0378:1638354:1638474 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 5
lrdn0120:2076463:2076578 [0] NCCL INFO Channel 00/0 : 7[0] -> 0[0] [receive] via NET/IB/5
lrdn0378:1638354:1638473 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn0120:2076463:2076578 [0] NCCL INFO Channel 01/0 : 7[0] -> 0[0] [receive] via NET/IB/6
lrdn0120:2076463:2076579 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 9
lrdn0120:2076463:2076578 [0] NCCL INFO Channel 02/0 : 7[0] -> 0[0] [receive] via NET/IB/5
lrdn0378:1638354:1638473 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn0378:1638354:1638473 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn0120:2076463:2076578 [0] NCCL INFO Channel 03/0 : 7[0] -> 0[0] [receive] via NET/IB/6
lrdn0378:1638354:1638473 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn0378:1638354:1638473 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn0120:2076463:2076578 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn0378:1638354:1638473 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn0378:1638354:1638473 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn0120:2076463:2076578 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn0120:2076463:2076578 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn0120:2076463:2076578 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1057:3554428:3554545 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [receive] via NET/IB/5
lrdn0905:2120263:2120376 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 7
lrdn0872:519656:519773 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [receive] via NET/IB/5
lrdn0872:519656:519774 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 6
lrdn1057:3554428:3554546 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 6
lrdn1057:3554428:3554545 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [receive] via NET/IB/6
lrdn1057:3554428:3554545 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [receive] via NET/IB/5
lrdn1067:2554046:2554205 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [receive] via NET/IB/5
lrdn1057:3554428:3554545 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [receive] via NET/IB/6
lrdn0872:519656:519773 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [receive] via NET/IB/6
lrdn1067:2554046:2554205 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [receive] via NET/IB/6
lrdn1067:2554046:2554205 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [receive] via NET/IB/5
lrdn1057:3554428:3554545 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [send] via NET/IB/5
lrdn1057:3554428:3554545 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [send] via NET/IB/6
lrdn0872:519656:519773 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [receive] via NET/IB/5
lrdn0905:2120263:2120375 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [receive] via NET/IB/5
lrdn0872:519656:519773 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [receive] via NET/IB/6
lrdn1067:2554046:2554206 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
lrdn1067:2554046:2554205 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [receive] via NET/IB/6
lrdn1067:2554046:2554205 [0] NCCL INFO Channel 00/0 : 7[0] -> 0[0] [send] via NET/IB/5
lrdn1057:3554428:3554545 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [send] via NET/IB/5
lrdn1057:3554428:3554545 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [send] via NET/IB/6
lrdn0872:519656:519773 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [send] via NET/IB/5
lrdn0905:2120263:2120375 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [receive] via NET/IB/6
lrdn1067:2554046:2554205 [0] NCCL INFO Channel 01/0 : 7[0] -> 0[0] [send] via NET/IB/6
lrdn0872:519656:519773 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [send] via NET/IB/6
lrdn1067:2554046:2554205 [0] NCCL INFO Channel 02/0 : 7[0] -> 0[0] [send] via NET/IB/5
lrdn1067:2554046:2554205 [0] NCCL INFO Channel 03/0 : 7[0] -> 0[0] [send] via NET/IB/6
lrdn0905:2120263:2120375 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [receive] via NET/IB/5
lrdn0872:519656:519773 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [send] via NET/IB/5
lrdn0905:2120263:2120375 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [receive] via NET/IB/6
lrdn0872:519656:519773 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [send] via NET/IB/6
lrdn0905:2120263:2120375 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [send] via NET/IB/5
lrdn0905:2120263:2120375 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [send] via NET/IB/6
lrdn0905:2120263:2120375 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [send] via NET/IB/5
lrdn0905:2120263:2120375 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [send] via NET/IB/6
lrdn0786:1856354:1856465 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0120:2076463:2076578 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1057:3554428:3554545 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0398:3637919:3638032 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0905:2120263:2120375 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0872:519656:519773 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0378:1638354:1638473 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1067:2554046:2554205 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0120:2076463:2076463 [0] NCCL INFO Comm config Blocking set to 1
lrdn0872:519656:519656 [0] NCCL INFO Comm config Blocking set to 1
lrdn0378:1638354:1638354 [0] NCCL INFO Comm config Blocking set to 1
lrdn1057:3554428:3554428 [0] NCCL INFO Comm config Blocking set to 1
lrdn1057:3554428:3554547 [0] NCCL INFO Using network IB
lrdn0905:2120263:2120263 [0] NCCL INFO Comm config Blocking set to 1
lrdn0398:3637919:3637919 [0] NCCL INFO Comm config Blocking set to 1
lrdn0786:1856354:1856354 [0] NCCL INFO Comm config Blocking set to 1
lrdn0905:2120263:2120377 [0] NCCL INFO Using network IB
lrdn0378:1638354:1638475 [0] NCCL INFO Using network IB
lrdn0872:519656:519775 [0] NCCL INFO Using network IB
lrdn0120:2076463:2076581 [0] NCCL INFO Using network IB
lrdn0120:2076463:2076581 [0] NCCL INFO ncclCommInitRankConfig comm 0x1152e050 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50bb09d047e07015 - Init START
lrdn0398:3637919:3638034 [0] NCCL INFO Using network IB
lrdn0872:519656:519775 [0] NCCL INFO ncclCommInitRankConfig comm 0xd709340 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50bb09d047e07015 - Init START
lrdn0378:1638354:1638475 [0] NCCL INFO ncclCommInitRankConfig comm 0xe3bc1c0 rank 1 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50bb09d047e07015 - Init START
lrdn1067:2554046:2554046 [0] NCCL INFO Comm config Blocking set to 1
lrdn1067:2554046:2554207 [0] NCCL INFO Using network IB
lrdn0905:2120263:2120377 [0] NCCL INFO ncclCommInitRankConfig comm 0xca16c90 rank 5 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50bb09d047e07015 - Init START
lrdn1057:3554428:3554547 [0] NCCL INFO ncclCommInitRankConfig comm 0xc0af840 rank 6 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50bb09d047e07015 - Init START
lrdn1067:2554046:2554207 [0] NCCL INFO ncclCommInitRankConfig comm 0xd034730 rank 7 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50bb09d047e07015 - Init START
lrdn0398:3637919:3638034 [0] NCCL INFO ncclCommInitRankConfig comm 0x16b9e6d0 rank 2 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50bb09d047e07015 - Init START
lrdn0786:1856354:1856467 [0] NCCL INFO Using network IB
lrdn0786:1856354:1856467 [0] NCCL INFO ncclCommInitRankConfig comm 0x2af292a0 rank 3 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50bb09d047e07015 - Init START
lrdn0378:1638354:1638475 [0] NCCL INFO Bootstrap timings total 0.001044 (create 0.000019, send 0.000060, recv 0.000509, ring 0.000355, delay 0.000000)
lrdn0398:3637919:3638034 [0] NCCL INFO Bootstrap timings total 0.000982 (create 0.000022, send 0.000078, recv 0.000448, ring 0.000310, delay 0.000000)
lrdn0786:1856354:1856467 [0] NCCL INFO Bootstrap timings total 0.000976 (create 0.000013, send 0.000069, recv 0.000480, ring 0.000313, delay 0.000000)
lrdn0905:2120263:2120377 [0] NCCL INFO Bootstrap timings total 0.001091 (create 0.000019, send 0.000065, recv 0.000216, ring 0.000573, delay 0.000000)
lrdn0872:519656:519775 [0] NCCL INFO Bootstrap timings total 0.001100 (create 0.000025, send 0.000072, recv 0.000290, ring 0.000315, delay 0.000000)
lrdn1057:3554428:3554547 [0] NCCL INFO Bootstrap timings total 0.001257 (create 0.000019, send 0.000083, recv 0.000531, ring 0.000510, delay 0.000000)
lrdn0120:2076463:2076581 [0] NCCL INFO Bootstrap timings total 0.001421 (create 0.000012, send 0.000053, recv 0.000265, ring 0.000603, delay 0.000000)
lrdn1067:2554046:2554207 [0] NCCL INFO Bootstrap timings total 0.001176 (create 0.000015, send 0.000058, recv 0.000384, ring 0.000563, delay 0.000000)
lrdn0786:1856354:1856467 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0872:519656:519775 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0905:2120263:2120377 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0398:3637919:3638034 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1057:3554428:3554547 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1067:2554046:2554207 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0120:2076463:2076581 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0378:1638354:1638475 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1067:2554046:2554207 [0] NCCL INFO comm 0xd034730 rank 7 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn1067:2554046:2554207 [0] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 3/-1/-1->7->-1 [3] 3/-1/-1->7->-1
lrdn1067:2554046:2554207 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0905:2120263:2120377 [0] NCCL INFO comm 0xca16c90 rank 5 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0905:2120263:2120377 [0] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] -1/-1/-1->5->6 [2] 6/4/-1->5->3 [3] 6/4/-1->5->3
lrdn0905:2120263:2120377 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0120:2076463:2076581 [0] NCCL INFO comm 0x1152e050 rank 0 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0120:2076463:2076581 [0] NCCL INFO Channel 00/04 : 0 1 2 3 4 5 6 7
lrdn0120:2076463:2076581 [0] NCCL INFO Channel 01/04 : 0 1 2 3 4 5 6 7
lrdn0120:2076463:2076581 [0] NCCL INFO Channel 02/04 : 0 1 2 3 4 5 6 7
lrdn0120:2076463:2076581 [0] NCCL INFO Channel 03/04 : 0 1 2 3 4 5 6 7
lrdn0120:2076463:2076581 [0] NCCL INFO Trees [0] 4/-1/-1->0->-1 [1] 4/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
lrdn0120:2076463:2076581 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0120:2076463:2076581 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0120:2076463:2076582 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn0905:2120263:2120378 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0905:2120263:2120379 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn1067:2554046:2554208 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0378:1638354:1638475 [0] NCCL INFO comm 0xe3bc1c0 rank 1 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0378:1638354:1638475 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] -1/-1/-1->1->2 [2] 2/0/-1->1->3 [3] 2/0/-1->1->3
lrdn0378:1638354:1638475 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1057:3554428:3554547 [0] NCCL INFO comm 0xc0af840 rank 6 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn1057:3554428:3554547 [0] NCCL INFO Trees [0] 5/7/-1->6->4 [1] 5/7/-1->6->4 [2] -1/-1/-1->6->5 [3] -1/-1/-1->6->5
lrdn1057:3554428:3554547 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1057:3554428:3554548 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn0872:519656:519775 [0] NCCL INFO comm 0xd709340 rank 4 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0872:519656:519775 [0] NCCL INFO Trees [0] 2/6/-1->4->0 [1] 2/6/-1->4->0 [2] -1/-1/-1->4->5 [3] -1/-1/-1->4->5
lrdn0872:519656:519775 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1057:3554428:3554549 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn0378:1638354:1638476 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn0398:3637919:3638034 [0] NCCL INFO comm 0x16b9e6d0 rank 2 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0398:3637919:3638034 [0] NCCL INFO Trees [0] 1/3/-1->2->4 [1] 1/3/-1->2->4 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1
lrdn0398:3637919:3638034 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0398:3637919:3638036 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn1067:2554046:2554209 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0378:1638354:1638477 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn0120:2076463:2076583 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 12
lrdn0872:519656:519776 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn0872:519656:519777 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn0398:3637919:3638035 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn0786:1856354:1856467 [0] NCCL INFO comm 0x2af292a0 rank 3 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0786:1856354:1856467 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 5/1/-1->3->7 [3] 5/1/-1->3->7
lrdn0786:1856354:1856467 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0786:1856354:1856468 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0786:1856354:1856469 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn0905:2120263:2120377 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0905:2120263:2120377 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0905:2120263:2120377 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0905:2120263:2120377 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1057:3554428:3554547 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1057:3554428:3554547 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1057:3554428:3554547 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn1057:3554428:3554547 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1067:2554046:2554207 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1067:2554046:2554207 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1067:2554046:2554207 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn1067:2554046:2554207 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0120:2076463:2076581 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0120:2076463:2076581 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0120:2076463:2076581 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             0              1              0              0              0              0              0  
       Reduce |        0         0         1   |             0              1              0              0              0              0              0  
    AllGather |        0         0         1   |             0              1              0              0              0              0              0  
ReduceScatter |        0         0         1   |             0              1              0              0              0              0              0  
    AllReduce |        0         0         1   |             0              1              0              0              0              0              0  

lrdn0120:2076463:2076581 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0120:2076463:2076581 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0378:1638354:1638475 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0378:1638354:1638475 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0378:1638354:1638475 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0378:1638354:1638475 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0398:3637919:3638034 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0398:3637919:3638034 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0786:1856354:1856467 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0786:1856354:1856467 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0398:3637919:3638034 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0398:3637919:3638034 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0786:1856354:1856467 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0786:1856354:1856467 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0120:2076463:2076581 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0872:519656:519775 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0872:519656:519775 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0872:519656:519775 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0872:519656:519775 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1057:3554428:3554547 [0] NCCL INFO ncclCommInitRankConfig comm 0xc0af840 rank 6 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50bb09d047e07015 - Init COMPLETE
lrdn1057:3554428:3554547 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 6 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0905:2120263:2120377 [0] NCCL INFO ncclCommInitRankConfig comm 0xca16c90 rank 5 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50bb09d047e07015 - Init COMPLETE
lrdn1067:2554046:2554207 [0] NCCL INFO ncclCommInitRankConfig comm 0xd034730 rank 7 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50bb09d047e07015 - Init COMPLETE
lrdn1067:2554046:2554207 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 7 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0905:2120263:2120377 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 5 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0120:2076463:2076581 [0] NCCL INFO ncclCommInitRankConfig comm 0x1152e050 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50bb09d047e07015 - Init COMPLETE
lrdn0378:1638354:1638475 [0] NCCL INFO ncclCommInitRankConfig comm 0xe3bc1c0 rank 1 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50bb09d047e07015 - Init COMPLETE
lrdn0378:1638354:1638475 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0120:2076463:2076581 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0398:3637919:3638034 [0] NCCL INFO ncclCommInitRankConfig comm 0x16b9e6d0 rank 2 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50bb09d047e07015 - Init COMPLETE
lrdn0398:3637919:3638034 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0786:1856354:1856467 [0] NCCL INFO ncclCommInitRankConfig comm 0x2af292a0 rank 3 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50bb09d047e07015 - Init COMPLETE
lrdn0786:1856354:1856467 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0872:519656:519775 [0] NCCL INFO ncclCommInitRankConfig comm 0xd709340 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50bb09d047e07015 - Init COMPLETE
lrdn0872:519656:519775 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 4 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0398:3637919:3638038 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 8
lrdn0398:3637919:3638037 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn0786:1856354:1856471 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 9
lrdn0786:1856354:1856470 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn0786:1856354:1856470 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn0398:3637919:3638037 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn0786:1856354:1856470 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn0398:3637919:3638037 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn0786:1856354:1856470 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn0398:3637919:3638037 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn0786:1856354:1856470 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [send] via NET/IB/5
lrdn0398:3637919:3638037 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn0786:1856354:1856470 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [send] via NET/IB/6
lrdn0398:3637919:3638037 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn0786:1856354:1856470 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [send] via NET/IB/5
lrdn0398:3637919:3638037 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn0786:1856354:1856470 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [send] via NET/IB/6
lrdn0398:3637919:3638037 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn0378:1638354:1638478 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn0378:1638354:1638479 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 11
lrdn0120:2076463:2076585 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 0
lrdn0120:2076463:2076584 [0] NCCL INFO Channel 00/0 : 7[0] -> 0[0] [receive] via NET/IB/5
lrdn0120:2076463:2076584 [0] NCCL INFO Channel 01/0 : 7[0] -> 0[0] [receive] via NET/IB/6
lrdn0120:2076463:2076584 [0] NCCL INFO Channel 02/0 : 7[0] -> 0[0] [receive] via NET/IB/5
lrdn0120:2076463:2076584 [0] NCCL INFO Channel 03/0 : 7[0] -> 0[0] [receive] via NET/IB/6
lrdn0120:2076463:2076584 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn0120:2076463:2076584 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn0120:2076463:2076584 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn0378:1638354:1638478 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn0120:2076463:2076584 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1057:3554428:3554551 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 9
lrdn0905:2120263:2120381 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 10
lrdn0905:2120263:2120380 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [receive] via NET/IB/5
lrdn0872:519656:519779 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 9
lrdn0378:1638354:1638478 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn0378:1638354:1638478 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn0905:2120263:2120380 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [receive] via NET/IB/6
lrdn1067:2554046:2554211 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 7
lrdn1067:2554046:2554210 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [receive] via NET/IB/5
lrdn1067:2554046:2554210 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [receive] via NET/IB/6
lrdn1067:2554046:2554210 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [receive] via NET/IB/5
lrdn0905:2120263:2120380 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [receive] via NET/IB/5
lrdn0378:1638354:1638478 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn0378:1638354:1638478 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn0905:2120263:2120380 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [receive] via NET/IB/6
lrdn1067:2554046:2554210 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [receive] via NET/IB/6
lrdn1067:2554046:2554210 [0] NCCL INFO Channel 00/0 : 7[0] -> 0[0] [send] via NET/IB/5
lrdn0378:1638354:1638478 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn0378:1638354:1638478 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn0905:2120263:2120380 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [send] via NET/IB/5
lrdn1067:2554046:2554210 [0] NCCL INFO Channel 01/0 : 7[0] -> 0[0] [send] via NET/IB/6
lrdn1067:2554046:2554210 [0] NCCL INFO Channel 02/0 : 7[0] -> 0[0] [send] via NET/IB/5
lrdn1057:3554428:3554550 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [receive] via NET/IB/5
lrdn0905:2120263:2120380 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [send] via NET/IB/6
lrdn0905:2120263:2120380 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [send] via NET/IB/5
lrdn1057:3554428:3554550 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [receive] via NET/IB/6
lrdn0872:519656:519778 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [receive] via NET/IB/5
lrdn0905:2120263:2120380 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [send] via NET/IB/6
lrdn1067:2554046:2554210 [0] NCCL INFO Channel 03/0 : 7[0] -> 0[0] [send] via NET/IB/6
lrdn1057:3554428:3554550 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [receive] via NET/IB/5
lrdn0872:519656:519778 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [receive] via NET/IB/6
lrdn1057:3554428:3554550 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [receive] via NET/IB/6
lrdn0872:519656:519778 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [receive] via NET/IB/5
lrdn0872:519656:519778 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [receive] via NET/IB/6
lrdn1057:3554428:3554550 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [send] via NET/IB/5
lrdn0872:519656:519778 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [send] via NET/IB/5
lrdn1057:3554428:3554550 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [send] via NET/IB/6
lrdn1057:3554428:3554550 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [send] via NET/IB/5
lrdn0872:519656:519778 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [send] via NET/IB/6
lrdn1057:3554428:3554550 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [send] via NET/IB/6
lrdn0872:519656:519778 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [send] via NET/IB/5
lrdn0872:519656:519778 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [send] via NET/IB/6
lrdn0786:1856354:1856470 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0905:2120263:2120380 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0398:3637919:3638037 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0120:2076463:2076584 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0378:1638354:1638478 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1057:3554428:3554550 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0872:519656:519778 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1067:2554046:2554210 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0120:2076463:2076463 [0] NCCL INFO Comm config Blocking set to 1
lrdn1067:2554046:2554046 [0] NCCL INFO Comm config Blocking set to 1
lrdn1057:3554428:3554428 [0] NCCL INFO Comm config Blocking set to 1
lrdn0905:2120263:2120263 [0] NCCL INFO Comm config Blocking set to 1
lrdn0786:1856354:1856354 [0] NCCL INFO Comm config Blocking set to 1
lrdn1067:2554046:2554212 [0] NCCL INFO Using network IB
lrdn0120:2076463:2076587 [0] NCCL INFO Using network IB
lrdn0120:2076463:2076587 [0] NCCL INFO ncclCommInitRankConfig comm 0x115a9be0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9be0adc8184cdff - Init START
lrdn0378:1638354:1638354 [0] NCCL INFO Comm config Blocking set to 1
lrdn0378:1638354:1638480 [0] NCCL INFO Using network IB
lrdn0905:2120263:2120382 [0] NCCL INFO Using network IB
lrdn1057:3554428:3554552 [0] NCCL INFO Using network IB
lrdn0378:1638354:1638480 [0] NCCL INFO ncclCommInitRankConfig comm 0xe437d50 rank 1 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9be0adc8184cdff - Init START
lrdn1067:2554046:2554212 [0] NCCL INFO ncclCommInitRankConfig comm 0xd0b02c0 rank 7 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9be0adc8184cdff - Init START
lrdn1057:3554428:3554552 [0] NCCL INFO ncclCommInitRankConfig comm 0xc12b3d0 rank 6 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9be0adc8184cdff - Init START
lrdn0872:519656:519656 [0] NCCL INFO Comm config Blocking set to 1
lrdn0905:2120263:2120382 [0] NCCL INFO ncclCommInitRankConfig comm 0xca92820 rank 5 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9be0adc8184cdff - Init START
lrdn0398:3637919:3637919 [0] NCCL INFO Comm config Blocking set to 1
lrdn0786:1856354:1856472 [0] NCCL INFO Using network IB
lrdn0786:1856354:1856472 [0] NCCL INFO ncclCommInitRankConfig comm 0x2afa4e30 rank 3 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9be0adc8184cdff - Init START
lrdn0398:3637919:3638039 [0] NCCL INFO Using network IB
lrdn0398:3637919:3638039 [0] NCCL INFO ncclCommInitRankConfig comm 0x16c1a260 rank 2 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9be0adc8184cdff - Init START
lrdn0872:519656:519780 [0] NCCL INFO Using network IB
lrdn0872:519656:519780 [0] NCCL INFO ncclCommInitRankConfig comm 0xd784ed0 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9be0adc8184cdff - Init START
lrdn0398:3637919:3638039 [0] NCCL INFO Bootstrap timings total 0.001084 (create 0.000027, send 0.000063, recv 0.000187, ring 0.000619, delay 0.000000)
lrdn0378:1638354:1638480 [0] NCCL INFO Bootstrap timings total 0.001515 (create 0.000019, send 0.000059, recv 0.000462, ring 0.000866, delay 0.000000)
lrdn0786:1856354:1856472 [0] NCCL INFO Bootstrap timings total 0.001376 (create 0.000017, send 0.000059, recv 0.000473, ring 0.000702, delay 0.000000)
lrdn0120:2076463:2076587 [0] NCCL INFO Bootstrap timings total 0.001707 (create 0.000013, send 0.000045, recv 0.000289, ring 0.000924, delay 0.000000)
lrdn0872:519656:519780 [0] NCCL INFO Bootstrap timings total 0.001253 (create 0.000017, send 0.000304, recv 0.000363, ring 0.000442, delay 0.000000)
lrdn0905:2120263:2120382 [0] NCCL INFO Bootstrap timings total 0.001502 (create 0.000017, send 0.000064, recv 0.000204, ring 0.000438, delay 0.000000)
lrdn1067:2554046:2554212 [0] NCCL INFO Bootstrap timings total 0.001726 (create 0.000015, send 0.000061, recv 0.000444, ring 0.001096, delay 0.000000)
lrdn1057:3554428:3554552 [0] NCCL INFO Bootstrap timings total 0.001652 (create 0.000023, send 0.000077, recv 0.000244, ring 0.001149, delay 0.000000)
lrdn0120:2076463:2076587 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1067:2554046:2554212 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0398:3637919:3638039 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1057:3554428:3554552 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0378:1638354:1638480 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0786:1856354:1856472 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0872:519656:519780 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0905:2120263:2120382 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1057:3554428:3554552 [0] NCCL INFO comm 0xc12b3d0 rank 6 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn1057:3554428:3554552 [0] NCCL INFO Trees [0] 5/7/-1->6->4 [1] 5/7/-1->6->4 [2] -1/-1/-1->6->5 [3] -1/-1/-1->6->5
lrdn1057:3554428:3554552 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0872:519656:519780 [0] NCCL INFO comm 0xd784ed0 rank 4 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0872:519656:519780 [0] NCCL INFO Trees [0] 2/6/-1->4->0 [1] 2/6/-1->4->0 [2] -1/-1/-1->4->5 [3] -1/-1/-1->4->5
lrdn0872:519656:519780 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0872:519656:519782 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn0905:2120263:2120382 [0] NCCL INFO comm 0xca92820 rank 5 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0905:2120263:2120382 [0] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] -1/-1/-1->5->6 [2] 6/4/-1->5->3 [3] 6/4/-1->5->3
lrdn0905:2120263:2120382 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0905:2120263:2120383 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn1067:2554046:2554212 [0] NCCL INFO comm 0xd0b02c0 rank 7 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn1067:2554046:2554212 [0] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 3/-1/-1->7->-1 [3] 3/-1/-1->7->-1
lrdn1067:2554046:2554212 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0120:2076463:2076587 [0] NCCL INFO comm 0x115a9be0 rank 0 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0120:2076463:2076587 [0] NCCL INFO Channel 00/04 : 0 1 2 3 4 5 6 7
lrdn0120:2076463:2076587 [0] NCCL INFO Channel 01/04 : 0 1 2 3 4 5 6 7
lrdn0120:2076463:2076587 [0] NCCL INFO Channel 02/04 : 0 1 2 3 4 5 6 7
lrdn0120:2076463:2076587 [0] NCCL INFO Channel 03/04 : 0 1 2 3 4 5 6 7
lrdn0120:2076463:2076587 [0] NCCL INFO Trees [0] 4/-1/-1->0->-1 [1] 4/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
lrdn0120:2076463:2076587 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0120:2076463:2076587 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0120:2076463:2076588 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0872:519656:519781 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn1057:3554428:3554554 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn1067:2554046:2554213 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn0120:2076463:2076589 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn0786:1856354:1856472 [0] NCCL INFO comm 0x2afa4e30 rank 3 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0786:1856354:1856472 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 5/1/-1->3->7 [3] 5/1/-1->3->7
lrdn0786:1856354:1856472 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0786:1856354:1856474 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 12
lrdn0786:1856354:1856473 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn0398:3637919:3638039 [0] NCCL INFO comm 0x16c1a260 rank 2 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0398:3637919:3638039 [0] NCCL INFO Trees [0] 1/3/-1->2->4 [1] 1/3/-1->2->4 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1
lrdn0398:3637919:3638039 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0398:3637919:3638040 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn0398:3637919:3638041 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn0905:2120263:2120384 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn0378:1638354:1638480 [0] NCCL INFO comm 0xe437d50 rank 1 nRanks 8 nNodes 8 localRanks 1 localRank 0 MNNVL 0
lrdn0378:1638354:1638480 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] -1/-1/-1->1->2 [2] 2/0/-1->1->3 [3] 2/0/-1->1->3
lrdn0378:1638354:1638480 [0] NCCL INFO P2P Chunksize set to 131072
lrdn0378:1638354:1638481 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1067:2554046:2554214 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn1057:3554428:3554553 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn0378:1638354:1638482 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn0398:3637919:3638039 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0398:3637919:3638039 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0398:3637919:3638039 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0398:3637919:3638039 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0786:1856354:1856472 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0786:1856354:1856472 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0786:1856354:1856472 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0786:1856354:1856472 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0398:3637919:3638039 [0] NCCL INFO ncclCommInitRankConfig comm 0x16c1a260 rank 2 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9be0adc8184cdff - Init COMPLETE
lrdn0398:3637919:3638039 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0872:519656:519780 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0872:519656:519780 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0872:519656:519780 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0872:519656:519780 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0905:2120263:2120382 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0905:2120263:2120382 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0905:2120263:2120382 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0905:2120263:2120382 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0378:1638354:1638480 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0378:1638354:1638480 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0378:1638354:1638480 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0378:1638354:1638480 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0786:1856354:1856472 [0] NCCL INFO ncclCommInitRankConfig comm 0x2afa4e30 rank 3 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9be0adc8184cdff - Init COMPLETE
lrdn0786:1856354:1856472 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1067:2554046:2554212 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1067:2554046:2554212 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1067:2554046:2554212 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn1067:2554046:2554212 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn0872:519656:519780 [0] NCCL INFO ncclCommInitRankConfig comm 0xd784ed0 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9be0adc8184cdff - Init COMPLETE
lrdn0872:519656:519780 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 4 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0905:2120263:2120382 [0] NCCL INFO ncclCommInitRankConfig comm 0xca92820 rank 5 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9be0adc8184cdff - Init COMPLETE
lrdn0905:2120263:2120382 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 5 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0378:1638354:1638480 [0] NCCL INFO ncclCommInitRankConfig comm 0xe437d50 rank 1 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9be0adc8184cdff - Init COMPLETE
lrdn0378:1638354:1638480 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0120:2076463:2076587 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn0120:2076463:2076587 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn0120:2076463:2076587 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             0              1              0              0              0              0              0  
       Reduce |        0         0         1   |             0              1              0              0              0              0              0  
    AllGather |        0         0         1   |             0              1              0              0              0              0              0  
ReduceScatter |        0         0         1   |             0              1              0              0              0              0              0  
    AllReduce |        0         0         1   |             0              1              0              0              0              0              0  

lrdn0120:2076463:2076587 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn0120:2076463:2076587 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1057:3554428:3554552 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1057:3554428:3554552 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1057:3554428:3554552 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
lrdn1057:3554428:3554552 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1067:2554046:2554212 [0] NCCL INFO ncclCommInitRankConfig comm 0xd0b02c0 rank 7 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9be0adc8184cdff - Init COMPLETE
lrdn1067:2554046:2554212 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 7 nranks 8 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0120:2076463:2076587 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0120:2076463:2076587 [0] NCCL INFO ncclCommInitRankConfig comm 0x115a9be0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9be0adc8184cdff - Init COMPLETE
lrdn0120:2076463:2076587 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 8 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1057:3554428:3554552 [0] NCCL INFO ncclCommInitRankConfig comm 0xc12b3d0 rank 6 nranks 8 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9be0adc8184cdff - Init COMPLETE
lrdn1057:3554428:3554552 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 6 nranks 8 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn0786:1856354:1856476 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 13
lrdn0786:1856354:1856475 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn0786:1856354:1856475 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn0786:1856354:1856475 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn0398:3637919:3638043 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 12
lrdn0398:3637919:3638042 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn0786:1856354:1856475 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn0786:1856354:1856475 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [send] via NET/IB/5
lrdn0398:3637919:3638042 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn0398:3637919:3638042 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn0786:1856354:1856475 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [send] via NET/IB/6
lrdn0398:3637919:3638042 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn0786:1856354:1856475 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [send] via NET/IB/5
lrdn0398:3637919:3638042 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn0786:1856354:1856475 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [send] via NET/IB/6
lrdn0398:3637919:3638042 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn0398:3637919:3638042 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn0398:3637919:3638042 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn0872:519656:519783 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [receive] via NET/IB/5
lrdn0872:519656:519784 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 12
lrdn0872:519656:519783 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [receive] via NET/IB/6
lrdn0872:519656:519783 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [receive] via NET/IB/5
lrdn0872:519656:519783 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [receive] via NET/IB/6
lrdn0872:519656:519783 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [send] via NET/IB/5
lrdn0872:519656:519783 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [send] via NET/IB/6
lrdn0872:519656:519783 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [send] via NET/IB/5
lrdn0872:519656:519783 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [send] via NET/IB/6
lrdn0905:2120263:2120385 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [receive] via NET/IB/5
lrdn0905:2120263:2120385 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [receive] via NET/IB/6
lrdn0905:2120263:2120386 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 12
lrdn0905:2120263:2120385 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [receive] via NET/IB/5
lrdn0905:2120263:2120385 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [receive] via NET/IB/6
lrdn0378:1638354:1638484 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 9
lrdn0378:1638354:1638483 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn0378:1638354:1638483 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn0378:1638354:1638483 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn0905:2120263:2120385 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [send] via NET/IB/5
lrdn0905:2120263:2120385 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [send] via NET/IB/6
lrdn0905:2120263:2120385 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [send] via NET/IB/5
lrdn0378:1638354:1638483 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn0378:1638354:1638483 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn0378:1638354:1638483 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn0378:1638354:1638483 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn0378:1638354:1638483 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn0905:2120263:2120385 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [send] via NET/IB/6
lrdn1057:3554428:3554555 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [receive] via NET/IB/5
lrdn1057:3554428:3554555 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [receive] via NET/IB/6
lrdn1057:3554428:3554556 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 13
lrdn1057:3554428:3554555 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [receive] via NET/IB/5
lrdn1057:3554428:3554555 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [receive] via NET/IB/6
lrdn1057:3554428:3554555 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [send] via NET/IB/5
lrdn1057:3554428:3554555 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [send] via NET/IB/6
lrdn1057:3554428:3554555 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [send] via NET/IB/5
lrdn1057:3554428:3554555 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [send] via NET/IB/6
lrdn1067:2554046:2554215 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [receive] via NET/IB/5
lrdn1067:2554046:2554216 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 12
lrdn1067:2554046:2554215 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [receive] via NET/IB/6
lrdn0120:2076463:2076590 [0] NCCL INFO Channel 00/0 : 7[0] -> 0[0] [receive] via NET/IB/5
lrdn0120:2076463:2076590 [0] NCCL INFO Channel 01/0 : 7[0] -> 0[0] [receive] via NET/IB/6
lrdn1067:2554046:2554215 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [receive] via NET/IB/5
lrdn0120:2076463:2076590 [0] NCCL INFO Channel 02/0 : 7[0] -> 0[0] [receive] via NET/IB/5
lrdn0120:2076463:2076590 [0] NCCL INFO Channel 03/0 : 7[0] -> 0[0] [receive] via NET/IB/6
lrdn1067:2554046:2554215 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [receive] via NET/IB/6
lrdn1067:2554046:2554215 [0] NCCL INFO Channel 00/0 : 7[0] -> 0[0] [send] via NET/IB/5
lrdn0120:2076463:2076590 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn0120:2076463:2076591 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 5
lrdn1067:2554046:2554215 [0] NCCL INFO Channel 01/0 : 7[0] -> 0[0] [send] via NET/IB/6
lrdn1067:2554046:2554215 [0] NCCL INFO Channel 02/0 : 7[0] -> 0[0] [send] via NET/IB/5
lrdn0120:2076463:2076590 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn0120:2076463:2076590 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1067:2554046:2554215 [0] NCCL INFO Channel 03/0 : 7[0] -> 0[0] [send] via NET/IB/6
lrdn0120:2076463:2076590 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn0786:1856354:1856475 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0905:2120263:2120385 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0378:1638354:1638483 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0120:2076463:2076590 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0398:3637919:3638042 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1057:3554428:3554555 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn0872:519656:519783 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1067:2554046:2554215 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
[38;20m2025-08-02 04:27:54,055 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 111.36/111.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:28:10,088 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 110.23/113.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:28:27,341 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 101.42/107.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:28:44,424 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 103.01/108.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:29:00,571 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 112.30/112.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:29:17,017 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 109.30/109.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:29:33,912 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 107.92/105.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:29:51,035 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 107.58/102.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:30:07,097 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 112.52/111.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:30:23,139 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 111.96/111.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:30:40,011 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 106.16/107.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:30:56,828 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 107.01/107.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:31:13,052 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 110.24/111.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:31:29,157 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 112.98/113.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:31:46,163 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 104.09/107.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:32:03,311 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 103.59/104.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:32:19,375 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 110.52/112.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:32:35,415 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 111.11/112.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:32:52,470 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 104.24/106.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:33:09,134 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 107.64/107.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:33:25,321 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 111.22/111.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:33:41,560 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 112.07/109.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:33:58,538 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 107.40/103.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:34:15,540 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 107.43/103.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:34:31,843 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 109.98/108.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:34:48,145 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 110.81/109.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:35:05,109 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 107.56/103.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:35:22,221 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 107.13/102.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:35:38,447 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 110.10/111.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:35:54,599 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 111.74/112.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:36:11,545 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 108.10/104.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:36:28,641 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 106.31/105.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:36:44,702 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 111.80/112.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:37:00,688 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 110.69/113.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:37:17,701 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 105.68/107.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:37:34,863 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 102.41/107.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:37:51,114 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 111.17/110.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:38:07,259 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 112.52/111.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:38:24,289 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 107.48/104.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:38:41,342 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 106.41/102.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:38:57,646 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 110.47/108.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:39:13,943 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 110.68/109.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:39:30,935 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 108.23/103.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:39:48,063 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 105.32/102.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:40:04,299 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 110.02/109.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:40:20,327 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 113.12/112.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:40:37,492 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 107.64/103.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:40:54,322 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 105.57/108.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:41:10,422 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 111.78/112.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:41:26,535 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 112.70/112.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:41:43,441 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 104.72/108.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:42:00,351 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 105.29/105.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:42:16,525 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 110.20/110.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:42:32,573 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 111.49/112.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:42:49,697 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 107.23/104.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:43:06,533 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 106.39/109.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:43:22,250 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 116.44/112.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:43:37,865 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.96 (max/real adjusted throughput: 114.89/114.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:43:53,340 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.96 (max/real adjusted throughput: 118.20/114.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:44:08,589 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.90 (max/real adjusted throughput: 118.35/118.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:44:24,054 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.93 (max/real adjusted throughput: 116.41/116.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:44:39,549 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.93 (max/real adjusted throughput: 115.41/116.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:44:55,021 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.91 (max/real adjusted throughput: 113.52/118.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:45:12,023 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 101.51/105.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:45:27,919 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 112.44/113.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:45:44,037 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 112.26/112.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:46:01,075 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 107.72/103.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:46:18,184 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 104.99/103.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:46:34,607 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 109.57/108.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:46:50,986 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 110.74/108.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:47:08,125 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 105.89/104.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:47:25,207 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 105.41/103.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:47:41,247 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 111.98/112.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:47:57,582 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 108.80/109.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:48:14,503 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 106.72/106.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:48:31,726 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 103.80/105.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:48:47,963 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 110.53/110.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:49:04,413 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 107.59/109.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:49:21,455 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 104.74/107.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:49:38,386 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 104.31/107.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:49:54,564 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 109.72/112.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:50:10,675 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 111.31/112.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:50:27,629 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 107.84/107.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:50:44,635 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 107.53/108.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:51:00,605 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 113.83/112.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:51:16,912 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 110.53/108.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:51:33,846 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 108.24/103.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:51:50,944 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 108.45/102.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:52:07,107 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 110.26/111.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:52:23,091 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 113.15/110.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:52:40,023 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 108.87/103.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:52:57,224 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 106.41/102.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:53:13,575 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 109.85/110.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:53:29,693 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 111.22/113.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:53:46,755 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 105.10/107.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:54:03,905 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 105.25/106.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:54:19,781 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.97 (max/real adjusted throughput: 112.89/113.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:54:35,850 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 111.88/111.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:54:52,889 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 104.59/107.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:55:10,007 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 104.68/107.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:55:25,961 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 113.80/112.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:55:42,071 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 112.72/110.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:55:59,025 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 109.06/103.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:56:15,912 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 108.60/103.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:56:31,944 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 112.01/110.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:56:47,925 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 113.64/110.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:57:05,088 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 106.39/102.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:57:22,195 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 104.60/103.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:57:38,218 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 112.19/111.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:57:54,201 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 112.89/113.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:58:11,354 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 105.72/106.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:58:28,583 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 103.08/105.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:58:44,745 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 108.96/112.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:59:00,673 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 111.89/113.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 04:59:18,157 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 98.47/106.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:59:34,911 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 106.94/106.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 04:59:50,940 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 111.99/112.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:00:06,970 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 112.88/111.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:00:24,151 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 105.31/101.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:00:41,219 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 107.95/103.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:00:57,275 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 113.39/110.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:01:13,295 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 112.58/110.91 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:01:30,339 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 106.68/104.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:01:47,412 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 106.90/103.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:02:03,484 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 112.65/111.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:02:19,600 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 112.15/111.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:02:36,317 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 107.53/109.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:02:53,278 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 105.77/107.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:03:09,344 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 112.47/112.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:03:25,329 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 111.45/112.74 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:03:42,281 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 103.17/107.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:03:59,042 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 105.00/110.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:04:15,081 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.97 (max/real adjusted throughput: 109.26/114.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:04:30,940 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.97 (max/real adjusted throughput: 113.10/114.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:04:47,848 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 109.00/104.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:05:04,667 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 108.29/107.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:05:20,639 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 112.18/113.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:05:36,447 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 114.07/113.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:05:52,988 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 108.60/109.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:06:09,694 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 108.26/108.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:06:25,557 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 115.15/113.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:06:41,565 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 110.94/113.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:06:58,518 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 103.46/108.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:07:15,474 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 102.57/108.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:07:31,526 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 111.18/113.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:07:47,437 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.97 (max/real adjusted throughput: 111.45/114.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:08:04,483 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 104.84/107.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:08:21,345 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 104.78/108.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:08:37,537 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 110.11/112.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:08:53,628 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 113.36/110.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:09:10,619 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 107.00/102.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:09:27,485 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 108.24/104.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:09:43,396 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 113.33/112.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:09:59,390 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 114.93/110.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:10:16,646 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 105.42/103.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:10:33,801 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 107.61/101.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:10:49,762 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 113.96/112.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:11:05,652 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.97 (max/real adjusted throughput: 113.96/114.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:11:22,609 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 106.27/105.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:11:39,632 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 106.74/106.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:11:55,505 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 114.74/112.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:12:11,380 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 113.87/112.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:12:28,387 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 105.82/105.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:12:45,379 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 104.56/106.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:13:01,352 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 112.74/112.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:13:17,253 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 112.82/113.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:13:34,217 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 106.26/107.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:13:51,197 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 107.05/104.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:14:07,284 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 110.76/111.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:14:23,164 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 112.88/113.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:14:40,260 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 104.88/104.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:14:57,139 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 107.04/105.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:15:13,060 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.97 (max/real adjusted throughput: 112.09/113.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:15:29,024 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 111.85/112.90 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:15:46,076 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 104.46/106.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:16:03,131 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 106.10/107.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:16:19,023 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 114.29/111.90 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:16:34,996 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 112.99/112.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:16:51,947 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 107.57/107.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:17:08,999 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 105.68/104.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:17:24,854 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.97 (max/real adjusted throughput: 112.12/114.15 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:17:40,624 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 114.69/113.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:17:57,568 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 106.36/105.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:18:14,458 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 105.87/106.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:18:30,353 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 113.58/112.74 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:18:46,210 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 113.31/112.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:19:03,095 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 107.92/106.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:19:20,309 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 104.85/104.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:19:36,154 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 114.06/112.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:19:51,955 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.97 (max/real adjusted throughput: 113.49/113.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:20:09,251 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 107.72/102.98 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:20:26,942 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 104.86/101.75 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:20:43,369 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 110.63/110.08 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:20:59,646 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 113.47/109.80 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:21:17,006 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 107.78/103.14 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:21:34,388 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 107.57/103.10 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:21:50,680 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 113.13/109.48 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:22:06,993 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 112.89/109.80 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:22:24,542 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 106.57/102.32 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:22:41,893 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 106.50/104.27 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:22:58,077 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 113.84/110.07 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:23:14,410 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 113.70/107.95 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:23:31,822 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 107.02/102.42 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:23:49,092 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 108.99/103.93 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:24:05,382 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 114.19/109.44 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:24:21,528 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 113.83/111.21 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:24:39,055 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 106.55/102.43 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:24:56,484 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 106.52/101.98 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:25:12,723 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 114.16/108.93 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:25:28,968 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 113.51/111.05 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:25:46,282 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 107.09/103.64 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:26:03,429 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 106.83/104.00 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:26:19,572 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 115.37/110.80 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:26:36,021 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 113.01/107.90 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:26:53,345 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 108.10/103.19 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:27:10,416 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 111.21/103.33 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:27:26,828 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 113.13/108.04 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:27:43,227 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 113.27/108.84 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:28:01,284 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.29 (max/real adjusted throughput: 101.96/98.08 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:28:19,229 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.30 (max/real adjusted throughput: 105.78/97.97 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:28:35,564 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 112.49/109.07 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:28:52,106 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 111.79/107.90 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:29:09,950 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 104.96/100.72 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:29:27,780 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 103.94/102.27 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:29:44,140 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 113.75/109.55 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:30:00,533 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 113.66/106.87 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:30:18,303 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 102.47/101.60 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:30:36,049 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 105.57/100.44 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:30:52,402 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 114.35/108.35 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:31:09,111 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 110.92/107.31 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:31:27,077 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 102.03/99.05 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:31:45,007 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 101.85/100.09 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:32:01,461 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 113.33/107.04 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:32:17,973 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 111.91/108.53 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:32:36,065 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 102.62/99.72 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:32:53,991 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 102.72/102.76 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:33:10,246 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 114.28/109.28 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:33:26,869 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 110.04/107.42 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:33:44,478 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 106.74/102.27 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:34:02,172 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 106.36/101.82 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:34:18,681 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 114.26/107.46 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:34:34,999 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 113.58/108.53 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:34:52,748 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 104.11/100.51 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:35:10,401 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 105.67/101.99 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:35:27,095 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 110.44/106.69 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:35:43,602 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 112.73/109.76 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:36:01,406 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 103.66/101.13 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:36:19,300 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 102.79/99.93 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:36:35,908 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 110.22/106.99 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:36:52,342 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 113.09/108.68 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:37:10,278 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 104.72/101.29 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:37:28,317 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 105.82/98.56 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:37:44,802 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 110.93/110.31 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:38:00,877 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 115.52/111.17 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:38:18,016 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 106.90/105.77 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:38:35,876 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 103.48/99.90 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:38:52,457 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 112.47/108.21 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:39:09,087 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 109.86/107.97 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:39:27,166 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 102.03/98.58 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:39:45,281 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.30 (max/real adjusted throughput: 103.01/97.64 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:40:01,742 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 112.81/106.48 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:40:18,156 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 113.44/108.74 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:40:35,965 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 105.52/100.81 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:40:53,838 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 105.39/101.68 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:41:10,368 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 111.47/107.41 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:41:26,817 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 113.53/106.83 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:41:44,663 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 104.93/98.43 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:42:02,416 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 103.24/100.70 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:42:18,671 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 114.30/109.67 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:42:34,991 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 112.52/110.02 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:42:53,115 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 102.00/99.13 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:43:11,135 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 103.98/100.76 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:43:27,524 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 112.62/109.71 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:43:44,043 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 113.31/105.16 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:44:01,884 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 103.51/100.74 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:44:19,688 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.26 (max/real adjusted throughput: 104.63/99.28 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:44:36,346 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 110.91/107.05 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:44:52,754 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 114.21/109.53 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:45:10,721 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.28 (max/real adjusted throughput: 104.81/98.81 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:45:28,737 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.26 (max/real adjusted throughput: 102.59/99.70 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:45:45,323 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 112.59/106.56 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:46:01,830 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 111.68/107.76 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:46:19,678 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 104.16/99.83 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:46:37,827 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.31 (max/real adjusted throughput: 102.66/97.27 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 05:46:54,594 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 109.02/108.12 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 05:47:10,968 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 112.02/109.76 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 05:47:28,434 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 103.15/104.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:47:46,060 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 102.27/101.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:48:02,101 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 112.23/111.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:48:17,944 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 114.07/113.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:48:35,230 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 103.59/103.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:48:52,725 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 103.30/102.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:49:08,557 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.97 (max/real adjusted throughput: 113.32/114.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:49:24,421 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.97 (max/real adjusted throughput: 113.29/113.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:49:41,589 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 104.15/103.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:49:58,857 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 103.72/103.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:50:14,909 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 109.13/113.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:50:30,908 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 113.21/112.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:50:48,396 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 102.96/101.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:51:05,639 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 104.12/103.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:51:21,625 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 110.90/113.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:51:37,550 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.97 (max/real adjusted throughput: 112.64/113.90 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:51:54,689 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 103.79/106.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:52:12,208 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 102.29/100.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:52:28,173 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 112.71/112.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:52:44,006 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 114.17/113.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:53:01,304 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 103.19/105.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:53:18,921 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 102.22/100.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:53:35,100 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 112.04/109.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:53:50,917 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 114.21/113.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:54:08,363 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 103.15/100.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:54:25,938 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 100.84/102.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:54:42,013 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 112.42/110.90 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:54:58,116 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 110.62/112.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:55:15,551 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 102.41/103.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:55:32,657 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 105.71/105.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:55:48,609 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 113.62/111.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:56:04,787 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 111.30/112.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:56:21,996 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 105.48/105.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:56:39,047 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 106.90/103.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:56:54,796 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.96 (max/real adjusted throughput: 113.31/114.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:57:10,716 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 114.91/111.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:57:27,982 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 103.50/102.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:57:45,016 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 104.43/105.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:58:01,081 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 110.51/113.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:58:17,273 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 112.59/110.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:58:34,449 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 104.08/104.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:58:51,756 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 103.34/101.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:59:07,971 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 112.87/109.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:59:24,323 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 110.54/110.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 05:59:41,834 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 103.83/102.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 05:59:58,774 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 106.05/104.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:00:14,969 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 109.06/112.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:00:31,164 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 111.88/110.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:00:48,625 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 103.04/102.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:01:05,909 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 104.71/105.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:01:22,106 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 112.24/109.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:01:38,348 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 110.02/110.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:01:55,756 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 104.60/102.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:02:12,905 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 106.70/102.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:02:29,115 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 111.62/111.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:02:45,369 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 109.54/110.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:03:02,526 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 104.50/104.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:03:20,148 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 102.64/103.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:03:36,261 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 112.14/112.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:03:52,422 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 111.54/111.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:04:09,798 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 102.25/102.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:04:26,993 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 103.81/104.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:04:42,865 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 113.84/112.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:04:59,121 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 110.46/110.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:05:16,637 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 104.11/103.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:05:34,038 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 102.72/102.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:05:50,027 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 112.19/111.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:06:06,177 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 110.16/112.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:06:23,653 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 104.90/103.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:06:40,926 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 105.62/104.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:06:56,962 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 113.08/110.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:07:13,448 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 108.64/111.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:07:30,937 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 100.46/105.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:07:48,327 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 104.23/104.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:08:04,571 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 112.43/109.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:08:20,570 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 112.36/112.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:08:37,898 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 105.71/102.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:08:55,189 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 102.15/106.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:09:11,314 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 112.73/112.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:09:27,429 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 111.76/111.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:09:44,723 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 103.12/106.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:10:01,875 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 103.52/106.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:10:18,259 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 110.60/111.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:10:34,097 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 114.40/111.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:10:51,392 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 104.63/101.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:11:08,870 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 104.40/101.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:11:24,951 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 112.27/111.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:11:41,071 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 112.20/112.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:11:58,477 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 102.06/102.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:12:15,793 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 102.02/103.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:12:32,076 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 109.40/112.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:12:48,124 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 113.22/112.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:13:05,192 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 106.22/103.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:13:22,150 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 106.06/105.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:13:38,205 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 113.75/110.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:13:54,678 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 108.84/110.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:14:12,512 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 103.18/101.39 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:14:30,623 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.27 (max/real adjusted throughput: 102.75/99.19 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:14:47,066 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 112.45/108.80 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:15:03,284 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 114.35/110.08 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:15:20,657 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 106.94/103.38 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:15:37,768 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 109.36/105.16 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:15:53,966 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 113.87/110.43 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:16:10,292 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 113.61/107.81 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:16:27,843 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 106.71/99.86 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:16:45,097 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 107.18/103.92 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:17:01,294 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 113.43/111.24 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:17:17,489 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 115.52/108.78 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:17:34,584 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 107.10/106.29 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:17:51,699 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 107.94/104.61 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:18:07,860 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 115.19/109.89 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:18:24,117 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 115.22/109.78 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:18:41,458 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 107.05/102.06 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:18:58,764 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 108.06/102.60 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:19:15,077 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 114.08/108.16 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:19:31,314 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 114.42/109.37 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:19:48,336 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 109.86/105.54 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:20:05,253 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.12 (max/real adjusted throughput: 111.17/106.00 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:20:21,369 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 115.34/110.59 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:20:37,606 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 114.16/109.49 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:20:55,051 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 108.74/103.23 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:21:12,048 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 113.23/103.46 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:21:28,391 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 113.63/108.85 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:21:44,706 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 111.95/110.27 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:22:02,231 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 106.26/102.10 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:22:19,711 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 105.51/102.56 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:22:35,940 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 114.48/109.77 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:22:52,061 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 114.44/110.51 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:23:09,379 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 107.14/103.37 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:23:26,846 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 108.08/100.85 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:23:43,183 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 113.23/108.69 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:23:59,579 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 113.41/108.37 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:24:16,888 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 108.37/104.89 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:24:34,061 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 108.50/103.71 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:24:50,202 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 114.28/110.59 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:25:06,438 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 114.12/109.36 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:25:23,496 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 109.15/105.34 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:25:40,630 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 109.26/105.07 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:25:56,867 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 114.81/110.04 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:26:13,231 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 112.45/109.29 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:26:30,459 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 107.66/104.22 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:26:47,807 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 108.74/103.14 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:27:04,251 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 112.13/107.94 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:27:20,489 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 115.07/109.20 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:27:37,931 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 107.69/103.24 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:27:55,182 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 107.84/104.63 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:28:11,536 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 112.91/108.28 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:28:27,728 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 114.07/109.89 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:28:45,031 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 108.79/103.62 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:29:02,366 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 106.96/103.65 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:29:18,577 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 114.49/110.19 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:29:35,060 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 112.81/108.54 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:29:52,231 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 107.99/104.43 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:30:09,657 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 106.33/100.73 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:30:25,885 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 113.90/110.39 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:30:42,111 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 114.52/110.09 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:30:59,426 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 107.56/102.62 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:31:16,888 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 106.57/100.51 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:31:33,219 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 112.16/110.40 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:31:49,249 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 115.32/112.72 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:32:05,501 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 116.42/108.79 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:32:21,724 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 115.09/111.64 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:32:37,460 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 117.74/113.44 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:32:53,233 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 117.40/113.32 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:33:09,031 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 118.32/113.81 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:33:24,883 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 117.48/112.96 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:33:40,644 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 117.43/113.22 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:33:56,849 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 113.84/109.64 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:34:14,474 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 105.40/100.67 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:34:31,945 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.20 (max/real adjusted throughput: 106.17/102.35 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:34:48,194 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 114.64/108.22 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:35:04,393 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 113.86/110.72 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:35:21,906 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 105.16/103.04 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:35:39,484 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 105.86/101.23 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:35:55,815 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 112.82/108.77 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:36:12,281 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 111.62/109.05 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:36:29,811 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 105.70/103.09 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:36:47,328 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 105.20/102.94 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:37:03,511 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 114.56/110.01 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:37:19,811 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 113.37/109.97 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:37:37,424 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 106.61/100.99 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:37:54,854 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 107.84/102.96 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:38:11,136 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 114.27/109.82 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:38:27,431 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 113.23/109.27 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:38:44,771 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 106.55/103.74 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:39:02,289 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.19 (max/real adjusted throughput: 106.77/102.49 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:39:18,469 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 114.39/110.44 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:39:34,714 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 114.32/109.40 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:39:52,048 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 106.40/103.24 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:40:09,334 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 106.61/104.42 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 06:40:25,704 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.06 (max/real adjusted throughput: 114.39/108.98 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 06:40:41,959 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 114.80/108.45 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 06:40:58,779 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.08 (max/real adjusted throughput: 106.60/108.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:41:15,529 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 106.70/107.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:41:31,505 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 112.23/112.74 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:41:47,328 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 114.12/113.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:42:04,190 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 106.81/106.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:42:21,086 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 107.09/106.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:42:36,975 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 112.84/112.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:42:52,785 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.97 (max/real adjusted throughput: 113.54/114.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:43:09,744 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.11 (max/real adjusted throughput: 105.08/106.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:43:26,551 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.09 (max/real adjusted throughput: 105.57/107.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:43:42,397 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 113.45/113.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:43:58,277 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.96 (max/real adjusted throughput: 113.45/114.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:44:15,315 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 104.59/105.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:44:32,174 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.07 (max/real adjusted throughput: 106.52/108.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:44:47,859 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 115.32/112.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:45:03,705 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 114.75/112.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:45:20,604 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.10 (max/real adjusted throughput: 106.73/107.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:45:37,567 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 105.95/103.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:45:53,469 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 113.65/112.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:46:09,509 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 113.74/111.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:46:26,732 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 105.65/103.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:46:43,848 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 105.70/105.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:46:59,955 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 111.71/109.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:47:15,967 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 113.52/113.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:47:33,315 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 103.78/101.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:47:50,623 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 105.20/101.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:48:06,542 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 112.69/112.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:48:22,665 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 112.03/112.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:48:40,073 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 103.09/103.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:48:57,549 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.16 (max/real adjusted throughput: 103.55/103.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:49:13,634 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 111.69/111.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:49:29,744 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 111.57/112.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:49:47,027 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 104.22/105.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:50:04,269 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 103.16/105.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:50:20,347 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 111.50/112.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:50:36,224 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 114.22/111.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:50:53,268 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 105.62/105.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:51:10,467 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 106.03/105.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:51:26,357 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 113.95/112.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:51:42,215 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 114.93/113.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:51:59,326 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 105.22/105.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:52:16,559 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.17 (max/real adjusted throughput: 103.58/103.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:52:32,643 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 110.39/112.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:52:48,735 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 111.96/111.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:53:06,005 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.14 (max/real adjusted throughput: 102.34/104.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:53:23,269 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 102.93/103.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:53:39,276 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 114.53/111.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:53:55,457 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 110.98/111.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:54:12,634 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.18 (max/real adjusted throughput: 106.48/103.15 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:54:29,854 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.15 (max/real adjusted throughput: 104.46/104.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:54:45,813 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 114.08/111.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:55:01,917 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 111.58/111.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:55:18,791 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 109.94/101.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:55:35,950 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.13 (max/real adjusted throughput: 104.30/105.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:55:52,026 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 113.33/109.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:56:08,142 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 110.52/111.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:56:26,161 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 98.08/99.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:56:44,183 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.29 (max/real adjusted throughput: 99.11/98.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:57:00,154 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 112.18/113.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:57:16,178 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 113.14/111.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:57:34,022 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 100.25/100.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:57:51,819 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 100.37/101.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:58:07,539 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 115.76/113.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:58:23,606 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.02 (max/real adjusted throughput: 112.80/111.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:58:41,664 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 100.15/100.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:58:59,676 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 100.03/101.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:59:15,692 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 112.10/111.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 06:59:31,739 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.04 (max/real adjusted throughput: 113.27/110.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 06:59:49,635 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.26 (max/real adjusted throughput: 100.34/99.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:00:07,463 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 100.48/100.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:00:23,434 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 113.19/112.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:00:39,363 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 114.58/112.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 07:00:57,285 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 98.36/99.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:01:15,254 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 99.74/101.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:01:31,299 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 111.88/112.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:01:47,153 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 113.21/113.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 07:02:05,372 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.29 (max/real adjusted throughput: 98.83/98.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:02:23,349 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.24 (max/real adjusted throughput: 99.17/100.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:02:39,602 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.05 (max/real adjusted throughput: 109.69/109.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:02:55,509 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.98 (max/real adjusted throughput: 113.37/113.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 07:03:13,755 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.29 (max/real adjusted throughput: 100.16/98.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:03:31,798 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.23 (max/real adjusted throughput: 99.30/100.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:03:47,863 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.01 (max/real adjusted throughput: 112.55/112.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:04:03,826 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.03 (max/real adjusted throughput: 113.62/110.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 07:04:21,846 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 98.44/100.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:04:39,783 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 98.49/101.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:04:55,703 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 113.59/112.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:05:11,576 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.97 (max/real adjusted throughput: 112.95/113.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 07:05:29,614 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.21 (max/real adjusted throughput: 100.69/101.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:05:47,462 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 102.49/100.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:06:03,482 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 114.32/112.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:06:19,340 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 112.81/112.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 07:06:37,073 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 2.22 (max/real adjusted throughput: 100.09/101.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:06:54,869 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 2.25 (max/real adjusted throughput: 100.66/99.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:07:10,763 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.99 (max/real adjusted throughput: 113.50/113.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:07:26,798 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 2.00 (max/real adjusted throughput: 113.25/112.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 07:07:26,798 | xffl.distributed.aggregation |     INFO | Dumping benchmarking results to /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs/llama3.1-8b_ns_8_fs_1_ppn_1.csv[0m
lrdn0120:2076463:2084502 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0120:2076463:2084502 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0120:2076463:2076588 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0120:2076463:2084502 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0378:1638354:1647021 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0378:1638354:1647021 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0378:1638354:1647021 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0378:1638354:1638481 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0398:3637919:3646998 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0398:3637919:3646998 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0398:3637919:3646998 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0398:3637919:3638040 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0786:1856354:1864555 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0786:1856354:1864555 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0786:1856354:1864555 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0786:1856354:1856473 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0872:519656:529321 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0872:519656:529321 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0872:519656:529321 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0872:519656:519781 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0905:2120263:2128292 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0905:2120263:2128292 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0905:2120263:2128292 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0905:2120263:2120383 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1057:3554428:3563190 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1057:3554428:3563190 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1057:3554428:3563190 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1057:3554428:3554553 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1067:2554046:2566008 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1067:2554046:2566008 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1067:2554046:2566008 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1067:2554046:2554213 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0120:2076463:2084502 [0] NCCL INFO comm 0x115a9be0 rank 0 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0378:1638354:1647021 [0] NCCL INFO comm 0xe437d50 rank 1 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0398:3637919:3646998 [0] NCCL INFO comm 0x16c1a260 rank 2 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0786:1856354:1864555 [0] NCCL INFO comm 0x2afa4e30 rank 3 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0872:519656:529321 [0] NCCL INFO comm 0xd784ed0 rank 4 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0905:2120263:2128292 [0] NCCL INFO comm 0xca92820 rank 5 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1057:3554428:3563190 [0] NCCL INFO comm 0xc12b3d0 rank 6 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1067:2554046:2566008 [0] NCCL INFO comm 0xd0b02c0 rank 7 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0120:2076463:2084505 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0120:2076463:2084505 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0120:2076463:2084505 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0120:2076463:2076582 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0378:1638354:1647024 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0378:1638354:1647024 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0378:1638354:1647024 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0378:1638354:1638476 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0398:3637919:3647001 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0398:3637919:3647001 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0398:3637919:3647001 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0398:3637919:3638035 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0872:519656:529324 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0872:519656:529324 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0872:519656:529324 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0872:519656:519776 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0786:1856354:1864558 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0786:1856354:1864558 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0786:1856354:1864558 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0786:1856354:1856468 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0905:2120263:2128295 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0905:2120263:2128295 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0905:2120263:2128295 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0905:2120263:2120378 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1057:3554428:3563193 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1057:3554428:3563193 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1057:3554428:3563193 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1057:3554428:3554548 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1067:2554046:2566027 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1067:2554046:2566027 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1067:2554046:2566027 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1067:2554046:2554208 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0120:2076463:2084505 [0] NCCL INFO comm 0x1152e050 rank 0 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0378:1638354:1647024 [0] NCCL INFO comm 0xe3bc1c0 rank 1 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0398:3637919:3647001 [0] NCCL INFO comm 0x16b9e6d0 rank 2 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0872:519656:529324 [0] NCCL INFO comm 0xd709340 rank 4 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0786:1856354:1864558 [0] NCCL INFO comm 0x2af292a0 rank 3 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0905:2120263:2128295 [0] NCCL INFO comm 0xca16c90 rank 5 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1057:3554428:3563193 [0] NCCL INFO comm 0xc0af840 rank 6 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1067:2554046:2566027 [0] NCCL INFO comm 0xd034730 rank 7 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0120:2076463:2084507 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0120:2076463:2084507 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0120:2076463:2084507 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0120:2076463:2076576 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0378:1638354:1647026 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0378:1638354:1647026 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0378:1638354:1647026 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0378:1638354:1638471 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0398:3637919:3647003 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0398:3637919:3647003 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0398:3637919:3647003 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0398:3637919:3638030 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0905:2120263:2128297 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0905:2120263:2128297 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0905:2120263:2128297 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0905:2120263:2120373 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0786:1856354:1864560 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0786:1856354:1864560 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0786:1856354:1864560 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0786:1856354:1856463 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1057:3554428:3563195 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1057:3554428:3563195 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1057:3554428:3563195 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1057:3554428:3554543 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0872:519656:529326 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0872:519656:529326 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0872:519656:529326 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0872:519656:519771 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1067:2554046:2566029 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1067:2554046:2554203 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1067:2554046:2566029 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1067:2554046:2566029 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0120:2076463:2084507 [0] NCCL INFO comm 0x114b24c0 rank 0 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0398:3637919:3647003 [0] NCCL INFO comm 0x16b22b40 rank 2 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0378:1638354:1647026 [0] NCCL INFO comm 0xe340630 rank 1 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0786:1856354:1864560 [0] NCCL INFO comm 0x2aead710 rank 3 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0905:2120263:2128297 [0] NCCL INFO comm 0xc99b100 rank 5 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1057:3554428:3563195 [0] NCCL INFO comm 0xc033cb0 rank 6 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0872:519656:529326 [0] NCCL INFO comm 0xd68d7b0 rank 4 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1067:2554046:2566029 [0] NCCL INFO comm 0xcfb8ba0 rank 7 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0120:2076463:2084509 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0120:2076463:2084509 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0120:2076463:2084509 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0120:2076463:2076568 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0378:1638354:1647028 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0378:1638354:1647028 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0378:1638354:1647028 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0378:1638354:1638461 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0398:3637919:3647005 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0398:3637919:3647005 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0398:3637919:3647005 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0398:3637919:3638021 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0786:1856354:1864562 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0786:1856354:1864562 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0786:1856354:1864562 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0786:1856354:1856455 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0872:519656:529328 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0872:519656:529328 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0872:519656:529328 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0872:519656:519761 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0905:2120263:2128299 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn0905:2120263:2128299 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn0905:2120263:2128299 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn0905:2120263:2120365 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1057:3554428:3563197 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1057:3554428:3563197 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1057:3554428:3563197 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1057:3554428:3554533 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1067:2554046:2566031 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1067:2554046:2566031 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1067:2554046:2566031 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1067:2554046:2554163 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn0120:2076463:2084509 [0] NCCL INFO comm 0x15f81490 rank 0 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0378:1638354:1647028 [0] NCCL INFO comm 0xe138980 rank 1 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0398:3637919:3647005 [0] NCCL INFO comm 0x1691af00 rank 2 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0786:1856354:1864562 [0] NCCL INFO comm 0x2185ed00 rank 3 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0872:519656:529328 [0] NCCL INFO comm 0xd486250 rank 4 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0905:2120263:2128299 [0] NCCL INFO comm 0x16467be0 rank 5 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1057:3554428:3563197 [0] NCCL INFO comm 0x296fa700 rank 6 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1067:2554046:2566031 [0] NCCL INFO comm 0x2ae7f150 rank 7 nranks 8 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0905:2120263:2128302 [0] NCCL INFO comm 0xd7043f0 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0786:1856354:1864565 [0] NCCL INFO comm 0xdc12d70 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1057:3554428:3563200 [0] NCCL INFO comm 0xcd1b400 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0872:519656:529331 [0] NCCL INFO comm 0xd039110 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0120:2076463:2084512 [0] NCCL INFO comm 0xd21ba40 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0398:3637919:3647008 [0] NCCL INFO comm 0xebd6540 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn0378:1638354:1647031 [0] NCCL INFO comm 0xdcec4d0 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1067:2554046:2566034 [0] NCCL INFO comm 0xdc9e8e0 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
[38;20m2025-08-02 07:07:31,388 | xffl.cli.simulate |     INFO | Total simulation execution time: 9655.93 seconds[0m
[38;20m2025-08-02 07:07:31,389 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
