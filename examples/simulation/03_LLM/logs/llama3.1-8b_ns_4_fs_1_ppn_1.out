[38;20m2025-08-02 00:12:13,478 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
[38;5;39m2025-08-02 00:12:13,480 | xffl.cli.simulate |    DEBUG | Using current virtual environment: /leonardo_scratch/fast/uToID_bench/xffl/.venv[0m
[38;5;39m2025-08-02 00:12:13,480 | xffl.cli.simulate |    DEBUG | New local simulation xFFL environment variables: {'XFFL_VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv', 'XFFL_WORLD_SIZE': '4', 'XFFL_NUM_NODES': '4', 'MASTER_ADDR': 'lrdn1207', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;5;39m2025-08-02 00:12:13,480 | xffl.cli.simulate |    DEBUG | Updated xFFL environment: {'XFFL_VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv', 'XFFL_WORLD_SIZE': '4', 'XFFL_NUM_NODES': '4', 'MASTER_ADDR': 'lrdn1207', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;20m2025-08-02 00:12:13,482 | xffl.cli.simulate |     INFO | Running local simulation...[0m
[38;5;39m2025-08-02 00:12:13,482 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1207: ssh -oStrictHostKeyChecking=no lrdn1207 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=4 XFFL_NUM_NODES=4 MASTER_ADDR=lrdn1207 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_4_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:13,483 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1268: ssh -oStrictHostKeyChecking=no lrdn1268 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=4 XFFL_NUM_NODES=4 MASTER_ADDR=lrdn1207 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_4_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:13,483 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1280: ssh -oStrictHostKeyChecking=no lrdn1280 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=4 XFFL_NUM_NODES=4 MASTER_ADDR=lrdn1207 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=2 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_4_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:13,483 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1370: ssh -oStrictHostKeyChecking=no lrdn1370 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=4 XFFL_NUM_NODES=4 MASTER_ADDR=lrdn1207 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=3 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_4_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 00:12:31,752 | xffl.distributed.distributed_state |    DEBUG | Setting Symmetric Federated Scaling with sizes (1, 1, 1, 1)[0m
[38;5;39m2025-08-02 00:12:31,785 | xffl.distributed.distributed |    DEBUG | [Rank 0]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn1207
                    Master port=29500
                    Rank=0
                    World size=4
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=0
                    Node world size=4
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1)
                    Federated rank=0
                    Federated world size=4
                MESHES:
                    FSDP=DeviceMesh('cuda', [0], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3]
                    Replica group=None
                    Federation=[0]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb9acd80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9c9fc0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf11430>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf11730>)
                [0m
[38;5;39m2025-08-02 00:12:31,785 |         __main__ |    DEBUG | Rendez-vous time: 7.08 seconds[0m
[38;5;39m2025-08-02 00:12:31,785 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:31,785 | xffl.distributed.distributed |    DEBUG | [Rank 2]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn1207
                    Master port=29500
                    Rank=2
                    World size=4
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=2
                    Node world size=4
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1)
                    Federated rank=2
                    Federated world size=4
                MESHES:
                    FSDP=DeviceMesh('cuda', [2], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3]
                    Replica group=None
                    Federation=[2]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcc98e40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xccb6080>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1fd930>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1fdc30>)
                [0m
[38;5;39m2025-08-02 00:12:31,785 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:31,786 | xffl.distributed.distributed |    DEBUG | [Rank 3]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn1207
                    Master port=29500
                    Rank=3
                    World size=4
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=3
                    Node world size=4
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1)
                    Federated rank=3
                    Federated world size=4
                MESHES:
                    FSDP=DeviceMesh('cuda', [3], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3]
                    Replica group=None
                    Federation=[3]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcf42790>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf5f9d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd4a6a60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd4a6d60>)
                [0m
[38;5;39m2025-08-02 00:12:31,786 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:31,786 | xffl.distributed.distributed |    DEBUG | [Rank 1]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn1207
                    Master port=29500
                    Rank=1
                    World size=4
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=1
                    Node world size=4
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1)
                    Federated rank=1
                    Federated world size=4
                MESHES:
                    FSDP=DeviceMesh('cuda', [1], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3]
                    Replica group=None
                    Federation=[1]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xce885f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcea5830>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd3ec320>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd3ec620>)
                [0m
[38;5;39m2025-08-02 00:12:31,786 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 00:12:32,545 | xffl.distributed.distributed |    DEBUG | [Rank 2]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 00:12:32,589 | xffl.distributed.distributed |    DEBUG | [Rank 1]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 00:12:32,598 | xffl.distributed.distributed |    DEBUG | [Rank 3]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 00:12:32,621 |         __main__ |    DEBUG | Model loading time: 0.84 seconds[0m
[38;5;39m2025-08-02 00:12:32,622 |         __main__ |    DEBUG | Training llama3.1-8b: 8030.26 million trainable parameters[0m
[38;5;39m2025-08-02 00:12:32,622 | xffl.distributed.distributed |    DEBUG | [Rank 0]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1280:1451550:1451550 [0] NCCL INFO Bootstrap: Using ib0:10.128.26.33<0>
lrdn1280:1451550:1451550 [0] NCCL INFO cudaDriverVersion 12020
lrdn1280:1451550:1451550 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1280:1451550:1451550 [0] NCCL INFO Comm config Blocking set to 1
lrdn1268:3521221:3521221 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.241<0>
lrdn1268:3521221:3521221 [0] NCCL INFO cudaDriverVersion 12020
lrdn1268:3521221:3521221 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1268:3521221:3521221 [0] NCCL INFO Comm config Blocking set to 1
lrdn1370:234521:234521 [0] NCCL INFO Bootstrap: Using ib0:10.128.27.137<0>
lrdn1370:234521:234521 [0] NCCL INFO cudaDriverVersion 12020
lrdn1370:234521:234521 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1370:234521:234521 [0] NCCL INFO Comm config Blocking set to 1
lrdn1207:1671835:1671835 [0] NCCL INFO Bootstrap: Using ib0:10.128.24.253<0>
lrdn1207:1671835:1671835 [0] NCCL INFO cudaDriverVersion 12020
lrdn1207:1671835:1671835 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1207:1671835:1671835 [0] NCCL INFO Comm config Blocking set to 1
lrdn1280:1451550:1451639 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1280:1451550:1451639 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1268:3521221:3521312 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1268:3521221:3521312 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1280:1451550:1451639 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.26.33<0>
lrdn1370:234521:234612 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1370:234521:234612 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1280:1451550:1451639 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1280:1451550:1451639 [0] NCCL INFO Using network IB
lrdn1280:1451550:1451639 [0] NCCL INFO ncclCommInitRankConfig comm 0xdd797f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7ee49cd291be79ce - Init START
lrdn1280:1451550:1451639 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1280:1451550:1451639 [0] NCCL INFO Bootstrap timings total 0.000368 (create 0.000017, send 0.000055, recv 0.000079, ring 0.000001, delay 0.000001)
lrdn1280:1451550:1451639 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1280:1451550:1451639 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1280:1451550:1451639 [0] NCCL INFO comm 0xdd797f0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 00/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 01/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 02/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 03/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 04/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 05/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 06/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 07/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 08/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 09/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 10/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 11/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 12/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 13/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 14/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 15/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 16/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 17/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 18/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 19/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 20/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 21/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 22/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 23/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 24/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 25/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 26/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 27/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 28/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 29/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 30/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 31/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 32/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 33/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 34/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 35/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 36/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 37/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 38/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 39/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 40/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 41/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 42/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 43/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 44/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 45/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 46/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 47/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 48/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 49/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 50/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 51/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 52/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 53/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 54/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 55/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 56/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 57/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 58/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 59/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 60/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 61/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 62/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Channel 63/64 : 0
lrdn1280:1451550:1451639 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1280:1451550:1451639 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1280:1451550:1451639 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1280:1451550:1451646 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1280:1451550:1451647 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn1268:3521221:3521312 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.241<0>
lrdn1268:3521221:3521312 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1268:3521221:3521312 [0] NCCL INFO Using network IB
lrdn1280:1451550:1451639 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1280:1451550:1451639 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1268:3521221:3521312 [0] NCCL INFO ncclCommInitRankConfig comm 0xe767a70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x49c7cbc52cccfdbe - Init START
lrdn1268:3521221:3521312 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1268:3521221:3521312 [0] NCCL INFO Bootstrap timings total 0.000399 (create 0.000019, send 0.000058, recv 0.000100, ring 0.000001, delay 0.000000)
lrdn1207:1671835:1671927 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1207:1671835:1671927 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1280:1451550:1451639 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1280:1451550:1451639 [0] NCCL INFO ncclCommInitRankConfig comm 0xdd797f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7ee49cd291be79ce - Init COMPLETE
lrdn1280:1451550:1451639 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn1370:234521:234612 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.27.137<0>
lrdn1268:3521221:3521312 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1268:3521221:3521312 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1268:3521221:3521312 [0] NCCL INFO comm 0xe767a70 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 00/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 01/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 02/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 03/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 04/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 05/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 06/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 07/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 08/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 09/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 10/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 11/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 12/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 13/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 14/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 15/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 16/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 17/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 18/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 19/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 20/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 21/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 22/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 23/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 24/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 25/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 26/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 27/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 28/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 29/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 30/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 31/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 32/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 33/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 34/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 35/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 36/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 37/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 38/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 39/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 40/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 41/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 42/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 43/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 44/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 45/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 46/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 47/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 48/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 49/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 50/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 51/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 52/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 53/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 54/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 55/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 56/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 57/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 58/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 59/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 60/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 61/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 62/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Channel 63/64 : 0
lrdn1268:3521221:3521312 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1268:3521221:3521312 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1268:3521221:3521312 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1268:3521221:3521335 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn1268:3521221:3521336 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn1370:234521:234612 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1370:234521:234612 [0] NCCL INFO Using network IB
lrdn1370:234521:234612 [0] NCCL INFO ncclCommInitRankConfig comm 0xe821710 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9b9cfdb2ce5f171f - Init START
lrdn1370:234521:234612 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1370:234521:234612 [0] NCCL INFO Bootstrap timings total 0.000410 (create 0.000023, send 0.000066, recv 0.000098, ring 0.000001, delay 0.000000)
lrdn1268:3521221:3521312 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1268:3521221:3521312 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1268:3521221:3521312 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1268:3521221:3521312 [0] NCCL INFO ncclCommInitRankConfig comm 0xe767a70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x49c7cbc52cccfdbe - Init COMPLETE
lrdn1268:3521221:3521312 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn1370:234521:234612 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1370:234521:234612 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1370:234521:234612 [0] NCCL INFO comm 0xe821710 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 00/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 01/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 02/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 03/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 04/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 05/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 06/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 07/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 08/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 09/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 10/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 11/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 12/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 13/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 14/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 15/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 16/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 17/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 18/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 19/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 20/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 21/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 22/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 23/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 24/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 25/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 26/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 27/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 28/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 29/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 30/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 31/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 32/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 33/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 34/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 35/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 36/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 37/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 38/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 39/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 40/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 41/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 42/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 43/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 44/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 45/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 46/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 47/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 48/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 49/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 50/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 51/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 52/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 53/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 54/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 55/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 56/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 57/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 58/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 59/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 60/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 61/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 62/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Channel 63/64 : 0
lrdn1370:234521:234612 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn1370:234521:234612 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1370:234521:234612 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1370:234521:234620 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn1370:234521:234619 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn1370:234521:234612 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1370:234521:234612 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1370:234521:234612 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1370:234521:234612 [0] NCCL INFO ncclCommInitRankConfig comm 0xe821710 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9b9cfdb2ce5f171f - Init COMPLETE
lrdn1370:234521:234612 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn1207:1671835:1671927 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.24.253<0>
lrdn1207:1671835:1671927 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1207:1671835:1671927 [0] NCCL INFO Using network IB
lrdn1207:1671835:1671927 [0] NCCL INFO ncclCommInitRankConfig comm 0xca8da80 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc7dae969e458d95 - Init START
lrdn1207:1671835:1671927 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1207:1671835:1671927 [0] NCCL INFO Bootstrap timings total 0.000427 (create 0.000024, send 0.000064, recv 0.000105, ring 0.000001, delay 0.000000)
lrdn1207:1671835:1671927 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1207:1671835:1671927 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1207:1671835:1671927 [0] NCCL INFO comm 0xca8da80 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 00/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 01/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 02/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 03/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 04/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 05/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 06/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 07/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 08/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 09/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 10/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 11/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 12/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 13/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 14/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 15/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 16/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 17/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 18/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 19/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 20/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 21/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 22/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 23/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 24/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 25/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 26/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 27/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 28/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 29/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 30/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 31/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 32/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 33/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 34/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 35/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 36/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 37/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 38/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 39/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 40/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 41/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 42/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 43/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 44/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 45/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 46/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 47/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 48/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 49/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 50/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 51/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 52/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 53/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 54/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 55/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 56/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 57/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 58/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 59/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 60/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 61/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 62/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Channel 63/64 : 0
lrdn1207:1671835:1671927 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1207:1671835:1671927 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1207:1671835:1671927 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1207:1671835:1671934 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1207:1671835:1671935 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 12
lrdn1207:1671835:1671927 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1207:1671835:1671927 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1207:1671835:1671927 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1207:1671835:1671927 [0] NCCL INFO ncclCommInitRankConfig comm 0xca8da80 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc7dae969e458d95 - Init COMPLETE
lrdn1207:1671835:1671927 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.28 (kernels 0.15, alloc 0.09, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
[38;5;39m2025-08-02 00:12:39,549 |         __main__ |    DEBUG | FSDP wrapping setup time: 6.93 seconds[0m
[38;5;39m2025-08-02 00:12:39,562 |         __main__ |    DEBUG | Total setup time: 14.86 seconds[0m
[38;5;39m2025-08-02 00:12:39,562 |         __main__ |    DEBUG | GPU RAM allocated before training: 16.06 GB[0m


lrdn1207:1671835:1671835 [0] NCCL INFO Comm config Blocking set to 1
lrdn1207:1671835:1671940 [0] NCCL INFO Using network IB
lrdn1207:1671835:1671940 [0] NCCL INFO ncclCommInitRankConfig comm 0xced9480 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1a501f9dbb9caa57 - Init START
lrdn1370:234521:234521 [0] NCCL INFO Comm config Blocking set to 1
lrdn1268:3521221:3521221 [0] NCCL INFO Comm config Blocking set to 1
lrdn1280:1451550:1451550 [0] NCCL INFO Comm config Blocking set to 1
lrdn1370:234521:234624 [0] NCCL INFO Using network IB
lrdn1268:3521221:3521340 [0] NCCL INFO Using network IB
lrdn1370:234521:234624 [0] NCCL INFO ncclCommInitRankConfig comm 0x142f36f0 rank 3 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1a501f9dbb9caa57 - Init START
lrdn1268:3521221:3521340 [0] NCCL INFO ncclCommInitRankConfig comm 0x2c1461c0 rank 1 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1a501f9dbb9caa57 - Init START
lrdn1280:1451550:1451651 [0] NCCL INFO Using network IB
lrdn1280:1451550:1451651 [0] NCCL INFO ncclCommInitRankConfig comm 0xe1c6990 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1a501f9dbb9caa57 - Init START
lrdn1280:1451550:1451651 [0] NCCL INFO Bootstrap timings total 0.002055 (create 0.000022, send 0.000282, recv 0.000514, ring 0.000355, delay 0.000000)
lrdn1207:1671835:1671940 [0] NCCL INFO Bootstrap timings total 0.002527 (create 0.000015, send 0.000054, recv 0.001321, ring 0.000446, delay 0.000000)
lrdn1370:234521:234624 [0] NCCL INFO Bootstrap timings total 0.002240 (create 0.000020, send 0.000382, recv 0.000990, ring 0.000411, delay 0.000000)
lrdn1268:3521221:3521340 [0] NCCL INFO Bootstrap timings total 0.002333 (create 0.000023, send 0.000933, recv 0.000496, ring 0.000770, delay 0.000000)
lrdn1370:234521:234624 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1268:3521221:3521340 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1370:234521:234624 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn1268:3521221:3521340 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn1280:1451550:1451651 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1280:1451550:1451651 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn1207:1671835:1671940 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1207:1671835:1671940 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 1.
lrdn1370:234521:234624 [0] NCCL INFO comm 0x142f36f0 rank 3 nRanks 4 nNodes 4 localRanks 1 localRank 0 MNNVL 0
lrdn1370:234521:234624 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 1/-1/-1->3->-1 [3] 1/-1/-1->3->-1
lrdn1370:234521:234624 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1280:1451550:1451651 [0] NCCL INFO comm 0xe1c6990 rank 2 nRanks 4 nNodes 4 localRanks 1 localRank 0 MNNVL 0
lrdn1280:1451550:1451651 [0] NCCL INFO Trees [0] 1/3/-1->2->0 [1] 1/3/-1->2->0 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1
lrdn1280:1451550:1451651 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1207:1671835:1671940 [0] NCCL INFO comm 0xced9480 rank 0 nRanks 4 nNodes 4 localRanks 1 localRank 0 MNNVL 0
lrdn1207:1671835:1671940 [0] NCCL INFO Channel 00/04 : 0 1 2 3
lrdn1207:1671835:1671940 [0] NCCL INFO Channel 01/04 : 0 1 2 3
lrdn1207:1671835:1671940 [0] NCCL INFO Channel 02/04 : 0 1 2 3
lrdn1207:1671835:1671940 [0] NCCL INFO Channel 03/04 : 0 1 2 3
lrdn1207:1671835:1671940 [0] NCCL INFO Trees [0] 2/-1/-1->0->-1 [1] 2/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
lrdn1207:1671835:1671940 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1207:1671835:1671940 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1207:1671835:1671942 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn1207:1671835:1671943 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn1280:1451550:1451652 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn1268:3521221:3521340 [0] NCCL INFO comm 0x2c1461c0 rank 1 nRanks 4 nNodes 4 localRanks 1 localRank 0 MNNVL 0
lrdn1268:3521221:3521340 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] -1/-1/-1->1->2 [2] 2/0/-1->1->3 [3] 2/0/-1->1->3
lrdn1268:3521221:3521340 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1268:3521221:3521341 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1268:3521221:3521342 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1370:234521:234625 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1370:234521:234626 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1280:1451550:1451653 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1207:1671835:1671940 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1207:1671835:1671940 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1207:1671835:1671940 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             0              1              0              0              0              0              0  
       Reduce |        0         0         1   |             0              1              0              0              0              0              0  
    AllGather |        0         0         1   |             0              1              0              0              0              0              0  
ReduceScatter |        0         0         1   |             0              1              0              0              0              0              0  
    AllReduce |        0         0         1   |             0              1              0              0              0              0              0  

lrdn1207:1671835:1671940 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
lrdn1207:1671835:1671940 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1207:1671835:1671940 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1268:3521221:3521340 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1268:3521221:3521340 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1268:3521221:3521340 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
lrdn1268:3521221:3521340 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1370:234521:234624 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1370:234521:234624 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1370:234521:234624 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
lrdn1370:234521:234624 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1207:1671835:1671940 [0] NCCL INFO ncclCommInitRankConfig comm 0xced9480 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1a501f9dbb9caa57 - Init COMPLETE
lrdn1207:1671835:1671940 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 4 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1280:1451550:1451651 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1280:1451550:1451651 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1280:1451550:1451651 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
lrdn1280:1451550:1451651 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1268:3521221:3521340 [0] NCCL INFO ncclCommInitRankConfig comm 0x2c1461c0 rank 1 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1a501f9dbb9caa57 - Init COMPLETE
lrdn1268:3521221:3521340 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 4 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.02, connections 0.00, rest 0.00)
lrdn1370:234521:234624 [0] NCCL INFO ncclCommInitRankConfig comm 0x142f36f0 rank 3 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1a501f9dbb9caa57 - Init COMPLETE
lrdn1370:234521:234624 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 4 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.02, connections 0.00, rest 0.00)
lrdn1280:1451550:1451651 [0] NCCL INFO ncclCommInitRankConfig comm 0xe1c6990 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1a501f9dbb9caa57 - Init COMPLETE
lrdn1280:1451550:1451651 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 4 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1207:1671835:1671944 [0] NCCL INFO Channel 00/0 : 3[0] -> 0[0] [receive] via NET/IB/5
lrdn1207:1671835:1671944 [0] NCCL INFO Channel 01/0 : 3[0] -> 0[0] [receive] via NET/IB/6
lrdn1207:1671835:1671944 [0] NCCL INFO Channel 02/0 : 3[0] -> 0[0] [receive] via NET/IB/5
lrdn1207:1671835:1671945 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 8
lrdn1207:1671835:1671944 [0] NCCL INFO Channel 03/0 : 3[0] -> 0[0] [receive] via NET/IB/6
lrdn1207:1671835:1671944 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1207:1671835:1671944 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1207:1671835:1671944 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1207:1671835:1671944 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1370:234521:234628 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 4
lrdn1370:234521:234627 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn1370:234521:234627 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn1370:234521:234627 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn1370:234521:234627 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn1370:234521:234627 [0] NCCL INFO Channel 00/0 : 3[0] -> 0[0] [send] via NET/IB/5
lrdn1268:3521221:3521344 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 3
lrdn1268:3521221:3521343 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1268:3521221:3521343 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn1268:3521221:3521343 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1370:234521:234627 [0] NCCL INFO Channel 01/0 : 3[0] -> 0[0] [send] via NET/IB/6
lrdn1370:234521:234627 [0] NCCL INFO Channel 02/0 : 3[0] -> 0[0] [send] via NET/IB/5
lrdn1268:3521221:3521343 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn1268:3521221:3521343 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn1268:3521221:3521343 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn1370:234521:234627 [0] NCCL INFO Channel 03/0 : 3[0] -> 0[0] [send] via NET/IB/6
lrdn1268:3521221:3521343 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn1268:3521221:3521343 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn1280:1451550:1451654 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn1280:1451550:1451654 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn1280:1451550:1451655 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 5
lrdn1280:1451550:1451654 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn1280:1451550:1451654 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn1280:1451550:1451654 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn1280:1451550:1451654 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn1280:1451550:1451654 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn1280:1451550:1451654 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn1268:3521221:3521343 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1280:1451550:1451654 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1207:1671835:1671944 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1370:234521:234627 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
[38;20m2025-08-02 00:12:53,391 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 113.01/112.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:13:07,076 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 111.88/111.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
lrdn1207:1671835:1671835 [0] NCCL INFO Comm config Blocking set to 1
lrdn1207:1671835:1671950 [0] NCCL INFO Using network IB
lrdn1268:3521221:3521221 [0] NCCL INFO Comm config Blocking set to 1
lrdn1280:1451550:1451550 [0] NCCL INFO Comm config Blocking set to 1
lrdn1268:3521221:3521364 [0] NCCL INFO Using network IB
lrdn1370:234521:234521 [0] NCCL INFO Comm config Blocking set to 1
lrdn1280:1451550:1451659 [0] NCCL INFO Using network IB
lrdn1207:1671835:1671950 [0] NCCL INFO ncclCommInitRankConfig comm 0xd0e1770 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x89845cbc7c6f03b5 - Init START
lrdn1370:234521:234633 [0] NCCL INFO Using network IB
lrdn1268:3521221:3521364 [0] NCCL INFO ncclCommInitRankConfig comm 0xda93540 rank 1 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x89845cbc7c6f03b5 - Init START
lrdn1370:234521:234633 [0] NCCL INFO ncclCommInitRankConfig comm 0x1450cf50 rank 3 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x89845cbc7c6f03b5 - Init START
lrdn1280:1451550:1451659 [0] NCCL INFO ncclCommInitRankConfig comm 0xe3e15b0 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x89845cbc7c6f03b5 - Init START
lrdn1207:1671835:1671950 [0] NCCL INFO Bootstrap timings total 0.001500 (create 0.000016, send 0.000060, recv 0.000975, ring 0.000253, delay 0.000000)
lrdn1268:3521221:3521364 [0] NCCL INFO Bootstrap timings total 0.001392 (create 0.000017, send 0.000204, recv 0.000701, ring 0.000341, delay 0.000000)
lrdn1370:234521:234633 [0] NCCL INFO Bootstrap timings total 0.001401 (create 0.000017, send 0.000375, recv 0.000431, ring 0.000324, delay 0.000000)
lrdn1280:1451550:1451659 [0] NCCL INFO Bootstrap timings total 0.001423 (create 0.000016, send 0.000251, recv 0.000367, ring 0.000328, delay 0.000000)
lrdn1207:1671835:1671950 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1370:234521:234633 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1268:3521221:3521364 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1280:1451550:1451659 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1370:234521:234633 [0] NCCL INFO comm 0x1450cf50 rank 3 nRanks 4 nNodes 4 localRanks 1 localRank 0 MNNVL 0
lrdn1370:234521:234633 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 1/-1/-1->3->-1 [3] 1/-1/-1->3->-1
lrdn1370:234521:234633 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1207:1671835:1671950 [0] NCCL INFO comm 0xd0e1770 rank 0 nRanks 4 nNodes 4 localRanks 1 localRank 0 MNNVL 0
lrdn1207:1671835:1671950 [0] NCCL INFO Channel 00/04 : 0 1 2 3
lrdn1207:1671835:1671950 [0] NCCL INFO Channel 01/04 : 0 1 2 3
lrdn1207:1671835:1671950 [0] NCCL INFO Channel 02/04 : 0 1 2 3
lrdn1207:1671835:1671950 [0] NCCL INFO Channel 03/04 : 0 1 2 3
lrdn1207:1671835:1671950 [0] NCCL INFO Trees [0] 2/-1/-1->0->-1 [1] 2/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
lrdn1207:1671835:1671950 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1207:1671835:1671950 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1207:1671835:1671951 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1207:1671835:1671952 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn1268:3521221:3521364 [0] NCCL INFO comm 0xda93540 rank 1 nRanks 4 nNodes 4 localRanks 1 localRank 0 MNNVL 0
lrdn1268:3521221:3521364 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] -1/-1/-1->1->2 [2] 2/0/-1->1->3 [3] 2/0/-1->1->3
lrdn1268:3521221:3521364 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1280:1451550:1451659 [0] NCCL INFO comm 0xe3e15b0 rank 2 nRanks 4 nNodes 4 localRanks 1 localRank 0 MNNVL 0
lrdn1280:1451550:1451659 [0] NCCL INFO Trees [0] 1/3/-1->2->0 [1] 1/3/-1->2->0 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1
lrdn1280:1451550:1451659 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1370:234521:234634 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1268:3521221:3521365 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1268:3521221:3521366 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1280:1451550:1451660 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn1280:1451550:1451661 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1370:234521:234635 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn1207:1671835:1671950 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1207:1671835:1671950 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1207:1671835:1671950 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             0              1              0              0              0              0              0  
       Reduce |        0         0         1   |             0              1              0              0              0              0              0  
    AllGather |        0         0         1   |             0              1              0              0              0              0              0  
ReduceScatter |        0         0         1   |             0              1              0              0              0              0              0  
    AllReduce |        0         0         1   |             0              1              0              0              0              0              0  

lrdn1370:234521:234633 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1370:234521:234633 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1370:234521:234633 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
lrdn1370:234521:234633 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1207:1671835:1671950 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
lrdn1207:1671835:1671950 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1268:3521221:3521364 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1268:3521221:3521364 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1268:3521221:3521364 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
lrdn1268:3521221:3521364 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1280:1451550:1451659 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1280:1451550:1451659 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1280:1451550:1451659 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
lrdn1280:1451550:1451659 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1207:1671835:1671950 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1280:1451550:1451659 [0] NCCL INFO ncclCommInitRankConfig comm 0xe3e15b0 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x89845cbc7c6f03b5 - Init COMPLETE
lrdn1280:1451550:1451659 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 4 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1268:3521221:3521364 [0] NCCL INFO ncclCommInitRankConfig comm 0xda93540 rank 1 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x89845cbc7c6f03b5 - Init COMPLETE
lrdn1268:3521221:3521364 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 4 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1370:234521:234633 [0] NCCL INFO ncclCommInitRankConfig comm 0x1450cf50 rank 3 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x89845cbc7c6f03b5 - Init COMPLETE
lrdn1207:1671835:1671950 [0] NCCL INFO ncclCommInitRankConfig comm 0xd0e1770 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x89845cbc7c6f03b5 - Init COMPLETE
lrdn1207:1671835:1671950 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 4 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1370:234521:234633 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 4 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1370:234521:234636 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn1370:234521:234636 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn1370:234521:234637 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 7
lrdn1370:234521:234636 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn1370:234521:234636 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn1370:234521:234636 [0] NCCL INFO Channel 00/0 : 3[0] -> 0[0] [send] via NET/IB/5
lrdn1370:234521:234636 [0] NCCL INFO Channel 01/0 : 3[0] -> 0[0] [send] via NET/IB/6
lrdn1370:234521:234636 [0] NCCL INFO Channel 02/0 : 3[0] -> 0[0] [send] via NET/IB/5
lrdn1280:1451550:1451662 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn1207:1671835:1671954 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 6
lrdn1207:1671835:1671953 [0] NCCL INFO Channel 00/0 : 3[0] -> 0[0] [receive] via NET/IB/5
lrdn1207:1671835:1671953 [0] NCCL INFO Channel 01/0 : 3[0] -> 0[0] [receive] via NET/IB/6
lrdn1207:1671835:1671953 [0] NCCL INFO Channel 02/0 : 3[0] -> 0[0] [receive] via NET/IB/5
lrdn1207:1671835:1671953 [0] NCCL INFO Channel 03/0 : 3[0] -> 0[0] [receive] via NET/IB/6
lrdn1207:1671835:1671953 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1268:3521221:3521367 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1268:3521221:3521367 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn1268:3521221:3521368 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 5
lrdn1268:3521221:3521367 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1268:3521221:3521367 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn1268:3521221:3521367 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn1370:234521:234636 [0] NCCL INFO Channel 03/0 : 3[0] -> 0[0] [send] via NET/IB/6
lrdn1207:1671835:1671953 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1268:3521221:3521367 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn1207:1671835:1671953 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1207:1671835:1671953 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1268:3521221:3521367 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn1268:3521221:3521367 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn1280:1451550:1451663 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 6
lrdn1280:1451550:1451662 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn1280:1451550:1451662 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn1280:1451550:1451662 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn1280:1451550:1451662 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn1280:1451550:1451662 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn1280:1451550:1451662 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn1280:1451550:1451662 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn1370:234521:234636 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1280:1451550:1451662 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1268:3521221:3521367 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1207:1671835:1671953 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1207:1671835:1671835 [0] NCCL INFO Comm config Blocking set to 1
lrdn1280:1451550:1451550 [0] NCCL INFO Comm config Blocking set to 1
lrdn1268:3521221:3521221 [0] NCCL INFO Comm config Blocking set to 1
lrdn1207:1671835:1671956 [0] NCCL INFO Using network IB
lrdn1280:1451550:1451664 [0] NCCL INFO Using network IB
lrdn1370:234521:234521 [0] NCCL INFO Comm config Blocking set to 1
lrdn1207:1671835:1671956 [0] NCCL INFO ncclCommInitRankConfig comm 0xd15d300 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x79d602e8d8120aaf - Init START
lrdn1280:1451550:1451664 [0] NCCL INFO ncclCommInitRankConfig comm 0xe45d140 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x79d602e8d8120aaf - Init START
lrdn1268:3521221:3521369 [0] NCCL INFO Using network IB
lrdn1370:234521:234638 [0] NCCL INFO Using network IB
lrdn1370:234521:234638 [0] NCCL INFO ncclCommInitRankConfig comm 0x14588ae0 rank 3 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x79d602e8d8120aaf - Init START
lrdn1268:3521221:3521369 [0] NCCL INFO ncclCommInitRankConfig comm 0xdb0f0d0 rank 1 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x79d602e8d8120aaf - Init START
lrdn1280:1451550:1451664 [0] NCCL INFO Bootstrap timings total 0.001245 (create 0.000014, send 0.000069, recv 0.000283, ring 0.000463, delay 0.000000)
lrdn1207:1671835:1671956 [0] NCCL INFO Bootstrap timings total 0.001296 (create 0.000022, send 0.000059, recv 0.000447, ring 0.000344, delay 0.000000)
lrdn1370:234521:234638 [0] NCCL INFO Bootstrap timings total 0.001166 (create 0.000017, send 0.000114, recv 0.000347, ring 0.000400, delay 0.000000)
lrdn1268:3521221:3521369 [0] NCCL INFO Bootstrap timings total 0.001107 (create 0.000017, send 0.000074, recv 0.000316, ring 0.000540, delay 0.000000)
lrdn1207:1671835:1671956 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1268:3521221:3521369 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1280:1451550:1451664 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1370:234521:234638 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1280:1451550:1451664 [0] NCCL INFO comm 0xe45d140 rank 2 nRanks 4 nNodes 4 localRanks 1 localRank 0 MNNVL 0
lrdn1280:1451550:1451664 [0] NCCL INFO Trees [0] 1/3/-1->2->0 [1] 1/3/-1->2->0 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1
lrdn1280:1451550:1451664 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1268:3521221:3521369 [0] NCCL INFO comm 0xdb0f0d0 rank 1 nRanks 4 nNodes 4 localRanks 1 localRank 0 MNNVL 0
lrdn1268:3521221:3521369 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] -1/-1/-1->1->2 [2] 2/0/-1->1->3 [3] 2/0/-1->1->3
lrdn1268:3521221:3521369 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1280:1451550:1451665 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1280:1451550:1451666 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn1268:3521221:3521370 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn1268:3521221:3521371 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn1370:234521:234638 [0] NCCL INFO comm 0x14588ae0 rank 3 nRanks 4 nNodes 4 localRanks 1 localRank 0 MNNVL 0
lrdn1370:234521:234638 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 1/-1/-1->3->-1 [3] 1/-1/-1->3->-1
lrdn1370:234521:234638 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1207:1671835:1671956 [0] NCCL INFO comm 0xd15d300 rank 0 nRanks 4 nNodes 4 localRanks 1 localRank 0 MNNVL 0
lrdn1207:1671835:1671956 [0] NCCL INFO Channel 00/04 : 0 1 2 3
lrdn1207:1671835:1671956 [0] NCCL INFO Channel 01/04 : 0 1 2 3
lrdn1207:1671835:1671956 [0] NCCL INFO Channel 02/04 : 0 1 2 3
lrdn1207:1671835:1671956 [0] NCCL INFO Channel 03/04 : 0 1 2 3
lrdn1207:1671835:1671956 [0] NCCL INFO Trees [0] 2/-1/-1->0->-1 [1] 2/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
lrdn1207:1671835:1671956 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1207:1671835:1671956 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1207:1671835:1671957 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn1207:1671835:1671958 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn1370:234521:234639 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1370:234521:234640 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn1268:3521221:3521369 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1268:3521221:3521369 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1268:3521221:3521369 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
lrdn1268:3521221:3521369 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1370:234521:234638 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1370:234521:234638 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1370:234521:234638 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
lrdn1207:1671835:1671956 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1207:1671835:1671956 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1207:1671835:1671956 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             0              1              0              0              0              0              0  
       Reduce |        0         0         1   |             0              1              0              0              0              0              0  
    AllGather |        0         0         1   |             0              1              0              0              0              0              0  
ReduceScatter |        0         0         1   |             0              1              0              0              0              0              0  
    AllReduce |        0         0         1   |             0              1              0              0              0              0              0  

lrdn1370:234521:234638 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1207:1671835:1671956 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
lrdn1207:1671835:1671956 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1280:1451550:1451664 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1280:1451550:1451664 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1280:1451550:1451664 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
lrdn1280:1451550:1451664 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1207:1671835:1671956 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1268:3521221:3521369 [0] NCCL INFO ncclCommInitRankConfig comm 0xdb0f0d0 rank 1 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x79d602e8d8120aaf - Init COMPLETE
lrdn1268:3521221:3521369 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 4 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1280:1451550:1451664 [0] NCCL INFO ncclCommInitRankConfig comm 0xe45d140 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x79d602e8d8120aaf - Init COMPLETE
lrdn1280:1451550:1451664 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 4 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1207:1671835:1671956 [0] NCCL INFO ncclCommInitRankConfig comm 0xd15d300 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x79d602e8d8120aaf - Init COMPLETE
lrdn1207:1671835:1671956 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 4 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1370:234521:234638 [0] NCCL INFO ncclCommInitRankConfig comm 0x14588ae0 rank 3 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x79d602e8d8120aaf - Init COMPLETE
lrdn1370:234521:234638 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 4 total 0.05 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1268:3521221:3521372 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1268:3521221:3521373 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 9
lrdn1268:3521221:3521372 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn1268:3521221:3521372 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1268:3521221:3521372 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn1268:3521221:3521372 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn1268:3521221:3521372 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn1268:3521221:3521372 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn1268:3521221:3521372 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn1207:1671835:1671960 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 12
lrdn1207:1671835:1671959 [0] NCCL INFO Channel 00/0 : 3[0] -> 0[0] [receive] via NET/IB/5
lrdn1207:1671835:1671959 [0] NCCL INFO Channel 01/0 : 3[0] -> 0[0] [receive] via NET/IB/6
lrdn1207:1671835:1671959 [0] NCCL INFO Channel 02/0 : 3[0] -> 0[0] [receive] via NET/IB/5
lrdn1207:1671835:1671959 [0] NCCL INFO Channel 03/0 : 3[0] -> 0[0] [receive] via NET/IB/6
lrdn1280:1451550:1451667 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn1207:1671835:1671959 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1207:1671835:1671959 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1207:1671835:1671959 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1280:1451550:1451668 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 8
lrdn1280:1451550:1451667 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn1207:1671835:1671959 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1280:1451550:1451667 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn1280:1451550:1451667 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn1280:1451550:1451667 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn1280:1451550:1451667 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn1280:1451550:1451667 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn1280:1451550:1451667 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn1370:234521:234641 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn1370:234521:234642 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 9
lrdn1370:234521:234641 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn1370:234521:234641 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn1370:234521:234641 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn1370:234521:234641 [0] NCCL INFO Channel 00/0 : 3[0] -> 0[0] [send] via NET/IB/5
lrdn1370:234521:234641 [0] NCCL INFO Channel 01/0 : 3[0] -> 0[0] [send] via NET/IB/6
lrdn1370:234521:234641 [0] NCCL INFO Channel 02/0 : 3[0] -> 0[0] [send] via NET/IB/5
lrdn1370:234521:234641 [0] NCCL INFO Channel 03/0 : 3[0] -> 0[0] [send] via NET/IB/6
lrdn1207:1671835:1671959 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1268:3521221:3521372 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1280:1451550:1451667 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1370:234521:234641 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1207:1671835:1671835 [0] NCCL INFO Comm config Blocking set to 1
lrdn1280:1451550:1451550 [0] NCCL INFO Comm config Blocking set to 1
lrdn1207:1671835:1671962 [0] NCCL INFO Using network IB
lrdn1207:1671835:1671962 [0] NCCL INFO ncclCommInitRankConfig comm 0xd1d8e90 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x61a688afe31479e4 - Init START
lrdn1280:1451550:1451669 [0] NCCL INFO Using network IB
lrdn1280:1451550:1451669 [0] NCCL INFO ncclCommInitRankConfig comm 0xe4d8cd0 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x61a688afe31479e4 - Init START
lrdn1268:3521221:3521221 [0] NCCL INFO Comm config Blocking set to 1
lrdn1370:234521:234521 [0] NCCL INFO Comm config Blocking set to 1
lrdn1370:234521:234643 [0] NCCL INFO Using network IB
lrdn1370:234521:234643 [0] NCCL INFO ncclCommInitRankConfig comm 0x14604670 rank 3 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x61a688afe31479e4 - Init START
lrdn1268:3521221:3521374 [0] NCCL INFO Using network IB
lrdn1268:3521221:3521374 [0] NCCL INFO ncclCommInitRankConfig comm 0xdb8ac60 rank 1 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x61a688afe31479e4 - Init START
lrdn1268:3521221:3521374 [0] NCCL INFO Bootstrap timings total 0.000951 (create 0.000016, send 0.000064, recv 0.000256, ring 0.000126, delay 0.000000)
lrdn1370:234521:234643 [0] NCCL INFO Bootstrap timings total 0.001150 (create 0.000026, send 0.000052, recv 0.000287, ring 0.000689, delay 0.000000)
lrdn1280:1451550:1451669 [0] NCCL INFO Bootstrap timings total 0.001215 (create 0.000015, send 0.000088, recv 0.000203, ring 0.000110, delay 0.000000)
lrdn1207:1671835:1671962 [0] NCCL INFO Bootstrap timings total 0.001321 (create 0.000015, send 0.000059, recv 0.000527, ring 0.000573, delay 0.000000)
lrdn1268:3521221:3521374 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1370:234521:234643 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1280:1451550:1451669 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1207:1671835:1671962 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1268:3521221:3521374 [0] NCCL INFO comm 0xdb8ac60 rank 1 nRanks 4 nNodes 4 localRanks 1 localRank 0 MNNVL 0
lrdn1268:3521221:3521374 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] -1/-1/-1->1->2 [2] 2/0/-1->1->3 [3] 2/0/-1->1->3
lrdn1268:3521221:3521374 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1280:1451550:1451669 [0] NCCL INFO comm 0xe4d8cd0 rank 2 nRanks 4 nNodes 4 localRanks 1 localRank 0 MNNVL 0
lrdn1280:1451550:1451669 [0] NCCL INFO Trees [0] 1/3/-1->2->0 [1] 1/3/-1->2->0 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1
lrdn1280:1451550:1451669 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1370:234521:234643 [0] NCCL INFO comm 0x14604670 rank 3 nRanks 4 nNodes 4 localRanks 1 localRank 0 MNNVL 0
lrdn1370:234521:234643 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 1/-1/-1->3->-1 [3] 1/-1/-1->3->-1
lrdn1370:234521:234643 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1268:3521221:3521375 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn1268:3521221:3521376 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn1207:1671835:1671962 [0] NCCL INFO comm 0xd1d8e90 rank 0 nRanks 4 nNodes 4 localRanks 1 localRank 0 MNNVL 0
lrdn1207:1671835:1671962 [0] NCCL INFO Channel 00/04 : 0 1 2 3
lrdn1207:1671835:1671962 [0] NCCL INFO Channel 01/04 : 0 1 2 3
lrdn1207:1671835:1671962 [0] NCCL INFO Channel 02/04 : 0 1 2 3
lrdn1207:1671835:1671962 [0] NCCL INFO Channel 03/04 : 0 1 2 3
lrdn1207:1671835:1671962 [0] NCCL INFO Trees [0] 2/-1/-1->0->-1 [1] 2/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
lrdn1207:1671835:1671962 [0] NCCL INFO P2P Chunksize set to 131072
lrdn1207:1671835:1671962 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1207:1671835:1671963 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1207:1671835:1671964 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1370:234521:234644 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn1370:234521:234645 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn1280:1451550:1451670 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn1280:1451550:1451671 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn1280:1451550:1451669 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1280:1451550:1451669 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1280:1451550:1451669 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
lrdn1280:1451550:1451669 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1370:234521:234643 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1370:234521:234643 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1370:234521:234643 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
lrdn1370:234521:234643 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1268:3521221:3521374 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1268:3521221:3521374 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1268:3521221:3521374 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
lrdn1268:3521221:3521374 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1207:1671835:1671962 [0] NCCL INFO NCCL_PROTO set by environment to SIMPLE
lrdn1207:1671835:1671962 [0] NCCL INFO NCCL_ALGO set by environment to ring
lrdn1207:1671835:1671962 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             0              1              0              0              0              0              0  
       Reduce |        0         0         1   |             0              1              0              0              0              0              0  
    AllGather |        0         0         1   |             0              1              0              0              0              0              0  
ReduceScatter |        0         0         1   |             0              1              0              0              0              0              0  
    AllReduce |        0         0         1   |             0              1              0              0              0              0              0  

lrdn1207:1671835:1671962 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
lrdn1207:1671835:1671962 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
lrdn1207:1671835:1671962 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1280:1451550:1451669 [0] NCCL INFO ncclCommInitRankConfig comm 0xe4d8cd0 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x61a688afe31479e4 - Init COMPLETE
lrdn1280:1451550:1451669 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 4 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1370:234521:234643 [0] NCCL INFO ncclCommInitRankConfig comm 0x14604670 rank 3 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x61a688afe31479e4 - Init COMPLETE
lrdn1370:234521:234643 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 4 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1268:3521221:3521374 [0] NCCL INFO ncclCommInitRankConfig comm 0xdb8ac60 rank 1 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x61a688afe31479e4 - Init COMPLETE
lrdn1268:3521221:3521374 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 4 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1207:1671835:1671962 [0] NCCL INFO ncclCommInitRankConfig comm 0xd1d8e90 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x61a688afe31479e4 - Init COMPLETE
lrdn1207:1671835:1671962 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 4 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.02, connections 0.00, rest 0.00)
lrdn1207:1671835:1671965 [0] NCCL INFO Channel 00/0 : 3[0] -> 0[0] [receive] via NET/IB/5
lrdn1207:1671835:1671965 [0] NCCL INFO Channel 01/0 : 3[0] -> 0[0] [receive] via NET/IB/6
lrdn1207:1671835:1671966 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 9
lrdn1207:1671835:1671965 [0] NCCL INFO Channel 02/0 : 3[0] -> 0[0] [receive] via NET/IB/5
lrdn1207:1671835:1671965 [0] NCCL INFO Channel 03/0 : 3[0] -> 0[0] [receive] via NET/IB/6
lrdn1268:3521221:3521377 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1268:3521221:3521378 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 11
lrdn1207:1671835:1671965 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1207:1671835:1671965 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1207:1671835:1671965 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/5
lrdn1207:1671835:1671965 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/6
lrdn1280:1451550:1451672 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn1280:1451550:1451672 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn1280:1451550:1451673 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 11
lrdn1280:1451550:1451672 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [receive] via NET/IB/5
lrdn1280:1451550:1451672 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [receive] via NET/IB/6
lrdn1280:1451550:1451672 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn1280:1451550:1451672 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn1280:1451550:1451672 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [send] via NET/IB/5
lrdn1280:1451550:1451672 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [send] via NET/IB/6
lrdn1370:234521:234646 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn1370:234521:234647 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 11
lrdn1268:3521221:3521377 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn1268:3521221:3521377 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/5
lrdn1268:3521221:3521377 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/6
lrdn1268:3521221:3521377 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn1268:3521221:3521377 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn1370:234521:234646 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn1370:234521:234646 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [receive] via NET/IB/5
lrdn1268:3521221:3521377 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [send] via NET/IB/5
lrdn1268:3521221:3521377 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [send] via NET/IB/6
lrdn1370:234521:234646 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [receive] via NET/IB/6
lrdn1370:234521:234646 [0] NCCL INFO Channel 00/0 : 3[0] -> 0[0] [send] via NET/IB/5
lrdn1370:234521:234646 [0] NCCL INFO Channel 01/0 : 3[0] -> 0[0] [send] via NET/IB/6
lrdn1370:234521:234646 [0] NCCL INFO Channel 02/0 : 3[0] -> 0[0] [send] via NET/IB/5
lrdn1370:234521:234646 [0] NCCL INFO Channel 03/0 : 3[0] -> 0[0] [send] via NET/IB/6
lrdn1207:1671835:1671965 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1268:3521221:3521377 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1370:234521:234646 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
lrdn1280:1451550:1451672 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
[38;20m2025-08-02 00:13:20,370 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 116.37/116.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:13:33,770 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.61/115.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:13:47,613 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 111.96/111.12 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:14:01,460 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 112.13/110.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:14:14,786 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 115.40/116.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:14:28,150 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 114.86/115.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:14:42,158 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 108.38/111.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:14:55,993 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 111.53/112.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:15:09,500 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 115.65/111.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:15:22,894 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 114.92/114.34 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:15:36,954 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.80 (max/real adjusted throughput: 110.32/106.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:15:50,803 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 111.98/112.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:16:04,201 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 114.31/115.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:16:17,607 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 114.20/115.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:16:31,421 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 111.90/112.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:16:45,630 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 106.61/109.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:16:58,980 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 115.16/115.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:17:12,375 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 114.77/114.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:17:26,493 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 106.08/111.34 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:17:40,234 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 116.31/109.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:17:53,610 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 114.94/115.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:18:06,954 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.25/115.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:18:21,144 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 108.98/109.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:18:35,067 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 112.69/110.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:18:48,451 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 114.45/115.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:19:01,769 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.77/114.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:19:15,594 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 111.68/111.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:19:29,866 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 107.31/110.09 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:19:43,282 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 115.93/113.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:19:56,757 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 114.32/113.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:20:10,623 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 110.92/111.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:20:24,832 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.80 (max/real adjusted throughput: 110.48/107.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:20:38,314 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 114.95/115.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:20:51,685 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.15/115.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:21:05,583 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 111.93/111.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:21:19,490 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 110.38/110.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:21:32,909 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 113.88/115.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:21:46,306 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 116.06/114.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:22:00,550 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 106.86/110.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:22:14,458 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 110.67/109.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:22:27,888 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.12/115.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:22:41,258 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.17/115.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:22:55,462 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.83 (max/real adjusted throughput: 111.39/105.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:23:09,292 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 112.01/111.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:23:22,747 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 115.63/113.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:23:36,145 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 114.31/114.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:23:50,084 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 111.05/111.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:24:04,301 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 106.78/111.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:24:17,750 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.05/114.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:24:31,152 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.11/114.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:24:45,357 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 106.89/111.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:24:59,321 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.78 (max/real adjusted throughput: 112.07/108.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:25:12,782 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 114.75/115.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:25:26,166 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 114.51/115.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:25:40,379 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.80 (max/real adjusted throughput: 109.84/106.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:25:54,344 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 110.65/109.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:26:07,817 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 115.40/114.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:26:21,143 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.66/115.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:26:34,973 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 109.91/112.83 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:26:49,253 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 108.11/108.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:27:02,679 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 114.94/114.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:27:16,119 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 114.91/114.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:27:29,959 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 111.21/110.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:27:43,733 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 112.05/112.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:27:57,277 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 114.16/113.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:28:10,680 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 114.82/115.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:28:24,369 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 110.96/116.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:28:37,772 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 116.02/114.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:28:51,104 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.73/115.34 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:29:04,471 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.92/115.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:29:18,647 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.85 (max/real adjusted throughput: 112.02/104.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:29:32,485 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 112.90/110.70 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:29:45,912 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.00/114.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:29:59,376 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 114.22/115.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:30:13,245 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 110.32/111.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:30:27,537 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 107.28/109.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:30:40,940 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 113.98/115.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:30:54,458 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 114.96/113.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:31:08,754 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 106.57/110.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:31:22,634 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 110.40/111.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:31:36,115 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 114.62/114.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:31:49,563 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 114.89/114.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:32:03,704 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.82 (max/real adjusted throughput: 110.97/106.04 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:32:17,677 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 108.93/110.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:32:31,163 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 115.22/113.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:32:44,670 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 113.98/114.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:32:58,565 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 110.04/110.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:33:12,988 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.79 (max/real adjusted throughput: 107.87/107.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:33:26,432 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 114.37/114.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:33:39,851 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.93/115.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:33:53,824 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 112.76/109.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:34:07,712 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 110.89/110.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:34:21,188 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 113.80/114.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:34:34,634 | xffl.distributed.aggregation |     INFO | layer_by_layer - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 115.31/113.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:34:48,902 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 103.33/110.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:35:02,770 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 110.68/111.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:35:16,206 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 113.66/115.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:35:29,517 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.69/115.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:35:43,592 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.80 (max/real adjusted throughput: 111.77/107.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:35:57,723 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 108.66/111.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:36:11,008 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 116.56/115.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:36:24,361 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 116.49/114.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:36:38,312 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 109.50/110.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:36:52,615 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.84 (max/real adjusted throughput: 111.12/105.00 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:37:05,990 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.96/114.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:37:19,375 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 114.28/115.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:37:33,603 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 107.01/111.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:37:47,572 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 111.33/109.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:38:00,926 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.21/115.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:38:14,205 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 116.58/116.45 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:38:28,146 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 110.65/111.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:38:42,517 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 103.69/110.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:38:55,909 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.47/114.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:39:09,278 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.29/115.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:39:23,264 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 109.59/110.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:39:37,359 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.83 (max/real adjusted throughput: 112.09/105.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:39:50,675 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 116.19/116.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:40:04,022 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.53/115.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:40:18,194 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 109.89/109.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:40:32,235 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 110.64/109.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:40:45,568 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 115.93/115.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:40:59,055 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 114.24/114.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:41:13,278 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.83 (max/real adjusted throughput: 111.58/105.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:41:27,291 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 110.60/109.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:41:40,606 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 116.29/115.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:41:53,990 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 114.98/115.88 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:42:07,851 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 110.91/111.20 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:42:22,035 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 104.84/110.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:42:35,363 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 115.71/116.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:42:48,735 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 115.14/115.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:43:03,019 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.78 (max/real adjusted throughput: 106.93/108.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:43:16,932 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 110.76/110.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:43:30,353 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 114.57/115.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:43:43,674 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 116.26/115.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:43:57,841 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.84 (max/real adjusted throughput: 111.24/104.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:44:11,712 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 112.31/111.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:44:25,021 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 115.71/115.91 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:44:38,362 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.66/115.34 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:44:51,971 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 113.22/113.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:45:05,990 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 111.09/111.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:45:19,410 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 113.57/115.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:45:32,659 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 116.93/116.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:45:46,316 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 114.51/112.34 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:45:59,923 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 114.82/112.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:46:13,204 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 116.52/115.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:46:26,596 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 116.00/114.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:46:40,505 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 111.12/111.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:46:54,149 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 113.10/112.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:47:07,462 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 115.56/116.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:47:20,710 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 116.39/116.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:47:34,352 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 112.49/114.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:47:48,314 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 108.30/113.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:48:01,677 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 116.36/115.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:48:14,938 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 116.57/116.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:48:28,526 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 114.57/112.32 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:48:42,580 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.84 (max/real adjusted throughput: 114.18/104.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:48:56,045 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 114.50/114.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:49:09,434 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.03/115.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:49:23,699 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 104.73/111.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:49:37,404 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 113.83/112.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:49:50,784 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.09/115.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:50:04,163 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.01/114.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:50:18,310 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.83 (max/real adjusted throughput: 111.18/105.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:50:32,547 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 107.85/110.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:50:45,904 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.49/115.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:50:59,258 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.09/115.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:51:13,158 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 112.10/109.91 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:51:27,653 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.85 (max/real adjusted throughput: 108.97/104.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:51:41,008 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.70/115.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:51:54,255 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 116.92/116.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:52:08,193 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 111.31/110.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:52:22,162 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 110.31/110.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:52:35,577 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 116.34/114.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:52:48,859 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 115.87/116.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:53:03,172 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 106.18/109.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:53:17,050 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 112.21/110.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:53:30,334 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 115.60/116.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:53:43,646 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 115.84/116.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:53:57,631 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.78 (max/real adjusted throughput: 111.93/108.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:54:11,884 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 105.61/112.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:54:25,287 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.25/115.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:54:38,673 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.48/115.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:54:52,633 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 109.31/112.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:55:06,917 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.86 (max/real adjusted throughput: 110.98/103.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:55:20,258 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 116.20/115.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:55:33,587 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 115.16/116.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:55:47,987 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 104.76/109.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:56:01,867 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 110.08/112.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:56:15,254 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.85/114.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 00:56:28,692 | xffl.distributed.aggregation |     INFO | layer_by_layer_optimized - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 115.02/114.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 00:56:42,723 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 112.88/109.14 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 00:56:56,953 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 111.95/110.17 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 00:57:10,597 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 117.77/111.81 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 00:57:24,237 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 116.93/112.20 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 00:57:38,506 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 111.55/108.75 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 00:57:52,610 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 113.30/109.29 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 00:58:06,338 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 114.31/112.68 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 00:58:19,896 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 117.12/112.75 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 00:58:34,143 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 112.36/108.73 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 00:58:48,455 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 110.62/109.07 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 00:59:02,097 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 116.24/112.78 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 00:59:15,722 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 117.32/112.00 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 00:59:29,840 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.79 (max/real adjusted throughput: 113.12/107.88 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 00:59:43,972 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 112.16/109.58 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 00:59:57,494 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 117.45/112.93 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:00:11,043 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 117.37/113.20 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:00:25,233 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 111.30/110.56 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:00:39,359 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 113.69/110.26 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:00:52,938 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 117.51/112.60 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:01:06,554 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 117.42/112.61 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:01:20,775 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.78 (max/real adjusted throughput: 111.78/108.41 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:01:34,940 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.78 (max/real adjusted throughput: 113.18/108.13 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:01:48,527 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 116.15/113.55 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:02:02,202 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 117.48/111.03 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:02:16,345 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.78 (max/real adjusted throughput: 113.43/108.52 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:02:30,485 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 111.52/109.84 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:02:44,086 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 116.96/112.45 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:02:57,693 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 115.67/113.24 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:03:11,886 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 111.51/110.28 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:03:26,100 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 112.95/109.81 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:03:39,693 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 117.49/111.59 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:03:53,328 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 117.14/111.38 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:04:07,531 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 111.48/109.06 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:04:21,730 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 112.97/108.68 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:04:35,361 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 117.19/111.82 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:04:48,985 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 117.25/112.16 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:05:03,158 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 111.85/109.39 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:05:17,382 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 111.16/109.65 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:05:31,036 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 116.11/111.80 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:05:44,755 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 117.24/109.39 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:05:58,949 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.78 (max/real adjusted throughput: 112.33/108.42 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:06:13,046 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 113.22/109.53 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:06:26,669 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 115.96/113.24 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:06:40,382 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 116.99/112.19 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:06:54,549 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 113.11/109.17 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:07:08,794 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 112.12/109.80 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:07:22,371 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 117.84/111.77 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:07:35,996 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 116.63/112.16 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:07:50,256 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 111.16/108.72 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:08:04,435 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 112.69/109.38 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:08:18,019 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 116.61/113.48 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:08:31,650 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 116.12/112.71 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:08:46,003 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.80 (max/real adjusted throughput: 110.88/106.78 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:09:00,346 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 109.79/109.07 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:09:13,925 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 117.41/111.86 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:09:27,479 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 116.61/113.77 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:09:41,792 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.80 (max/real adjusted throughput: 110.79/107.19 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:09:56,074 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.79 (max/real adjusted throughput: 111.64/107.60 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:10:09,660 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 116.38/113.08 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:10:23,183 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 116.96/113.91 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:10:37,376 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 111.89/109.69 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:10:51,686 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.78 (max/real adjusted throughput: 110.04/108.44 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:11:05,310 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 117.67/111.15 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:11:18,938 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 116.27/111.95 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:11:33,192 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.78 (max/real adjusted throughput: 110.73/108.21 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:11:47,476 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.78 (max/real adjusted throughput: 112.03/108.18 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:12:01,206 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 115.11/112.19 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:12:14,869 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 115.90/112.34 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:12:29,099 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 111.63/108.85 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:12:43,240 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 113.05/109.45 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:12:56,901 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 116.23/112.11 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:13:10,529 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 116.60/112.94 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:13:24,845 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 110.83/108.71 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:13:39,129 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.79 (max/real adjusted throughput: 112.32/107.68 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:13:52,717 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 117.65/112.78 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:14:06,321 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 116.75/112.75 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:14:20,587 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.80 (max/real adjusted throughput: 113.12/107.03 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:14:34,724 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 113.28/109.58 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:14:48,311 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 116.26/114.00 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:15:01,841 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 117.54/113.04 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:15:16,055 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.78 (max/real adjusted throughput: 112.31/108.04 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:15:30,208 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 111.43/109.26 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:15:43,833 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 115.44/113.86 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:15:57,494 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 117.38/111.57 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:16:11,581 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 112.84/111.65 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:16:25,726 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 113.07/108.83 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:16:39,276 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 117.46/113.16 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:16:52,976 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 117.11/112.37 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:17:07,103 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 113.08/109.71 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:17:21,296 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 110.87/110.64 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:17:34,972 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 116.67/111.92 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:17:48,667 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 116.85/111.26 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:18:02,761 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 112.09/110.57 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:18:16,904 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 111.05/110.91 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:18:30,526 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 116.34/113.07 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m
[38;20m2025-08-02 01:18:44,151 | xffl.distributed.aggregation |     INFO | bucket_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 117.37/112.85 Gb/s - Max GPU RAM allocated: 37.71 GB)[0m


[38;20m2025-08-02 01:18:57,935 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 112.28/113.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:19:11,731 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 111.20/111.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:19:24,749 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 118.46/118.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:19:37,836 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.62/118.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:19:51,183 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 116.10/114.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:20:04,234 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 118.53/117.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:20:17,229 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 118.95/118.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:20:30,232 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 118.53/118.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:20:43,324 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 118.18/117.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:20:56,448 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.75/117.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:21:09,460 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.62 (max/real adjusted throughput: 118.63/118.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:21:22,521 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 118.24/118.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:21:36,251 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 112.76/112.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:21:49,832 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 113.22/114.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:22:02,925 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 118.16/117.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:22:15,990 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 118.52/117.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:22:29,730 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 112.18/111.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:22:43,837 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 108.29/110.77 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:22:57,007 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 116.85/116.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:23:10,130 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 116.53/117.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:23:23,753 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 113.97/113.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:23:37,866 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 107.44/111.15 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:23:50,951 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.69/117.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:24:04,068 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.66/117.58 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:24:18,072 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.81 (max/real adjusted throughput: 112.55/106.63 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:24:31,727 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 112.63/112.91 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:24:44,819 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.47/117.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:24:57,940 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.52/117.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:25:12,541 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.80 (max/real adjusted throughput: 112.40/107.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:25:26,286 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 112.87/112.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:25:39,431 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 116.89/117.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:25:52,621 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 117.24/116.06 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:26:06,368 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 113.63/113.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:26:20,550 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 104.06/114.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:26:33,655 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 116.77/118.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:26:46,767 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.90/117.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:27:00,759 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 107.50/112.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:27:14,521 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 113.28/110.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:27:27,650 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.92/117.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:27:40,793 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.04/117.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:27:54,478 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 112.68/112.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:28:08,140 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 114.11/111.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:28:21,349 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 116.31/116.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:28:34,504 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 117.08/116.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:28:48,113 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 113.34/113.34 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:29:01,768 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 113.13/112.61 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:29:14,940 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 117.26/116.73 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:29:28,604 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 115.89/116.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:29:42,226 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 113.50/113.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:29:55,906 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 112.43/112.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:30:09,022 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.34/117.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:30:22,237 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 116.15/116.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:30:35,853 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 113.51/113.17 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:30:49,484 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 111.64/113.90 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:31:02,700 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 116.75/116.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:31:15,807 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.30/118.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:31:29,375 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 113.34/114.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:31:42,979 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 112.86/113.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:31:56,232 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 117.36/115.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:32:09,321 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.77/118.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:32:22,977 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 113.45/112.26 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:32:36,823 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.79 (max/real adjusted throughput: 114.34/107.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:32:49,961 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.20/117.38 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:33:03,175 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 117.38/116.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:33:16,831 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 111.90/112.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:33:30,609 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 113.55/109.65 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:33:43,738 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.06/117.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:33:56,823 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.88/117.52 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:34:10,588 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 111.40/111.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:34:24,479 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 107.68/113.02 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:34:37,572 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 118.17/117.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:34:50,699 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.36/117.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:35:04,407 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 111.58/112.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:35:18,414 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 105.12/113.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:35:31,590 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 117.19/116.24 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:35:44,651 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.58/118.50 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:35:58,648 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 108.94/111.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:36:12,230 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 112.68/114.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:36:25,403 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 116.94/117.13 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:36:38,541 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.50/117.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:36:52,535 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.78 (max/real adjusted throughput: 112.99/108.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:37:06,104 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 114.42/113.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:37:19,218 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 116.94/117.87 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:37:32,397 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 117.95/116.21 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:37:46,266 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 109.75/112.36 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:37:59,885 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 113.98/112.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:38:13,006 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.05/118.15 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:38:26,105 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.63/117.46 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:38:40,172 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 105.96/112.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:38:53,866 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 113.26/112.01 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:39:06,991 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.92/117.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:39:20,171 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 116.64/117.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:39:33,776 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 112.86/114.57 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:39:47,415 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 113.46/112.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:40:00,578 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 118.13/116.89 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 01:40:13,826 | xffl.distributed.aggregation |     INFO | bucket_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 116.72/115.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 01:40:27,762 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 113.24/110.15 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:40:41,827 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 111.90/111.64 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:40:55,283 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 118.31/113.68 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:41:08,779 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 118.12/113.71 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:41:22,834 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 113.20/110.51 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:41:36,962 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 112.43/109.74 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:41:50,368 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 118.46/114.52 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:42:03,864 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 118.16/113.51 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:42:17,953 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 112.93/110.11 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:42:32,081 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 112.72/110.13 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:42:45,514 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 118.61/113.96 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:42:59,023 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 117.76/113.70 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:43:13,098 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 114.18/109.72 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:43:27,233 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 113.35/108.98 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:43:40,683 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 118.21/113.78 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:43:54,072 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 118.39/115.12 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:44:08,198 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 112.98/109.51 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:44:22,426 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.82 (max/real adjusted throughput: 114.35/106.05 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:44:35,917 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 118.44/113.14 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:44:49,360 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 117.95/114.36 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:45:03,449 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 114.46/109.00 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:45:17,756 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.82 (max/real adjusted throughput: 113.60/105.96 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:45:31,219 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 118.59/113.96 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:45:44,656 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 118.49/114.15 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:45:58,862 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.78 (max/real adjusted throughput: 113.09/108.19 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:46:13,310 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 107.31/110.18 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:46:26,788 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 118.18/113.45 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:46:40,291 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 118.39/113.26 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:46:54,661 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.83 (max/real adjusted throughput: 112.79/105.54 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:47:09,061 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 110.65/109.65 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:47:22,513 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 118.28/114.57 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:47:36,023 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 117.98/113.12 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:47:50,336 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 110.86/109.78 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:48:04,426 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 113.93/109.25 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:48:17,887 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 117.97/114.30 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:48:31,373 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 117.95/113.93 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:48:45,609 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 113.02/109.89 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:48:59,806 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 113.36/108.62 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:49:13,260 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 118.25/114.52 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:49:26,707 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 118.40/114.00 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:49:40,739 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 114.18/110.11 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:49:54,812 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 113.60/109.24 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:50:08,302 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 118.49/113.83 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:50:21,790 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 118.13/113.68 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:50:35,890 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 112.43/110.90 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:50:50,019 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 112.60/109.20 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:51:03,496 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 118.13/113.76 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:51:17,000 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 117.72/113.80 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:51:31,105 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 112.55/111.63 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:51:45,190 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 112.86/110.37 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:51:58,636 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 118.57/114.21 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:52:12,118 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 118.21/114.09 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:52:26,278 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 112.19/109.16 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:52:40,699 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.82 (max/real adjusted throughput: 110.22/106.15 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:52:54,153 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 118.28/113.61 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:53:07,678 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 117.74/113.65 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:53:21,883 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 113.16/108.77 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:53:36,145 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.80 (max/real adjusted throughput: 112.96/107.14 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:53:49,642 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 118.46/113.57 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:54:03,125 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 118.17/113.76 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:54:17,289 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 112.88/108.61 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:54:31,861 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 106.54/111.19 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:54:45,349 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 117.22/114.13 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:54:58,930 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 117.73/112.28 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:55:13,186 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.80 (max/real adjusted throughput: 112.34/107.24 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:55:27,697 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.78 (max/real adjusted throughput: 108.16/108.31 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:55:41,224 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 118.13/112.77 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:55:54,729 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 117.71/113.46 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:56:09,211 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.81 (max/real adjusted throughput: 111.59/106.45 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:56:23,311 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 112.62/111.01 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:56:36,756 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 118.29/114.19 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:56:50,241 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 118.00/113.59 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:57:04,793 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.87 (max/real adjusted throughput: 112.89/103.17 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:57:18,916 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 114.01/108.78 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:57:32,444 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 117.26/113.66 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:57:45,919 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 117.67/114.17 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:58:00,424 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.79 (max/real adjusted throughput: 111.33/107.67 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:58:14,649 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.77 (max/real adjusted throughput: 112.44/108.66 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:58:28,169 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 118.29/113.02 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:58:41,657 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 117.45/113.67 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:58:55,605 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 113.54/111.20 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:59:09,517 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 115.23/110.62 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 01:59:22,977 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 117.80/114.63 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 01:59:36,390 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 118.44/114.31 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 01:59:50,268 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 115.24/111.43 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:00:04,225 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 114.48/110.38 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:00:17,704 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 118.51/113.44 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:00:31,150 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 118.14/114.25 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:00:44,984 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 114.80/112.71 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:00:58,834 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 114.92/112.41 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:01:12,321 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 117.79/113.98 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:01:25,838 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 117.65/113.71 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:01:39,729 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 116.17/111.06 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:01:53,705 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.75 (max/real adjusted throughput: 114.68/110.43 Gb/s - Max GPU RAM allocated: 48.18 GB)[0m
[38;20m2025-08-02 02:02:07,232 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 117.83/113.71 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m
[38;20m2025-08-02 02:02:20,734 | xffl.distributed.aggregation |     INFO | bucket_optimized_flatten - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 118.43/113.08 Gb/s - Max GPU RAM allocated: 36.48 GB)[0m


[38;20m2025-08-02 02:02:34,352 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 113.80/114.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:02:48,054 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 109.11/115.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:03:01,146 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.19/118.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:03:14,227 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 118.28/117.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:03:27,754 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 113.97/114.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:03:41,595 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.73 (max/real adjusted throughput: 111.83/111.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:03:54,713 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.70/117.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:04:07,797 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.97/118.16 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:04:21,488 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.74 (max/real adjusted throughput: 115.01/110.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:04:35,062 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 113.11/114.25 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:04:48,113 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 118.22/118.07 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:05:01,210 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=ring - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 118.07/117.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:05:14,743 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 113.11/113.91 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:05:28,533 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 110.35/114.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:05:41,623 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.87/117.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:05:54,697 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 118.51/117.56 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:06:08,455 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.76 (max/real adjusted throughput: 114.72/109.68 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:06:21,903 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.96/114.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:06:34,976 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 118.21/117.49 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:06:48,047 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.76/118.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:07:01,919 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.79 (max/real adjusted throughput: 114.30/107.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:07:15,403 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 114.43/114.42 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:07:28,534 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.13/117.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:07:41,626 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=tree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 118.27/117.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:07:55,406 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 109.86/114.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:08:08,873 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 113.91/114.92 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:08:21,951 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 118.62/117.74 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:08:35,051 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 118.59/117.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:08:48,934 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 108.75/114.69 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:09:02,413 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 113.80/114.81 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:09:15,480 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 118.02/118.08 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:09:28,548 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.96/118.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:09:42,267 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 112.35/113.55 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:09:55,703 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 116.20/113.22 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:10:08,815 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 116.96/118.31 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:10:21,916 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnet - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.84/117.66 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:10:35,663 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 112.58/111.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:10:49,248 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 113.54/114.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:11:02,353 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.48/117.40 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:11:15,451 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.59/117.82 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:11:29,138 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 110.65/114.30 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:11:42,641 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 115.13/113.78 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:11:55,718 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 118.80/116.93 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:12:08,808 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.43/118.03 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:12:22,029 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 115.83/117.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:12:35,219 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 117.33/115.95 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:12:48,267 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.90/118.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:13:01,347 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetchain - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.98/117.53 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:13:14,877 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 112.37/115.71 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:13:28,283 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.11/114.41 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:13:41,349 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.85/118.11 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:13:54,405 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.90/118.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:14:08,203 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 110.45/113.33 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:14:21,740 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.72 (max/real adjusted throughput: 115.33/112.27 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:14:34,839 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.67/117.44 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:14:47,959 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.98/117.80 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:15:01,362 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 115.16/115.14 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:15:14,852 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 114.96/115.18 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:15:27,924 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.84/117.97 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:15:41,002 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=collnetdirect - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 118.36/117.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:15:54,759 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 106.96/115.35 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:16:08,227 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.71 (max/real adjusted throughput: 115.29/112.84 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:16:21,305 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.41/118.05 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:16:34,366 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 118.10/117.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:16:47,887 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 114.37/114.94 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:17:01,275 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.64/114.59 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:17:14,369 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 118.46/117.51 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:17:27,486 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.74/117.67 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:17:41,125 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 112.23/115.23 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:17:54,619 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 114.94/114.19 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:18:07,708 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 118.23/116.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:18:20,782 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvls - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.98/117.75 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:18:34,211 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 115.12/113.62 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:18:47,709 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.70 (max/real adjusted throughput: 115.10/113.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:19:00,794 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.49/118.29 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:19:13,857 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 118.20/118.37 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:19:27,284 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 114.25/114.39 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:19:40,654 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.95/114.79 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:19:53,763 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.65 (max/real adjusted throughput: 118.04/117.10 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:20:06,844 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 118.35/117.64 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:20:20,298 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 113.45/115.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:20:33,745 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.40/114.43 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:20:46,817 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.74/117.96 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:20:59,921 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=nvlstree - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 118.07/117.76 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:21:13,399 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.66 (max/real adjusted throughput: 112.93/115.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:21:26,832 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.25/114.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:21:39,912 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 117.40/118.28 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:21:53,036 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=SIMPLE - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.57/117.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:22:06,519 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.69 (max/real adjusted throughput: 115.49/113.98 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:22:19,928 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 115.71/114.47 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:22:33,012 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.70/117.85 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:22:46,097 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.70/117.86 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m


[38;20m2025-08-02 02:22:59,550 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory False:
 Average communication time over 3 iterations: 1.67 (max/real adjusted throughput: 114.25/115.54 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:23:13,059 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams False, Contiguous memory True:
 Average communication time over 3 iterations: 1.68 (max/real adjusted throughput: 113.75/114.72 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:23:26,203 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory False:
 Average communication time over 3 iterations: 1.64 (max/real adjusted throughput: 117.06/117.60 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:23:39,225 | xffl.distributed.aggregation |     INFO | bucket_optimized_coalesced - NCCL_ALGO=pat - NCCL_PROTO=LL128 - Multiple CUDA streams True, Contiguous memory True:
 Average communication time over 3 iterations: 1.63 (max/real adjusted throughput: 118.35/118.48 Gb/s - Max GPU RAM allocated: 16.06 GB)[0m
[38;20m2025-08-02 02:23:39,225 | xffl.distributed.aggregation |     INFO | Dumping benchmarking results to /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs/llama3.1-8b_ns_4_fs_1_ppn_1.csv[0m
lrdn1207:1671835:1678931 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1207:1671835:1678931 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1207:1671835:1678931 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1207:1671835:1671963 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1268:3521221:3531026 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1268:3521221:3531026 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1268:3521221:3531026 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1268:3521221:3521375 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1280:1451550:1458657 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1280:1451550:1458657 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1280:1451550:1458657 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1280:1451550:1451670 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1370:234521:242413 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1370:234521:242413 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1370:234521:234644 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1370:234521:242413 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1207:1671835:1678931 [0] NCCL INFO comm 0xd1d8e90 rank 0 nranks 4 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1268:3521221:3531026 [0] NCCL INFO comm 0xdb8ac60 rank 1 nranks 4 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1280:1451550:1458657 [0] NCCL INFO comm 0xe4d8cd0 rank 2 nranks 4 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1370:234521:242413 [0] NCCL INFO comm 0x14604670 rank 3 nranks 4 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1268:3521221:3531028 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1268:3521221:3531028 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1268:3521221:3531028 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1268:3521221:3521370 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1207:1671835:1678933 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1207:1671835:1678933 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1207:1671835:1678933 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1207:1671835:1671957 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1280:1451550:1458659 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1280:1451550:1458659 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1280:1451550:1458659 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1280:1451550:1451665 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1370:234521:242415 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1370:234521:242415 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1370:234521:242415 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1370:234521:234639 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1207:1671835:1678933 [0] NCCL INFO comm 0xd15d300 rank 0 nranks 4 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1268:3521221:3531028 [0] NCCL INFO comm 0xdb0f0d0 rank 1 nranks 4 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1280:1451550:1458659 [0] NCCL INFO comm 0xe45d140 rank 2 nranks 4 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1370:234521:242415 [0] NCCL INFO comm 0x14588ae0 rank 3 nranks 4 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1207:1671835:1678935 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1207:1671835:1678935 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1207:1671835:1678935 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1207:1671835:1671951 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1268:3521221:3531030 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1268:3521221:3531030 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1268:3521221:3531030 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1268:3521221:3521365 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1280:1451550:1458661 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1280:1451550:1458661 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1280:1451550:1458661 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1280:1451550:1451660 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1370:234521:242417 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1370:234521:242417 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1370:234521:242417 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1370:234521:234634 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1207:1671835:1678935 [0] NCCL INFO comm 0xd0e1770 rank 0 nranks 4 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1268:3521221:3531030 [0] NCCL INFO comm 0xda93540 rank 1 nranks 4 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1370:234521:242417 [0] NCCL INFO comm 0x1450cf50 rank 3 nranks 4 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1280:1451550:1458661 [0] NCCL INFO comm 0xe3e15b0 rank 2 nranks 4 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1207:1671835:1678937 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1207:1671835:1678937 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1207:1671835:1678937 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1207:1671835:1671942 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1268:3521221:3531032 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1268:3521221:3531032 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1268:3521221:3531032 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1268:3521221:3521341 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1280:1451550:1458663 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1280:1451550:1458663 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1280:1451550:1458663 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1280:1451550:1451652 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1370:234521:242419 [0] NCCL INFO misc/socket.cc:64 -> 3
lrdn1370:234521:242419 [0] NCCL INFO misc/socket.cc:80 -> 3
lrdn1370:234521:242419 [0] NCCL INFO misc/socket.cc:829 -> 3
lrdn1370:234521:234625 [0] NCCL INFO misc/socket.cc:881 -> 3
lrdn1207:1671835:1678937 [0] NCCL INFO comm 0xced9480 rank 0 nranks 4 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1268:3521221:3531032 [0] NCCL INFO comm 0x2c1461c0 rank 1 nranks 4 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1280:1451550:1458663 [0] NCCL INFO comm 0xe1c6990 rank 2 nranks 4 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1370:234521:242419 [0] NCCL INFO comm 0x142f36f0 rank 3 nranks 4 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1370:234521:242422 [0] NCCL INFO comm 0xe821710 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1280:1451550:1458666 [0] NCCL INFO comm 0xdd797f0 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1268:3521221:3531035 [0] NCCL INFO comm 0xe767a70 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1207:1671835:1678940 [0] NCCL INFO comm 0xca8da80 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
[38;20m2025-08-02 02:23:43,562 | xffl.cli.simulate |     INFO | Total simulation execution time: 7890.08 seconds[0m
[38;20m2025-08-02 02:23:43,563 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
