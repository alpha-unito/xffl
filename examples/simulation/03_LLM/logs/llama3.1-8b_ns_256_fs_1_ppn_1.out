[38;20m2025-08-02 08:46:51,214 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
[38;5;39m2025-08-02 08:46:51,215 | xffl.cli.simulate |    DEBUG | Using current virtual environment: /leonardo_scratch/fast/uToID_bench/xffl/.venv[0m
[38;5;39m2025-08-02 08:46:51,215 | xffl.cli.simulate |    DEBUG | New local simulation xFFL environment variables: {'XFFL_VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv', 'XFFL_WORLD_SIZE': '256', 'XFFL_NUM_NODES': '256', 'MASTER_ADDR': 'lrdn0014', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;5;39m2025-08-02 08:46:51,215 | xffl.cli.simulate |    DEBUG | Updated xFFL environment: {'XFFL_VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv', 'XFFL_WORLD_SIZE': '256', 'XFFL_NUM_NODES': '256', 'MASTER_ADDR': 'lrdn0014', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;20m2025-08-02 08:46:51,217 | xffl.cli.simulate |     INFO | Running local simulation...[0m
[38;5;39m2025-08-02 08:46:51,217 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0014: ssh -oStrictHostKeyChecking=no lrdn0014 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,217 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0020: ssh -oStrictHostKeyChecking=no lrdn0020 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,217 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0029: ssh -oStrictHostKeyChecking=no lrdn0029 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=2 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,218 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0038: ssh -oStrictHostKeyChecking=no lrdn0038 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=3 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,218 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0060: ssh -oStrictHostKeyChecking=no lrdn0060 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=4 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,218 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0080: ssh -oStrictHostKeyChecking=no lrdn0080 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=5 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,218 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0089: ssh -oStrictHostKeyChecking=no lrdn0089 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=6 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,218 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0090: ssh -oStrictHostKeyChecking=no lrdn0090 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=7 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,219 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0110: ssh -oStrictHostKeyChecking=no lrdn0110 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=8 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,219 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0114: ssh -oStrictHostKeyChecking=no lrdn0114 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=9 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,219 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0117: ssh -oStrictHostKeyChecking=no lrdn0117 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=10 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,219 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0134: ssh -oStrictHostKeyChecking=no lrdn0134 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=11 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,220 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0147: ssh -oStrictHostKeyChecking=no lrdn0147 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=12 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,220 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0150: ssh -oStrictHostKeyChecking=no lrdn0150 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=13 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,220 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0163: ssh -oStrictHostKeyChecking=no lrdn0163 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=14 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,220 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0164: ssh -oStrictHostKeyChecking=no lrdn0164 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=15 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,220 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0170: ssh -oStrictHostKeyChecking=no lrdn0170 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=16 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,221 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0175: ssh -oStrictHostKeyChecking=no lrdn0175 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=17 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,221 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0177: ssh -oStrictHostKeyChecking=no lrdn0177 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=18 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,221 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0178: ssh -oStrictHostKeyChecking=no lrdn0178 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=19 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,221 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0179: ssh -oStrictHostKeyChecking=no lrdn0179 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=20 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,222 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0193: ssh -oStrictHostKeyChecking=no lrdn0193 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=21 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,222 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0194: ssh -oStrictHostKeyChecking=no lrdn0194 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=22 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,222 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0200: ssh -oStrictHostKeyChecking=no lrdn0200 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=23 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,222 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0204: ssh -oStrictHostKeyChecking=no lrdn0204 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=24 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,222 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0205: ssh -oStrictHostKeyChecking=no lrdn0205 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=25 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,222 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0210: ssh -oStrictHostKeyChecking=no lrdn0210 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=26 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,223 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0223: ssh -oStrictHostKeyChecking=no lrdn0223 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=27 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,223 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0228: ssh -oStrictHostKeyChecking=no lrdn0228 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=28 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,223 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0230: ssh -oStrictHostKeyChecking=no lrdn0230 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=29 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,223 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0238: ssh -oStrictHostKeyChecking=no lrdn0238 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=30 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,224 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0248: ssh -oStrictHostKeyChecking=no lrdn0248 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=31 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,224 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0254: ssh -oStrictHostKeyChecking=no lrdn0254 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=32 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,224 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0266: ssh -oStrictHostKeyChecking=no lrdn0266 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=33 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,224 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0267: ssh -oStrictHostKeyChecking=no lrdn0267 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=34 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,224 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0270: ssh -oStrictHostKeyChecking=no lrdn0270 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=35 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,225 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0283: ssh -oStrictHostKeyChecking=no lrdn0283 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=36 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,225 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0284: ssh -oStrictHostKeyChecking=no lrdn0284 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=37 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,225 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0288: ssh -oStrictHostKeyChecking=no lrdn0288 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=38 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,225 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0289: ssh -oStrictHostKeyChecking=no lrdn0289 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=39 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,225 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0295: ssh -oStrictHostKeyChecking=no lrdn0295 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=40 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,226 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0297: ssh -oStrictHostKeyChecking=no lrdn0297 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=41 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,226 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0298: ssh -oStrictHostKeyChecking=no lrdn0298 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=42 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,226 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0300: ssh -oStrictHostKeyChecking=no lrdn0300 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=43 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,226 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0305: ssh -oStrictHostKeyChecking=no lrdn0305 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=44 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,226 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0307: ssh -oStrictHostKeyChecking=no lrdn0307 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=45 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,227 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0318: ssh -oStrictHostKeyChecking=no lrdn0318 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=46 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,227 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0326: ssh -oStrictHostKeyChecking=no lrdn0326 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=47 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,227 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0329: ssh -oStrictHostKeyChecking=no lrdn0329 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=48 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,227 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0358: ssh -oStrictHostKeyChecking=no lrdn0358 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=49 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,227 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0360: ssh -oStrictHostKeyChecking=no lrdn0360 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=50 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,227 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0367: ssh -oStrictHostKeyChecking=no lrdn0367 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=51 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,228 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0385: ssh -oStrictHostKeyChecking=no lrdn0385 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=52 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,228 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0387: ssh -oStrictHostKeyChecking=no lrdn0387 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=53 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,228 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0389: ssh -oStrictHostKeyChecking=no lrdn0389 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=54 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,228 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0390: ssh -oStrictHostKeyChecking=no lrdn0390 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=55 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,229 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0404: ssh -oStrictHostKeyChecking=no lrdn0404 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=56 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,229 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0409: ssh -oStrictHostKeyChecking=no lrdn0409 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=57 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,229 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0413: ssh -oStrictHostKeyChecking=no lrdn0413 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=58 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,229 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0416: ssh -oStrictHostKeyChecking=no lrdn0416 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=59 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,230 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0434: ssh -oStrictHostKeyChecking=no lrdn0434 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=60 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,231 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0447: ssh -oStrictHostKeyChecking=no lrdn0447 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=61 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,231 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0458: ssh -oStrictHostKeyChecking=no lrdn0458 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=62 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,231 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0475: ssh -oStrictHostKeyChecking=no lrdn0475 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=63 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,231 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0478: ssh -oStrictHostKeyChecking=no lrdn0478 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=64 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,231 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0479: ssh -oStrictHostKeyChecking=no lrdn0479 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=65 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,232 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0480: ssh -oStrictHostKeyChecking=no lrdn0480 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=66 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,232 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0485: ssh -oStrictHostKeyChecking=no lrdn0485 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=67 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,232 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0486: ssh -oStrictHostKeyChecking=no lrdn0486 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=68 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,232 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0493: ssh -oStrictHostKeyChecking=no lrdn0493 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=69 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,232 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0499: ssh -oStrictHostKeyChecking=no lrdn0499 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=70 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,232 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0506: ssh -oStrictHostKeyChecking=no lrdn0506 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=71 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,233 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0507: ssh -oStrictHostKeyChecking=no lrdn0507 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=72 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,233 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0510: ssh -oStrictHostKeyChecking=no lrdn0510 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=73 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,233 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0523: ssh -oStrictHostKeyChecking=no lrdn0523 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=74 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,233 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0528: ssh -oStrictHostKeyChecking=no lrdn0528 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=75 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,233 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0529: ssh -oStrictHostKeyChecking=no lrdn0529 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=76 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,233 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0535: ssh -oStrictHostKeyChecking=no lrdn0535 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=77 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,234 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0536: ssh -oStrictHostKeyChecking=no lrdn0536 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=78 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,234 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0539: ssh -oStrictHostKeyChecking=no lrdn0539 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=79 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,234 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0542: ssh -oStrictHostKeyChecking=no lrdn0542 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=80 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,234 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0544: ssh -oStrictHostKeyChecking=no lrdn0544 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=81 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,235 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0553: ssh -oStrictHostKeyChecking=no lrdn0553 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=82 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,236 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0565: ssh -oStrictHostKeyChecking=no lrdn0565 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=83 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,236 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0568: ssh -oStrictHostKeyChecking=no lrdn0568 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=84 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,236 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0569: ssh -oStrictHostKeyChecking=no lrdn0569 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=85 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,236 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0570: ssh -oStrictHostKeyChecking=no lrdn0570 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=86 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,236 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0583: ssh -oStrictHostKeyChecking=no lrdn0583 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=87 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,237 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0590: ssh -oStrictHostKeyChecking=no lrdn0590 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=88 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,237 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0596: ssh -oStrictHostKeyChecking=no lrdn0596 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=89 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,237 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0597: ssh -oStrictHostKeyChecking=no lrdn0597 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=90 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,237 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0598: ssh -oStrictHostKeyChecking=no lrdn0598 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=91 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,237 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0599: ssh -oStrictHostKeyChecking=no lrdn0599 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=92 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,238 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0604: ssh -oStrictHostKeyChecking=no lrdn0604 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=93 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,238 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0606: ssh -oStrictHostKeyChecking=no lrdn0606 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=94 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,238 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0611: ssh -oStrictHostKeyChecking=no lrdn0611 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=95 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,238 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0614: ssh -oStrictHostKeyChecking=no lrdn0614 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=96 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,240 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0618: ssh -oStrictHostKeyChecking=no lrdn0618 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=97 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,240 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0624: ssh -oStrictHostKeyChecking=no lrdn0624 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=98 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,240 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0627: ssh -oStrictHostKeyChecking=no lrdn0627 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=99 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,240 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0628: ssh -oStrictHostKeyChecking=no lrdn0628 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=100 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,240 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0632: ssh -oStrictHostKeyChecking=no lrdn0632 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=101 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,241 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0634: ssh -oStrictHostKeyChecking=no lrdn0634 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=102 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,241 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0637: ssh -oStrictHostKeyChecking=no lrdn0637 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=103 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,245 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0642: ssh -oStrictHostKeyChecking=no lrdn0642 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=104 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,245 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0646: ssh -oStrictHostKeyChecking=no lrdn0646 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=105 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,246 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0648: ssh -oStrictHostKeyChecking=no lrdn0648 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=106 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,246 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0657: ssh -oStrictHostKeyChecking=no lrdn0657 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=107 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,246 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0663: ssh -oStrictHostKeyChecking=no lrdn0663 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=108 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,246 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0664: ssh -oStrictHostKeyChecking=no lrdn0664 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=109 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,246 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0668: ssh -oStrictHostKeyChecking=no lrdn0668 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=110 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,247 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0672: ssh -oStrictHostKeyChecking=no lrdn0672 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=111 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,247 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0676: ssh -oStrictHostKeyChecking=no lrdn0676 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=112 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,247 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0680: ssh -oStrictHostKeyChecking=no lrdn0680 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=113 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,247 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0681: ssh -oStrictHostKeyChecking=no lrdn0681 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=114 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,248 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0693: ssh -oStrictHostKeyChecking=no lrdn0693 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=115 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,248 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0696: ssh -oStrictHostKeyChecking=no lrdn0696 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=116 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,248 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0697: ssh -oStrictHostKeyChecking=no lrdn0697 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=117 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,248 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0699: ssh -oStrictHostKeyChecking=no lrdn0699 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=118 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,249 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0705: ssh -oStrictHostKeyChecking=no lrdn0705 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=119 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,249 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0709: ssh -oStrictHostKeyChecking=no lrdn0709 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=120 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,249 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0716: ssh -oStrictHostKeyChecking=no lrdn0716 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=121 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,249 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0717: ssh -oStrictHostKeyChecking=no lrdn0717 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=122 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,251 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0721: ssh -oStrictHostKeyChecking=no lrdn0721 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=123 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,251 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0727: ssh -oStrictHostKeyChecking=no lrdn0727 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=124 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,252 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0739: ssh -oStrictHostKeyChecking=no lrdn0739 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=125 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,252 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0740: ssh -oStrictHostKeyChecking=no lrdn0740 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=126 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,253 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0746: ssh -oStrictHostKeyChecking=no lrdn0746 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=127 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,254 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0763: ssh -oStrictHostKeyChecking=no lrdn0763 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=128 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,254 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0766: ssh -oStrictHostKeyChecking=no lrdn0766 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=129 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,254 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0773: ssh -oStrictHostKeyChecking=no lrdn0773 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=130 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,254 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0789: ssh -oStrictHostKeyChecking=no lrdn0789 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=131 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,254 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0791: ssh -oStrictHostKeyChecking=no lrdn0791 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=132 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,255 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0803: ssh -oStrictHostKeyChecking=no lrdn0803 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=133 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,256 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0815: ssh -oStrictHostKeyChecking=no lrdn0815 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=134 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,256 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0819: ssh -oStrictHostKeyChecking=no lrdn0819 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=135 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,256 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0823: ssh -oStrictHostKeyChecking=no lrdn0823 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=136 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,256 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0828: ssh -oStrictHostKeyChecking=no lrdn0828 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=137 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,257 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0830: ssh -oStrictHostKeyChecking=no lrdn0830 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=138 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,258 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0834: ssh -oStrictHostKeyChecking=no lrdn0834 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=139 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,258 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0842: ssh -oStrictHostKeyChecking=no lrdn0842 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=140 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,258 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0843: ssh -oStrictHostKeyChecking=no lrdn0843 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=141 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,258 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0849: ssh -oStrictHostKeyChecking=no lrdn0849 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=142 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,259 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0850: ssh -oStrictHostKeyChecking=no lrdn0850 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=143 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,259 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0852: ssh -oStrictHostKeyChecking=no lrdn0852 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=144 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,259 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0854: ssh -oStrictHostKeyChecking=no lrdn0854 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=145 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,262 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0855: ssh -oStrictHostKeyChecking=no lrdn0855 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=146 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,263 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0864: ssh -oStrictHostKeyChecking=no lrdn0864 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=147 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,263 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0865: ssh -oStrictHostKeyChecking=no lrdn0865 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=148 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,263 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0872: ssh -oStrictHostKeyChecking=no lrdn0872 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=149 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,264 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0875: ssh -oStrictHostKeyChecking=no lrdn0875 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=150 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,264 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0877: ssh -oStrictHostKeyChecking=no lrdn0877 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=151 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,265 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0888: ssh -oStrictHostKeyChecking=no lrdn0888 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=152 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,265 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0890: ssh -oStrictHostKeyChecking=no lrdn0890 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=153 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,267 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0891: ssh -oStrictHostKeyChecking=no lrdn0891 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=154 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,268 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0893: ssh -oStrictHostKeyChecking=no lrdn0893 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=155 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,268 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0896: ssh -oStrictHostKeyChecking=no lrdn0896 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=156 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,268 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0905: ssh -oStrictHostKeyChecking=no lrdn0905 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=157 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,269 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0907: ssh -oStrictHostKeyChecking=no lrdn0907 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=158 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,271 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0912: ssh -oStrictHostKeyChecking=no lrdn0912 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=159 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,271 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0916: ssh -oStrictHostKeyChecking=no lrdn0916 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=160 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,271 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0919: ssh -oStrictHostKeyChecking=no lrdn0919 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=161 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,272 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0923: ssh -oStrictHostKeyChecking=no lrdn0923 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=162 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,272 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0927: ssh -oStrictHostKeyChecking=no lrdn0927 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=163 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,272 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0932: ssh -oStrictHostKeyChecking=no lrdn0932 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=164 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,272 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0933: ssh -oStrictHostKeyChecking=no lrdn0933 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=165 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,273 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0936: ssh -oStrictHostKeyChecking=no lrdn0936 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=166 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,273 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0941: ssh -oStrictHostKeyChecking=no lrdn0941 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=167 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,273 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0942: ssh -oStrictHostKeyChecking=no lrdn0942 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=168 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,273 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0951: ssh -oStrictHostKeyChecking=no lrdn0951 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=169 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,274 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0953: ssh -oStrictHostKeyChecking=no lrdn0953 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=170 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,274 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0963: ssh -oStrictHostKeyChecking=no lrdn0963 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=171 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,274 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0964: ssh -oStrictHostKeyChecking=no lrdn0964 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=172 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,274 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0965: ssh -oStrictHostKeyChecking=no lrdn0965 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=173 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,275 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0970: ssh -oStrictHostKeyChecking=no lrdn0970 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=174 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,275 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0972: ssh -oStrictHostKeyChecking=no lrdn0972 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=175 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,276 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0975: ssh -oStrictHostKeyChecking=no lrdn0975 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=176 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,276 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0981: ssh -oStrictHostKeyChecking=no lrdn0981 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=177 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,277 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0982: ssh -oStrictHostKeyChecking=no lrdn0982 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=178 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,278 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0991: ssh -oStrictHostKeyChecking=no lrdn0991 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=179 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,278 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0992: ssh -oStrictHostKeyChecking=no lrdn0992 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=180 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,278 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1001: ssh -oStrictHostKeyChecking=no lrdn1001 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=181 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,278 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1002: ssh -oStrictHostKeyChecking=no lrdn1002 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=182 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,279 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1005: ssh -oStrictHostKeyChecking=no lrdn1005 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=183 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,280 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1006: ssh -oStrictHostKeyChecking=no lrdn1006 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=184 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,280 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1027: ssh -oStrictHostKeyChecking=no lrdn1027 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=185 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,281 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1029: ssh -oStrictHostKeyChecking=no lrdn1029 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=186 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,281 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1030: ssh -oStrictHostKeyChecking=no lrdn1030 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=187 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,281 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1042: ssh -oStrictHostKeyChecking=no lrdn1042 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=188 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,281 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1043: ssh -oStrictHostKeyChecking=no lrdn1043 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=189 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,282 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1053: ssh -oStrictHostKeyChecking=no lrdn1053 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=190 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,282 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1054: ssh -oStrictHostKeyChecking=no lrdn1054 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=191 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,282 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1061: ssh -oStrictHostKeyChecking=no lrdn1061 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=192 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,283 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1065: ssh -oStrictHostKeyChecking=no lrdn1065 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=193 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,283 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1068: ssh -oStrictHostKeyChecking=no lrdn1068 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=194 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,283 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1075: ssh -oStrictHostKeyChecking=no lrdn1075 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=195 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,284 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1082: ssh -oStrictHostKeyChecking=no lrdn1082 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=196 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,284 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1083: ssh -oStrictHostKeyChecking=no lrdn1083 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=197 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,284 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1089: ssh -oStrictHostKeyChecking=no lrdn1089 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=198 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,284 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1090: ssh -oStrictHostKeyChecking=no lrdn1090 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=199 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,285 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1103: ssh -oStrictHostKeyChecking=no lrdn1103 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=200 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,285 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1104: ssh -oStrictHostKeyChecking=no lrdn1104 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=201 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,286 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1111: ssh -oStrictHostKeyChecking=no lrdn1111 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=202 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,286 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1112: ssh -oStrictHostKeyChecking=no lrdn1112 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=203 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,286 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1120: ssh -oStrictHostKeyChecking=no lrdn1120 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=204 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,287 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1121: ssh -oStrictHostKeyChecking=no lrdn1121 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=205 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,287 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1131: ssh -oStrictHostKeyChecking=no lrdn1131 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=206 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,287 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1132: ssh -oStrictHostKeyChecking=no lrdn1132 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=207 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,288 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1141: ssh -oStrictHostKeyChecking=no lrdn1141 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=208 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,288 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1145: ssh -oStrictHostKeyChecking=no lrdn1145 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=209 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,288 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1152: ssh -oStrictHostKeyChecking=no lrdn1152 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=210 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,289 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1157: ssh -oStrictHostKeyChecking=no lrdn1157 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=211 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,289 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1162: ssh -oStrictHostKeyChecking=no lrdn1162 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=212 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,289 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1163: ssh -oStrictHostKeyChecking=no lrdn1163 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=213 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,290 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1171: ssh -oStrictHostKeyChecking=no lrdn1171 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=214 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,290 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1172: ssh -oStrictHostKeyChecking=no lrdn1172 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=215 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,290 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1179: ssh -oStrictHostKeyChecking=no lrdn1179 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=216 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,291 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1181: ssh -oStrictHostKeyChecking=no lrdn1181 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=217 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,291 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1186: ssh -oStrictHostKeyChecking=no lrdn1186 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=218 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,291 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1191: ssh -oStrictHostKeyChecking=no lrdn1191 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=219 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,291 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1201: ssh -oStrictHostKeyChecking=no lrdn1201 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=220 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,291 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1203: ssh -oStrictHostKeyChecking=no lrdn1203 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=221 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,292 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1210: ssh -oStrictHostKeyChecking=no lrdn1210 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=222 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,292 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1211: ssh -oStrictHostKeyChecking=no lrdn1211 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=223 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,292 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1221: ssh -oStrictHostKeyChecking=no lrdn1221 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=224 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,292 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1223: ssh -oStrictHostKeyChecking=no lrdn1223 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=225 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,293 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1232: ssh -oStrictHostKeyChecking=no lrdn1232 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=226 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,295 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1234: ssh -oStrictHostKeyChecking=no lrdn1234 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=227 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,295 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1240: ssh -oStrictHostKeyChecking=no lrdn1240 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=228 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,296 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1241: ssh -oStrictHostKeyChecking=no lrdn1241 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=229 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,296 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1245: ssh -oStrictHostKeyChecking=no lrdn1245 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=230 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,296 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1246: ssh -oStrictHostKeyChecking=no lrdn1246 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=231 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,297 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1261: ssh -oStrictHostKeyChecking=no lrdn1261 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=232 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,297 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1262: ssh -oStrictHostKeyChecking=no lrdn1262 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=233 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,297 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1271: ssh -oStrictHostKeyChecking=no lrdn1271 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=234 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,298 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1276: ssh -oStrictHostKeyChecking=no lrdn1276 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=235 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,298 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1283: ssh -oStrictHostKeyChecking=no lrdn1283 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=236 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,298 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1290: ssh -oStrictHostKeyChecking=no lrdn1290 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=237 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,298 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1294: ssh -oStrictHostKeyChecking=no lrdn1294 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=238 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,299 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1295: ssh -oStrictHostKeyChecking=no lrdn1295 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=239 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,299 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1301: ssh -oStrictHostKeyChecking=no lrdn1301 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=240 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,299 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1313: ssh -oStrictHostKeyChecking=no lrdn1313 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=241 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,299 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1318: ssh -oStrictHostKeyChecking=no lrdn1318 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=242 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,299 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1324: ssh -oStrictHostKeyChecking=no lrdn1324 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=243 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,300 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1331: ssh -oStrictHostKeyChecking=no lrdn1331 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=244 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,300 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1335: ssh -oStrictHostKeyChecking=no lrdn1335 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=245 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,300 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1344: ssh -oStrictHostKeyChecking=no lrdn1344 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=246 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,300 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1375: ssh -oStrictHostKeyChecking=no lrdn1375 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=247 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,300 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1376: ssh -oStrictHostKeyChecking=no lrdn1376 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=248 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,301 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1389: ssh -oStrictHostKeyChecking=no lrdn1389 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=249 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,301 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1397: ssh -oStrictHostKeyChecking=no lrdn1397 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=250 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,301 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1413: ssh -oStrictHostKeyChecking=no lrdn1413 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=251 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,302 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1419: ssh -oStrictHostKeyChecking=no lrdn1419 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=252 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,302 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1424: ssh -oStrictHostKeyChecking=no lrdn1424 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=253 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,302 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1428: ssh -oStrictHostKeyChecking=no lrdn1428 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=254 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:46:51,302 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1438: ssh -oStrictHostKeyChecking=no lrdn1438 " XFFL_VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv XFFL_WORLD_SIZE=256 XFFL_NUM_NODES=256 MASTER_ADDR=lrdn0014 XFFL_FACILITY=leonardo XFFL_SIMULATION=true  XFFL_NODEID=255 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py --debug --model llama3.1-8b --dataset clean_mc4_it --seed 42 --federated-scaling 1 --benchmark 3 --workspace /leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/logs --csv llama3.1-8b_ns_256_fs_1_ppn_1.csv "[0m
[38;5;39m2025-08-02 08:47:35,592 | xffl.distributed.distributed_state |    DEBUG | Setting Symmetric Federated Scaling with sizes (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)[0m
[38;5;39m2025-08-02 08:47:41,382 | xffl.distributed.distributed |    DEBUG | [Rank 0]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=0
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=0
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=0
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [0], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[0]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbfd6e70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbff40b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc53b1b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc53b4b0>)
                [0m
[38;5;39m2025-08-02 08:47:41,382 |         __main__ |    DEBUG | Rendez-vous time: 38.98 seconds[0m
[38;5;39m2025-08-02 08:47:41,382 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,739 | xffl.distributed.distributed |    DEBUG | [Rank 255]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=255
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=255
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=255
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [255], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[255]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc1073e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc124620>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc66be70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc66c170>)
                [0m
[38;5;39m2025-08-02 08:47:41,739 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,741 | xffl.distributed.distributed |    DEBUG | [Rank 4]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=4
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=4
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=4
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [4], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[4]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcd4b7b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcd689f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd2afe40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd2b0140>)
                [0m
[38;5;39m2025-08-02 08:47:41,741 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,743 | xffl.distributed.distributed |    DEBUG | [Rank 5]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=5
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=5
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=5
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [5], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[5]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb7b8e90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb7d60d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd1dc60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd1df60>)
                [0m
[38;5;39m2025-08-02 08:47:41,743 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,756 | xffl.distributed.distributed |    DEBUG | [Rank 13]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=13
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=13
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=13
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [13], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[13]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcb1c940>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcb39b80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd0816d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd0819d0>)
                [0m
[38;5;39m2025-08-02 08:47:41,756 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,766 | xffl.distributed.distributed |    DEBUG | [Rank 20]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=20
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=20
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=20
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [20], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[20]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbfa14b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbfbe6f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc505ea0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc5061a0>)
                [0m
[38;5;39m2025-08-02 08:47:41,766 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,780 | xffl.distributed.distributed |    DEBUG | [Rank 253]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=253
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=253
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=253
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [253], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[253]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcac8060>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcae52a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd02c2c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd02c5c0>)
                [0m
[38;5;39m2025-08-02 08:47:41,780 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,781 | xffl.distributed.distributed |    DEBUG | [Rank 25]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=25
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=25
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=25
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [25], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[25]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb4da3c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb4f7600>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba3f160>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba3f460>)
                [0m
[38;5;39m2025-08-02 08:47:41,781 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,781 | xffl.distributed.distributed |    DEBUG | [Rank 254]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=254
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=254
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=254
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [254], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[254]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbae57c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb02a00>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc04a1d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc04a4d0>)
                [0m
[38;5;39m2025-08-02 08:47:41,781 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,800 | xffl.distributed.distributed |    DEBUG | [Rank 9]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=9
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=9
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=9
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [9], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[9]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbcfbdc0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd19000>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc260750>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc260a50>)
                [0m
[38;5;39m2025-08-02 08:47:41,800 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,802 | xffl.distributed.distributed |    DEBUG | [Rank 31]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=31
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=31
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=31
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [31], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[31]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb44b8a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb468ae0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9b01b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9b04b0>)
                [0m
[38;5;39m2025-08-02 08:47:41,802 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,805 | xffl.distributed.distributed |    DEBUG | [Rank 11]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=11
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=11
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=11
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [11], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[11]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb355140>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb372380>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb8b9c20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb8b9f20>)
                [0m
[38;5;39m2025-08-02 08:47:41,806 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,812 | xffl.distributed.distributed |    DEBUG | [Rank 39]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=39
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=39
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=39
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [39], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[39]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb3b1ff0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb3cf230>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb916300>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb916600>)
                [0m
[38;5;39m2025-08-02 08:47:41,812 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,814 | xffl.distributed.distributed |    DEBUG | [Rank 6]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=6
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=6
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=6
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [6], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[6]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc98da50>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc9aac90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcef2800>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcef2b00>)
                [0m
[38;5;39m2025-08-02 08:47:41,814 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,819 | xffl.distributed.distributed |    DEBUG | [Rank 8]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=8
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=8
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=8
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [8], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[8]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcac0490>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcadd6d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd024f10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd025210>)
                [0m
[38;5;39m2025-08-02 08:47:41,819 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,824 | xffl.distributed.distributed |    DEBUG | [Rank 18]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=18
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=18
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=18
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [18], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[18]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbae7f60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb051a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc04cad0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc04cdd0>)
                [0m
[38;5;39m2025-08-02 08:47:41,824 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,827 | xffl.distributed.distributed |    DEBUG | [Rank 40]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=40
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=40
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=40
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [40], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[40]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xca4ce90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xca6a0d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcfb1b60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcfb1e60>)
                [0m
[38;5;39m2025-08-02 08:47:41,827 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,835 | xffl.distributed.distributed |    DEBUG | [Rank 49]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=49
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=49
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=49
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [49], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[49]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb826be0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb843e20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb84e410>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd8b9a0>)
                [0m
[38;5;39m2025-08-02 08:47:41,835 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,837 | xffl.distributed.distributed |    DEBUG | [Rank 15]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=15
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=15
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=15
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [15], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[15]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xd15ac80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd177ea0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd6bf040>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd6bf340>)
                [0m
[38;5;39m2025-08-02 08:47:41,837 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,840 | xffl.distributed.distributed |    DEBUG | [Rank 44]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=44
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=44
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=44
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [44], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[44]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xd0b7020>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd0d4280>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd61bc50>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd61bf50>)
                [0m
[38;5;39m2025-08-02 08:47:41,840 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,841 | xffl.distributed.distributed |    DEBUG | [Rank 1]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=1
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=1
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=1
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [1], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[1]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc5c39a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc5e0be0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcb28690>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcb28990>)
                [0m
[38;5;39m2025-08-02 08:47:41,841 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,841 | xffl.distributed.distributed |    DEBUG | [Rank 17]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=17
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=17
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=17
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [17], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[17]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb74a630>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb767870>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbcaee90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbcaf190>)
                [0m
[38;5;39m2025-08-02 08:47:41,841 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,842 | xffl.distributed.distributed |    DEBUG | [Rank 53]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=53
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=53
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=53
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [53], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[53]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb9e6340>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba03580>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf4aa30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf4ad30>)
                [0m
[38;5;39m2025-08-02 08:47:41,842 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,844 | xffl.distributed.distributed |    DEBUG | [Rank 45]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=45
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=45
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=45
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [45], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[45]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcdd55e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcdf2820>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd339870>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd339b70>)
                [0m
[38;5;39m2025-08-02 08:47:41,844 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,848 | xffl.distributed.distributed |    DEBUG | [Rank 26]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=26
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=26
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=26
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [26], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[26]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xba69720>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba86960>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbfce370>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbfce670>)
                [0m
[38;5;39m2025-08-02 08:47:41,848 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,849 | xffl.distributed.distributed |    DEBUG | [Rank 27]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=27
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=27
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=27
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [27], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[27]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xca42140>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xca5f380>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcfa6470>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcfa6770>)
                [0m
[38;5;39m2025-08-02 08:47:41,850 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,860 | xffl.distributed.distributed |    DEBUG | [Rank 7]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=7
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=7
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=7
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [7], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[7]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xce3f1f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce5c430>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd3a32c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd3a35c0>)
                [0m
[38;5;39m2025-08-02 08:47:41,860 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,862 | xffl.distributed.distributed |    DEBUG | [Rank 32]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=32
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=32
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=32
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [32], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[32]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbc820e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc82080>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc1e6400>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc1e6700>)
                [0m
[38;5;39m2025-08-02 08:47:41,862 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,867 | xffl.distributed.distributed |    DEBUG | [Rank 28]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=28
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=28
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=28
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [28], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[28]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb33a400>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb357640>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb89ed10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb89f010>)
                [0m
[38;5;39m2025-08-02 08:47:41,867 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,881 | xffl.distributed.distributed |    DEBUG | [Rank 41]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=41
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=41
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=41
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [41], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[41]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbe704b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbbaced0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc3d4e10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc3d5110>)
                [0m
[38;5;39m2025-08-02 08:47:41,881 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,883 | xffl.distributed.distributed |    DEBUG | [Rank 36]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=36
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=36
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=36
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [36], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[36]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc7bc4a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc4f8ec0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcd20db0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcd210b0>)
                [0m
[38;5;39m2025-08-02 08:47:41,883 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,886 | xffl.distributed.distributed |    DEBUG | [Rank 62]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=62
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=62
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=62
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [62], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[62]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xd0b10c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd0ce300>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd615d30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd616030>)
                [0m
[38;5;39m2025-08-02 08:47:41,886 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,888 | xffl.distributed.distributed |    DEBUG | [Rank 63]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=63
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=63
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=63
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [63], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[63]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcc609a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc7dbe0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1c5420>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1c5720>)
                [0m
[38;5;39m2025-08-02 08:47:41,888 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,894 | xffl.distributed.distributed |    DEBUG | [Rank 14]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=14
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=14
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=14
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [14], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[14]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xd06b390>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd0885d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd5cfb80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd5cfe80>)
                [0m
[38;5;39m2025-08-02 08:47:41,894 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,897 | xffl.distributed.distributed |    DEBUG | [Rank 19]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=19
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=19
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=19
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [19], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[19]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbcb99f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbcd6c50>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc21e540>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc21e840>)
                [0m
[38;5;39m2025-08-02 08:47:41,897 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,899 | xffl.distributed.distributed |    DEBUG | [Rank 50]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=50
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=50
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=50
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [50], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[50]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb38d870>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb3aaab0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb8f2400>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb8f2700>)
                [0m
[38;5;39m2025-08-02 08:47:41,899 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,912 | xffl.distributed.distributed |    DEBUG | [Rank 51]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=51
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=51
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=51
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [51], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[51]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xce2d9b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce4abf0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd392450>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd392750>)
                [0m
[38;5;39m2025-08-02 08:47:41,912 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,921 | xffl.distributed.distributed |    DEBUG | [Rank 77]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=77
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=77
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=77
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [77], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[77]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcab3b20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcad0d60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd017fa0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd0182a0>)
                [0m
[38;5;39m2025-08-02 08:47:41,921 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,921 | xffl.distributed.distributed |    DEBUG | [Rank 22]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=22
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=22
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=22
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [22], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[22]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbe9d260>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbeba480>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc401490>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc401790>)
                [0m
[38;5;39m2025-08-02 08:47:41,921 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,923 | xffl.distributed.distributed |    DEBUG | [Rank 57]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=57
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=57
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=57
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [57], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[57]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbf41760>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf5e9a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc4a6410>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc4a6710>)
                [0m
[38;5;39m2025-08-02 08:47:41,923 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,926 | xffl.distributed.distributed |    DEBUG | [Rank 75]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=75
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=75
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=75
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [75], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[75]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcb0b0c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcb28300>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd06fab0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd06fdb0>)
                [0m
[38;5;39m2025-08-02 08:47:41,926 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,930 | xffl.distributed.distributed |    DEBUG | [Rank 24]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=24
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=24
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=24
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [24], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[24]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xba2c8e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba49b20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf90f50>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf91250>)
                [0m
[38;5;39m2025-08-02 08:47:41,930 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,934 | xffl.distributed.distributed |    DEBUG | [Rank 78]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=78
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=78
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=78
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [78], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[78]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xd0622e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd07f520>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd5c6910>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd5c6c10>)
                [0m
[38;5;39m2025-08-02 08:47:41,934 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,938 | xffl.distributed.distributed |    DEBUG | [Rank 81]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=81
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=81
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=81
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [81], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[81]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xce36dd0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce54010>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd39ba30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd39bd30>)
                [0m
[38;5;39m2025-08-02 08:47:41,938 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,941 | xffl.distributed.distributed |    DEBUG | [Rank 30]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=30
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=30
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=30
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [30], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[30]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb7b6850>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb7d3a90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd1b360>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd1b660>)
                [0m
[38;5;39m2025-08-02 08:47:41,941 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,942 | xffl.distributed.distributed |    DEBUG | [Rank 69]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=69
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=69
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=69
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [69], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[69]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc028ad0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc045d10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc58d7d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc58dad0>)
                [0m
[38;5;39m2025-08-02 08:47:41,942 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,950 | xffl.distributed.distributed |    DEBUG | [Rank 74]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=74
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=74
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=74
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [74], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[74]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb57d860>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb59aaa0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbae2010>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbae2310>)
                [0m
[38;5;39m2025-08-02 08:47:41,950 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,958 | xffl.distributed.distributed |    DEBUG | [Rank 33]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=33
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=33
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=33
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [33], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[33]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb3dc6e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb118e40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9414f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9417f0>)
                [0m
[38;5;39m2025-08-02 08:47:41,958 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,959 | xffl.distributed.distributed |    DEBUG | [Rank 37]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=37
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=37
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=37
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [37], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[37]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbf27b90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf44dd0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc48c460>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc48c760>)
                [0m
[38;5;39m2025-08-02 08:47:41,959 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,966 | xffl.distributed.distributed |    DEBUG | [Rank 82]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=82
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=82
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=82
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [82], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[82]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xccac640>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xccc9880>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd210920>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd210c20>)
                [0m
[38;5;39m2025-08-02 08:47:41,966 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,973 | xffl.distributed.distributed |    DEBUG | [Rank 85]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=85
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=85
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=85
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [85], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[85]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc92a440>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc947680>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce8ede0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce8f0e0>)
                [0m
[38;5;39m2025-08-02 08:47:41,973 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,976 | xffl.distributed.distributed |    DEBUG | [Rank 43]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=43
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=43
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=43
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [43], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[43]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcbc73c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcbe4600>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd12c0e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd12c3e0>)
                [0m
[38;5;39m2025-08-02 08:47:41,976 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,976 | xffl.distributed.distributed |    DEBUG | [Rank 84]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=84
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=84
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=84
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [84], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[84]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcd50010>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcd6d250>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd2b42c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd2b45c0>)
                [0m
[38;5;39m2025-08-02 08:47:41,976 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,982 | xffl.distributed.distributed |    DEBUG | [Rank 89]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=89
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=89
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=89
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [89], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[89]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb9eaee0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba08120>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf4fb20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf4fe20>)
                [0m
[38;5;39m2025-08-02 08:47:41,982 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:41,987 | xffl.distributed.distributed |    DEBUG | [Rank 48]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=48
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=48
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=48
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [48], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[48]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc9fbb30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xca18d70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf608d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf60bd0>)
                [0m
[38;5;39m2025-08-02 08:47:41,987 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,003 | xffl.distributed.distributed |    DEBUG | [Rank 52]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=52
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=52
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=52
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [52], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[52]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc86ac20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc887e60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcdcf670>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcdcf970>)
                [0m
[38;5;39m2025-08-02 08:47:42,003 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,005 | xffl.distributed.distributed |    DEBUG | [Rank 55]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=55
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=55
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=55
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [55], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[55]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc81c410>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc839650>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcd810e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcd813e0>)
                [0m
[38;5;39m2025-08-02 08:47:42,005 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,005 | xffl.distributed.distributed |    DEBUG | [Rank 56]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=56
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=56
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=56
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [56], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[56]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbe79650>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe96890>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc3ddf40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc3de240>)
                [0m
[38;5;39m2025-08-02 08:47:42,005 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,011 | xffl.distributed.distributed |    DEBUG | [Rank 58]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=58
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=58
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=58
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [58], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[58]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcbaedb0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcbcbff0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd113b10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd113e10>)
                [0m
[38;5;39m2025-08-02 08:47:42,012 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,013 | xffl.distributed.distributed |    DEBUG | [Rank 104]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=104
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=104
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=104
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [104], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[104]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xccb3380>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xccd05c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd217e50>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd218150>)
                [0m
[38;5;39m2025-08-02 08:47:42,013 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,021 | xffl.distributed.distributed |    DEBUG | [Rank 61]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=61
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=61
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=61
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [61], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[61]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb236590>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb2537d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb79a980>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb79ac80>)
                [0m
[38;5;39m2025-08-02 08:47:42,021 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,021 | xffl.distributed.distributed |    DEBUG | [Rank 64]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=64
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=64
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=64
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [64], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[64]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbc05230>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc22470>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc169ba0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc169ea0>)
                [0m
[38;5;39m2025-08-02 08:47:42,021 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,023 | xffl.distributed.distributed |    DEBUG | [Rank 106]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=106
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=106
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=106
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [106], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[106]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xd038bd0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd055e10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd59d460>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd59d760>)
                [0m
[38;5;39m2025-08-02 08:47:42,023 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,025 | xffl.distributed.distributed |    DEBUG | [Rank 66]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=66
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=66
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=66
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [66], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[66]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc216f40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc234180>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc77b130>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc77b430>)
                [0m
[38;5;39m2025-08-02 08:47:42,025 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,031 | xffl.distributed.distributed |    DEBUG | [Rank 108]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=108
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=108
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=108
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [108], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[108]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xca34ec0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xca52100>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf99770>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf99a70>)
                [0m
[38;5;39m2025-08-02 08:47:42,031 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,033 | xffl.distributed.distributed |    DEBUG | [Rank 67]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=67
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=67
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=67
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [67], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[67]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb50edc0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb52c000>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba736c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba739c0>)
                [0m
[38;5;39m2025-08-02 08:47:42,034 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,036 | xffl.distributed.distributed |    DEBUG | [Rank 71]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=71
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=71
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=71
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [71], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[71]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc256900>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc273b40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc7bb8b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc7bbbb0>)
                [0m
[38;5;39m2025-08-02 08:47:42,036 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,038 | xffl.distributed.distributed |    DEBUG | [Rank 2]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=2
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=2
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=2
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [2], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[2]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb945900>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb962b40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbea9940>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbea9c40>)
                [0m
[38;5;39m2025-08-02 08:47:42,038 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,038 | xffl.distributed.distributed |    DEBUG | [Rank 3]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=3
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=3
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=3
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [3], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[3]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb4c2630>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb4df870>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba26ea0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba271a0>)
                [0m
[38;5;39m2025-08-02 08:47:42,038 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,038 | xffl.distributed.distributed |    DEBUG | [Rank 73]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=73
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=73
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=73
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [73], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[73]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb980920>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb99db60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbee5450>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbee5750>)
                [0m
[38;5;39m2025-08-02 08:47:42,039 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,043 | xffl.distributed.distributed |    DEBUG | [Rank 72]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=72
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=72
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=72
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [72], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[72]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbff8660>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc0158a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc55d270>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc55d570>)
                [0m
[38;5;39m2025-08-02 08:47:42,043 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,046 | xffl.distributed.distributed |    DEBUG | [Rank 76]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=76
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=76
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=76
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [76], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[76]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xd164840>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd181a80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd6c9160>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd6c9460>)
                [0m
[38;5;39m2025-08-02 08:47:42,046 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,052 | xffl.distributed.distributed |    DEBUG | [Rank 112]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=112
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=112
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=112
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [112], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[112]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb323a20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb340c60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb888460>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb888760>)
                [0m
[38;5;39m2025-08-02 08:47:42,052 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,064 | xffl.distributed.distributed |    DEBUG | [Rank 86]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=86
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=86
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=86
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [86], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[86]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb5a63b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb5c35f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb0af70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb0b270>)
                [0m
[38;5;39m2025-08-02 08:47:42,064 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,066 | xffl.distributed.distributed |    DEBUG | [Rank 115]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=115
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=115
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=115
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [115], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[115]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc1f8560>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2157a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc75cc10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc75cf10>)
                [0m
[38;5;39m2025-08-02 08:47:42,066 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,066 | xffl.distributed.distributed |    DEBUG | [Rank 10]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=10
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=10
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=10
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [10], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[10]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xba26200>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba43440>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf8aed0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf8b1d0>)
                [0m
[38;5;39m2025-08-02 08:47:42,066 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,069 | xffl.distributed.distributed |    DEBUG | [Rank 83]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=83
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=83
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=83
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [83], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[83]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcf73e60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf910a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd4d8840>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd4d8b40>)
                [0m
[38;5;39m2025-08-02 08:47:42,070 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,071 | xffl.distributed.distributed |    DEBUG | [Rank 12]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=12
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=12
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=12
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [12], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[12]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb711320>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb72e560>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc75460>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc75760>)
                [0m
[38;5;39m2025-08-02 08:47:42,071 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,074 | xffl.distributed.distributed |    DEBUG | [Rank 117]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=117
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=117
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=117
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [117], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[117]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc89db10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc8bad30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce025d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce028d0>)
                [0m
[38;5;39m2025-08-02 08:47:42,074 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,075 | xffl.distributed.distributed |    DEBUG | [Rank 119]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=119
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=119
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=119
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [119], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[119]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xd05de80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd07b0c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd5c21b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd5c24b0>)
                [0m
[38;5;39m2025-08-02 08:47:42,075 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,080 | xffl.distributed.distributed |    DEBUG | [Rank 91]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=91
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=91
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=91
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [91], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[91]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xd0b5040>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd0d2260>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd619d00>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd61a000>)
                [0m
[38;5;39m2025-08-02 08:47:42,081 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,082 | xffl.distributed.distributed |    DEBUG | [Rank 111]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=111
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=111
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=111
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [111], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[111]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc996e20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc9b4060>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcefbae0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcefbde0>)
                [0m
[38;5;39m2025-08-02 08:47:42,082 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,082 | xffl.distributed.distributed |    DEBUG | [Rank 120]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=120
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=120
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=120
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [120], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[120]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbe3d490>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe5a6d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc3a19c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc3a1cc0>)
                [0m
[38;5;39m2025-08-02 08:47:42,082 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,084 | xffl.distributed.distributed |    DEBUG | [Rank 105]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=105
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=105
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=105
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [105], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[105]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc3704d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc38d710>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc8d4a90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc8d4d90>)
                [0m
[38;5;39m2025-08-02 08:47:42,084 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,088 | xffl.distributed.distributed |    DEBUG | [Rank 101]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=101
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=101
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=101
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [101], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[101]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb687450>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb6a4690>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbbeb790>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbbeba90>)
                [0m
[38;5;39m2025-08-02 08:47:42,088 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,088 | xffl.distributed.distributed |    DEBUG | [Rank 122]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=122
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=122
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=122
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [122], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[122]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xba88f60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbaa61a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbfed9c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbfedcc0>)
                [0m
[38;5;39m2025-08-02 08:47:42,088 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,089 | xffl.distributed.distributed |    DEBUG | [Rank 95]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=95
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=95
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=95
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [95], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[95]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb616590>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb6337d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb7b3d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb7b6d0>)
                [0m
[38;5;39m2025-08-02 08:47:42,089 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,096 | xffl.distributed.distributed |    DEBUG | [Rank 124]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=124
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=124
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=124
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [124], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[124]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc0ab520>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc0c8760>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc60ff20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc610220>)
                [0m
[38;5;39m2025-08-02 08:47:42,097 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,097 | xffl.distributed.distributed |    DEBUG | [Rank 100]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=100
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=100
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=100
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [100], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[100]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xd05b930>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd078b70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd5c04b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd5c07b0>)
                [0m
[38;5;39m2025-08-02 08:47:42,097 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,099 | xffl.distributed.distributed |    DEBUG | [Rank 125]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=125
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=125
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=125
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [125], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[125]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcc74d00>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc91f40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1d9b20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1d9e20>)
                [0m
[38;5;39m2025-08-02 08:47:42,099 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,101 | xffl.distributed.distributed |    DEBUG | [Rank 126]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=126
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=126
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=126
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [126], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[126]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb991c60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9aee80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbef6400>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbef6700>)
                [0m
[38;5;39m2025-08-02 08:47:42,101 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,102 | xffl.distributed.distributed |    DEBUG | [Rank 21]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=21
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=21
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=21
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [21], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[21]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbbd7030>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbbf4270>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc13bcd0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc13bfd0>)
                [0m
[38;5;39m2025-08-02 08:47:42,102 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,111 | xffl.distributed.distributed |    DEBUG | [Rank 127]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=127
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=127
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=127
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [127], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[127]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc0f2710>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc10f930>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc657290>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc657590>)
                [0m
[38;5;39m2025-08-02 08:47:42,111 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,111 | xffl.distributed.distributed |    DEBUG | [Rank 118]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=118
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=118
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=118
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [118], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[118]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcdacfd0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcdca230>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd3118c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd311bc0>)
                [0m
[38;5;39m2025-08-02 08:47:42,111 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,116 | xffl.distributed.distributed |    DEBUG | [Rank 123]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=123
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=123
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=123
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [123], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[123]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc6d0f60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc6ee1a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc35c80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc35f80>)
                [0m
[38;5;39m2025-08-02 08:47:42,116 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,118 | xffl.distributed.distributed |    DEBUG | [Rank 129]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=129
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=129
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=129
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [129], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[129]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcd84e10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcda2050>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd2e9080>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd2e9380>)
                [0m
[38;5;39m2025-08-02 08:47:42,118 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,122 | xffl.distributed.distributed |    DEBUG | [Rank 130]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=130
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=130
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=130
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [130], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[130]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbb04560>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb21780>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc069070>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc069370>)
                [0m
[38;5;39m2025-08-02 08:47:42,122 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,125 | xffl.distributed.distributed |    DEBUG | [Rank 132]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=132
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=132
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=132
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [132], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[132]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc9a07a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc9bd9e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf05230>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf05530>)
                [0m
[38;5;39m2025-08-02 08:47:42,126 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,127 | xffl.distributed.distributed |    DEBUG | [Rank 133]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=133
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=133
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=133
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [133], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[133]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbba1f90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbbbf1d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc106cc0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc106fc0>)
                [0m
[38;5;39m2025-08-02 08:47:42,127 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,127 | xffl.distributed.distributed |    DEBUG | [Rank 16]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=16
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=16
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=16
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [16], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[16]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc4ed1f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc50a430>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xca51110>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xca51410>)
                [0m
[38;5;39m2025-08-02 08:47:42,128 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,132 | xffl.distributed.distributed |    DEBUG | [Rank 135]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=135
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=135
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=135
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [135], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[135]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc32ab00>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc347d40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc88f600>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc88f900>)
                [0m
[38;5;39m2025-08-02 08:47:42,132 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,134 | xffl.distributed.distributed |    DEBUG | [Rank 134]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=134
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=134
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=134
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [134], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[134]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc00aad0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc027d10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc56f800>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc56fb00>)
                [0m
[38;5;39m2025-08-02 08:47:42,134 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,136 | xffl.distributed.distributed |    DEBUG | [Rank 131]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=131
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=131
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=131
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [131], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[131]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb309860>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb326aa0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb86df20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb86e220>)
                [0m
[38;5;39m2025-08-02 08:47:42,136 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,145 | xffl.distributed.distributed |    DEBUG | [Rank 157]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=157
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=157
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=157
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [157], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[157]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb992940>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9afb80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbef6a20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbef6d20>)
                [0m
[38;5;39m2025-08-02 08:47:42,145 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,145 | xffl.distributed.distributed |    DEBUG | [Rank 155]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=155
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=155
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=155
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [155], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[155]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb573200>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb590440>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbad70b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbad73b0>)
                [0m
[38;5;39m2025-08-02 08:47:42,145 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,147 | xffl.distributed.distributed |    DEBUG | [Rank 156]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=156
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=156
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=156
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [156], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[156]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcbdb240>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcbf8480>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd13fe50>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd140150>)
                [0m
[38;5;39m2025-08-02 08:47:42,147 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,151 | xffl.distributed.distributed |    DEBUG | [Rank 158]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=158
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=158
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=158
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [158], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[158]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xd158780>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce951a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd6bd420>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd6bd720>)
                [0m
[38;5;39m2025-08-02 08:47:42,151 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,151 | xffl.distributed.distributed |    DEBUG | [Rank 159]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=159
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=159
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=159
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [159], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[159]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcf1f0b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf3c2f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd483b90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd483e90>)
                [0m
[38;5;39m2025-08-02 08:47:42,151 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,152 | xffl.distributed.distributed |    DEBUG | [Rank 23]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=23
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=23
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=23
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [23], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[23]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc6ff0f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc71c330>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc63d40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc64040>)
                [0m
[38;5;39m2025-08-02 08:47:42,152 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,154 | xffl.distributed.distributed |    DEBUG | [Rank 139]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=139
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=139
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=139
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [139], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[139]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc75a340>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc75a2e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xccbeff0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xccbf2f0>)
                [0m
[38;5;39m2025-08-02 08:47:42,154 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,158 | xffl.distributed.distributed |    DEBUG | [Rank 142]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=142
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=142
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=142
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [142], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[142]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc4d2380>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc4ef5c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xca36760>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xca36a60>)
                [0m
[38;5;39m2025-08-02 08:47:42,158 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,160 | xffl.distributed.distributed |    DEBUG | [Rank 143]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=143
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=143
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=143
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [143], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[143]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb90a7d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb927a10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe6f4f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe6f7f0>)
                [0m
[38;5;39m2025-08-02 08:47:42,160 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,164 | xffl.distributed.distributed |    DEBUG | [Rank 162]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=162
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=162
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=162
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [162], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[162]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb914040>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb931280>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe78780>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe78a80>)
                [0m
[38;5;39m2025-08-02 08:47:42,164 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,168 | xffl.distributed.distributed |    DEBUG | [Rank 145]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=145
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=145
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=145
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [145], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[145]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb5a24c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb5bf700>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb07210>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb07510>)
                [0m
[38;5;39m2025-08-02 08:47:42,168 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,171 | xffl.distributed.distributed |    DEBUG | [Rank 165]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=165
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=165
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=165
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [165], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[165]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb2c50c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb2e2300>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb829db0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb82a0b0>)
                [0m
[38;5;39m2025-08-02 08:47:42,172 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,172 | xffl.distributed.distributed |    DEBUG | [Rank 164]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=164
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=164
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=164
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [164], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[164]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb366be0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb383e20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb8cb5a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb8cb8a0>)
                [0m
[38;5;39m2025-08-02 08:47:42,172 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,175 | xffl.distributed.distributed |    DEBUG | [Rank 149]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=149
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=149
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=149
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [149], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[149]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb60fba0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb62cde0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb745e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb748e0>)
                [0m
[38;5;39m2025-08-02 08:47:42,175 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,175 | xffl.distributed.distributed |    DEBUG | [Rank 166]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=166
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=166
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=166
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [166], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[166]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb7c3860>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb7e0aa0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd28480>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd28780>)
                [0m
[38;5;39m2025-08-02 08:47:42,176 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,176 | xffl.distributed.distributed |    DEBUG | [Rank 147]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=147
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=147
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=147
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [147], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[147]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb7eff70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb80d1b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd542a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd545a0>)
                [0m
[38;5;39m2025-08-02 08:47:42,176 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,177 | xffl.distributed.distributed |    DEBUG | [Rank 148]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=148
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=148
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=148
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [148], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[148]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb4a2310>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb4bf550>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba06cc0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba06fc0>)
                [0m
[38;5;39m2025-08-02 08:47:42,177 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,180 | xffl.distributed.distributed |    DEBUG | [Rank 29]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=29
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=29
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=29
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [29], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[29]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb2ce3a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb2ce340>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb8326e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb8329e0>)
                [0m
[38;5;39m2025-08-02 08:47:42,180 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,180 | xffl.distributed.distributed |    DEBUG | [Rank 168]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=168
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=168
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=168
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [168], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[168]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbde9fc0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe07200>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc34eae0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc34ede0>)
                [0m
[38;5;39m2025-08-02 08:47:42,181 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,181 | xffl.distributed.distributed |    DEBUG | [Rank 152]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=152
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=152
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=152
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [152], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[152]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbcb6c10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbcd3e50>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc21b8a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc21bba0>)
                [0m
[38;5;39m2025-08-02 08:47:42,182 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,184 | xffl.distributed.distributed |    DEBUG | [Rank 169]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=169
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=169
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=169
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [169], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[169]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbd4dc30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd6ae90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2b2ca0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2b2fa0>)
                [0m
[38;5;39m2025-08-02 08:47:42,185 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,185 | xffl.distributed.distributed |    DEBUG | [Rank 151]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=151
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=151
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=151
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [151], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[151]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc692830>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc6afa70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcbf74a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcbf77a0>)
                [0m
[38;5;39m2025-08-02 08:47:42,185 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,188 | xffl.distributed.distributed |    DEBUG | [Rank 170]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=170
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=170
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=170
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [170], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[170]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb467820>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb484a60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9cc5c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9cc8c0>)
                [0m
[38;5;39m2025-08-02 08:47:42,188 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,190 | xffl.distributed.distributed |    DEBUG | [Rank 46]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=46
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=46
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=46
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [46], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[46]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb8e4240>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb901480>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe48a90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe48d90>)
                [0m
[38;5;39m2025-08-02 08:47:42,190 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,192 | xffl.distributed.distributed |    DEBUG | [Rank 171]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=171
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=171
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=171
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [171], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[171]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc584650>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc5a1890>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcae8d50>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcae9050>)
                [0m
[38;5;39m2025-08-02 08:47:42,192 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,194 | xffl.distributed.distributed |    DEBUG | [Rank 34]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=34
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=34
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=34
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [34], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[34]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb38aaa0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb0c7200>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb8ef5b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb8ef8b0>)
                [0m
[38;5;39m2025-08-02 08:47:42,194 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,194 | xffl.distributed.distributed |    DEBUG | [Rank 172]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=172
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=172
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=172
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [172], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[172]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb2306e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb24d920>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb794f40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb795240>)
                [0m
[38;5;39m2025-08-02 08:47:42,195 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,197 | xffl.distributed.distributed |    DEBUG | [Rank 173]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=173
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=173
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=173
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [173], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[173]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc976c50>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc976bf0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcedb9c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcedbcc0>)
                [0m
[38;5;39m2025-08-02 08:47:42,197 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,198 | xffl.distributed.distributed |    DEBUG | [Rank 35]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=35
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=35
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=35
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [35], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[35]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcc0cf50>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc2a190>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd171cd0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd171fd0>)
                [0m
[38;5;39m2025-08-02 08:47:42,198 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,198 | xffl.distributed.distributed |    DEBUG | [Rank 174]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=174
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=174
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=174
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [174], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[174]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc89f1b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc89f170>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce03ba0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce03ea0>)
                [0m
[38;5;39m2025-08-02 08:47:42,198 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,203 | xffl.distributed.distributed |    DEBUG | [Rank 176]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=176
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=176
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=176
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [176], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[176]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc17f190>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc19c3d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc6e3e00>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc6e4100>)
                [0m
[38;5;39m2025-08-02 08:47:42,203 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,204 | xffl.distributed.distributed |    DEBUG | [Rank 38]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=38
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=38
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=38
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [38], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[38]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbc16600>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc33840>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc17af70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc17b270>)
                [0m
[38;5;39m2025-08-02 08:47:42,204 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,206 | xffl.distributed.distributed |    DEBUG | [Rank 177]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=177
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=177
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=177
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [177], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[177]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbccc890>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbce9ad0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2311f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2314f0>)
                [0m
[38;5;39m2025-08-02 08:47:42,206 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,210 | xffl.distributed.distributed |    DEBUG | [Rank 178]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=178
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=178
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=178
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [178], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[178]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbc10180>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc2d3c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc174c80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc174f80>)
                [0m
[38;5;39m2025-08-02 08:47:42,210 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,211 | xffl.distributed.distributed |    DEBUG | [Rank 42]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=42
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=42
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=42
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [42], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[42]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb49b5c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb4b8800>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba003b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba006b0>)
                [0m
[38;5;39m2025-08-02 08:47:42,211 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,212 | xffl.distributed.distributed |    DEBUG | [Rank 54]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=54
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=54
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=54
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [54], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[54]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc9e77d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xca04a10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf4c8c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf4cbc0>)
                [0m
[38;5;39m2025-08-02 08:47:42,212 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,214 | xffl.distributed.distributed |    DEBUG | [Rank 180]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=180
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=180
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=180
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [180], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[180]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbd4a8a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd67ae0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2af510>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2af810>)
                [0m
[38;5;39m2025-08-02 08:47:42,214 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,219 | xffl.distributed.distributed |    DEBUG | [Rank 182]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=182
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=182
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=182
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [182], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[182]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb5f2bf0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb60fe30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb57740>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb57a40>)
                [0m
[38;5;39m2025-08-02 08:47:42,219 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,220 | xffl.distributed.distributed |    DEBUG | [Rank 47]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=47
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=47
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=47
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [47], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[47]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbe8f580>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbeac7c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc3f4090>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc3f4390>)
                [0m
[38;5;39m2025-08-02 08:47:42,220 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,224 | xffl.distributed.distributed |    DEBUG | [Rank 183]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=183
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=183
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=183
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [183], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[183]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcca7670>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xccc48b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd20bfe0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd20c2e0>)
                [0m
[38;5;39m2025-08-02 08:47:42,224 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,226 | xffl.distributed.distributed |    DEBUG | [Rank 184]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=184
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=184
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=184
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [184], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[184]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc9e87f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xca05a30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf4d530>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf4d830>)
                [0m
[38;5;39m2025-08-02 08:47:42,226 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,227 | xffl.distributed.distributed |    DEBUG | [Rank 60]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=60
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=60
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=60
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [60], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[60]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcea1290>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcebe4d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd405df0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd4060f0>)
                [0m
[38;5;39m2025-08-02 08:47:42,228 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,234 | xffl.distributed.distributed |    DEBUG | [Rank 187]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=187
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=187
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=187
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [187], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[187]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb5db0a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb317c70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb3fc90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb3ff90>)
                [0m
[38;5;39m2025-08-02 08:47:42,234 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,237 | xffl.distributed.distributed |    DEBUG | [Rank 188]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=188
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=188
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=188
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [188], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[188]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb216c10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb233e50>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb77b950>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb77bc50>)
                [0m
[38;5;39m2025-08-02 08:47:42,237 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,240 | xffl.distributed.distributed |    DEBUG | [Rank 190]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=190
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=190
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=190
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [190], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[190]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbe88d80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbea5fc0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc3eda80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc3edd80>)
                [0m
[38;5;39m2025-08-02 08:47:42,240 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,243 | xffl.distributed.distributed |    DEBUG | [Rank 191]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=191
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=191
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=191
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [191], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[191]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbdb9750>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbdb96f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc31df30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc31e230>)
                [0m
[38;5;39m2025-08-02 08:47:42,243 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,244 | xffl.distributed.distributed |    DEBUG | [Rank 214]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=214
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=214
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=214
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [214], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[214]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbdc42c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbde1500>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc329060>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc329360>)
                [0m
[38;5;39m2025-08-02 08:47:42,244 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,245 | xffl.distributed.distributed |    DEBUG | [Rank 65]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=65
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=65
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=65
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [65], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[65]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb31ac30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb337e70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb87f9d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb87fcd0>)
                [0m
[38;5;39m2025-08-02 08:47:42,246 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,246 | xffl.distributed.distributed |    DEBUG | [Rank 215]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=215
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=215
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=215
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [215], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[215]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb9eb7c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba08a00>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf505b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf508b0>)
                [0m
[38;5;39m2025-08-02 08:47:42,247 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,248 | xffl.distributed.distributed |    DEBUG | [Rank 68]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=68
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=68
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=68
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [68], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[68]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb349880>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb366ac0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb8ae500>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb8ae800>)
                [0m
[38;5;39m2025-08-02 08:47:42,248 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,249 | xffl.distributed.distributed |    DEBUG | [Rank 193]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=193
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=193
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=193
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [193], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[193]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb41a900>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb437b40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb97f280>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb97f580>)
                [0m
[38;5;39m2025-08-02 08:47:42,249 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,249 | xffl.distributed.distributed |    DEBUG | [Rank 59]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=59
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=59
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=59
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [59], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[59]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xce715b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce8e7f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd3d6060>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd3d6360>)
                [0m
[38;5;39m2025-08-02 08:47:42,250 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,250 | xffl.distributed.distributed |    DEBUG | [Rank 216]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=216
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=216
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=216
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [216], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[216]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcd59c30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcd76e70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd2be9a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd2beca0>)
                [0m
[38;5;39m2025-08-02 08:47:42,250 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,250 | xffl.distributed.distributed |    DEBUG | [Rank 196]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=196
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=196
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=196
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [196], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[196]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb9c20a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9df2e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf26a80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf26d80>)
                [0m
[38;5;39m2025-08-02 08:47:42,250 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,251 | xffl.distributed.distributed |    DEBUG | [Rank 197]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=197
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=197
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=197
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [197], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[197]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb19d320>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb1ba560>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb701bd0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb701ed0>)
                [0m
[38;5;39m2025-08-02 08:47:42,251 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,251 | xffl.distributed.distributed |    DEBUG | [Rank 194]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=194
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=194
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=194
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [194], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[194]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb809270>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb8264b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd6d690>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd6d990>)
                [0m
[38;5;39m2025-08-02 08:47:42,251 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,251 | xffl.distributed.distributed |    DEBUG | [Rank 160]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=160
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=160
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=160
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [160], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[160]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbd6beb0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd890f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2d0820>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2d0b20>)
                [0m
[38;5;39m2025-08-02 08:47:42,252 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,252 | xffl.distributed.distributed |    DEBUG | [Rank 217]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=217
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=217
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=217
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [217], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[217]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb90d6d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb92a910>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe71930>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe71c30>)
                [0m
[38;5;39m2025-08-02 08:47:42,252 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,253 | xffl.distributed.distributed |    DEBUG | [Rank 70]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=70
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=70
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=70
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [70], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[70]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcf92cd0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcfaff10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd4f7de0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd4f80e0>)
                [0m
[38;5;39m2025-08-02 08:47:42,254 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,254 | xffl.distributed.distributed |    DEBUG | [Rank 218]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=218
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=218
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=218
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [218], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[218]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb726250>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb743490>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc8aca0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc8afa0>)
                [0m
[38;5;39m2025-08-02 08:47:42,254 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,254 | xffl.distributed.distributed |    DEBUG | [Rank 161]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=161
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=161
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=161
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [161], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[161]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc48be00>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc4a9040>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc9f0cc0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc9f0fc0>)
                [0m
[38;5;39m2025-08-02 08:47:42,254 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,255 | xffl.distributed.distributed |    DEBUG | [Rank 198]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=198
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=198
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=198
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [198], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[198]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb7072c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb724500>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc6bf00>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc6c200>)
                [0m
[38;5;39m2025-08-02 08:47:42,255 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,256 | xffl.distributed.distributed |    DEBUG | [Rank 219]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=219
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=219
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=219
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [219], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[219]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb816300>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb833540>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd7b1c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd7b4c0>)
                [0m
[38;5;39m2025-08-02 08:47:42,256 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,256 | xffl.distributed.distributed |    DEBUG | [Rank 199]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=199
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=199
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=199
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [199], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[199]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcaa9c90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcac6ed0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd00ea40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd00ed40>)
                [0m
[38;5;39m2025-08-02 08:47:42,256 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,258 | xffl.distributed.distributed |    DEBUG | [Rank 220]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=220
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=220
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=220
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [220], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[220]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcc01170>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc1e3d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd165fe0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1662e0>)
                [0m
[38;5;39m2025-08-02 08:47:42,258 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,258 | xffl.distributed.distributed |    DEBUG | [Rank 200]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=200
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=200
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=200
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [200], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[200]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xce05800>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce22a40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd369d50>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd36a050>)
                [0m
[38;5;39m2025-08-02 08:47:42,258 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,260 | xffl.distributed.distributed |    DEBUG | [Rank 221]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=221
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=221
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=221
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [221], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[221]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb491690>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb4ae8d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9f5d20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9f6020>)
                [0m
[38;5;39m2025-08-02 08:47:42,260 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,261 | xffl.distributed.distributed |    DEBUG | [Rank 201]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=201
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=201
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=201
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [201], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[201]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb47c6d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb499910>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9e1480>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9e1780>)
                [0m
[38;5;39m2025-08-02 08:47:42,261 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,262 | xffl.distributed.distributed |    DEBUG | [Rank 163]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=163
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=163
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=163
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [163], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[163]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb23ca60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb259ca0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb7a1530>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb7a1830>)
                [0m
[38;5;39m2025-08-02 08:47:42,262 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,262 | xffl.distributed.distributed |    DEBUG | [Rank 222]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=222
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=222
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=222
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [222], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[222]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xba574c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba74700>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbfbc0d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbfbc3d0>)
                [0m
[38;5;39m2025-08-02 08:47:42,262 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,263 | xffl.distributed.distributed |    DEBUG | [Rank 202]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=202
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=202
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=202
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [202], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[202]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbccd2a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba09f90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc231370>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc231670>)
                [0m
[38;5;39m2025-08-02 08:47:42,263 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,263 | xffl.distributed.distributed |    DEBUG | [Rank 223]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=223
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=223
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=223
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [223], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[223]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb6569c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb673c00>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbbbb3d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbbbb6d0>)
                [0m
[38;5;39m2025-08-02 08:47:42,263 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,265 | xffl.distributed.distributed |    DEBUG | [Rank 224]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=224
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=224
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=224
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [224], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[224]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb239670>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb2568b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb79d370>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb79d670>)
                [0m
[38;5;39m2025-08-02 08:47:42,265 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,266 | xffl.distributed.distributed |    DEBUG | [Rank 203]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=203
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=203
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=203
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [203], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[203]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc806fc0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc824200>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcd6bca0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcd6bfa0>)
                [0m
[38;5;39m2025-08-02 08:47:42,266 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,266 | xffl.distributed.distributed |    DEBUG | [Rank 225]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=225
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=225
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=225
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [225], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[225]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc5e4ea0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc602100>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcb49d80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcb4a080>)
                [0m
[38;5;39m2025-08-02 08:47:42,266 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,267 | xffl.distributed.distributed |    DEBUG | [Rank 226]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=226
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=226
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=226
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [226], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[226]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xce2ad00>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce47f40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd38f170>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd38f470>)
                [0m
[38;5;39m2025-08-02 08:47:42,267 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,268 | xffl.distributed.distributed |    DEBUG | [Rank 167]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=167
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=167
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=167
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [167], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[167]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xba60b90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba7ddf0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbfc5730>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbfc5a30>)
                [0m
[38;5;39m2025-08-02 08:47:42,268 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,269 | xffl.distributed.distributed |    DEBUG | [Rank 204]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=204
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=204
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=204
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [204], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[204]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc1f4270>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2114b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc7590c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc7593c0>)
                [0m
[38;5;39m2025-08-02 08:47:42,269 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,269 | xffl.distributed.distributed |    DEBUG | [Rank 227]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=227
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=227
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=227
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [227], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[227]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc071c70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc08eed0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc5d6060>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc5d6360>)
                [0m
[38;5;39m2025-08-02 08:47:42,269 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,270 | xffl.distributed.distributed |    DEBUG | [Rank 228]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=228
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=228
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=228
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [228], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[228]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc715ae0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc732d20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc7a7d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc7aad0>)
                [0m
[38;5;39m2025-08-02 08:47:42,270 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,271 | xffl.distributed.distributed |    DEBUG | [Rank 109]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=109
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=109
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=109
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [109], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[109]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xba7f8b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba9caf0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbfe43a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbfe46a0>)
                [0m
[38;5;39m2025-08-02 08:47:42,271 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,272 | xffl.distributed.distributed |    DEBUG | [Rank 229]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=229
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=229
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=229
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [229], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[229]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcde8c70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcb253d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd34d900>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd34dc00>)
                [0m
[38;5;39m2025-08-02 08:47:42,272 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,273 | xffl.distributed.distributed |    DEBUG | [Rank 110]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=110
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=110
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=110
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [110], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[110]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xba3b300>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba58540>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf9fc40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf9ff40>)
                [0m
[38;5;39m2025-08-02 08:47:42,273 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,273 | xffl.distributed.distributed |    DEBUG | [Rank 230]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=230
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=230
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=230
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [230], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[230]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc807080>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc8242c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcd6bbe0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcd6bee0>)
                [0m
[38;5;39m2025-08-02 08:47:42,273 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,274 | xffl.distributed.distributed |    DEBUG | [Rank 231]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=231
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=231
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=231
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [231], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[231]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xca65290>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xca824d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcfc9ab0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcfc9db0>)
                [0m
[38;5;39m2025-08-02 08:47:42,274 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,275 | xffl.distributed.distributed |    DEBUG | [Rank 232]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=232
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=232
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=232
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [232], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[232]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb648660>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb6658a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbbacd80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbbad080>)
                [0m
[38;5;39m2025-08-02 08:47:42,275 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,277 | xffl.distributed.distributed |    DEBUG | [Rank 113]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=113
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=113
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=113
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [113], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[113]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc1d3ca0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc1f0f00>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc738770>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc738a70>)
                [0m
[38;5;39m2025-08-02 08:47:42,277 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,277 | xffl.distributed.distributed |    DEBUG | [Rank 175]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=175
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=175
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=175
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [175], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[175]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcc494b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc666f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1adf00>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1ae200>)
                [0m
[38;5;39m2025-08-02 08:47:42,277 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,279 | xffl.distributed.distributed |    DEBUG | [Rank 235]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=235
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=235
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=235
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [235], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[235]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb92c370>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9495b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe90530>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe90830>)
                [0m
[38;5;39m2025-08-02 08:47:42,279 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,279 | xffl.distributed.distributed |    DEBUG | [Rank 208]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=208
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=208
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=208
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [208], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[208]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc42e450>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc44b690>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc992a80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc992d80>)
                [0m
[38;5;39m2025-08-02 08:47:42,279 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,279 | xffl.distributed.distributed |    DEBUG | [Rank 94]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=94
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=94
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=94
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [94], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[94]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcf81660>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf9e8a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd4e6020>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd4e6320>)
                [0m
[38;5;39m2025-08-02 08:47:42,279 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,279 | xffl.distributed.distributed |    DEBUG | [Rank 233]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=233
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=233
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=233
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [233], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[233]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbd54460>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd71680>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2b8b50>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2b8e50>)
                [0m
[38;5;39m2025-08-02 08:47:42,279 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,279 | xffl.distributed.distributed |    DEBUG | [Rank 107]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=107
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=107
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=107
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [107], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[107]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcbf4aa0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc11ce0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1590a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1593a0>)
                [0m
[38;5;39m2025-08-02 08:47:42,279 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,279 | xffl.distributed.distributed |    DEBUG | [Rank 234]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=234
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=234
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=234
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [234], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[234]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xba66b90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba83dd0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbfcb6c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbfcb9c0>)
                [0m
[38;5;39m2025-08-02 08:47:42,280 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,280 | xffl.distributed.distributed |    DEBUG | [Rank 236]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=236
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=236
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=236
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [236], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[236]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcb69320>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcb86560>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd0cdaf0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd0cddf0>)
                [0m
[38;5;39m2025-08-02 08:47:42,280 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,280 | xffl.distributed.distributed |    DEBUG | [Rank 114]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=114
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=114
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=114
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [114], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[114]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc3ea9b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc407bf0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc94f430>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc94f730>)
                [0m
[38;5;39m2025-08-02 08:47:42,280 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,281 | xffl.distributed.distributed |    DEBUG | [Rank 179]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=179
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=179
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=179
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [179], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[179]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc0ed210>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc10a450>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc651c80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc651f80>)
                [0m
[38;5;39m2025-08-02 08:47:42,281 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,281 | xffl.distributed.distributed |    DEBUG | [Rank 237]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=237
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=237
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=237
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [237], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[237]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcae2170>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcaff390>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd046db0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd0470b0>)
                [0m
[38;5;39m2025-08-02 08:47:42,281 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,281 | xffl.distributed.distributed |    DEBUG | [Rank 96]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=96
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=96
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=96
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [96], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[96]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcc49120>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc66360>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1adc90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd1adf90>)
                [0m
[38;5;39m2025-08-02 08:47:42,281 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,282 | xffl.distributed.distributed |    DEBUG | [Rank 238]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=238
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=238
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=238
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [238], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[238]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc1863b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc1a35f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc6ea940>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc6eac40>)
                [0m
[38;5;39m2025-08-02 08:47:42,282 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,283 | xffl.distributed.distributed |    DEBUG | [Rank 210]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=210
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=210
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=210
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [210], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[210]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbfb5e00>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbfd3040>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc51a9a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc51aca0>)
                [0m
[38;5;39m2025-08-02 08:47:42,283 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,283 | xffl.distributed.distributed |    DEBUG | [Rank 181]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=181
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=181
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=181
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [181], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[181]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcd0bfb0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcd291f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd2709b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd270cb0>)
                [0m
[38;5;39m2025-08-02 08:47:42,283 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,283 | xffl.distributed.distributed |    DEBUG | [Rank 79]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=79
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=79
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=79
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [79], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[79]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb4c2d40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb4dff60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba277b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xba27ab0>)
                [0m
[38;5;39m2025-08-02 08:47:42,283 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,283 | xffl.distributed.distributed |    DEBUG | [Rank 97]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=97
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=97
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=97
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [97], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[97]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb3f0270>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb40d4b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb954e30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb955130>)
                [0m
[38;5;39m2025-08-02 08:47:42,283 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,283 | xffl.distributed.distributed |    DEBUG | [Rank 88]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=88
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=88
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=88
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [88], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[88]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcce1930>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xccfeb70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd246530>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd246830>)
                [0m
[38;5;39m2025-08-02 08:47:42,284 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,284 | xffl.distributed.distributed |    DEBUG | [Rank 239]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=239
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=239
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=239
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [239], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[239]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc656680>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc6738c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcbbb3e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcbbb6e0>)
                [0m
[38;5;39m2025-08-02 08:47:42,284 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,284 | xffl.distributed.distributed |    DEBUG | [Rank 211]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=211
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=211
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=211
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [211], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[211]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb8a8760>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb8c59a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe0d360>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe0d660>)
                [0m
[38;5;39m2025-08-02 08:47:42,284 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,285 | xffl.distributed.distributed |    DEBUG | [Rank 90]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=90
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=90
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=90
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [90], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[90]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb96d350>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb98a590>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbed1d10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbed2010>)
                [0m
[38;5;39m2025-08-02 08:47:42,285 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,285 | xffl.distributed.distributed |    DEBUG | [Rank 240]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=240
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=240
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=240
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [240], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[240]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc94d210>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc96a450>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xceb1cf0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xceb1ff0>)
                [0m
[38;5;39m2025-08-02 08:47:42,285 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,285 | xffl.distributed.distributed |    DEBUG | [Rank 80]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=80
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=80
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=80
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [80], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[80]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb6a9080>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb6c62c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc0ddc0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc0e0c0>)
                [0m
[38;5;39m2025-08-02 08:47:42,285 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,286 | xffl.distributed.distributed |    DEBUG | [Rank 212]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=212
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=212
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=212
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [212], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[212]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcf9bae0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcfb8d20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd500610>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd500910>)
                [0m
[38;5;39m2025-08-02 08:47:42,286 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,286 | xffl.distributed.distributed |    DEBUG | [Rank 92]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=92
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=92
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=92
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [92], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[92]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb6defe0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb6fc220>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc43a70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc43d70>)
                [0m
[38;5;39m2025-08-02 08:47:42,286 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,286 | xffl.distributed.distributed |    DEBUG | [Rank 241]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=241
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=241
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=241
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [241], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[241]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb22f970>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb24cbb0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb7942e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb7945e0>)
                [0m
[38;5;39m2025-08-02 08:47:42,286 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,286 | xffl.distributed.distributed |    DEBUG | [Rank 186]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=186
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=186
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=186
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [186], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[186]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc430730>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc44d970>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc994b90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc994e90>)
                [0m
[38;5;39m2025-08-02 08:47:42,287 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,287 | xffl.distributed.distributed |    DEBUG | [Rank 242]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=242
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=242
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=242
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [242], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[242]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbd8b880>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbda8ac0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2f00c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2f03c0>)
                [0m
[38;5;39m2025-08-02 08:47:42,287 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,287 | xffl.distributed.distributed |    DEBUG | [Rank 185]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=185
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=185
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=185
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [185], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[185]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb59ce40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb5ba080>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb019d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb01cd0>)
                [0m
[38;5;39m2025-08-02 08:47:42,287 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,287 | xffl.distributed.distributed |    DEBUG | [Rank 141]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=141
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=141
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=141
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [141], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[141]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xce0fbe0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce2ce20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd374970>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd374c70>)
                [0m
[38;5;39m2025-08-02 08:47:42,288 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,288 | xffl.distributed.distributed |    DEBUG | [Rank 243]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=243
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=243
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=243
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [243], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[243]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbc57000>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc74240>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc1bb9c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc1bbcc0>)
                [0m
[38;5;39m2025-08-02 08:47:42,288 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,288 | xffl.distributed.distributed |    DEBUG | [Rank 93]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=93
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=93
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=93
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [93], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[93]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb5bfb90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb5dcdd0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb24550>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbb24850>)
                [0m
[38;5;39m2025-08-02 08:47:42,288 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,288 | xffl.distributed.distributed |    DEBUG | [Rank 244]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=244
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=244
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=244
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [244], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[244]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc3dc420>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc3f9660>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc941090>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc941390>)
                [0m
[38;5;39m2025-08-02 08:47:42,289 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,288 | xffl.distributed.distributed |    DEBUG | [Rank 252]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=252
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=252
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=252
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [252], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[252]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc9b8de0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc9d6020>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf1dd30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcf1e030>)
                [0m
[38;5;39m2025-08-02 08:47:42,288 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,288 | xffl.distributed.distributed |    DEBUG | [Rank 87]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=87
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=87
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=87
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [87], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[87]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc6cc600>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc6e9840>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc30f70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc31270>)
                [0m
[38;5;39m2025-08-02 08:47:42,289 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,288 | xffl.distributed.distributed |    DEBUG | [Rank 116]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=116
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=116
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=116
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [116], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[116]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc321a70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc33ecb0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc886820>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc886b20>)
                [0m
[38;5;39m2025-08-02 08:47:42,288 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,288 | xffl.distributed.distributed |    DEBUG | [Rank 121]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=121
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=121
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=121
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [121], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[121]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb9d6270>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9f34b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf3ae70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbf3b170>)
                [0m
[38;5;39m2025-08-02 08:47:42,289 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,289 | xffl.distributed.distributed |    DEBUG | [Rank 128]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=128
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=128
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=128
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [128], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[128]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcdd99a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcdf6be0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd33e560>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd33e860>)
                [0m
[38;5;39m2025-08-02 08:47:42,289 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,289 | xffl.distributed.distributed |    DEBUG | [Rank 245]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=245
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=245
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=245
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [245], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[245]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcdf2f70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce101b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd357b10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd357e10>)
                [0m
[38;5;39m2025-08-02 08:47:42,289 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,289 | xffl.distributed.distributed |    DEBUG | [Rank 146]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=146
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=146
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=146
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [146], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[146]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc374a40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc391c80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc8d9740>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc8d9a40>)
                [0m
[38;5;39m2025-08-02 08:47:42,289 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,289 | xffl.distributed.distributed |    DEBUG | [Rank 137]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=137
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=137
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=137
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [137], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[137]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbadb7d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbaf8a10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc0402e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc0405e0>)
                [0m
[38;5;39m2025-08-02 08:47:42,289 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,289 | xffl.distributed.distributed |    DEBUG | [Rank 189]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=189
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=189
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=189
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [189], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[189]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc7167e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc7339e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc7af60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcc7b260>)
                [0m
[38;5;39m2025-08-02 08:47:42,289 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,289 | xffl.distributed.distributed |    DEBUG | [Rank 144]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=144
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=144
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=144
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [144], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[144]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc1bbcb0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc1d8ef0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc7209f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc720cf0>)
                [0m
[38;5;39m2025-08-02 08:47:42,290 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,290 | xffl.distributed.distributed |    DEBUG | [Rank 246]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=246
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=246
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=246
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [246], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[246]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb4914e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb4ae720>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9f60b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9f63b0>)
                [0m
[38;5;39m2025-08-02 08:47:42,290 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,290 | xffl.distributed.distributed |    DEBUG | [Rank 247]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=247
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=247
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=247
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [247], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[247]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb8e7260>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb9044a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe4c080>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbe4c380>)
                [0m
[38;5;39m2025-08-02 08:47:42,291 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,291 | xffl.distributed.distributed |    DEBUG | [Rank 98]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=98
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=98
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=98
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [98], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[98]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbd6cec0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd6ce80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2d1b20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2d1e20>)
                [0m
[38;5;39m2025-08-02 08:47:42,291 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,291 | xffl.distributed.distributed |    DEBUG | [Rank 192]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=192
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=192
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=192
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [192], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[192]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xce01340>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcb3daa0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd365650>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd365950>)
                [0m
[38;5;39m2025-08-02 08:47:42,291 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,291 | xffl.distributed.distributed |    DEBUG | [Rank 248]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=248
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=248
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=248
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [248], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[248]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbabc120>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbad9360>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc0201e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc0204e0>)
                [0m
[38;5;39m2025-08-02 08:47:42,291 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,291 | xffl.distributed.distributed |    DEBUG | [Rank 99]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=99
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=99
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=99
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [99], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[99]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc3ef170>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc40c3d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc953730>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc953a30>)
                [0m
[38;5;39m2025-08-02 08:47:42,291 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,291 | xffl.distributed.distributed |    DEBUG | [Rank 195]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=195
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=195
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=195
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [195], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[195]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb6e20e0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb6ff320>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc46ce0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc46fe0>)
                [0m
[38;5;39m2025-08-02 08:47:42,291 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,291 | xffl.distributed.distributed |    DEBUG | [Rank 249]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=249
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=249
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=249
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [249], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[249]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xca47df0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xca65030>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcfac010>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcfac310>)
                [0m
[38;5;39m2025-08-02 08:47:42,292 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,292 | xffl.distributed.distributed |    DEBUG | [Rank 150]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=150
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=150
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=150
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [150], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[150]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbd7e2b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd9b4f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2e2e40>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc2e3140>)
                [0m
[38;5;39m2025-08-02 08:47:42,292 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,291 | xffl.distributed.distributed |    DEBUG | [Rank 103]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=103
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=103
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=103
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [103], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[103]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcab2a80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcacfcc0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd0176f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd0179f0>)
                [0m
[38;5;39m2025-08-02 08:47:42,292 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,292 | xffl.distributed.distributed |    DEBUG | [Rank 138]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=138
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=138
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=138
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [138], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[138]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc7cd6b0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc7ea8f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcd32360>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcd32660>)
                [0m
[38;5;39m2025-08-02 08:47:42,292 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,292 | xffl.distributed.distributed |    DEBUG | [Rank 250]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=250
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=250
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=250
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [250], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[250]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc07d870>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc07d810>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc5e21c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc5e24c0>)
                [0m
[38;5;39m2025-08-02 08:47:42,292 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,292 | xffl.distributed.distributed |    DEBUG | [Rank 251]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=251
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=251
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=251
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [251], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[251]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbdce700>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbdeb940>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc333100>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc333400>)
                [0m
[38;5;39m2025-08-02 08:47:42,292 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,292 | xffl.distributed.distributed |    DEBUG | [Rank 140]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=140
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=140
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=140
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [140], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[140]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xce3abe0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xce57e20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd39f8a0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd39fba0>)
                [0m
[38;5;39m2025-08-02 08:47:42,292 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,292 | xffl.distributed.distributed |    DEBUG | [Rank 205]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=205
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=205
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=205
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [205], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[205]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb7eae50>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb808090>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd4f590>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbd4f890>)
                [0m
[38;5;39m2025-08-02 08:47:42,293 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,293 | xffl.distributed.distributed |    DEBUG | [Rank 102]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=102
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=102
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=102
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [102], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[102]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc1e9fc0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc207200>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc74ea80>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc74ed80>)
                [0m
[38;5;39m2025-08-02 08:47:42,293 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,293 | xffl.distributed.distributed |    DEBUG | [Rank 206]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=206
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=206
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=206
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [206], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[206]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xb71fa20>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xb73cc60>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc843f0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbc846f0>)
                [0m
[38;5;39m2025-08-02 08:47:42,293 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,293 | xffl.distributed.distributed |    DEBUG | [Rank 136]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=136
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=136
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=136
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [136], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[136]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc44fc30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc46ce70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc9b4810>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc9b4b10>)
                [0m
[38;5;39m2025-08-02 08:47:42,293 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,293 | xffl.distributed.distributed |    DEBUG | [Rank 207]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=207
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=207
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=207
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [207], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[207]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcff6240>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd013480>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd55a7c0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd55aac0>)
                [0m
[38;5;39m2025-08-02 08:47:42,294 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,294 | xffl.distributed.distributed |    DEBUG | [Rank 154]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=154
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=154
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=154
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [154], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[154]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xc2f7610>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc314850>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc85c0d0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc85c3d0>)
                [0m
[38;5;39m2025-08-02 08:47:42,294 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,294 | xffl.distributed.distributed |    DEBUG | [Rank 209]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=209
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=209
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=209
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [209], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[209]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xbf89ad0>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xbfa6d10>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc4eea00>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xc4eed00>)
                [0m
[38;5;39m2025-08-02 08:47:42,294 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,294 | xffl.distributed.distributed |    DEBUG | [Rank 213]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=213
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=213
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=213
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [213], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[213]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcaad630>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcaca870>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd012360>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd012660>)
                [0m
[38;5;39m2025-08-02 08:47:42,294 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:47:42,295 | xffl.distributed.distributed |    DEBUG | [Rank 153]: distributed setup: 

                GLOBAL:
                    Backend=nccl
                    Master address=lrdn0014
                    Master port=29500
                    Rank=153
                    World size=256
                NODE:
                    Node local rank=0
                    Node local size=1
                    Node rank=153
                    Node world size=256
                REPLICA:
                    Replica local rank=None
                    Replica local size=None
                    Replica rank=None
                    Replica world size=None
                FEDERATION:
                    Federated local rank=0
                    Federated local size=(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
                    Federated rank=153
                    Federated world size=256
                MESHES:
                    FSDP=DeviceMesh('cuda', [153], mesh_dim_names=('shard',))
                    HSDP=None
                    Is sender=True
                    Receives from=None
                    Federated group=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255]
                    Replica group=None
                    Federation=[153]
                DEVICE:
                    Device type=cuda
                    Current device=cuda:0
                    Initialization device=cpu
                    Meta initialization=True
                    Streams=(<torch.cuda.Stream device=cuda:0 cuda_stream=0xcb1ee70>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xcb1ee30>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd083b90>, <torch.cuda.Stream device=cuda:0 cuda_stream=0xd083e90>)
                [0m
[38;5;39m2025-08-02 08:47:42,295 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-08-02 08:48:16,216 |         __main__ |    DEBUG | Model loading time: 34.83 seconds[0m
[38;5;39m2025-08-02 08:48:16,217 |         __main__ |    DEBUG | Training llama3.1-8b: 8030.26 million trainable parameters[0m
[38;5;39m2025-08-02 08:48:16,217 | xffl.distributed.distributed |    DEBUG | [Rank 0]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0014:3257777:3257777 [0] NCCL INFO Bootstrap: Using ib0:10.128.6.89<0>
lrdn0014:3257777:3257777 [0] NCCL INFO cudaDriverVersion 12020
lrdn0014:3257777:3257777 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0014:3257777:3257777 [0] NCCL INFO Comm config Blocking set to 1
lrdn0014:3257777:3257870 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0014:3257777:3257870 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0014:3257777:3257870 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.6.89<0>
lrdn0014:3257777:3257870 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0014:3257777:3257870 [0] NCCL INFO Using network IB
lrdn0014:3257777:3257870 [0] NCCL INFO ncclCommInitRankConfig comm 0xd27b180 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xca3601382b28a6fe - Init START
lrdn0014:3257777:3257870 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0014:3257777:3257870 [0] NCCL INFO Bootstrap timings total 0.000392 (create 0.000019, send 0.000058, recv 0.000094, ring 0.000001, delay 0.000001)
lrdn0014:3257777:3257870 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0014:3257777:3257870 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0014:3257777:3257870 [0] NCCL INFO comm 0xd27b180 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 00/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 01/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 02/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 03/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 04/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 05/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 06/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 07/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 08/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 09/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 10/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 11/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 12/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 13/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 14/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 15/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 16/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 17/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 18/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 19/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 20/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 21/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 22/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 23/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 24/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 25/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 26/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 27/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 28/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 29/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 30/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 31/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 32/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 33/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 34/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 35/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 36/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 37/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 38/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 39/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 40/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 41/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 42/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 43/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 44/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 45/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 46/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 47/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 48/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 49/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 50/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 51/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 52/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 53/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 54/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 55/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 56/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 57/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 58/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 59/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 60/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 61/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 62/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Channel 63/64 : 0
lrdn0014:3257777:3257870 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0014:3257777:3257870 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0014:3257777:3257870 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0014:3257777:3257876 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn0014:3257777:3257877 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn0014:3257777:3257870 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0014:3257777:3257870 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0014:3257777:3257870 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0014:3257777:3257870 [0] NCCL INFO ncclCommInitRankConfig comm 0xd27b180 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xca3601382b28a6fe - Init COMPLETE
lrdn0014:3257777:3257870 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
[38;5;39m2025-08-02 08:48:22,911 |         __main__ |    DEBUG | FSDP wrapping setup time: 6.69 seconds[0m
[38;5;39m2025-08-02 08:48:22,924 |         __main__ |    DEBUG | Total setup time: 80.52 seconds[0m
[38;5;39m2025-08-02 08:48:22,925 |         __main__ |    DEBUG | GPU RAM allocated before training: 16.06 GB[0m


[38;5;39m2025-08-02 08:48:23,856 | xffl.distributed.distributed |    DEBUG | [Rank 41]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:48:24,015 | xffl.distributed.distributed |    DEBUG | [Rank 255]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0297:281832:281832 [0] NCCL INFO Bootstrap: Using ib0:10.128.10.197<0>
lrdn0297:281832:281832 [0] NCCL INFO cudaDriverVersion 12020
lrdn0297:281832:281832 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0297:281832:281832 [0] NCCL INFO Comm config Blocking set to 1
lrdn1438:3364708:3364708 [0] NCCL INFO Bootstrap: Using ib0:10.128.28.153<0>
lrdn1438:3364708:3364708 [0] NCCL INFO cudaDriverVersion 12020
lrdn1438:3364708:3364708 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0297:281832:281931 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0297:281832:281931 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1438:3364708:3364708 [0] NCCL INFO Comm config Blocking set to 1
lrdn0297:281832:281931 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.10.197<0>
lrdn0297:281832:281931 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0297:281832:281931 [0] NCCL INFO Using network IB
lrdn0297:281832:281931 [0] NCCL INFO ncclCommInitRankConfig comm 0xd116af0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3940d290d826c832 - Init START
lrdn0297:281832:281931 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0297:281832:281931 [0] NCCL INFO Bootstrap timings total 0.000396 (create 0.000021, send 0.000063, recv 0.000085, ring 0.000001, delay 0.000000)
[38;5;39m2025-08-02 08:48:24,409 | xffl.distributed.distributed |    DEBUG | [Rank 49]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0297:281832:281931 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0297:281832:281931 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0297:281832:281931 [0] NCCL INFO comm 0xd116af0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 00/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 01/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 02/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 03/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 04/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 05/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 06/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 07/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 08/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 09/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 10/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 11/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 12/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 13/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 14/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 15/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 16/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 17/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 18/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 19/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 20/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 21/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 22/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 23/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 24/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 25/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 26/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 27/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 28/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 29/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 30/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 31/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 32/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 33/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 34/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 35/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 36/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 37/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 38/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 39/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 40/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 41/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 42/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 43/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 44/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 45/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 46/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 47/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 48/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 49/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 50/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 51/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 52/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 53/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 54/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 55/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 56/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 57/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 58/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 59/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 60/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 61/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 62/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Channel 63/64 : 0
lrdn0297:281832:281931 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0297:281832:281931 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0297:281832:281931 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0297:281832:281937 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0297:281832:281938 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0297:281832:281931 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0297:281832:281931 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0297:281832:281931 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0297:281832:281931 [0] NCCL INFO ncclCommInitRankConfig comm 0xd116af0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3940d290d826c832 - Init COMPLETE
lrdn0297:281832:281931 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.16, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1438:3364708:3364809 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1438:3364708:3364809 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1438:3364708:3364809 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.28.153<0>
lrdn1438:3364708:3364809 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1438:3364708:3364809 [0] NCCL INFO Using network IB
lrdn1438:3364708:3364809 [0] NCCL INFO ncclCommInitRankConfig comm 0x323aa020 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xea1dfdecdc626d65 - Init START
lrdn1438:3364708:3364809 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1438:3364708:3364809 [0] NCCL INFO Bootstrap timings total 0.000428 (create 0.000024, send 0.000070, recv 0.000097, ring 0.000002, delay 0.000001)
lrdn1438:3364708:3364809 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1438:3364708:3364809 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1438:3364708:3364809 [0] NCCL INFO comm 0x323aa020 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 00/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 01/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 02/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 03/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 04/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 05/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 06/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 07/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 08/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 09/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 10/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 11/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 12/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 13/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 14/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 15/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 16/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 17/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 18/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 19/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 20/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 21/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 22/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 23/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 24/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 25/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 26/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 27/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 28/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 29/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 30/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 31/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 32/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 33/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 34/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 35/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 36/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 37/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 38/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 39/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 40/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 41/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 42/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 43/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 44/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 45/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 46/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 47/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 48/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 49/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 50/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 51/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 52/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 53/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 54/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 55/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 56/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 57/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 58/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 59/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 60/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 61/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 62/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Channel 63/64 : 0
lrdn1438:3364708:3364809 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1438:3364708:3364809 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1438:3364708:3364809 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1438:3364708:3364815 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn1438:3364708:3364816 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1438:3364708:3364809 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1438:3364708:3364809 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1438:3364708:3364809 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1438:3364708:3364809 [0] NCCL INFO ncclCommInitRankConfig comm 0x323aa020 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xea1dfdecdc626d65 - Init COMPLETE
lrdn1438:3364708:3364809 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0358:1512606:1512606 [0] NCCL INFO Bootstrap: Using ib0:10.128.11.185<0>
lrdn0358:1512606:1512606 [0] NCCL INFO cudaDriverVersion 12020
lrdn0358:1512606:1512606 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0358:1512606:1512606 [0] NCCL INFO Comm config Blocking set to 1
lrdn0358:1512606:1512698 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0358:1512606:1512698 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0358:1512606:1512698 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.11.185<0>
lrdn0358:1512606:1512698 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0358:1512606:1512698 [0] NCCL INFO Using network IB
lrdn0358:1512606:1512698 [0] NCCL INFO ncclCommInitRankConfig comm 0xdac94a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xeb05772030673b33 - Init START
lrdn0358:1512606:1512698 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0358:1512606:1512698 [0] NCCL INFO Bootstrap timings total 0.000448 (create 0.000026, send 0.000072, recv 0.000101, ring 0.000001, delay 0.000000)
lrdn0358:1512606:1512698 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0358:1512606:1512698 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0358:1512606:1512698 [0] NCCL INFO comm 0xdac94a0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 00/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 01/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 02/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 03/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 04/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 05/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 06/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 07/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 08/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 09/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 10/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 11/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 12/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 13/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 14/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 15/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 16/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 17/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 18/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 19/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 20/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 21/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 22/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 23/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 24/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 25/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 26/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 27/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 28/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 29/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 30/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 31/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 32/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 33/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 34/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 35/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 36/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 37/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 38/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 39/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 40/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 41/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 42/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 43/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 44/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 45/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 46/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 47/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 48/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 49/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 50/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 51/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 52/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 53/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 54/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 55/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 56/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 57/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 58/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 59/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 60/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 61/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 62/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Channel 63/64 : 0
lrdn0358:1512606:1512698 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0358:1512606:1512698 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0358:1512606:1512698 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0358:1512606:1512704 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn0358:1512606:1512705 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn0358:1512606:1512698 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0358:1512606:1512698 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0358:1512606:1512698 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0358:1512606:1512698 [0] NCCL INFO ncclCommInitRankConfig comm 0xdac94a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xeb05772030673b33 - Init COMPLETE
lrdn0358:1512606:1512698 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:25,300 | xffl.distributed.distributed |    DEBUG | [Rank 75]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:48:25,550 | xffl.distributed.distributed |    DEBUG | [Rank 72]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0528:2919634:2919634 [0] NCCL INFO Bootstrap: Using ib0:10.128.14.97<0>
lrdn0528:2919634:2919634 [0] NCCL INFO cudaDriverVersion 12020
lrdn0528:2919634:2919634 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0528:2919634:2919634 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:48:25,684 | xffl.distributed.distributed |    DEBUG | [Rank 131]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0528:2919634:2919732 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0528:2919634:2919732 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0528:2919634:2919732 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.14.97<0>
lrdn0528:2919634:2919732 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0528:2919634:2919732 [0] NCCL INFO Using network IB
lrdn0528:2919634:2919732 [0] NCCL INFO ncclCommInitRankConfig comm 0xe5adb40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xad6d26072878a566 - Init START
lrdn0528:2919634:2919732 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0528:2919634:2919732 [0] NCCL INFO Bootstrap timings total 0.000386 (create 0.000018, send 0.000058, recv 0.000091, ring 0.000001, delay 0.000000)
lrdn0528:2919634:2919732 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0528:2919634:2919732 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0528:2919634:2919732 [0] NCCL INFO comm 0xe5adb40 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 00/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 01/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 02/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 03/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 04/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 05/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 06/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 07/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 08/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 09/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 10/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 11/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 12/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 13/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 14/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 15/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 16/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 17/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 18/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 19/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 20/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 21/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 22/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 23/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 24/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 25/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 26/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 27/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 28/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 29/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 30/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 31/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 32/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 33/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 34/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 35/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 36/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 37/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 38/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 39/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 40/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 41/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 42/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 43/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 44/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 45/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 46/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 47/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 48/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 49/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 50/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 51/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 52/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 53/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 54/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 55/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 56/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 57/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 58/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 59/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 60/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 61/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 62/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Channel 63/64 : 0
lrdn0528:2919634:2919732 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0528:2919634:2919732 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0528:2919634:2919732 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0528:2919634:2919738 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn0528:2919634:2919739 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1
lrdn0507:1445318:1445318 [0] NCCL INFO Bootstrap: Using ib0:10.128.14.13<0>
lrdn0507:1445318:1445318 [0] NCCL INFO cudaDriverVersion 12020
lrdn0507:1445318:1445318 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0507:1445318:1445318 [0] NCCL INFO Comm config Blocking set to 1
lrdn0528:2919634:2919732 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0528:2919634:2919732 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0528:2919634:2919732 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0528:2919634:2919732 [0] NCCL INFO ncclCommInitRankConfig comm 0xe5adb40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xad6d26072878a566 - Init COMPLETE
lrdn0528:2919634:2919732 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:25,931 | xffl.distributed.distributed |    DEBUG | [Rank 237]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0789:1852874:1852874 [0] NCCL INFO Bootstrap: Using ib0:10.128.18.117<0>
lrdn0789:1852874:1852874 [0] NCCL INFO cudaDriverVersion 12020
lrdn0789:1852874:1852874 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0789:1852874:1852874 [0] NCCL INFO Comm config Blocking set to 1
lrdn0507:1445318:1445411 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0507:1445318:1445411 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0507:1445318:1445411 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.14.13<0>
lrdn0507:1445318:1445411 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0507:1445318:1445411 [0] NCCL INFO Using network IB
lrdn0507:1445318:1445411 [0] NCCL INFO ncclCommInitRankConfig comm 0xda99b10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x37b7633b9fbe8e20 - Init START
lrdn0507:1445318:1445411 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0507:1445318:1445411 [0] NCCL INFO Bootstrap timings total 0.000428 (create 0.000026, send 0.000070, recv 0.000100, ring 0.000001, delay 0.000000)
lrdn0507:1445318:1445411 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0507:1445318:1445411 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0507:1445318:1445411 [0] NCCL INFO comm 0xda99b10 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 00/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 01/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 02/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 03/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 04/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 05/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 06/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 07/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 08/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 09/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 10/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 11/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 12/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 13/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 14/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 15/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 16/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 17/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 18/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 19/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 20/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 21/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 22/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 23/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 24/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 25/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 26/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 27/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 28/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 29/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 30/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 31/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 32/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 33/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 34/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 35/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 36/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 37/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 38/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 39/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 40/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 41/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 42/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 43/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 44/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 45/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 46/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 47/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 48/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 49/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 50/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 51/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 52/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 53/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 54/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 55/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 56/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 57/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 58/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 59/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 60/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 61/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 62/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Channel 63/64 : 0
lrdn0507:1445318:1445411 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0507:1445318:1445411 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0507:1445318:1445411 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0507:1445318:1445418 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn0507:1445318:1445417 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn0507:1445318:1445411 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0507:1445318:1445411 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0507:1445318:1445411 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0507:1445318:1445411 [0] NCCL INFO ncclCommInitRankConfig comm 0xda99b10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x37b7633b9fbe8e20 - Init COMPLETE
lrdn0507:1445318:1445411 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0789:1852874:1852973 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0789:1852874:1852973 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0789:1852874:1852973 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.18.117<0>
lrdn0789:1852874:1852973 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0789:1852874:1852973 [0] NCCL INFO Using network IB
lrdn0789:1852874:1852973 [0] NCCL INFO ncclCommInitRankConfig comm 0xe5ab220 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xcf2aa0ed221629b6 - Init START
lrdn0789:1852874:1852973 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0789:1852874:1852973 [0] NCCL INFO Bootstrap timings total 0.000396 (create 0.000019, send 0.000059, recv 0.000095, ring 0.000001, delay 0.000000)
lrdn1290:1434779:1434779 [0] NCCL INFO Bootstrap: Using ib0:10.128.26.73<0>
lrdn1290:1434779:1434779 [0] NCCL INFO cudaDriverVersion 12020
lrdn1290:1434779:1434779 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1290:1434779:1434779 [0] NCCL INFO Comm config Blocking set to 1
lrdn0789:1852874:1852973 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0789:1852874:1852973 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0789:1852874:1852973 [0] NCCL INFO comm 0xe5ab220 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 00/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 01/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 02/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 03/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 04/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 05/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 06/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 07/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 08/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 09/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 10/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 11/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 12/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 13/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 14/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 15/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 16/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 17/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 18/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 19/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 20/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 21/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 22/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 23/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 24/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 25/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 26/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 27/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 28/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 29/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 30/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 31/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 32/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 33/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 34/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 35/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 36/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 37/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 38/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 39/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 40/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 41/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 42/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 43/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 44/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 45/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 46/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 47/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 48/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 49/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 50/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 51/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 52/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 53/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 54/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 55/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 56/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 57/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 58/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 59/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 60/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 61/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 62/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Channel 63/64 : 0
lrdn0789:1852874:1852973 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0789:1852874:1852973 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0789:1852874:1852973 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0789:1852874:1852979 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn0789:1852874:1852980 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0789:1852874:1852973 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0789:1852874:1852973 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0789:1852874:1852973 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0789:1852874:1852973 [0] NCCL INFO ncclCommInitRankConfig comm 0xe5ab220 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xcf2aa0ed221629b6 - Init COMPLETE
lrdn0789:1852874:1852973 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1290:1434779:1434871 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1290:1434779:1434871 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1290:1434779:1434871 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.26.73<0>
lrdn1290:1434779:1434871 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1290:1434779:1434871 [0] NCCL INFO Using network IB
lrdn1290:1434779:1434871 [0] NCCL INFO ncclCommInitRankConfig comm 0xdd85e40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xa0615840c360312f - Init START
lrdn1290:1434779:1434871 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1290:1434779:1434871 [0] NCCL INFO Bootstrap timings total 0.000452 (create 0.000024, send 0.000071, recv 0.000113, ring 0.000001, delay 0.000000)
lrdn1290:1434779:1434871 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1290:1434779:1434871 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1290:1434779:1434871 [0] NCCL INFO comm 0xdd85e40 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 00/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 01/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 02/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 03/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 04/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 05/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 06/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 07/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 08/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 09/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 10/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 11/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 12/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 13/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 14/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 15/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 16/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 17/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 18/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 19/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 20/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 21/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 22/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 23/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 24/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 25/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 26/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 27/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 28/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 29/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 30/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 31/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 32/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 33/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 34/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 35/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 36/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 37/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 38/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 39/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 40/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 41/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 42/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 43/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 44/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 45/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 46/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 47/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 48/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 49/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 50/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 51/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 52/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 53/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 54/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 55/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 56/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 57/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 58/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 59/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 60/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 61/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 62/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Channel 63/64 : 0
lrdn1290:1434779:1434871 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1290:1434779:1434871 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1290:1434779:1434871 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1290:1434779:1434877 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn1290:1434779:1434878 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 15
lrdn1290:1434779:1434871 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1290:1434779:1434871 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1290:1434779:1434871 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1290:1434779:1434871 [0] NCCL INFO ncclCommInitRankConfig comm 0xdd85e40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xa0615840c360312f - Init COMPLETE
lrdn1290:1434779:1434871 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:26,555 | xffl.distributed.distributed |    DEBUG | [Rank 204]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:48:26,578 | xffl.distributed.distributed |    DEBUG | [Rank 17]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:48:26,678 | xffl.distributed.distributed |    DEBUG | [Rank 208]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:48:26,841 | xffl.distributed.distributed |    DEBUG | [Rank 190]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1120:1800335:1800335 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.161<0>
lrdn1120:1800335:1800335 [0] NCCL INFO cudaDriverVersion 12020
lrdn1120:1800335:1800335 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1120:1800335:1800335 [0] NCCL INFO Comm config Blocking set to 1
lrdn0175:1493980:1493980 [0] NCCL INFO Bootstrap: Using ib0:10.128.8.221<0>
lrdn0175:1493980:1493980 [0] NCCL INFO cudaDriverVersion 12020
lrdn0175:1493980:1493980 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0175:1493980:1493980 [0] NCCL INFO Comm config Blocking set to 1
lrdn1141:1960198:1960198 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.245<0>
lrdn1141:1960198:1960198 [0] NCCL INFO cudaDriverVersion 12020
lrdn1141:1960198:1960198 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1141:1960198:1960198 [0] NCCL INFO Comm config Blocking set to 1
lrdn1120:1800335:1800428 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1120:1800335:1800428 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0175:1493980:1494072 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0175:1493980:1494072 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1120:1800335:1800428 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.161<0>
lrdn1120:1800335:1800428 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1120:1800335:1800428 [0] NCCL INFO Using network IB
lrdn1120:1800335:1800428 [0] NCCL INFO ncclCommInitRankConfig comm 0xe495540 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1095397ce8ee967a - Init START
lrdn1120:1800335:1800428 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1120:1800335:1800428 [0] NCCL INFO Bootstrap timings total 0.000435 (create 0.000024, send 0.000074, recv 0.000095, ring 0.000001, delay 0.000000)
lrdn1120:1800335:1800428 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1120:1800335:1800428 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1120:1800335:1800428 [0] NCCL INFO comm 0xe495540 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 00/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 01/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 02/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 03/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 04/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 05/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 06/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 07/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 08/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 09/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 10/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 11/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 12/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 13/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 14/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 15/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 16/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 17/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 18/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 19/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 20/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 21/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 22/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 23/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 24/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 25/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 26/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 27/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 28/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 29/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 30/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 31/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 32/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 33/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 34/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 35/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 36/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 37/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 38/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 39/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 40/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 41/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 42/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 43/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 44/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 45/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 46/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 47/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 48/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 49/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 50/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 51/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 52/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 53/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 54/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 55/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 56/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 57/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 58/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 59/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 60/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 61/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 62/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Channel 63/64 : 0
lrdn1120:1800335:1800428 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1120:1800335:1800428 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1120:1800335:1800428 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1120:1800335:1800434 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn1120:1800335:1800435 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn1120:1800335:1800428 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1120:1800335:1800428 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1120:1800335:1800428 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1120:1800335:1800428 [0] NCCL INFO ncclCommInitRankConfig comm 0xe495540 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1095397ce8ee967a - Init COMPLETE
lrdn1120:1800335:1800428 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.04, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0175:1493980:1494072 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.8.221<0>
lrdn0175:1493980:1494072 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0175:1493980:1494072 [0] NCCL INFO Using network IB
lrdn0175:1493980:1494072 [0] NCCL INFO ncclCommInitRankConfig comm 0xc9eb0b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd1f9ab9623a7dc41 - Init START
lrdn0175:1493980:1494072 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0175:1493980:1494072 [0] NCCL INFO Bootstrap timings total 0.000407 (create 0.000022, send 0.000064, recv 0.000099, ring 0.000001, delay 0.000001)
lrdn0175:1493980:1494072 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0175:1493980:1494072 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0175:1493980:1494072 [0] NCCL INFO comm 0xc9eb0b0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 00/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 01/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 02/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 03/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 04/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 05/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 06/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 07/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 08/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 09/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 10/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 11/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 12/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 13/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 14/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 15/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 16/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 17/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 18/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 19/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 20/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 21/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 22/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 23/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 24/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 25/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 26/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 27/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 28/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 29/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 30/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 31/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 32/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 33/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 34/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 35/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 36/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 37/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 38/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 39/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 40/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 41/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 42/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 43/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 44/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 45/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 46/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 47/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 48/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 49/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 50/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 51/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 52/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 53/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 54/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 55/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 56/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 57/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 58/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 59/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 60/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 61/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 62/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Channel 63/64 : 0
lrdn0175:1493980:1494072 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0175:1493980:1494072 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0175:1493980:1494072 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0175:1493980:1494078 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0175:1493980:1494079 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn1141:1960198:1960296 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1141:1960198:1960296 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0175:1493980:1494072 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0175:1493980:1494072 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0175:1493980:1494072 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0175:1493980:1494072 [0] NCCL INFO ncclCommInitRankConfig comm 0xc9eb0b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd1f9ab9623a7dc41 - Init COMPLETE
lrdn0175:1493980:1494072 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1053:3252281:3252281 [0] NCCL INFO Bootstrap: Using ib0:10.128.22.149<0>
lrdn1053:3252281:3252281 [0] NCCL INFO cudaDriverVersion 12020
lrdn1053:3252281:3252281 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1053:3252281:3252281 [0] NCCL INFO Comm config Blocking set to 1
lrdn1141:1960198:1960296 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.245<0>
lrdn1141:1960198:1960296 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1141:1960198:1960296 [0] NCCL INFO Using network IB
lrdn1141:1960198:1960296 [0] NCCL INFO ncclCommInitRankConfig comm 0xdecf680 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x17af74cff87092bc - Init START
lrdn1141:1960198:1960296 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1141:1960198:1960296 [0] NCCL INFO Bootstrap timings total 0.000430 (create 0.000023, send 0.000068, recv 0.000099, ring 0.000001, delay 0.000000)
lrdn1141:1960198:1960296 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1141:1960198:1960296 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1141:1960198:1960296 [0] NCCL INFO comm 0xdecf680 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 00/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 01/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 02/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 03/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 04/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 05/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 06/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 07/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 08/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 09/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 10/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 11/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 12/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 13/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 14/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 15/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 16/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 17/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 18/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 19/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 20/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 21/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 22/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 23/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 24/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 25/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 26/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 27/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 28/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 29/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 30/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 31/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 32/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 33/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 34/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 35/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 36/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 37/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 38/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 39/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 40/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 41/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 42/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 43/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 44/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 45/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 46/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 47/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 48/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 49/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 50/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 51/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 52/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 53/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 54/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 55/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 56/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 57/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 58/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 59/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 60/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 61/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 62/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Channel 63/64 : 0
lrdn1141:1960198:1960296 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1141:1960198:1960296 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1141:1960198:1960296 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1141:1960198:1960303 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn1141:1960198:1960302 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn1141:1960198:1960296 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1141:1960198:1960296 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1141:1960198:1960296 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1141:1960198:1960296 [0] NCCL INFO ncclCommInitRankConfig comm 0xdecf680 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x17af74cff87092bc - Init COMPLETE
lrdn1141:1960198:1960296 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1053:3252281:3252380 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1053:3252281:3252380 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1053:3252281:3252380 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.22.149<0>
lrdn1053:3252281:3252380 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1053:3252281:3252380 [0] NCCL INFO Using network IB
lrdn1053:3252281:3252380 [0] NCCL INFO ncclCommInitRankConfig comm 0xf12e330 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xdeb161126a64da6e - Init START
lrdn1053:3252281:3252380 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1053:3252281:3252380 [0] NCCL INFO Bootstrap timings total 0.000393 (create 0.000020, send 0.000059, recv 0.000097, ring 0.000001, delay 0.000000)
lrdn1053:3252281:3252380 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1053:3252281:3252380 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1053:3252281:3252380 [0] NCCL INFO comm 0xf12e330 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 00/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 01/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 02/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 03/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 04/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 05/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 06/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 07/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 08/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 09/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 10/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 11/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 12/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 13/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 14/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 15/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 16/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 17/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 18/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 19/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 20/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 21/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 22/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 23/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 24/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 25/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 26/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 27/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 28/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 29/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 30/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 31/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 32/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 33/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 34/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 35/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 36/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 37/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 38/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 39/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 40/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 41/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 42/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 43/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 44/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 45/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 46/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 47/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 48/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 49/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 50/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 51/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 52/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 53/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 54/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 55/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 56/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 57/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 58/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 59/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 60/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 61/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 62/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Channel 63/64 : 0
lrdn1053:3252281:3252380 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1053:3252281:3252380 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1053:3252281:3252380 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1053:3252281:3252386 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1053:3252281:3252387 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1053:3252281:3252380 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1053:3252281:3252380 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1053:3252281:3252380 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1053:3252281:3252380 [0] NCCL INFO ncclCommInitRankConfig comm 0xf12e330 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xdeb161126a64da6e - Init COMPLETE
lrdn1053:3252281:3252380 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:28,778 | xffl.distributed.distributed |    DEBUG | [Rank 135]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0819:1714151:1714151 [0] NCCL INFO Bootstrap: Using ib0:10.128.18.237<0>
lrdn0819:1714151:1714151 [0] NCCL INFO cudaDriverVersion 12020
lrdn0819:1714151:1714151 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0819:1714151:1714151 [0] NCCL INFO Comm config Blocking set to 1
lrdn0819:1714151:1714244 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0819:1714151:1714244 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:48:29,284 | xffl.distributed.distributed |    DEBUG | [Rank 193]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0819:1714151:1714244 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.18.237<0>
lrdn0819:1714151:1714244 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0819:1714151:1714244 [0] NCCL INFO Using network IB
lrdn0819:1714151:1714244 [0] NCCL INFO ncclCommInitRankConfig comm 0xd5cd050 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd5b6f5faa51910df - Init START
lrdn0819:1714151:1714244 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0819:1714151:1714244 [0] NCCL INFO Bootstrap timings total 0.000397 (create 0.000021, send 0.000063, recv 0.000098, ring 0.000001, delay 0.000000)
lrdn0819:1714151:1714244 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0819:1714151:1714244 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0819:1714151:1714244 [0] NCCL INFO comm 0xd5cd050 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 00/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 01/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 02/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 03/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 04/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 05/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 06/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 07/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 08/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 09/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 10/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 11/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 12/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 13/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 14/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 15/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 16/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 17/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 18/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 19/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 20/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 21/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 22/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 23/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 24/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 25/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 26/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 27/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 28/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 29/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 30/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 31/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 32/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 33/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 34/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 35/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 36/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 37/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 38/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 39/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 40/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 41/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 42/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 43/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 44/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 45/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 46/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 47/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 48/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 49/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 50/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 51/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 52/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 53/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 54/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 55/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 56/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 57/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 58/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 59/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 60/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 61/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 62/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Channel 63/64 : 0
lrdn0819:1714151:1714244 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0819:1714151:1714244 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0819:1714151:1714244 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0819:1714151:1714253 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn0819:1714151:1714254 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn0819:1714151:1714244 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0819:1714151:1714244 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0819:1714151:1714244 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0819:1714151:1714244 [0] NCCL INFO ncclCommInitRankConfig comm 0xd5cd050 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd5b6f5faa51910df - Init COMPLETE
lrdn0819:1714151:1714244 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1065:1831945:1831945 [0] NCCL INFO Bootstrap: Using ib0:10.128.22.197<0>
lrdn1065:1831945:1831945 [0] NCCL INFO cudaDriverVersion 12020
lrdn1065:1831945:1831945 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1065:1831945:1831945 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:48:29,618 | xffl.distributed.distributed |    DEBUG | [Rank 34]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1065:1831945:1832037 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1065:1831945:1832037 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1065:1831945:1832037 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.22.197<0>
lrdn1065:1831945:1832037 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1065:1831945:1832037 [0] NCCL INFO Using network IB
lrdn1065:1831945:1832037 [0] NCCL INFO ncclCommInitRankConfig comm 0xc6bd7e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x24bbe1c9a9fa794d - Init START
lrdn1065:1831945:1832037 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1065:1831945:1832037 [0] NCCL INFO Bootstrap timings total 0.000391 (create 0.000020, send 0.000061, recv 0.000100, ring 0.000001, delay 0.000001)
lrdn1065:1831945:1832037 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1065:1831945:1832037 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1065:1831945:1832037 [0] NCCL INFO comm 0xc6bd7e0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 00/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 01/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 02/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 03/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 04/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 05/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 06/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 07/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 08/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 09/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 10/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 11/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 12/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 13/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 14/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 15/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 16/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 17/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 18/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 19/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 20/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 21/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 22/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 23/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 24/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 25/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 26/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 27/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 28/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 29/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 30/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 31/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 32/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 33/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 34/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 35/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 36/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 37/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 38/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 39/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 40/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 41/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 42/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 43/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 44/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 45/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 46/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 47/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 48/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 49/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 50/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 51/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 52/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 53/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 54/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 55/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 56/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 57/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 58/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 59/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 60/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 61/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 62/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Channel 63/64 : 0
lrdn1065:1831945:1832037 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1065:1831945:1832037 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1065:1831945:1832037 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1065:1831945:1832043 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn1065:1831945:1832044 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn1065:1831945:1832037 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1065:1831945:1832037 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1065:1831945:1832037 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1065:1831945:1832037 [0] NCCL INFO ncclCommInitRankConfig comm 0xc6bd7e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x24bbe1c9a9fa794d - Init COMPLETE
lrdn1065:1831945:1832037 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0267:2029357:2029357 [0] NCCL INFO Bootstrap: Using ib0:10.128.10.77<0>
lrdn0267:2029357:2029357 [0] NCCL INFO cudaDriverVersion 12020
lrdn0267:2029357:2029357 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0267:2029357:2029357 [0] NCCL INFO Comm config Blocking set to 1
lrdn0267:2029357:2029450 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0267:2029357:2029450 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0267:2029357:2029450 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.10.77<0>
lrdn0267:2029357:2029450 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0267:2029357:2029450 [0] NCCL INFO Using network IB
lrdn0267:2029357:2029450 [0] NCCL INFO ncclCommInitRankConfig comm 0xc62ca80 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x26b8693c865310ab - Init START
lrdn0267:2029357:2029450 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0267:2029357:2029450 [0] NCCL INFO Bootstrap timings total 0.000410 (create 0.000020, send 0.000062, recv 0.000103, ring 0.000001, delay 0.000001)
lrdn0267:2029357:2029450 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0267:2029357:2029450 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0267:2029357:2029450 [0] NCCL INFO comm 0xc62ca80 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 00/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 01/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 02/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 03/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 04/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 05/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 06/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 07/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 08/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 09/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 10/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 11/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 12/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 13/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 14/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 15/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 16/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 17/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 18/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 19/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 20/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 21/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 22/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 23/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 24/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 25/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 26/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 27/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 28/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 29/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 30/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 31/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 32/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 33/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 34/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 35/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 36/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 37/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 38/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 39/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 40/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 41/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 42/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 43/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 44/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 45/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 46/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 47/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 48/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 49/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 50/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 51/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 52/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 53/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 54/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 55/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 56/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 57/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 58/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 59/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 60/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 61/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 62/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Channel 63/64 : 0
lrdn0267:2029357:2029450 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0267:2029357:2029450 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0267:2029357:2029450 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0267:2029357:2029456 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn0267:2029357:2029457 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0267:2029357:2029450 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0267:2029357:2029450 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0267:2029357:2029450 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0267:2029357:2029450 [0] NCCL INFO ncclCommInitRankConfig comm 0xc62ca80 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x26b8693c865310ab - Init COMPLETE
lrdn0267:2029357:2029450 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:30,698 | xffl.distributed.distributed |    DEBUG | [Rank 52]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0385:1468992:1468992 [0] NCCL INFO Bootstrap: Using ib0:10.128.12.37<0>
lrdn0385:1468992:1468992 [0] NCCL INFO cudaDriverVersion 12020
lrdn0385:1468992:1468992 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0385:1468992:1468992 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:48:31,124 | xffl.distributed.distributed |    DEBUG | [Rank 100]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0385:1468992:1469084 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0385:1468992:1469084 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0385:1468992:1469084 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.12.37<0>
lrdn0385:1468992:1469084 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0385:1468992:1469084 [0] NCCL INFO Using network IB
lrdn0385:1468992:1469084 [0] NCCL INFO ncclCommInitRankConfig comm 0xe30e9b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3fb06bfe74d6763d - Init START
lrdn0385:1468992:1469084 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0385:1468992:1469084 [0] NCCL INFO Bootstrap timings total 0.000521 (create 0.000020, send 0.000062, recv 0.000215, ring 0.000001, delay 0.000001)
lrdn0385:1468992:1469084 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0385:1468992:1469084 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0385:1468992:1469084 [0] NCCL INFO comm 0xe30e9b0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 00/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 01/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 02/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 03/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 04/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 05/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 06/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 07/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 08/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 09/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 10/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 11/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 12/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 13/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 14/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 15/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 16/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 17/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 18/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 19/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 20/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 21/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 22/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 23/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 24/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 25/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 26/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 27/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 28/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 29/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 30/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 31/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 32/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 33/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 34/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 35/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 36/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 37/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 38/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 39/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 40/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 41/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 42/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 43/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 44/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 45/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 46/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 47/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 48/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 49/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 50/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 51/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 52/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 53/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 54/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 55/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 56/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 57/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 58/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 59/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 60/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 61/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 62/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Channel 63/64 : 0
lrdn0385:1468992:1469084 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0385:1468992:1469084 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0385:1468992:1469084 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0385:1468992:1469092 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0385:1468992:1469091 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn0385:1468992:1469084 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0385:1468992:1469084 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0385:1468992:1469084 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0385:1468992:1469084 [0] NCCL INFO ncclCommInitRankConfig comm 0xe30e9b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3fb06bfe74d6763d - Init COMPLETE
lrdn0385:1468992:1469084 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0628:1681364:1681364 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.241<0>
lrdn0628:1681364:1681364 [0] NCCL INFO cudaDriverVersion 12020
lrdn0628:1681364:1681364 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0628:1681364:1681364 [0] NCCL INFO Comm config Blocking set to 1
lrdn0628:1681364:1681458 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0628:1681364:1681458 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0628:1681364:1681458 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.241<0>
lrdn0628:1681364:1681458 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0628:1681364:1681458 [0] NCCL INFO Using network IB
lrdn0628:1681364:1681458 [0] NCCL INFO ncclCommInitRankConfig comm 0xe2ff060 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50e80caad59494e6 - Init START
lrdn0628:1681364:1681458 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0628:1681364:1681458 [0] NCCL INFO Bootstrap timings total 0.000432 (create 0.000021, send 0.000068, recv 0.000101, ring 0.000001, delay 0.000000)
lrdn0628:1681364:1681458 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0628:1681364:1681458 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0628:1681364:1681458 [0] NCCL INFO comm 0xe2ff060 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 00/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 01/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 02/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 03/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 04/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 05/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 06/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 07/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 08/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 09/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 10/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 11/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 12/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 13/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 14/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 15/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 16/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 17/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 18/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 19/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 20/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 21/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 22/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 23/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 24/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 25/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 26/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 27/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 28/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 29/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 30/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 31/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 32/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 33/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 34/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 35/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 36/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 37/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 38/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 39/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 40/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 41/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 42/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 43/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 44/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 45/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 46/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 47/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 48/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 49/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 50/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 51/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 52/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 53/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 54/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 55/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 56/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 57/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 58/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 59/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 60/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 61/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 62/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Channel 63/64 : 0
lrdn0628:1681364:1681458 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0628:1681364:1681458 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0628:1681364:1681458 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0628:1681364:1681465 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn0628:1681364:1681466 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn0628:1681364:1681458 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0628:1681364:1681458 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0628:1681364:1681458 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0628:1681364:1681458 [0] NCCL INFO ncclCommInitRankConfig comm 0xe2ff060 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x50e80caad59494e6 - Init COMPLETE
lrdn0628:1681364:1681458 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:32,749 | xffl.distributed.distributed |    DEBUG | [Rank 43]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0300:1343763:1343763 [0] NCCL INFO Bootstrap: Using ib0:10.128.10.209<0>
lrdn0300:1343763:1343763 [0] NCCL INFO cudaDriverVersion 12020
lrdn0300:1343763:1343763 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0300:1343763:1343763 [0] NCCL INFO Comm config Blocking set to 1
lrdn0300:1343763:1343858 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0300:1343763:1343858 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0300:1343763:1343858 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.10.209<0>
lrdn0300:1343763:1343858 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0300:1343763:1343858 [0] NCCL INFO Using network IB
lrdn0300:1343763:1343858 [0] NCCL INFO ncclCommInitRankConfig comm 0xde6bda0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x33eaf3a8d2c862ea - Init START
lrdn0300:1343763:1343858 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0300:1343763:1343858 [0] NCCL INFO Bootstrap timings total 0.000438 (create 0.000022, send 0.000069, recv 0.000111, ring 0.000001, delay 0.000000)
lrdn0300:1343763:1343858 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0300:1343763:1343858 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0300:1343763:1343858 [0] NCCL INFO comm 0xde6bda0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 00/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 01/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 02/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 03/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 04/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 05/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 06/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 07/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 08/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 09/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 10/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 11/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 12/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 13/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 14/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 15/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 16/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 17/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 18/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 19/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 20/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 21/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 22/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 23/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 24/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 25/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 26/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 27/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 28/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 29/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 30/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 31/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 32/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 33/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 34/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 35/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 36/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 37/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 38/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 39/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 40/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 41/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 42/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 43/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 44/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 45/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 46/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 47/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 48/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 49/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 50/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 51/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 52/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 53/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 54/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 55/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 56/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 57/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 58/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 59/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 60/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 61/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 62/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Channel 63/64 : 0
lrdn0300:1343763:1343858 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0300:1343763:1343858 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0300:1343763:1343858 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0300:1343763:1343864 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn0300:1343763:1343865 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0300:1343763:1343858 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0300:1343763:1343858 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0300:1343763:1343858 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0300:1343763:1343858 [0] NCCL INFO ncclCommInitRankConfig comm 0xde6bda0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x33eaf3a8d2c862ea - Init COMPLETE
lrdn0300:1343763:1343858 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:33,377 | xffl.distributed.distributed |    DEBUG | [Rank 73]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:48:33,382 | xffl.distributed.distributed |    DEBUG | [Rank 158]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0907:3310455:3310455 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.77<0>
lrdn0907:3310455:3310455 [0] NCCL INFO cudaDriverVersion 12020
lrdn0907:3310455:3310455 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0510:719419:719419 [0] NCCL INFO Bootstrap: Using ib0:10.128.14.25<0>
lrdn0907:3310455:3310455 [0] NCCL INFO Comm config Blocking set to 1
lrdn0510:719419:719419 [0] NCCL INFO cudaDriverVersion 12020
lrdn0510:719419:719419 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0510:719419:719419 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:48:33,780 | xffl.distributed.distributed |    DEBUG | [Rank 26]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:48:33,818 | xffl.distributed.distributed |    DEBUG | [Rank 84]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0510:719419:719515 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0510:719419:719515 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0907:3310455:3310557 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0907:3310455:3310557 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0510:719419:719515 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.14.25<0>
lrdn0907:3310455:3310557 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.77<0>
lrdn0510:719419:719515 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0510:719419:719515 [0] NCCL INFO Using network IB
lrdn0510:719419:719515 [0] NCCL INFO ncclCommInitRankConfig comm 0xd4223c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x247f578fa1e250cb - Init START
lrdn0510:719419:719515 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0510:719419:719515 [0] NCCL INFO Bootstrap timings total 0.000484 (create 0.000019, send 0.000062, recv 0.000180, ring 0.000001, delay 0.000000)
lrdn0907:3310455:3310557 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0907:3310455:3310557 [0] NCCL INFO Using network IB
lrdn0907:3310455:3310557 [0] NCCL INFO ncclCommInitRankConfig comm 0xe3fa110 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe51b435150831b37 - Init START
lrdn0907:3310455:3310557 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0907:3310455:3310557 [0] NCCL INFO Bootstrap timings total 0.000444 (create 0.000025, send 0.000067, recv 0.000107, ring 0.000001, delay 0.000000)
lrdn0510:719419:719515 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0510:719419:719515 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0510:719419:719515 [0] NCCL INFO comm 0xd4223c0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 00/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 01/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 02/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 03/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 04/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 05/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 06/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 07/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 08/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 09/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 10/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 11/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 12/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 13/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 14/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 15/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 16/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 17/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 18/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 19/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 20/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 21/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 22/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 23/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 24/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 25/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 26/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 27/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 28/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 29/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 30/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 31/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 32/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 33/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 34/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 35/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 36/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 37/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 38/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 39/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 40/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 41/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 42/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 43/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 44/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 45/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 46/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 47/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 48/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 49/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 50/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 51/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 52/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 53/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 54/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 55/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 56/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 57/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 58/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 59/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 60/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 61/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 62/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Channel 63/64 : 0
lrdn0510:719419:719515 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0510:719419:719515 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0510:719419:719515 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0510:719419:719521 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn0510:719419:719522 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0907:3310455:3310557 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0907:3310455:3310557 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0907:3310455:3310557 [0] NCCL INFO comm 0xe3fa110 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 00/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 01/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 02/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 03/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 04/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 05/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 06/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 07/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 08/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 09/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 10/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 11/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 12/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 13/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 14/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 15/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 16/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 17/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 18/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 19/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 20/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 21/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 22/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 23/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 24/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 25/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 26/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 27/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 28/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 29/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 30/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 31/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 32/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 33/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 34/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 35/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 36/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 37/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 38/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 39/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 40/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 41/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 42/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 43/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 44/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 45/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 46/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 47/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 48/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 49/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 50/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 51/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 52/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 53/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 54/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 55/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 56/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 57/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 58/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 59/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 60/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 61/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 62/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Channel 63/64 : 0
lrdn0907:3310455:3310557 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0907:3310455:3310557 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0907:3310455:3310557 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0907:3310455:3310564 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0907:3310455:3310563 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn0510:719419:719515 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0510:719419:719515 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0907:3310455:3310557 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0907:3310455:3310557 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0510:719419:719515 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0510:719419:719515 [0] NCCL INFO ncclCommInitRankConfig comm 0xd4223c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x247f578fa1e250cb - Init COMPLETE
lrdn0510:719419:719515 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn0907:3310455:3310557 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0907:3310455:3310557 [0] NCCL INFO ncclCommInitRankConfig comm 0xe3fa110 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe51b435150831b37 - Init COMPLETE
lrdn0907:3310455:3310557 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.27 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0210:1733148:1733148 [0] NCCL INFO Bootstrap: Using ib0:10.128.9.105<0>
lrdn0210:1733148:1733148 [0] NCCL INFO cudaDriverVersion 12020
lrdn0210:1733148:1733148 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0210:1733148:1733148 [0] NCCL INFO Comm config Blocking set to 1
lrdn0568:2431945:2431945 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.1<0>
lrdn0568:2431945:2431945 [0] NCCL INFO cudaDriverVersion 12020
lrdn0568:2431945:2431945 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0568:2431945:2431945 [0] NCCL INFO Comm config Blocking set to 1
lrdn0210:1733148:1733244 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0210:1733148:1733244 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0210:1733148:1733244 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.9.105<0>
lrdn0210:1733148:1733244 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0210:1733148:1733244 [0] NCCL INFO Using network IB
lrdn0210:1733148:1733244 [0] NCCL INFO ncclCommInitRankConfig comm 0xd509560 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x6043d06a591e21d6 - Init START
lrdn0210:1733148:1733244 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0210:1733148:1733244 [0] NCCL INFO Bootstrap timings total 0.000411 (create 0.000020, send 0.000069, recv 0.000096, ring 0.000001, delay 0.000001)
lrdn0210:1733148:1733244 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0210:1733148:1733244 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0210:1733148:1733244 [0] NCCL INFO comm 0xd509560 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 00/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 01/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 02/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 03/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 04/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 05/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 06/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 07/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 08/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 09/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 10/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 11/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 12/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 13/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 14/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 15/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 16/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 17/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 18/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 19/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 20/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 21/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 22/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 23/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 24/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 25/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 26/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 27/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 28/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 29/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 30/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 31/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 32/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 33/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 34/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 35/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 36/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 37/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 38/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 39/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 40/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 41/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 42/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 43/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 44/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 45/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 46/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 47/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 48/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 49/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 50/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 51/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 52/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 53/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 54/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 55/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 56/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 57/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 58/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 59/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 60/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 61/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 62/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Channel 63/64 : 0
lrdn0210:1733148:1733244 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0210:1733148:1733244 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0210:1733148:1733244 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0210:1733148:1733250 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn0210:1733148:1733251 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 15
lrdn0210:1733148:1733244 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0210:1733148:1733244 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0210:1733148:1733244 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0210:1733148:1733244 [0] NCCL INFO ncclCommInitRankConfig comm 0xd509560 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x6043d06a591e21d6 - Init COMPLETE
lrdn0210:1733148:1733244 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0568:2431945:2432039 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0568:2431945:2432039 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0568:2431945:2432039 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.1<0>
lrdn0568:2431945:2432039 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0568:2431945:2432039 [0] NCCL INFO Using network IB
lrdn0568:2431945:2432039 [0] NCCL INFO ncclCommInitRankConfig comm 0xdff2f70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb8b361900739a3e - Init START
lrdn0568:2431945:2432039 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0568:2431945:2432039 [0] NCCL INFO Bootstrap timings total 0.000437 (create 0.000025, send 0.000070, recv 0.000104, ring 0.000001, delay 0.000000)
lrdn0568:2431945:2432039 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0568:2431945:2432039 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0568:2431945:2432039 [0] NCCL INFO comm 0xdff2f70 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 00/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 01/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 02/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 03/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 04/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 05/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 06/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 07/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 08/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 09/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 10/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 11/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 12/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 13/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 14/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 15/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 16/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 17/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 18/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 19/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 20/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 21/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 22/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 23/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 24/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 25/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 26/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 27/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 28/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 29/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 30/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 31/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 32/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 33/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 34/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 35/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 36/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 37/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 38/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 39/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 40/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 41/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 42/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 43/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 44/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 45/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 46/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 47/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 48/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 49/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 50/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 51/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 52/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 53/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 54/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 55/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 56/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 57/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 58/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 59/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 60/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 61/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 62/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Channel 63/64 : 0
lrdn0568:2431945:2432039 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0568:2431945:2432039 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0568:2431945:2432039 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0568:2431945:2432045 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn0568:2431945:2432046 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0568:2431945:2432039 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0568:2431945:2432039 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0568:2431945:2432039 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0568:2431945:2432039 [0] NCCL INFO ncclCommInitRankConfig comm 0xdff2f70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb8b361900739a3e - Init COMPLETE
lrdn0568:2431945:2432039 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:35,837 | xffl.distributed.distributed |    DEBUG | [Rank 148]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0865:1721410:1721410 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.165<0>
lrdn0865:1721410:1721410 [0] NCCL INFO cudaDriverVersion 12020
lrdn0865:1721410:1721410 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0865:1721410:1721410 [0] NCCL INFO Comm config Blocking set to 1
lrdn0865:1721410:1721514 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0865:1721410:1721514 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0865:1721410:1721514 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.165<0>
lrdn0865:1721410:1721514 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0865:1721410:1721514 [0] NCCL INFO Using network IB
lrdn0865:1721410:1721514 [0] NCCL INFO ncclCommInitRankConfig comm 0xc747ed0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3c45f534f4212ba6 - Init START
lrdn0865:1721410:1721514 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0865:1721410:1721514 [0] NCCL INFO Bootstrap timings total 0.000405 (create 0.000020, send 0.000061, recv 0.000111, ring 0.000001, delay 0.000000)
lrdn0865:1721410:1721514 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0865:1721410:1721514 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0865:1721410:1721514 [0] NCCL INFO comm 0xc747ed0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 00/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 01/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 02/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 03/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 04/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 05/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 06/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 07/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 08/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 09/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 10/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 11/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 12/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 13/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 14/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 15/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 16/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 17/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 18/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 19/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 20/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 21/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 22/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 23/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 24/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 25/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 26/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 27/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 28/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 29/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 30/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 31/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 32/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 33/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 34/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 35/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 36/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 37/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 38/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 39/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 40/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 41/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 42/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 43/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 44/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 45/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 46/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 47/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 48/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 49/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 50/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 51/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 52/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 53/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 54/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 55/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 56/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 57/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 58/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 59/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 60/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 61/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 62/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Channel 63/64 : 0
lrdn0865:1721410:1721514 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0865:1721410:1721514 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0865:1721410:1721514 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0865:1721410:1721520 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn0865:1721410:1721521 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn0865:1721410:1721514 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0865:1721410:1721514 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0865:1721410:1721514 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0865:1721410:1721514 [0] NCCL INFO ncclCommInitRankConfig comm 0xc747ed0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3c45f534f4212ba6 - Init COMPLETE
lrdn0865:1721410:1721514 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:36,566 | xffl.distributed.distributed |    DEBUG | [Rank 247]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1375:1499504:1499504 [0] NCCL INFO Bootstrap: Using ib0:10.128.27.157<0>
lrdn1375:1499504:1499504 [0] NCCL INFO cudaDriverVersion 12020
lrdn1375:1499504:1499504 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1375:1499504:1499504 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:48:36,979 | xffl.distributed.distributed |    DEBUG | [Rank 151]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1375:1499504:1499607 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1375:1499504:1499607 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1375:1499504:1499607 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.27.157<0>
lrdn1375:1499504:1499607 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1375:1499504:1499607 [0] NCCL INFO Using network IB
lrdn1375:1499504:1499607 [0] NCCL INFO ncclCommInitRankConfig comm 0xcb89a30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb2ba06a3c7317eba - Init START
lrdn1375:1499504:1499607 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1375:1499504:1499607 [0] NCCL INFO Bootstrap timings total 0.000428 (create 0.000024, send 0.000071, recv 0.000090, ring 0.000001, delay 0.000000)
lrdn1375:1499504:1499607 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1375:1499504:1499607 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1375:1499504:1499607 [0] NCCL INFO comm 0xcb89a30 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 00/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 01/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 02/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 03/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 04/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 05/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 06/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 07/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 08/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 09/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 10/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 11/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 12/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 13/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 14/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 15/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 16/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 17/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 18/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 19/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 20/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 21/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 22/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 23/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 24/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 25/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 26/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 27/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 28/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 29/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 30/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 31/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 32/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 33/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 34/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 35/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 36/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 37/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 38/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 39/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 40/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 41/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 42/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 43/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 44/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 45/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 46/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 47/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 48/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 49/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 50/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 51/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 52/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 53/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 54/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 55/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 56/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 57/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 58/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 59/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 60/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 61/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 62/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Channel 63/64 : 0
lrdn1375:1499504:1499607 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1375:1499504:1499607 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1375:1499504:1499607 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1375:1499504:1499613 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn1375:1499504:1499614 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1
lrdn1375:1499504:1499607 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1375:1499504:1499607 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1375:1499504:1499607 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1375:1499504:1499607 [0] NCCL INFO ncclCommInitRankConfig comm 0xcb89a30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb2ba06a3c7317eba - Init COMPLETE
lrdn1375:1499504:1499607 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0877:1585785:1585785 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.213<0>
lrdn0877:1585785:1585785 [0] NCCL INFO cudaDriverVersion 12020
lrdn0877:1585785:1585785 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0877:1585785:1585785 [0] NCCL INFO Comm config Blocking set to 1
lrdn0877:1585785:1585881 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0877:1585785:1585881 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0877:1585785:1585881 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.213<0>
lrdn0877:1585785:1585881 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0877:1585785:1585881 [0] NCCL INFO Using network IB
lrdn0877:1585785:1585881 [0] NCCL INFO ncclCommInitRankConfig comm 0xd936f90 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xfd500af4848855c5 - Init START
lrdn0877:1585785:1585881 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0877:1585785:1585881 [0] NCCL INFO Bootstrap timings total 0.000427 (create 0.000023, send 0.000062, recv 0.000110, ring 0.000001, delay 0.000000)
lrdn0877:1585785:1585881 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0877:1585785:1585881 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0877:1585785:1585881 [0] NCCL INFO comm 0xd936f90 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 00/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 01/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 02/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 03/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 04/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 05/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 06/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 07/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 08/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 09/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 10/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 11/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 12/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 13/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 14/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 15/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 16/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 17/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 18/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 19/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 20/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 21/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 22/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 23/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 24/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 25/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 26/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 27/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 28/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 29/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 30/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 31/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 32/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 33/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 34/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 35/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 36/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 37/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 38/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 39/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 40/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 41/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 42/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 43/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 44/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 45/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 46/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 47/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 48/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 49/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 50/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 51/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 52/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 53/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 54/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 55/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 56/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 57/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 58/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 59/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 60/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 61/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 62/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Channel 63/64 : 0
lrdn0877:1585785:1585881 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0877:1585785:1585881 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0877:1585785:1585881 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0877:1585785:1585888 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn0877:1585785:1585887 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0877:1585785:1585881 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0877:1585785:1585881 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0877:1585785:1585881 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0877:1585785:1585881 [0] NCCL INFO ncclCommInitRankConfig comm 0xd936f90 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xfd500af4848855c5 - Init COMPLETE
lrdn0877:1585785:1585881 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:39,122 | xffl.distributed.distributed |    DEBUG | [Rank 105]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0646:1603546:1603546 [0] NCCL INFO Bootstrap: Using ib0:10.128.16.57<0>
lrdn0646:1603546:1603546 [0] NCCL INFO cudaDriverVersion 12020
lrdn0646:1603546:1603546 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0646:1603546:1603546 [0] NCCL INFO Comm config Blocking set to 1
lrdn0646:1603546:1603644 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0646:1603546:1603644 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0646:1603546:1603644 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.16.57<0>
lrdn0646:1603546:1603644 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0646:1603546:1603644 [0] NCCL INFO Using network IB
lrdn0646:1603546:1603644 [0] NCCL INFO ncclCommInitRankConfig comm 0xe612c10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8789f5dc268b43ea - Init START
lrdn0646:1603546:1603644 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0646:1603546:1603644 [0] NCCL INFO Bootstrap timings total 0.000453 (create 0.000024, send 0.000069, recv 0.000112, ring 0.000001, delay 0.000000)
lrdn0646:1603546:1603644 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0646:1603546:1603644 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0646:1603546:1603644 [0] NCCL INFO comm 0xe612c10 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 00/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 01/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 02/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 03/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 04/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 05/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 06/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 07/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 08/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 09/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 10/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 11/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 12/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 13/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 14/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 15/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 16/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 17/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 18/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 19/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 20/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 21/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 22/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 23/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 24/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 25/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 26/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 27/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 28/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 29/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 30/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 31/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 32/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 33/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 34/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 35/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 36/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 37/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 38/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 39/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 40/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 41/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 42/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 43/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 44/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 45/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 46/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 47/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 48/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 49/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 50/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 51/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 52/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 53/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 54/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 55/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 56/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 57/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 58/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 59/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 60/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 61/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 62/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Channel 63/64 : 0
lrdn0646:1603546:1603644 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0646:1603546:1603644 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0646:1603546:1603644 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0646:1603546:1603650 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn0646:1603546:1603651 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0646:1603546:1603644 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0646:1603546:1603644 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0646:1603546:1603644 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0646:1603546:1603644 [0] NCCL INFO ncclCommInitRankConfig comm 0xe612c10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8789f5dc268b43ea - Init COMPLETE
lrdn0646:1603546:1603644 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:41,893 | xffl.distributed.distributed |    DEBUG | [Rank 207]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1132:1427407:1427407 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.209<0>
lrdn1132:1427407:1427407 [0] NCCL INFO cudaDriverVersion 12020
lrdn1132:1427407:1427407 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1132:1427407:1427407 [0] NCCL INFO Comm config Blocking set to 1
lrdn1132:1427407:1427502 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1132:1427407:1427502 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1132:1427407:1427502 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.209<0>
lrdn1132:1427407:1427502 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1132:1427407:1427502 [0] NCCL INFO Using network IB
lrdn1132:1427407:1427502 [0] NCCL INFO ncclCommInitRankConfig comm 0xea97bd0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x83f23c23192a9adf - Init START
lrdn1132:1427407:1427502 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1132:1427407:1427502 [0] NCCL INFO Bootstrap timings total 0.000391 (create 0.000018, send 0.000060, recv 0.000093, ring 0.000001, delay 0.000000)
lrdn1132:1427407:1427502 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1132:1427407:1427502 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1132:1427407:1427502 [0] NCCL INFO comm 0xea97bd0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 00/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 01/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 02/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 03/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 04/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 05/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 06/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 07/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 08/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 09/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 10/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 11/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 12/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 13/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 14/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 15/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 16/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 17/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 18/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 19/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 20/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 21/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 22/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 23/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 24/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 25/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 26/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 27/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 28/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 29/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 30/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 31/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 32/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 33/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 34/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 35/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 36/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 37/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 38/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 39/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 40/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 41/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 42/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 43/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 44/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 45/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 46/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 47/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 48/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 49/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 50/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 51/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 52/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 53/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 54/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 55/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 56/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 57/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 58/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 59/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 60/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 61/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 62/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Channel 63/64 : 0
lrdn1132:1427407:1427502 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1132:1427407:1427502 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1132:1427407:1427502 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1132:1427407:1427508 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn1132:1427407:1427509 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 15
lrdn1132:1427407:1427502 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1132:1427407:1427502 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1132:1427407:1427502 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1132:1427407:1427502 [0] NCCL INFO ncclCommInitRankConfig comm 0xea97bd0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x83f23c23192a9adf - Init COMPLETE
lrdn1132:1427407:1427502 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:42,996 | xffl.distributed.distributed |    DEBUG | [Rank 19]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:48:43,239 | xffl.distributed.distributed |    DEBUG | [Rank 245]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0178:2619425:2619425 [0] NCCL INFO Bootstrap: Using ib0:10.128.8.233<0>
lrdn0178:2619425:2619425 [0] NCCL INFO cudaDriverVersion 12020
lrdn0178:2619425:2619425 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0178:2619425:2619425 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:48:43,391 | xffl.distributed.distributed |    DEBUG | [Rank 93]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0178:2619425:2619527 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0178:2619425:2619527 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0178:2619425:2619527 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.8.233<0>
lrdn0178:2619425:2619527 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0178:2619425:2619527 [0] NCCL INFO Using network IB
lrdn0178:2619425:2619527 [0] NCCL INFO ncclCommInitRankConfig comm 0xd75b4c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xafaa9bd20524f92b - Init START
lrdn0178:2619425:2619527 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0178:2619425:2619527 [0] NCCL INFO Bootstrap timings total 0.000411 (create 0.000020, send 0.000063, recv 0.000096, ring 0.000001, delay 0.000001)
lrdn1335:2311607:2311607 [0] NCCL INFO Bootstrap: Using ib0:10.128.26.253<0>
lrdn1335:2311607:2311607 [0] NCCL INFO cudaDriverVersion 12020
lrdn1335:2311607:2311607 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1335:2311607:2311607 [0] NCCL INFO Comm config Blocking set to 1
lrdn0178:2619425:2619527 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0178:2619425:2619527 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0178:2619425:2619527 [0] NCCL INFO comm 0xd75b4c0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 00/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 01/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 02/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 03/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 04/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 05/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 06/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 07/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 08/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 09/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 10/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 11/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 12/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 13/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 14/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 15/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 16/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 17/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 18/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 19/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 20/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 21/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 22/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 23/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 24/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 25/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 26/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 27/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 28/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 29/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 30/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 31/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 32/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 33/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 34/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 35/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 36/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 37/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 38/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 39/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 40/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 41/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 42/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 43/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 44/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 45/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 46/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 47/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 48/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 49/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 50/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 51/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 52/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 53/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 54/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 55/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 56/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 57/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 58/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 59/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 60/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 61/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 62/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Channel 63/64 : 0
lrdn0178:2619425:2619527 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0178:2619425:2619527 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0178:2619425:2619527 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0178:2619425:2619533 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0178:2619425:2619534 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0178:2619425:2619527 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0178:2619425:2619527 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0178:2619425:2619527 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0178:2619425:2619527 [0] NCCL INFO ncclCommInitRankConfig comm 0xd75b4c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xafaa9bd20524f92b - Init COMPLETE
lrdn0178:2619425:2619527 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1335:2311607:2311725 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1335:2311607:2311725 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0604:1730089:1730089 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.145<0>
lrdn0604:1730089:1730089 [0] NCCL INFO cudaDriverVersion 12020
lrdn0604:1730089:1730089 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0604:1730089:1730089 [0] NCCL INFO Comm config Blocking set to 1
lrdn1335:2311607:2311725 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.26.253<0>
lrdn1335:2311607:2311725 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1335:2311607:2311725 [0] NCCL INFO Using network IB
lrdn1335:2311607:2311725 [0] NCCL INFO ncclCommInitRankConfig comm 0xe892350 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc61e6a8a6b8c1d66 - Init START
lrdn1335:2311607:2311725 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1335:2311607:2311725 [0] NCCL INFO Bootstrap timings total 0.000372 (create 0.000018, send 0.000057, recv 0.000089, ring 0.000001, delay 0.000000)
lrdn1335:2311607:2311725 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1335:2311607:2311725 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1335:2311607:2311725 [0] NCCL INFO comm 0xe892350 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 00/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 01/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 02/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 03/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 04/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 05/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 06/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 07/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 08/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 09/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 10/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 11/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 12/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 13/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 14/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 15/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 16/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 17/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 18/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 19/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 20/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 21/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 22/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 23/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 24/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 25/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 26/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 27/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 28/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 29/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 30/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 31/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 32/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 33/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 34/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 35/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 36/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 37/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 38/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 39/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 40/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 41/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 42/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 43/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 44/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 45/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 46/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 47/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 48/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 49/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 50/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 51/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 52/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 53/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 54/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 55/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 56/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 57/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 58/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 59/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 60/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 61/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 62/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Channel 63/64 : 0
lrdn1335:2311607:2311725 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1335:2311607:2311725 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1335:2311607:2311725 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1335:2311607:2311731 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn1335:2311607:2311732 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn1335:2311607:2311725 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1335:2311607:2311725 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1335:2311607:2311725 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1335:2311607:2311725 [0] NCCL INFO ncclCommInitRankConfig comm 0xe892350 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc61e6a8a6b8c1d66 - Init COMPLETE
lrdn1335:2311607:2311725 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0604:1730089:1730207 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0604:1730089:1730207 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0604:1730089:1730207 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.145<0>
lrdn0604:1730089:1730207 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0604:1730089:1730207 [0] NCCL INFO Using network IB
lrdn0604:1730089:1730207 [0] NCCL INFO ncclCommInitRankConfig comm 0xc8636d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7b0941afe7e615d4 - Init START
lrdn0604:1730089:1730207 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0604:1730089:1730207 [0] NCCL INFO Bootstrap timings total 0.000412 (create 0.000023, send 0.000064, recv 0.000092, ring 0.000001, delay 0.000000)
lrdn0604:1730089:1730207 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0604:1730089:1730207 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0604:1730089:1730207 [0] NCCL INFO comm 0xc8636d0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 00/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 01/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 02/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 03/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 04/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 05/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 06/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 07/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 08/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 09/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 10/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 11/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 12/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 13/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 14/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 15/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 16/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 17/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 18/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 19/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 20/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 21/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 22/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 23/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 24/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 25/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 26/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 27/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 28/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 29/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 30/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 31/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 32/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 33/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 34/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 35/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 36/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 37/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 38/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 39/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 40/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 41/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 42/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 43/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 44/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 45/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 46/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 47/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 48/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 49/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 50/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 51/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 52/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 53/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 54/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 55/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 56/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 57/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 58/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 59/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 60/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 61/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 62/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Channel 63/64 : 0
lrdn0604:1730089:1730207 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0604:1730089:1730207 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0604:1730089:1730207 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0604:1730089:1730213 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn0604:1730089:1730214 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn0604:1730089:1730207 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0604:1730089:1730207 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0604:1730089:1730207 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0604:1730089:1730207 [0] NCCL INFO ncclCommInitRankConfig comm 0xc8636d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7b0941afe7e615d4 - Init COMPLETE
lrdn0604:1730089:1730207 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:45,104 | xffl.distributed.distributed |    DEBUG | [Rank 24]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0204:1656830:1656830 [0] NCCL INFO Bootstrap: Using ib0:10.128.9.81<0>
lrdn0204:1656830:1656830 [0] NCCL INFO cudaDriverVersion 12020
lrdn0204:1656830:1656830 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0204:1656830:1656830 [0] NCCL INFO Comm config Blocking set to 1
lrdn0204:1656830:1656974 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0204:1656830:1656974 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0204:1656830:1656974 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.9.81<0>
lrdn0204:1656830:1656974 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0204:1656830:1656974 [0] NCCL INFO Using network IB
lrdn0204:1656830:1656974 [0] NCCL INFO ncclCommInitRankConfig comm 0xd4ce9e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb25a3eb52c2c832a - Init START
lrdn0204:1656830:1656974 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0204:1656830:1656974 [0] NCCL INFO Bootstrap timings total 0.000609 (create 0.000023, send 0.000071, recv 0.000269, ring 0.000001, delay 0.000000)
lrdn0204:1656830:1656974 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0204:1656830:1656974 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0204:1656830:1656974 [0] NCCL INFO comm 0xd4ce9e0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 00/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 01/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 02/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 03/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 04/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 05/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 06/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 07/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 08/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 09/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 10/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 11/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 12/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 13/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 14/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 15/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 16/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 17/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 18/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 19/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 20/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 21/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 22/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 23/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 24/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 25/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 26/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 27/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 28/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 29/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 30/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 31/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 32/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 33/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 34/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 35/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 36/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 37/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 38/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 39/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 40/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 41/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 42/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 43/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 44/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 45/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 46/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 47/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 48/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 49/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 50/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 51/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 52/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 53/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 54/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 55/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 56/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 57/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 58/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 59/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 60/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 61/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 62/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Channel 63/64 : 0
lrdn0204:1656830:1656974 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0204:1656830:1656974 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0204:1656830:1656974 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0204:1656830:1656980 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0204:1656830:1656981 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 12
lrdn0204:1656830:1656974 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0204:1656830:1656974 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0204:1656830:1656974 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0204:1656830:1656974 [0] NCCL INFO ncclCommInitRankConfig comm 0xd4ce9e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb25a3eb52c2c832a - Init COMPLETE
lrdn0204:1656830:1656974 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:46,014 | xffl.distributed.distributed |    DEBUG | [Rank 161]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0919:1801558:1801558 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.125<0>
lrdn0919:1801558:1801558 [0] NCCL INFO cudaDriverVersion 12020
lrdn0919:1801558:1801558 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0919:1801558:1801558 [0] NCCL INFO Comm config Blocking set to 1
lrdn0919:1801558:1801661 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0919:1801558:1801661 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0919:1801558:1801661 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.125<0>
lrdn0919:1801558:1801661 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0919:1801558:1801661 [0] NCCL INFO Using network IB
lrdn0919:1801558:1801661 [0] NCCL INFO ncclCommInitRankConfig comm 0xdf2e0e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xeb107b6e0b813bd0 - Init START
lrdn0919:1801558:1801661 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0919:1801558:1801661 [0] NCCL INFO Bootstrap timings total 0.000400 (create 0.000020, send 0.000059, recv 0.000091, ring 0.000001, delay 0.000000)
lrdn0919:1801558:1801661 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0919:1801558:1801661 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0919:1801558:1801661 [0] NCCL INFO comm 0xdf2e0e0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 00/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 01/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 02/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 03/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 04/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 05/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 06/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 07/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 08/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 09/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 10/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 11/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 12/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 13/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 14/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 15/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 16/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 17/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 18/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 19/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 20/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 21/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 22/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 23/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 24/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 25/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 26/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 27/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 28/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 29/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 30/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 31/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 32/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 33/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 34/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 35/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 36/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 37/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 38/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 39/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 40/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 41/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 42/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 43/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 44/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 45/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 46/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 47/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 48/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 49/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 50/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 51/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 52/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 53/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 54/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 55/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 56/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 57/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 58/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 59/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 60/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 61/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 62/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Channel 63/64 : 0
lrdn0919:1801558:1801661 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0919:1801558:1801661 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0919:1801558:1801661 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0919:1801558:1801668 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0919:1801558:1801667 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn0919:1801558:1801661 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0919:1801558:1801661 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0919:1801558:1801661 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0919:1801558:1801661 [0] NCCL INFO ncclCommInitRankConfig comm 0xdf2e0e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xeb107b6e0b813bd0 - Init COMPLETE
lrdn0919:1801558:1801661 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
[38;5;39m2025-08-02 08:48:46,710 | xffl.distributed.distributed |    DEBUG | [Rank 176]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0975:1356366:1356366 [0] NCCL INFO Bootstrap: Using ib0:10.128.21.93<0>
lrdn0975:1356366:1356366 [0] NCCL INFO cudaDriverVersion 12020
lrdn0975:1356366:1356366 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0975:1356366:1356366 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:48:47,090 | xffl.distributed.distributed |    DEBUG | [Rank 103]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0975:1356366:1356461 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0975:1356366:1356461 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0975:1356366:1356461 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.21.93<0>
lrdn0975:1356366:1356461 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0975:1356366:1356461 [0] NCCL INFO Using network IB
lrdn0975:1356366:1356461 [0] NCCL INFO ncclCommInitRankConfig comm 0xd423e40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xa1492c6d24c17e1c - Init START
lrdn0975:1356366:1356461 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0975:1356366:1356461 [0] NCCL INFO Bootstrap timings total 0.000578 (create 0.000024, send 0.000073, recv 0.000244, ring 0.000001, delay 0.000001)
lrdn0975:1356366:1356461 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0975:1356366:1356461 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0975:1356366:1356461 [0] NCCL INFO comm 0xd423e40 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 00/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 01/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 02/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 03/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 04/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 05/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 06/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 07/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 08/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 09/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 10/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 11/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 12/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 13/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 14/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 15/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 16/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 17/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 18/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 19/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 20/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 21/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 22/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 23/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 24/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 25/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 26/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 27/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 28/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 29/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 30/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 31/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 32/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 33/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 34/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 35/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 36/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 37/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 38/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 39/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 40/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 41/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 42/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 43/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 44/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 45/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 46/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 47/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 48/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 49/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 50/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 51/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 52/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 53/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 54/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 55/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 56/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 57/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 58/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 59/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 60/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 61/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 62/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Channel 63/64 : 0
lrdn0975:1356366:1356461 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0975:1356366:1356461 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0975:1356366:1356461 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0975:1356366:1356467 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn0975:1356366:1356468 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1
lrdn0975:1356366:1356461 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0975:1356366:1356461 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0975:1356366:1356461 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0975:1356366:1356461 [0] NCCL INFO ncclCommInitRankConfig comm 0xd423e40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xa1492c6d24c17e1c - Init COMPLETE
lrdn0975:1356366:1356461 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0637:1712537:1712537 [0] NCCL INFO Bootstrap: Using ib0:10.128.16.21<0>
lrdn0637:1712537:1712537 [0] NCCL INFO cudaDriverVersion 12020
lrdn0637:1712537:1712537 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0637:1712537:1712537 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:48:47,459 | xffl.distributed.distributed |    DEBUG | [Rank 160]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0637:1712537:1712633 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0637:1712537:1712633 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0637:1712537:1712633 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.16.21<0>
lrdn0637:1712537:1712633 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0637:1712537:1712633 [0] NCCL INFO Using network IB
lrdn0637:1712537:1712633 [0] NCCL INFO ncclCommInitRankConfig comm 0xdd553f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xebd6a58ca3082f0a - Init START
lrdn0637:1712537:1712633 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0637:1712537:1712633 [0] NCCL INFO Bootstrap timings total 0.000437 (create 0.000025, send 0.000067, recv 0.000104, ring 0.000001, delay 0.000000)
lrdn0637:1712537:1712633 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0637:1712537:1712633 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0637:1712537:1712633 [0] NCCL INFO comm 0xdd553f0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 00/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 01/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 02/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 03/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 04/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 05/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 06/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 07/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 08/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 09/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 10/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 11/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 12/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 13/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 14/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 15/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 16/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 17/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 18/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 19/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 20/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 21/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 22/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 23/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 24/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 25/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 26/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 27/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 28/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 29/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 30/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 31/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 32/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 33/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 34/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 35/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 36/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 37/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 38/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 39/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 40/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 41/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 42/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 43/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 44/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 45/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 46/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 47/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 48/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 49/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 50/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 51/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 52/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 53/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 54/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 55/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 56/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 57/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 58/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 59/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 60/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 61/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 62/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Channel 63/64 : 0
lrdn0637:1712537:1712633 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0637:1712537:1712633 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0637:1712537:1712633 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0637:1712537:1712639 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn0637:1712537:1712640 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0637:1712537:1712633 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0637:1712537:1712633 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0637:1712537:1712633 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0637:1712537:1712633 [0] NCCL INFO ncclCommInitRankConfig comm 0xdd553f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xebd6a58ca3082f0a - Init COMPLETE
lrdn0637:1712537:1712633 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0916:3269502:3269502 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.113<0>
lrdn0916:3269502:3269502 [0] NCCL INFO cudaDriverVersion 12020
lrdn0916:3269502:3269502 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0916:3269502:3269502 [0] NCCL INFO Comm config Blocking set to 1
lrdn0916:3269502:3269606 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0916:3269502:3269606 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0916:3269502:3269606 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.113<0>
lrdn0916:3269502:3269606 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0916:3269502:3269606 [0] NCCL INFO Using network IB
lrdn0916:3269502:3269606 [0] NCCL INFO ncclCommInitRankConfig comm 0xd80dfb0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x957adedac79867dd - Init START
lrdn0916:3269502:3269606 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0916:3269502:3269606 [0] NCCL INFO Bootstrap timings total 0.000424 (create 0.000024, send 0.000073, recv 0.000090, ring 0.000001, delay 0.000001)
lrdn0916:3269502:3269606 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0916:3269502:3269606 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0916:3269502:3269606 [0] NCCL INFO comm 0xd80dfb0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 00/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 01/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 02/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 03/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 04/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 05/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 06/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 07/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 08/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 09/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 10/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 11/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 12/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 13/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 14/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 15/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 16/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 17/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 18/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 19/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 20/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 21/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 22/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 23/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 24/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 25/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 26/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 27/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 28/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 29/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 30/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 31/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 32/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 33/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 34/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 35/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 36/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 37/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 38/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 39/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 40/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 41/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 42/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 43/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 44/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 45/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 46/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 47/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 48/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 49/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 50/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 51/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 52/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 53/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 54/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 55/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 56/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 57/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 58/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 59/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 60/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 61/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 62/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Channel 63/64 : 0
lrdn0916:3269502:3269606 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0916:3269502:3269606 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0916:3269502:3269606 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0916:3269502:3269612 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn0916:3269502:3269613 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
[38;5;39m2025-08-02 08:48:48,062 | xffl.distributed.distributed |    DEBUG | [Rank 35]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0916:3269502:3269606 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0916:3269502:3269606 [0] NCCL INFO CC Off, workFifoBytes 1048576
[38;5;39m2025-08-02 08:48:48,075 | xffl.distributed.distributed |    DEBUG | [Rank 212]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0916:3269502:3269606 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0916:3269502:3269606 [0] NCCL INFO ncclCommInitRankConfig comm 0xd80dfb0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x957adedac79867dd - Init COMPLETE
lrdn0916:3269502:3269606 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0270:1598262:1598262 [0] NCCL INFO Bootstrap: Using ib0:10.128.10.89<0>
lrdn0270:1598262:1598262 [0] NCCL INFO cudaDriverVersion 12020
lrdn0270:1598262:1598262 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0270:1598262:1598262 [0] NCCL INFO Comm config Blocking set to 1
lrdn1162:1559282:1559282 [0] NCCL INFO Bootstrap: Using ib0:10.128.24.73<0>
lrdn1162:1559282:1559282 [0] NCCL INFO cudaDriverVersion 12020
lrdn1162:1559282:1559282 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1162:1559282:1559282 [0] NCCL INFO Comm config Blocking set to 1
lrdn0270:1598262:1598356 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0270:1598262:1598356 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0270:1598262:1598356 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.10.89<0>
lrdn0270:1598262:1598356 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0270:1598262:1598356 [0] NCCL INFO Using network IB
lrdn0270:1598262:1598356 [0] NCCL INFO ncclCommInitRankConfig comm 0xe6b0020 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xde0469f8f71c86c - Init START
lrdn0270:1598262:1598356 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0270:1598262:1598356 [0] NCCL INFO Bootstrap timings total 0.000362 (create 0.000018, send 0.000058, recv 0.000086, ring 0.000001, delay 0.000000)
lrdn1162:1559282:1559379 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1162:1559282:1559379 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0270:1598262:1598356 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0270:1598262:1598356 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0270:1598262:1598356 [0] NCCL INFO comm 0xe6b0020 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 00/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 01/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 02/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 03/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 04/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 05/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 06/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 07/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 08/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 09/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 10/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 11/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 12/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 13/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 14/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 15/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 16/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 17/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 18/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 19/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 20/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 21/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 22/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 23/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 24/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 25/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 26/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 27/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 28/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 29/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 30/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 31/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 32/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 33/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 34/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 35/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 36/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 37/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 38/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 39/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 40/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 41/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 42/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 43/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 44/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 45/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 46/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 47/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 48/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 49/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 50/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 51/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 52/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 53/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 54/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 55/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 56/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 57/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 58/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 59/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 60/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 61/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 62/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Channel 63/64 : 0
lrdn0270:1598262:1598356 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0270:1598262:1598356 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0270:1598262:1598356 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0270:1598262:1598362 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn0270:1598262:1598363 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0270:1598262:1598356 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0270:1598262:1598356 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0270:1598262:1598356 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0270:1598262:1598356 [0] NCCL INFO ncclCommInitRankConfig comm 0xe6b0020 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xde0469f8f71c86c - Init COMPLETE
lrdn0270:1598262:1598356 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1162:1559282:1559379 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.24.73<0>
lrdn1162:1559282:1559379 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1162:1559282:1559379 [0] NCCL INFO Using network IB
lrdn1162:1559282:1559379 [0] NCCL INFO ncclCommInitRankConfig comm 0xea3bc40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf3ff8671776213f2 - Init START
lrdn1162:1559282:1559379 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1162:1559282:1559379 [0] NCCL INFO Bootstrap timings total 0.000528 (create 0.000023, send 0.000069, recv 0.000194, ring 0.000001, delay 0.000001)
lrdn1162:1559282:1559379 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1162:1559282:1559379 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1162:1559282:1559379 [0] NCCL INFO comm 0xea3bc40 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 00/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 01/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 02/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 03/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 04/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 05/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 06/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 07/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 08/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 09/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 10/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 11/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 12/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 13/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 14/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 15/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 16/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 17/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 18/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 19/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 20/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 21/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 22/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 23/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 24/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 25/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 26/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 27/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 28/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 29/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 30/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 31/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 32/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 33/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 34/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 35/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 36/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 37/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 38/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 39/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 40/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 41/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 42/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 43/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 44/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 45/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 46/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 47/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 48/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 49/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 50/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 51/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 52/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 53/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 54/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 55/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 56/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 57/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 58/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 59/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 60/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 61/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 62/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Channel 63/64 : 0
lrdn1162:1559282:1559379 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1162:1559282:1559379 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1162:1559282:1559379 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1162:1559282:1559385 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn1162:1559282:1559386 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn1162:1559282:1559379 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1162:1559282:1559379 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1162:1559282:1559379 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1162:1559282:1559379 [0] NCCL INFO ncclCommInitRankConfig comm 0xea3bc40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf3ff8671776213f2 - Init COMPLETE
lrdn1162:1559282:1559379 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:49,023 | xffl.distributed.distributed |    DEBUG | [Rank 91]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:48:49,286 | xffl.distributed.distributed |    DEBUG | [Rank 194]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0598:2941148:2941148 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.121<0>
lrdn0598:2941148:2941148 [0] NCCL INFO cudaDriverVersion 12020
lrdn0598:2941148:2941148 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0598:2941148:2941148 [0] NCCL INFO Comm config Blocking set to 1
lrdn0598:2941148:2941252 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0598:2941148:2941252 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0598:2941148:2941252 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.121<0>
lrdn0598:2941148:2941252 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0598:2941148:2941252 [0] NCCL INFO Using network IB
lrdn0598:2941148:2941252 [0] NCCL INFO ncclCommInitRankConfig comm 0xe378540 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x49d4223f00e741ff - Init START
lrdn0598:2941148:2941252 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0598:2941148:2941252 [0] NCCL INFO Bootstrap timings total 0.000401 (create 0.000020, send 0.000060, recv 0.000094, ring 0.000001, delay 0.000000)
lrdn0598:2941148:2941252 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0598:2941148:2941252 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0598:2941148:2941252 [0] NCCL INFO comm 0xe378540 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 00/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 01/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 02/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 03/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 04/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 05/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 06/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 07/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 08/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 09/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 10/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 11/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 12/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 13/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 14/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 15/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 16/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 17/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 18/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 19/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 20/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 21/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 22/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 23/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 24/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 25/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 26/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 27/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 28/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 29/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 30/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 31/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 32/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 33/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 34/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 35/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 36/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 37/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 38/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 39/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 40/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 41/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 42/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 43/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 44/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 45/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 46/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 47/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 48/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 49/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 50/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 51/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 52/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 53/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 54/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 55/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 56/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 57/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 58/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 59/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 60/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 61/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 62/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Channel 63/64 : 0
lrdn0598:2941148:2941252 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0598:2941148:2941252 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0598:2941148:2941252 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0598:2941148:2941259 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1
lrdn0598:2941148:2941258 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
[38;5;39m2025-08-02 08:48:49,603 | xffl.distributed.distributed |    DEBUG | [Rank 38]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0598:2941148:2941252 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0598:2941148:2941252 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0598:2941148:2941252 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0598:2941148:2941252 [0] NCCL INFO ncclCommInitRankConfig comm 0xe378540 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x49d4223f00e741ff - Init COMPLETE
lrdn0598:2941148:2941252 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1068:1524729:1524729 [0] NCCL INFO Bootstrap: Using ib0:10.128.22.209<0>
lrdn1068:1524729:1524729 [0] NCCL INFO cudaDriverVersion 12020
lrdn1068:1524729:1524729 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1068:1524729:1524729 [0] NCCL INFO Comm config Blocking set to 1
lrdn1068:1524729:1524825 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1068:1524729:1524825 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1068:1524729:1524825 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.22.209<0>
lrdn1068:1524729:1524825 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1068:1524729:1524825 [0] NCCL INFO Using network IB
lrdn1068:1524729:1524825 [0] NCCL INFO ncclCommInitRankConfig comm 0xeaacf30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf8db7b636998f6c0 - Init START
lrdn1068:1524729:1524825 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1068:1524729:1524825 [0] NCCL INFO Bootstrap timings total 0.000426 (create 0.000022, send 0.000070, recv 0.000102, ring 0.000001, delay 0.000001)
[38;5;39m2025-08-02 08:48:49,840 | xffl.distributed.distributed |    DEBUG | [Rank 122]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1068:1524729:1524825 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1068:1524729:1524825 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1068:1524729:1524825 [0] NCCL INFO comm 0xeaacf30 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 00/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 01/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 02/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 03/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 04/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 05/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 06/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 07/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 08/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 09/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 10/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 11/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 12/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 13/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 14/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 15/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 16/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 17/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 18/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 19/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 20/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 21/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 22/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 23/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 24/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 25/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 26/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 27/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 28/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 29/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 30/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 31/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 32/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 33/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 34/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 35/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 36/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 37/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 38/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 39/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 40/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 41/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 42/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 43/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 44/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 45/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 46/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 47/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 48/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 49/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 50/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 51/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 52/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 53/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 54/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 55/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 56/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 57/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 58/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 59/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 60/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 61/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 62/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Channel 63/64 : 0
lrdn1068:1524729:1524825 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1068:1524729:1524825 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1068:1524729:1524825 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1068:1524729:1524831 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn1068:1524729:1524832 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn1068:1524729:1524825 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1068:1524729:1524825 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1068:1524729:1524825 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1068:1524729:1524825 [0] NCCL INFO ncclCommInitRankConfig comm 0xeaacf30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf8db7b636998f6c0 - Init COMPLETE
lrdn1068:1524729:1524825 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn0288:1681676:1681676 [0] NCCL INFO Bootstrap: Using ib0:10.128.10.161<0>
lrdn0288:1681676:1681676 [0] NCCL INFO cudaDriverVersion 12020
lrdn0288:1681676:1681676 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0288:1681676:1681676 [0] NCCL INFO Comm config Blocking set to 1
lrdn0288:1681676:1681769 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0288:1681676:1681769 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0288:1681676:1681769 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.10.161<0>
lrdn0288:1681676:1681769 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0288:1681676:1681769 [0] NCCL INFO Using network IB
lrdn0288:1681676:1681769 [0] NCCL INFO ncclCommInitRankConfig comm 0xd6b68a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5bf4886c6b99eaf3 - Init START
lrdn0288:1681676:1681769 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0288:1681676:1681769 [0] NCCL INFO Bootstrap timings total 0.000509 (create 0.000019, send 0.000061, recv 0.000219, ring 0.000001, delay 0.000001)
lrdn0288:1681676:1681769 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0288:1681676:1681769 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0288:1681676:1681769 [0] NCCL INFO comm 0xd6b68a0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 00/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 01/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 02/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 03/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 04/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 05/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 06/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 07/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 08/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 09/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 10/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 11/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 12/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 13/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 14/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 15/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 16/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 17/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 18/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 19/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 20/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 21/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 22/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 23/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 24/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 25/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 26/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 27/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 28/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 29/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 30/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 31/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 32/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 33/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 34/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 35/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 36/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 37/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 38/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 39/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 40/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 41/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 42/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 43/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 44/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 45/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 46/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 47/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 48/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 49/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 50/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 51/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 52/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 53/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 54/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 55/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 56/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 57/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 58/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 59/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 60/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 61/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 62/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Channel 63/64 : 0
lrdn0288:1681676:1681769 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0288:1681676:1681769 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0288:1681676:1681769 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0288:1681676:1681776 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn0288:1681676:1681775 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn0288:1681676:1681769 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0288:1681676:1681769 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0288:1681676:1681769 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0288:1681676:1681769 [0] NCCL INFO ncclCommInitRankConfig comm 0xd6b68a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5bf4886c6b99eaf3 - Init COMPLETE
lrdn0288:1681676:1681769 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.14, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0717:2059502:2059502 [0] NCCL INFO Bootstrap: Using ib0:10.128.17.85<0>
lrdn0717:2059502:2059502 [0] NCCL INFO cudaDriverVersion 12020
lrdn0717:2059502:2059502 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0717:2059502:2059502 [0] NCCL INFO Comm config Blocking set to 1
lrdn0717:2059502:2059606 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0717:2059502:2059606 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0717:2059502:2059606 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.17.85<0>
lrdn0717:2059502:2059606 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0717:2059502:2059606 [0] NCCL INFO Using network IB
lrdn0717:2059502:2059606 [0] NCCL INFO ncclCommInitRankConfig comm 0xd52bc70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x881f3f831fa57ef - Init START
lrdn0717:2059502:2059606 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0717:2059502:2059606 [0] NCCL INFO Bootstrap timings total 0.000449 (create 0.000025, send 0.000068, recv 0.000110, ring 0.000001, delay 0.000000)
lrdn0717:2059502:2059606 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0717:2059502:2059606 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0717:2059502:2059606 [0] NCCL INFO comm 0xd52bc70 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 00/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 01/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 02/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 03/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 04/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 05/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 06/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 07/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 08/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 09/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 10/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 11/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 12/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 13/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 14/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 15/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 16/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 17/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 18/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 19/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 20/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 21/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 22/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 23/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 24/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 25/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 26/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 27/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 28/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 29/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 30/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 31/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 32/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 33/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 34/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 35/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 36/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 37/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 38/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 39/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 40/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 41/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 42/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 43/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 44/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 45/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 46/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 47/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 48/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 49/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 50/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 51/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 52/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 53/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 54/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 55/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 56/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 57/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 58/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 59/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 60/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 61/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 62/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Channel 63/64 : 0
lrdn0717:2059502:2059606 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0717:2059502:2059606 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0717:2059502:2059606 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0717:2059502:2059613 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0717:2059502:2059612 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn0717:2059502:2059606 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0717:2059502:2059606 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0717:2059502:2059606 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0717:2059502:2059606 [0] NCCL INFO ncclCommInitRankConfig comm 0xd52bc70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x881f3f831fa57ef - Init COMPLETE
lrdn0717:2059502:2059606 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
[38;5;39m2025-08-02 08:48:51,336 | xffl.distributed.distributed |    DEBUG | [Rank 199]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1090:1739574:1739574 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.41<0>
lrdn1090:1739574:1739574 [0] NCCL INFO cudaDriverVersion 12020
lrdn1090:1739574:1739574 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1090:1739574:1739574 [0] NCCL INFO Comm config Blocking set to 1
lrdn1090:1739574:1739669 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1090:1739574:1739669 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:48:51,834 | xffl.distributed.distributed |    DEBUG | [Rank 109]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1090:1739574:1739669 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.41<0>
lrdn1090:1739574:1739669 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1090:1739574:1739669 [0] NCCL INFO Using network IB
lrdn1090:1739574:1739669 [0] NCCL INFO ncclCommInitRankConfig comm 0xe54acd0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd57106a26e003a31 - Init START
lrdn1090:1739574:1739669 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1090:1739574:1739669 [0] NCCL INFO Bootstrap timings total 0.000381 (create 0.000017, send 0.000061, recv 0.000091, ring 0.000001, delay 0.000000)
lrdn1090:1739574:1739669 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1090:1739574:1739669 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1090:1739574:1739669 [0] NCCL INFO comm 0xe54acd0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 00/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 01/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 02/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 03/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 04/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 05/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 06/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 07/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 08/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 09/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 10/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 11/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 12/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 13/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 14/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 15/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 16/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 17/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 18/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 19/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 20/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 21/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 22/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 23/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 24/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 25/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 26/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 27/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 28/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 29/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 30/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 31/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 32/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 33/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 34/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 35/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 36/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 37/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 38/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 39/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 40/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 41/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 42/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 43/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 44/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 45/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 46/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 47/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 48/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 49/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 50/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 51/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 52/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 53/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 54/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 55/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 56/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 57/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 58/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 59/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 60/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 61/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 62/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Channel 63/64 : 0
lrdn1090:1739574:1739669 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1090:1739574:1739669 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1090:1739574:1739669 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1090:1739574:1739675 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn1090:1739574:1739676 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn1090:1739574:1739669 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1090:1739574:1739669 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1090:1739574:1739669 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1090:1739574:1739669 [0] NCCL INFO ncclCommInitRankConfig comm 0xe54acd0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd57106a26e003a31 - Init COMPLETE
lrdn1090:1739574:1739669 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:51,876 | xffl.distributed.distributed |    DEBUG | [Rank 87]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0664:1570998:1570998 [0] NCCL INFO Bootstrap: Using ib0:10.128.16.129<0>
lrdn0664:1570998:1570998 [0] NCCL INFO cudaDriverVersion 12020
lrdn0664:1570998:1570998 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0664:1570998:1570998 [0] NCCL INFO Comm config Blocking set to 1
lrdn0583:1639762:1639762 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.61<0>
lrdn0583:1639762:1639762 [0] NCCL INFO cudaDriverVersion 12020
lrdn0583:1639762:1639762 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0583:1639762:1639762 [0] NCCL INFO Comm config Blocking set to 1
lrdn0664:1570998:1571101 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0664:1570998:1571101 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0583:1639762:1639867 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0583:1639762:1639867 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0664:1570998:1571101 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.16.129<0>
lrdn0664:1570998:1571101 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0664:1570998:1571101 [0] NCCL INFO Using network IB
lrdn0664:1570998:1571101 [0] NCCL INFO ncclCommInitRankConfig comm 0xd520a40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8ead0d221184f35b - Init START
lrdn0664:1570998:1571101 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0664:1570998:1571101 [0] NCCL INFO Bootstrap timings total 0.000403 (create 0.000020, send 0.000059, recv 0.000105, ring 0.000001, delay 0.000000)
lrdn0664:1570998:1571101 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0664:1570998:1571101 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0664:1570998:1571101 [0] NCCL INFO comm 0xd520a40 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 00/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 01/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 02/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 03/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 04/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 05/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 06/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 07/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 08/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 09/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 10/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 11/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 12/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 13/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 14/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 15/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 16/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 17/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 18/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 19/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 20/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 21/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 22/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 23/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 24/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 25/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 26/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 27/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 28/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 29/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 30/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 31/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 32/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 33/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 34/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 35/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 36/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 37/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 38/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 39/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 40/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 41/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 42/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 43/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 44/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 45/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 46/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 47/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 48/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 49/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 50/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 51/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 52/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 53/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 54/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 55/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 56/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 57/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 58/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 59/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 60/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 61/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 62/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Channel 63/64 : 0
lrdn0664:1570998:1571101 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0664:1570998:1571101 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0664:1570998:1571101 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0664:1570998:1571107 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn0664:1570998:1571108 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0664:1570998:1571101 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0664:1570998:1571101 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0583:1639762:1639867 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.61<0>
lrdn0664:1570998:1571101 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0664:1570998:1571101 [0] NCCL INFO ncclCommInitRankConfig comm 0xd520a40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8ead0d221184f35b - Init COMPLETE
lrdn0664:1570998:1571101 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0583:1639762:1639867 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0583:1639762:1639867 [0] NCCL INFO Using network IB
lrdn0583:1639762:1639867 [0] NCCL INFO ncclCommInitRankConfig comm 0xd970040 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf66937d1ccedce7 - Init START
lrdn0583:1639762:1639867 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0583:1639762:1639867 [0] NCCL INFO Bootstrap timings total 0.000536 (create 0.000023, send 0.000067, recv 0.000216, ring 0.000001, delay 0.000000)
lrdn0583:1639762:1639867 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0583:1639762:1639867 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0583:1639762:1639867 [0] NCCL INFO comm 0xd970040 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 00/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 01/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 02/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 03/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 04/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 05/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 06/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 07/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 08/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 09/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 10/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 11/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 12/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 13/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 14/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 15/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 16/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 17/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 18/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 19/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 20/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 21/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 22/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 23/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 24/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 25/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 26/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 27/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 28/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 29/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 30/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 31/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 32/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 33/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 34/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 35/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 36/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 37/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 38/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 39/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 40/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 41/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 42/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 43/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 44/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 45/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 46/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 47/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 48/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 49/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 50/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 51/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 52/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 53/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 54/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 55/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 56/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 57/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 58/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 59/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 60/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 61/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 62/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Channel 63/64 : 0
lrdn0583:1639762:1639867 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0583:1639762:1639867 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0583:1639762:1639867 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0583:1639762:1639874 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn0583:1639762:1639873 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn0583:1639762:1639867 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0583:1639762:1639867 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0583:1639762:1639867 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0583:1639762:1639867 [0] NCCL INFO ncclCommInitRankConfig comm 0xd970040 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf66937d1ccedce7 - Init COMPLETE
lrdn0583:1639762:1639867 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:52,659 | xffl.distributed.distributed |    DEBUG | [Rank 51]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:48:52,853 | xffl.distributed.distributed |    DEBUG | [Rank 118]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0367:1637221:1637221 [0] NCCL INFO Bootstrap: Using ib0:10.128.11.221<0>
lrdn0367:1637221:1637221 [0] NCCL INFO cudaDriverVersion 12020
lrdn0367:1637221:1637221 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0367:1637221:1637221 [0] NCCL INFO Comm config Blocking set to 1
lrdn0699:3525052:3525052 [0] NCCL INFO Bootstrap: Using ib0:10.128.17.13<0>
lrdn0699:3525052:3525052 [0] NCCL INFO cudaDriverVersion 12020
lrdn0699:3525052:3525052 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0699:3525052:3525052 [0] NCCL INFO Comm config Blocking set to 1
lrdn0367:1637221:1637316 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0367:1637221:1637316 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0367:1637221:1637316 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.11.221<0>
lrdn0367:1637221:1637316 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0367:1637221:1637316 [0] NCCL INFO Using network IB
lrdn0367:1637221:1637316 [0] NCCL INFO ncclCommInitRankConfig comm 0xe0d0850 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1d4b75f998e7d7c6 - Init START
lrdn0367:1637221:1637316 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0367:1637221:1637316 [0] NCCL INFO Bootstrap timings total 0.000432 (create 0.000023, send 0.000068, recv 0.000101, ring 0.000002, delay 0.000000)
lrdn0367:1637221:1637316 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0367:1637221:1637316 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0367:1637221:1637316 [0] NCCL INFO comm 0xe0d0850 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 00/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 01/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 02/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 03/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 04/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 05/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 06/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 07/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 08/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 09/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 10/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 11/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 12/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 13/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 14/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 15/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 16/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 17/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 18/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 19/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 20/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 21/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 22/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 23/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 24/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 25/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 26/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 27/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 28/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 29/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 30/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 31/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 32/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 33/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 34/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 35/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 36/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 37/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 38/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 39/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 40/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 41/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 42/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 43/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 44/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 45/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 46/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 47/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 48/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 49/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 50/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 51/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 52/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 53/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 54/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 55/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 56/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 57/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 58/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 59/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 60/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 61/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 62/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Channel 63/64 : 0
lrdn0367:1637221:1637316 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0367:1637221:1637316 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0367:1637221:1637316 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0367:1637221:1637322 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0367:1637221:1637323 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn0367:1637221:1637316 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0367:1637221:1637316 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0367:1637221:1637316 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0367:1637221:1637316 [0] NCCL INFO ncclCommInitRankConfig comm 0xe0d0850 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1d4b75f998e7d7c6 - Init COMPLETE
lrdn0367:1637221:1637316 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0699:3525052:3525148 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0699:3525052:3525148 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0699:3525052:3525148 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.17.13<0>
lrdn0699:3525052:3525148 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0699:3525052:3525148 [0] NCCL INFO Using network IB
lrdn0699:3525052:3525148 [0] NCCL INFO ncclCommInitRankConfig comm 0xe84e8c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x16f87c7fa06116c4 - Init START
lrdn0699:3525052:3525148 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0699:3525052:3525148 [0] NCCL INFO Bootstrap timings total 0.000543 (create 0.000022, send 0.000065, recv 0.000218, ring 0.000001, delay 0.000000)
lrdn0699:3525052:3525148 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0699:3525052:3525148 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0699:3525052:3525148 [0] NCCL INFO comm 0xe84e8c0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 00/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 01/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 02/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 03/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 04/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 05/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 06/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 07/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 08/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 09/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 10/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 11/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 12/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 13/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 14/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 15/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 16/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 17/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 18/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 19/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 20/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 21/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 22/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 23/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 24/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 25/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 26/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 27/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 28/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 29/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 30/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 31/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 32/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 33/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 34/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 35/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 36/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 37/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 38/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 39/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 40/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 41/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 42/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 43/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 44/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 45/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 46/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 47/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 48/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 49/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 50/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 51/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 52/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 53/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 54/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 55/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 56/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 57/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 58/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 59/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 60/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 61/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 62/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Channel 63/64 : 0
lrdn0699:3525052:3525148 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0699:3525052:3525148 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0699:3525052:3525148 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0699:3525052:3525154 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0699:3525052:3525155 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0699:3525052:3525148 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0699:3525052:3525148 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0699:3525052:3525148 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0699:3525052:3525148 [0] NCCL INFO ncclCommInitRankConfig comm 0xe84e8c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x16f87c7fa06116c4 - Init COMPLETE
lrdn0699:3525052:3525148 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:53,630 | xffl.distributed.distributed |    DEBUG | [Rank 224]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1221:1683744:1683744 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.53<0>
lrdn1221:1683744:1683744 [0] NCCL INFO cudaDriverVersion 12020
lrdn1221:1683744:1683744 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1221:1683744:1683744 [0] NCCL INFO Comm config Blocking set to 1
lrdn1221:1683744:1683849 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1221:1683744:1683849 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1221:1683744:1683849 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.53<0>
lrdn1221:1683744:1683849 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1221:1683744:1683849 [0] NCCL INFO Using network IB
lrdn1221:1683744:1683849 [0] NCCL INFO ncclCommInitRankConfig comm 0xccda3a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc025f2a436a66857 - Init START
lrdn1221:1683744:1683849 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1221:1683744:1683849 [0] NCCL INFO Bootstrap timings total 0.000391 (create 0.000019, send 0.000063, recv 0.000094, ring 0.000001, delay 0.000000)
lrdn1221:1683744:1683849 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1221:1683744:1683849 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1221:1683744:1683849 [0] NCCL INFO comm 0xccda3a0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 00/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 01/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 02/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 03/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 04/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 05/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 06/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 07/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 08/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 09/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 10/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 11/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 12/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 13/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 14/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 15/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 16/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 17/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 18/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 19/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 20/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 21/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 22/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 23/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 24/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 25/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 26/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 27/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 28/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 29/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 30/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 31/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 32/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 33/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 34/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 35/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 36/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 37/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 38/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 39/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 40/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 41/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 42/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 43/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 44/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 45/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 46/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 47/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 48/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 49/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 50/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 51/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 52/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 53/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 54/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 55/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 56/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 57/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 58/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 59/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 60/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 61/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 62/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Channel 63/64 : 0
lrdn1221:1683744:1683849 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1221:1683744:1683849 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1221:1683744:1683849 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1221:1683744:1683855 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn1221:1683744:1683856 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn1221:1683744:1683849 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1221:1683744:1683849 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1221:1683744:1683849 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1221:1683744:1683849 [0] NCCL INFO ncclCommInitRankConfig comm 0xccda3a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc025f2a436a66857 - Init COMPLETE
lrdn1221:1683744:1683849 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:54,596 | xffl.distributed.distributed |    DEBUG | [Rank 14]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0163:3297581:3297581 [0] NCCL INFO Bootstrap: Using ib0:10.128.8.173<0>
lrdn0163:3297581:3297581 [0] NCCL INFO cudaDriverVersion 12020
lrdn0163:3297581:3297581 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0163:3297581:3297581 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:48:55,001 | xffl.distributed.distributed |    DEBUG | [Rank 143]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0163:3297581:3297678 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0163:3297581:3297678 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0163:3297581:3297678 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.8.173<0>
lrdn0163:3297581:3297678 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0163:3297581:3297678 [0] NCCL INFO Using network IB
lrdn0163:3297581:3297678 [0] NCCL INFO ncclCommInitRankConfig comm 0xeb0f220 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8947e177abfeef1b - Init START
lrdn0163:3297581:3297678 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0163:3297581:3297678 [0] NCCL INFO Bootstrap timings total 0.000396 (create 0.000019, send 0.000059, recv 0.000094, ring 0.000001, delay 0.000000)
lrdn0163:3297581:3297678 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0163:3297581:3297678 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0163:3297581:3297678 [0] NCCL INFO comm 0xeb0f220 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 00/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 01/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 02/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 03/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 04/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 05/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 06/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 07/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 08/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 09/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 10/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 11/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 12/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 13/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 14/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 15/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 16/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 17/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 18/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 19/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 20/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 21/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 22/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 23/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 24/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 25/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 26/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 27/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 28/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 29/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 30/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 31/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 32/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 33/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 34/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 35/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 36/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 37/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 38/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 39/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 40/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 41/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 42/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 43/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 44/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 45/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 46/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 47/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 48/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 49/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 50/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 51/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 52/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 53/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 54/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 55/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 56/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 57/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 58/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 59/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 60/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 61/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 62/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Channel 63/64 : 0
lrdn0163:3297581:3297678 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0163:3297581:3297678 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0163:3297581:3297678 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0163:3297581:3297684 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0163:3297581:3297685 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0163:3297581:3297678 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0163:3297581:3297678 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0163:3297581:3297678 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0163:3297581:3297678 [0] NCCL INFO ncclCommInitRankConfig comm 0xeb0f220 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8947e177abfeef1b - Init COMPLETE
lrdn0163:3297581:3297678 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0850:3805411:3805411 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.105<0>
lrdn0850:3805411:3805411 [0] NCCL INFO cudaDriverVersion 12020
lrdn0850:3805411:3805411 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0850:3805411:3805411 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:48:55,407 | xffl.distributed.distributed |    DEBUG | [Rank 57]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0850:3805411:3805509 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0850:3805411:3805509 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:48:55,530 | xffl.distributed.distributed |    DEBUG | [Rank 182]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0850:3805411:3805509 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.105<0>
lrdn0850:3805411:3805509 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0850:3805411:3805509 [0] NCCL INFO Using network IB
lrdn0850:3805411:3805509 [0] NCCL INFO ncclCommInitRankConfig comm 0xcbad950 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xac8fef3901f86772 - Init START
lrdn0850:3805411:3805509 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0850:3805411:3805509 [0] NCCL INFO Bootstrap timings total 0.000388 (create 0.000018, send 0.000063, recv 0.000087, ring 0.000001, delay 0.000000)
lrdn0850:3805411:3805509 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0850:3805411:3805509 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0850:3805411:3805509 [0] NCCL INFO comm 0xcbad950 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 00/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 01/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 02/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 03/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 04/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 05/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 06/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 07/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 08/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 09/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 10/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 11/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 12/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 13/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 14/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 15/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 16/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 17/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 18/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 19/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 20/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 21/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 22/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 23/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 24/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 25/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 26/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 27/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 28/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 29/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 30/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 31/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 32/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 33/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 34/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 35/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 36/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 37/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 38/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 39/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 40/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 41/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 42/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 43/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 44/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 45/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 46/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 47/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 48/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 49/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 50/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 51/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 52/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 53/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 54/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 55/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 56/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 57/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 58/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 59/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 60/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 61/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 62/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Channel 63/64 : 0
lrdn0850:3805411:3805509 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0850:3805411:3805509 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0850:3805411:3805509 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0850:3805411:3805515 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0850:3805411:3805516 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 12
lrdn0850:3805411:3805509 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0850:3805411:3805509 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0850:3805411:3805509 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0850:3805411:3805509 [0] NCCL INFO ncclCommInitRankConfig comm 0xcbad950 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xac8fef3901f86772 - Init COMPLETE
lrdn0850:3805411:3805509 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.02, rest 0.00)
lrdn0409:1639191:1639191 [0] NCCL INFO Bootstrap: Using ib0:10.128.12.133<0>
lrdn0409:1639191:1639191 [0] NCCL INFO cudaDriverVersion 12020
lrdn0409:1639191:1639191 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0409:1639191:1639191 [0] NCCL INFO Comm config Blocking set to 1
lrdn1002:1707642:1707642 [0] NCCL INFO Bootstrap: Using ib0:10.128.21.201<0>
lrdn1002:1707642:1707642 [0] NCCL INFO cudaDriverVersion 12020
lrdn1002:1707642:1707642 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1002:1707642:1707642 [0] NCCL INFO Comm config Blocking set to 1
lrdn0409:1639191:1639287 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0409:1639191:1639287 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0409:1639191:1639287 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.12.133<0>
lrdn0409:1639191:1639287 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0409:1639191:1639287 [0] NCCL INFO Using network IB
lrdn0409:1639191:1639287 [0] NCCL INFO ncclCommInitRankConfig comm 0xd1e3e60 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x6b1aaba5b779f6a0 - Init START
lrdn0409:1639191:1639287 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0409:1639191:1639287 [0] NCCL INFO Bootstrap timings total 0.000571 (create 0.000026, send 0.000072, recv 0.000224, ring 0.000001, delay 0.000001)
lrdn0409:1639191:1639287 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0409:1639191:1639287 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0409:1639191:1639287 [0] NCCL INFO comm 0xd1e3e60 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 00/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 01/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 02/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 03/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 04/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 05/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 06/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 07/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 08/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 09/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 10/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 11/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 12/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 13/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 14/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 15/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 16/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 17/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 18/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 19/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 20/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 21/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 22/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 23/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 24/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 25/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 26/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 27/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 28/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 29/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 30/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 31/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 32/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 33/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 34/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 35/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 36/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 37/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 38/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 39/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 40/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 41/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 42/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 43/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 44/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 45/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 46/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 47/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 48/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 49/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 50/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 51/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 52/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 53/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 54/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 55/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 56/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 57/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 58/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 59/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 60/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 61/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 62/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Channel 63/64 : 0
lrdn0409:1639191:1639287 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0409:1639191:1639287 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0409:1639191:1639287 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0409:1639191:1639293 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0409:1639191:1639294 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0409:1639191:1639287 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0409:1639191:1639287 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0409:1639191:1639287 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0409:1639191:1639287 [0] NCCL INFO ncclCommInitRankConfig comm 0xd1e3e60 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x6b1aaba5b779f6a0 - Init COMPLETE
lrdn0409:1639191:1639287 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1002:1707642:1707739 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1002:1707642:1707739 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1002:1707642:1707739 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.21.201<0>
lrdn1002:1707642:1707739 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1002:1707642:1707739 [0] NCCL INFO Using network IB
lrdn1002:1707642:1707739 [0] NCCL INFO ncclCommInitRankConfig comm 0xd092a10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5a4ba7e038bc41f0 - Init START
lrdn1002:1707642:1707739 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1002:1707642:1707739 [0] NCCL INFO Bootstrap timings total 0.000715 (create 0.000025, send 0.000070, recv 0.000383, ring 0.000001, delay 0.000000)
lrdn1002:1707642:1707739 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1002:1707642:1707739 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1002:1707642:1707739 [0] NCCL INFO comm 0xd092a10 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 00/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 01/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 02/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 03/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 04/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 05/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 06/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 07/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 08/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 09/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 10/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 11/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 12/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 13/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 14/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 15/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 16/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 17/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 18/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 19/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 20/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 21/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 22/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 23/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 24/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 25/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 26/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 27/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 28/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 29/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 30/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 31/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 32/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 33/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 34/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 35/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 36/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 37/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 38/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 39/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 40/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 41/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 42/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 43/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 44/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 45/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 46/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 47/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 48/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 49/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 50/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 51/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 52/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 53/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 54/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 55/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 56/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 57/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 58/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 59/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 60/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 61/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 62/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Channel 63/64 : 0
lrdn1002:1707642:1707739 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1002:1707642:1707739 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1002:1707642:1707739 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1002:1707642:1707745 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn1002:1707642:1707746 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn1002:1707642:1707739 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1002:1707642:1707739 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1002:1707642:1707739 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1002:1707642:1707739 [0] NCCL INFO ncclCommInitRankConfig comm 0xd092a10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5a4ba7e038bc41f0 - Init COMPLETE
lrdn1002:1707642:1707739 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:57,360 | xffl.distributed.distributed |    DEBUG | [Rank 175]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:48:57,491 | xffl.distributed.distributed |    DEBUG | [Rank 67]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0972:1487882:1487882 [0] NCCL INFO Bootstrap: Using ib0:10.128.21.81<0>
lrdn0972:1487882:1487882 [0] NCCL INFO cudaDriverVersion 12020
lrdn0972:1487882:1487882 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0972:1487882:1487882 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:48:57,763 | xffl.distributed.distributed |    DEBUG | [Rank 170]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0485:1537110:1537110 [0] NCCL INFO Bootstrap: Using ib0:10.128.13.181<0>
lrdn0485:1537110:1537110 [0] NCCL INFO cudaDriverVersion 12020
lrdn0485:1537110:1537110 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0485:1537110:1537110 [0] NCCL INFO Comm config Blocking set to 1
lrdn0972:1487882:1487977 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0972:1487882:1487977 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0972:1487882:1487977 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.21.81<0>
lrdn0972:1487882:1487977 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0972:1487882:1487977 [0] NCCL INFO Using network IB
lrdn0972:1487882:1487977 [0] NCCL INFO ncclCommInitRankConfig comm 0xdeec9b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x290058db8f6546c2 - Init START
lrdn0972:1487882:1487977 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0972:1487882:1487977 [0] NCCL INFO Bootstrap timings total 0.000446 (create 0.000027, send 0.000070, recv 0.000096, ring 0.000001, delay 0.000001)
lrdn0972:1487882:1487977 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0972:1487882:1487977 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0972:1487882:1487977 [0] NCCL INFO comm 0xdeec9b0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 00/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 01/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 02/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 03/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 04/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 05/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 06/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 07/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 08/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 09/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 10/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 11/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 12/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 13/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 14/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 15/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 16/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 17/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 18/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 19/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 20/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 21/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 22/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 23/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 24/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 25/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 26/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 27/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 28/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 29/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 30/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 31/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 32/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 33/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 34/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 35/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 36/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 37/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 38/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 39/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 40/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 41/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 42/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 43/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 44/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 45/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 46/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 47/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 48/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 49/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 50/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 51/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 52/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 53/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 54/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 55/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 56/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 57/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 58/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 59/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 60/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 61/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 62/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Channel 63/64 : 0
lrdn0972:1487882:1487977 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0972:1487882:1487977 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0972:1487882:1487977 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0972:1487882:1487983 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn0972:1487882:1487984 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn0972:1487882:1487977 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0972:1487882:1487977 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0972:1487882:1487977 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0972:1487882:1487977 [0] NCCL INFO ncclCommInitRankConfig comm 0xdeec9b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x290058db8f6546c2 - Init COMPLETE
lrdn0972:1487882:1487977 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0485:1537110:1537222 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0485:1537110:1537222 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0485:1537110:1537222 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.13.181<0>
lrdn0485:1537110:1537222 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0485:1537110:1537222 [0] NCCL INFO Using network IB
lrdn0485:1537110:1537222 [0] NCCL INFO ncclCommInitRankConfig comm 0xd7b2aa0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9c2055a821c2bb1b - Init START
lrdn0485:1537110:1537222 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0485:1537110:1537222 [0] NCCL INFO Bootstrap timings total 0.000441 (create 0.000025, send 0.000064, recv 0.000108, ring 0.000001, delay 0.000000)
lrdn0485:1537110:1537222 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0485:1537110:1537222 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0485:1537110:1537222 [0] NCCL INFO comm 0xd7b2aa0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 00/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 01/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 02/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 03/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 04/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 05/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 06/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 07/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 08/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 09/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 10/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 11/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 12/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 13/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 14/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 15/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 16/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 17/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 18/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 19/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 20/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 21/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 22/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 23/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 24/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 25/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 26/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 27/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 28/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 29/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 30/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 31/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 32/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 33/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 34/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 35/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 36/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 37/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 38/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 39/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 40/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 41/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 42/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 43/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 44/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 45/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 46/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 47/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 48/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 49/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 50/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 51/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 52/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 53/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 54/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 55/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 56/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 57/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 58/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 59/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 60/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 61/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 62/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Channel 63/64 : 0
lrdn0485:1537110:1537222 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0485:1537110:1537222 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0485:1537110:1537222 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0485:1537110:1537228 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0485:1537110:1537229 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
[38;5;39m2025-08-02 08:48:58,053 | xffl.distributed.distributed |    DEBUG | [Rank 102]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0485:1537110:1537222 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0485:1537110:1537222 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0485:1537110:1537222 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0485:1537110:1537222 [0] NCCL INFO ncclCommInitRankConfig comm 0xd7b2aa0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9c2055a821c2bb1b - Init COMPLETE
lrdn0485:1537110:1537222 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0953:1677008:1677008 [0] NCCL INFO Bootstrap: Using ib0:10.128.21.5<0>
lrdn0953:1677008:1677008 [0] NCCL INFO cudaDriverVersion 12020
lrdn0953:1677008:1677008 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0953:1677008:1677008 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:48:58,237 | xffl.distributed.distributed |    DEBUG | [Rank 220]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0953:1677008:1677112 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0953:1677008:1677112 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0953:1677008:1677112 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.21.5<0>
lrdn0953:1677008:1677112 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0953:1677008:1677112 [0] NCCL INFO Using network IB
lrdn0953:1677008:1677112 [0] NCCL INFO ncclCommInitRankConfig comm 0xcf09150 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5e76d2e6d10e828e - Init START
lrdn0953:1677008:1677112 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0953:1677008:1677112 [0] NCCL INFO Bootstrap timings total 0.000438 (create 0.000024, send 0.000068, recv 0.000109, ring 0.000001, delay 0.000001)
lrdn0953:1677008:1677112 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0953:1677008:1677112 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0953:1677008:1677112 [0] NCCL INFO comm 0xcf09150 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 00/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 01/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 02/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 03/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 04/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 05/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 06/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 07/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 08/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 09/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 10/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 11/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 12/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 13/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 14/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 15/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 16/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 17/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 18/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 19/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 20/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 21/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 22/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 23/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 24/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 25/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 26/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 27/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 28/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 29/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 30/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 31/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 32/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 33/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 34/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 35/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 36/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 37/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 38/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 39/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 40/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 41/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 42/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 43/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 44/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 45/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 46/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 47/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 48/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 49/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 50/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 51/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 52/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 53/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 54/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 55/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 56/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 57/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 58/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 59/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 60/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 61/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 62/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Channel 63/64 : 0
lrdn0953:1677008:1677112 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0953:1677008:1677112 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0953:1677008:1677112 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0953:1677008:1677118 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn0953:1677008:1677119 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn0953:1677008:1677112 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0953:1677008:1677112 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0953:1677008:1677112 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0953:1677008:1677112 [0] NCCL INFO ncclCommInitRankConfig comm 0xcf09150 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5e76d2e6d10e828e - Init COMPLETE
lrdn0953:1677008:1677112 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:58,458 | xffl.distributed.distributed |    DEBUG | [Rank 5]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0634:836731:836731 [0] NCCL INFO Bootstrap: Using ib0:10.128.16.9<0>
lrdn0634:836731:836731 [0] NCCL INFO cudaDriverVersion 12020
lrdn0634:836731:836731 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0634:836731:836731 [0] NCCL INFO Comm config Blocking set to 1
lrdn1201:1761739:1761739 [0] NCCL INFO Bootstrap: Using ib0:10.128.24.229<0>
lrdn1201:1761739:1761739 [0] NCCL INFO cudaDriverVersion 12020
lrdn1201:1761739:1761739 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1201:1761739:1761739 [0] NCCL INFO Comm config Blocking set to 1
lrdn0634:836731:836827 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0634:836731:836827 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0634:836731:836827 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.16.9<0>
lrdn0634:836731:836827 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0634:836731:836827 [0] NCCL INFO Using network IB
lrdn0634:836731:836827 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc8bb10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1cf11cbc81d47d45 - Init START
lrdn0634:836731:836827 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0634:836731:836827 [0] NCCL INFO Bootstrap timings total 0.000435 (create 0.000024, send 0.000066, recv 0.000108, ring 0.000001, delay 0.000000)
lrdn0634:836731:836827 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0634:836731:836827 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0634:836731:836827 [0] NCCL INFO comm 0xdc8bb10 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 00/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 01/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 02/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 03/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 04/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 05/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 06/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 07/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 08/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 09/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 10/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 11/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 12/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 13/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 14/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 15/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 16/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 17/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 18/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 19/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 20/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 21/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 22/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 23/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 24/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 25/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 26/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 27/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 28/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 29/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 30/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 31/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 32/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 33/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 34/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 35/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 36/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 37/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 38/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 39/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 40/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 41/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 42/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 43/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 44/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 45/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 46/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 47/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 48/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 49/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 50/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 51/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 52/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 53/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 54/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 55/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 56/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 57/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 58/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 59/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 60/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 61/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 62/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Channel 63/64 : 0
lrdn0634:836731:836827 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0634:836731:836827 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0634:836731:836827 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0634:836731:836833 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn0634:836731:836834 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0634:836731:836827 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0634:836731:836827 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0634:836731:836827 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0634:836731:836827 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc8bb10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1cf11cbc81d47d45 - Init COMPLETE
lrdn0634:836731:836827 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1201:1761739:1761836 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1201:1761739:1761836 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1201:1761739:1761836 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.24.229<0>
lrdn1201:1761739:1761836 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1201:1761739:1761836 [0] NCCL INFO Using network IB
lrdn1201:1761739:1761836 [0] NCCL INFO ncclCommInitRankConfig comm 0x176c6a30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb74f372f7789e4b7 - Init START
lrdn1201:1761739:1761836 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1201:1761739:1761836 [0] NCCL INFO Bootstrap timings total 0.000425 (create 0.000024, send 0.000070, recv 0.000091, ring 0.000001, delay 0.000001)
lrdn1201:1761739:1761836 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1201:1761739:1761836 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1201:1761739:1761836 [0] NCCL INFO comm 0x176c6a30 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 00/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 01/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 02/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 03/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 04/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 05/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 06/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 07/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 08/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 09/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 10/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 11/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 12/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 13/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 14/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 15/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 16/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 17/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 18/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 19/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 20/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 21/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 22/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 23/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 24/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 25/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 26/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 27/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 28/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 29/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 30/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 31/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 32/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 33/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 34/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 35/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 36/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 37/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 38/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 39/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 40/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 41/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 42/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 43/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 44/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 45/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 46/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 47/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 48/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 49/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 50/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 51/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 52/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 53/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 54/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 55/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 56/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 57/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 58/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 59/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 60/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 61/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 62/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Channel 63/64 : 0
lrdn1201:1761739:1761836 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1201:1761739:1761836 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1201:1761739:1761836 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1201:1761739:1761843 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1201:1761739:1761842 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn0080:2202741:2202741 [0] NCCL INFO Bootstrap: Using ib0:10.128.7.97<0>
lrdn0080:2202741:2202741 [0] NCCL INFO cudaDriverVersion 12020
lrdn0080:2202741:2202741 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0080:2202741:2202741 [0] NCCL INFO Comm config Blocking set to 1
lrdn1201:1761739:1761836 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1201:1761739:1761836 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1201:1761739:1761836 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1201:1761739:1761836 [0] NCCL INFO ncclCommInitRankConfig comm 0x176c6a30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb74f372f7789e4b7 - Init COMPLETE
lrdn1201:1761739:1761836 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0080:2202741:2202851 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0080:2202741:2202851 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0080:2202741:2202851 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.7.97<0>
lrdn0080:2202741:2202851 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0080:2202741:2202851 [0] NCCL INFO Using network IB
lrdn0080:2202741:2202851 [0] NCCL INFO ncclCommInitRankConfig comm 0xd25ad80 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x2c16eb2d15ef84e6 - Init START
lrdn0080:2202741:2202851 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0080:2202741:2202851 [0] NCCL INFO Bootstrap timings total 0.000420 (create 0.000024, send 0.000066, recv 0.000093, ring 0.000001, delay 0.000001)
lrdn0080:2202741:2202851 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0080:2202741:2202851 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0080:2202741:2202851 [0] NCCL INFO comm 0xd25ad80 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 00/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 01/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 02/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 03/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 04/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 05/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 06/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 07/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 08/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 09/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 10/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 11/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 12/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 13/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 14/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 15/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 16/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 17/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 18/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 19/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 20/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 21/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 22/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 23/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 24/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 25/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 26/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 27/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 28/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 29/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 30/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 31/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 32/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 33/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 34/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 35/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 36/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 37/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 38/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 39/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 40/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 41/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 42/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 43/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 44/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 45/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 46/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 47/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 48/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 49/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 50/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 51/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 52/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 53/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 54/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 55/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 56/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 57/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 58/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 59/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 60/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 61/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 62/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Channel 63/64 : 0
lrdn0080:2202741:2202851 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0080:2202741:2202851 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0080:2202741:2202851 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0080:2202741:2202858 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn0080:2202741:2202859 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1
lrdn0080:2202741:2202851 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0080:2202741:2202851 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0080:2202741:2202851 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0080:2202741:2202851 [0] NCCL INFO ncclCommInitRankConfig comm 0xd25ad80 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x2c16eb2d15ef84e6 - Init COMPLETE
lrdn0080:2202741:2202851 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:48:59,159 | xffl.distributed.distributed |    DEBUG | [Rank 28]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:48:59,177 | xffl.distributed.distributed |    DEBUG | [Rank 186]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0228:333030:333030 [0] NCCL INFO Bootstrap: Using ib0:10.128.9.177<0>
lrdn0228:333030:333030 [0] NCCL INFO cudaDriverVersion 12020
lrdn0228:333030:333030 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0228:333030:333030 [0] NCCL INFO Comm config Blocking set to 1
lrdn1029:3237462:3237462 [0] NCCL INFO Bootstrap: Using ib0:10.128.22.53<0>
lrdn1029:3237462:3237462 [0] NCCL INFO cudaDriverVersion 12020
lrdn1029:3237462:3237462 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1029:3237462:3237462 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:48:59,626 | xffl.distributed.distributed |    DEBUG | [Rank 226]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0228:333030:333139 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0228:333030:333139 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:48:59,680 | xffl.distributed.distributed |    DEBUG | [Rank 68]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1029:3237462:3237568 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1029:3237462:3237568 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0228:333030:333139 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.9.177<0>
lrdn0228:333030:333139 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0228:333030:333139 [0] NCCL INFO Using network IB
lrdn0228:333030:333139 [0] NCCL INFO ncclCommInitRankConfig comm 0xcddb780 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x935a0fe6ea02a931 - Init START
lrdn0228:333030:333139 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0228:333030:333139 [0] NCCL INFO Bootstrap timings total 0.000427 (create 0.000024, send 0.000071, recv 0.000095, ring 0.000001, delay 0.000000)
lrdn1029:3237462:3237568 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.22.53<0>
lrdn1029:3237462:3237568 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1029:3237462:3237568 [0] NCCL INFO Using network IB
lrdn1029:3237462:3237568 [0] NCCL INFO ncclCommInitRankConfig comm 0xd6d2b50 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7d3863e2a11f354d - Init START
lrdn1029:3237462:3237568 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1029:3237462:3237568 [0] NCCL INFO Bootstrap timings total 0.000534 (create 0.000023, send 0.000072, recv 0.000204, ring 0.000001, delay 0.000000)
lrdn0228:333030:333139 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0228:333030:333139 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0228:333030:333139 [0] NCCL INFO comm 0xcddb780 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 00/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 01/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 02/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 03/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 04/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 05/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 06/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 07/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 08/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 09/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 10/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 11/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 12/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 13/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 14/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 15/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 16/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 17/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 18/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 19/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 20/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 21/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 22/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 23/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 24/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 25/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 26/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 27/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 28/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 29/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 30/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 31/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 32/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 33/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 34/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 35/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 36/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 37/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 38/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 39/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 40/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 41/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 42/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 43/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 44/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 45/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 46/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 47/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 48/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 49/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 50/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 51/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 52/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 53/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 54/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 55/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 56/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 57/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 58/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 59/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 60/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 61/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 62/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Channel 63/64 : 0
lrdn0228:333030:333139 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0228:333030:333139 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0228:333030:333139 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0228:333030:333145 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn0228:333030:333146 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1
lrdn1029:3237462:3237568 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1029:3237462:3237568 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1029:3237462:3237568 [0] NCCL INFO comm 0xd6d2b50 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 00/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 01/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 02/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 03/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 04/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 05/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 06/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 07/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 08/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 09/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 10/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 11/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 12/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 13/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 14/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 15/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 16/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 17/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 18/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 19/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 20/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 21/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 22/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 23/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 24/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 25/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 26/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 27/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 28/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 29/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 30/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 31/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 32/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 33/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 34/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 35/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 36/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 37/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 38/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 39/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 40/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 41/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 42/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 43/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 44/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 45/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 46/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 47/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 48/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 49/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 50/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 51/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 52/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 53/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 54/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 55/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 56/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 57/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 58/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 59/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 60/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 61/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 62/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Channel 63/64 : 0
lrdn1029:3237462:3237568 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1029:3237462:3237568 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1029:3237462:3237568 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1029:3237462:3237574 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn1029:3237462:3237575 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn0228:333030:333139 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0228:333030:333139 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0228:333030:333139 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0228:333030:333139 [0] NCCL INFO ncclCommInitRankConfig comm 0xcddb780 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x935a0fe6ea02a931 - Init COMPLETE
lrdn0228:333030:333139 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1029:3237462:3237568 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1029:3237462:3237568 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1029:3237462:3237568 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1029:3237462:3237568 [0] NCCL INFO ncclCommInitRankConfig comm 0xd6d2b50 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7d3863e2a11f354d - Init COMPLETE
lrdn1029:3237462:3237568 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1232:2814890:2814890 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.97<0>
lrdn1232:2814890:2814890 [0] NCCL INFO cudaDriverVersion 12020
lrdn1232:2814890:2814890 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1232:2814890:2814890 [0] NCCL INFO Comm config Blocking set to 1
lrdn0486:1880055:1880055 [0] NCCL INFO Bootstrap: Using ib0:10.128.13.185<0>
lrdn0486:1880055:1880055 [0] NCCL INFO cudaDriverVersion 12020
lrdn0486:1880055:1880055 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0486:1880055:1880055 [0] NCCL INFO Comm config Blocking set to 1
lrdn1232:2814890:2815050 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1232:2814890:2815050 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:00,206 | xffl.distributed.distributed |    DEBUG | [Rank 126]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1232:2814890:2815050 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.97<0>
lrdn1232:2814890:2815050 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1232:2814890:2815050 [0] NCCL INFO Using network IB
lrdn1232:2814890:2815050 [0] NCCL INFO ncclCommInitRankConfig comm 0xe8cd570 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc840bbeb97fe5f17 - Init START
lrdn1232:2814890:2815050 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1232:2814890:2815050 [0] NCCL INFO Bootstrap timings total 0.000603 (create 0.000025, send 0.000070, recv 0.000270, ring 0.000001, delay 0.000000)
lrdn1232:2814890:2815050 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1232:2814890:2815050 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1232:2814890:2815050 [0] NCCL INFO comm 0xe8cd570 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 00/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 01/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 02/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 03/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 04/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 05/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 06/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 07/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 08/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 09/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 10/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 11/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 12/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 13/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 14/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 15/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 16/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 17/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 18/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 19/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 20/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 21/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 22/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 23/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 24/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 25/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 26/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 27/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 28/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 29/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 30/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 31/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 32/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 33/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 34/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 35/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 36/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 37/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 38/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 39/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 40/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 41/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 42/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 43/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 44/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 45/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 46/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 47/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 48/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 49/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 50/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 51/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 52/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 53/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 54/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 55/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 56/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 57/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 58/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 59/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 60/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 61/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 62/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Channel 63/64 : 0
lrdn1232:2814890:2815050 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1232:2814890:2815050 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1232:2814890:2815050 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1232:2814890:2815057 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1232:2814890:2815056 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0486:1880055:1880216 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0486:1880055:1880216 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1232:2814890:2815050 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1232:2814890:2815050 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1232:2814890:2815050 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1232:2814890:2815050 [0] NCCL INFO ncclCommInitRankConfig comm 0xe8cd570 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc840bbeb97fe5f17 - Init COMPLETE
lrdn1232:2814890:2815050 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0486:1880055:1880216 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.13.185<0>
lrdn0486:1880055:1880216 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0486:1880055:1880216 [0] NCCL INFO Using network IB
lrdn0486:1880055:1880216 [0] NCCL INFO ncclCommInitRankConfig comm 0xcdea9d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf1b9cd654dde1290 - Init START
lrdn0486:1880055:1880216 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0486:1880055:1880216 [0] NCCL INFO Bootstrap timings total 0.000445 (create 0.000025, send 0.000072, recv 0.000103, ring 0.000001, delay 0.000000)
[38;5;39m2025-08-02 08:49:00,340 | xffl.distributed.distributed |    DEBUG | [Rank 112]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0486:1880055:1880216 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0486:1880055:1880216 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0486:1880055:1880216 [0] NCCL INFO comm 0xcdea9d0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 00/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 01/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 02/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 03/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 04/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 05/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 06/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 07/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 08/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 09/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 10/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 11/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 12/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 13/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 14/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 15/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 16/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 17/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 18/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 19/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 20/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 21/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 22/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 23/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 24/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 25/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 26/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 27/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 28/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 29/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 30/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 31/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 32/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 33/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 34/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 35/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 36/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 37/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 38/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 39/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 40/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 41/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 42/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 43/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 44/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 45/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 46/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 47/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 48/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 49/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 50/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 51/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 52/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 53/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 54/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 55/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 56/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 57/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 58/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 59/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 60/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 61/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 62/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Channel 63/64 : 0
lrdn0486:1880055:1880216 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0486:1880055:1880216 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0486:1880055:1880216 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0486:1880055:1880222 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn0486:1880055:1880223 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0486:1880055:1880216 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0486:1880055:1880216 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0486:1880055:1880216 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0486:1880055:1880216 [0] NCCL INFO ncclCommInitRankConfig comm 0xcdea9d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf1b9cd654dde1290 - Init COMPLETE
lrdn0486:1880055:1880216 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn0740:4052818:4052818 [0] NCCL INFO Bootstrap: Using ib0:10.128.17.177<0>
lrdn0740:4052818:4052818 [0] NCCL INFO cudaDriverVersion 12020
lrdn0740:4052818:4052818 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0740:4052818:4052818 [0] NCCL INFO Comm config Blocking set to 1
lrdn0740:4052818:4052924 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0740:4052818:4052924 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0676:1759834:1759834 [0] NCCL INFO Bootstrap: Using ib0:10.128.16.177<0>
lrdn0676:1759834:1759834 [0] NCCL INFO cudaDriverVersion 12020
lrdn0676:1759834:1759834 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0676:1759834:1759834 [0] NCCL INFO Comm config Blocking set to 1
lrdn0740:4052818:4052924 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.17.177<0>
lrdn0740:4052818:4052924 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0740:4052818:4052924 [0] NCCL INFO Using network IB
lrdn0740:4052818:4052924 [0] NCCL INFO ncclCommInitRankConfig comm 0xcc33130 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x945c9913e8557bca - Init START
lrdn0740:4052818:4052924 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0740:4052818:4052924 [0] NCCL INFO Bootstrap timings total 0.000398 (create 0.000023, send 0.000063, recv 0.000090, ring 0.000001, delay 0.000000)
lrdn0740:4052818:4052924 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0740:4052818:4052924 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0740:4052818:4052924 [0] NCCL INFO comm 0xcc33130 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 00/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 01/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 02/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 03/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 04/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 05/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 06/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 07/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 08/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 09/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 10/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 11/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 12/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 13/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 14/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 15/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 16/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 17/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 18/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 19/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 20/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 21/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 22/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 23/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 24/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 25/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 26/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 27/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 28/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 29/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 30/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 31/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 32/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 33/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 34/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 35/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 36/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 37/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 38/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 39/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 40/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 41/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 42/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 43/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 44/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 45/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 46/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 47/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 48/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 49/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 50/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 51/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 52/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 53/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 54/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 55/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 56/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 57/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 58/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 59/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 60/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 61/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 62/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Channel 63/64 : 0
lrdn0740:4052818:4052924 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0740:4052818:4052924 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0740:4052818:4052924 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0740:4052818:4052930 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn0740:4052818:4052931 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0740:4052818:4052924 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0740:4052818:4052924 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0740:4052818:4052924 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0740:4052818:4052924 [0] NCCL INFO ncclCommInitRankConfig comm 0xcc33130 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x945c9913e8557bca - Init COMPLETE
lrdn0740:4052818:4052924 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0676:1759834:1759931 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0676:1759834:1759931 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0676:1759834:1759931 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.16.177<0>
lrdn0676:1759834:1759931 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0676:1759834:1759931 [0] NCCL INFO Using network IB
lrdn0676:1759834:1759931 [0] NCCL INFO ncclCommInitRankConfig comm 0xc5c70a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc1f3b2462f5fe6a4 - Init START
lrdn0676:1759834:1759931 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0676:1759834:1759931 [0] NCCL INFO Bootstrap timings total 0.000395 (create 0.000019, send 0.000061, recv 0.000094, ring 0.000001, delay 0.000000)
lrdn0676:1759834:1759931 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0676:1759834:1759931 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0676:1759834:1759931 [0] NCCL INFO comm 0xc5c70a0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 00/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 01/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 02/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 03/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 04/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 05/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 06/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 07/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 08/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 09/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 10/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 11/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 12/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 13/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 14/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 15/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 16/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 17/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 18/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 19/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 20/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 21/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 22/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 23/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 24/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 25/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 26/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 27/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 28/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 29/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 30/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 31/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 32/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 33/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 34/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 35/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 36/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 37/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 38/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 39/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 40/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 41/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 42/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 43/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 44/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 45/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 46/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 47/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 48/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 49/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 50/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 51/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 52/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 53/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 54/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 55/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 56/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 57/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 58/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 59/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 60/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 61/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 62/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Channel 63/64 : 0
lrdn0676:1759834:1759931 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0676:1759834:1759931 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0676:1759834:1759931 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0676:1759834:1759937 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0676:1759834:1759938 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0676:1759834:1759931 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0676:1759834:1759931 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0676:1759834:1759931 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0676:1759834:1759931 [0] NCCL INFO ncclCommInitRankConfig comm 0xc5c70a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc1f3b2462f5fe6a4 - Init COMPLETE
lrdn0676:1759834:1759931 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:01,269 | xffl.distributed.distributed |    DEBUG | [Rank 29]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:01,329 | xffl.distributed.distributed |    DEBUG | [Rank 202]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0230:167967:167967 [0] NCCL INFO Bootstrap: Using ib0:10.128.9.185<0>
lrdn0230:167967:167967 [0] NCCL INFO cudaDriverVersion 12020
lrdn0230:167967:167967 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0230:167967:167967 [0] NCCL INFO Comm config Blocking set to 1
lrdn1111:1712948:1712948 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.125<0>
lrdn1111:1712948:1712948 [0] NCCL INFO cudaDriverVersion 12020
lrdn1111:1712948:1712948 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1111:1712948:1712948 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:01,729 | xffl.distributed.distributed |    DEBUG | [Rank 173]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:01,744 | xffl.distributed.distributed |    DEBUG | [Rank 23]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0230:167967:168072 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0230:167967:168072 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:01,817 | xffl.distributed.distributed |    DEBUG | [Rank 116]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0230:167967:168072 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.9.185<0>
lrdn1111:1712948:1713053 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1111:1712948:1713053 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0230:167967:168072 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0230:167967:168072 [0] NCCL INFO Using network IB
lrdn0230:167967:168072 [0] NCCL INFO ncclCommInitRankConfig comm 0xcd6e250 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xba8f78b5036df81f - Init START
lrdn0230:167967:168072 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0230:167967:168072 [0] NCCL INFO Bootstrap timings total 0.000383 (create 0.000021, send 0.000059, recv 0.000082, ring 0.000001, delay 0.000000)
lrdn0230:167967:168072 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0230:167967:168072 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0230:167967:168072 [0] NCCL INFO comm 0xcd6e250 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 00/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 01/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 02/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 03/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 04/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 05/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 06/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 07/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 08/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 09/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 10/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 11/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 12/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 13/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 14/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 15/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 16/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 17/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 18/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 19/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 20/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 21/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 22/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 23/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 24/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 25/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 26/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 27/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 28/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 29/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 30/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 31/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 32/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 33/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 34/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 35/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 36/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 37/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 38/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 39/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 40/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 41/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 42/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 43/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 44/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 45/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 46/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 47/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 48/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 49/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 50/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 51/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 52/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 53/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 54/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 55/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 56/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 57/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 58/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 59/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 60/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 61/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 62/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Channel 63/64 : 0
lrdn0230:167967:168072 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0230:167967:168072 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0230:167967:168072 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0230:167967:168078 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn0230:167967:168079 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0230:167967:168072 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0230:167967:168072 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0230:167967:168072 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0230:167967:168072 [0] NCCL INFO ncclCommInitRankConfig comm 0xcd6e250 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xba8f78b5036df81f - Init COMPLETE
lrdn0230:167967:168072 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.02, rest 0.00)
lrdn1111:1712948:1713053 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.125<0>
lrdn1111:1712948:1713053 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1111:1712948:1713053 [0] NCCL INFO Using network IB
lrdn1111:1712948:1713053 [0] NCCL INFO ncclCommInitRankConfig comm 0xdf6cf90 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x84173958ac6a471d - Init START
lrdn1111:1712948:1713053 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1111:1712948:1713053 [0] NCCL INFO Bootstrap timings total 0.000422 (create 0.000023, send 0.000071, recv 0.000100, ring 0.000001, delay 0.000000)
lrdn1111:1712948:1713053 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1111:1712948:1713053 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1111:1712948:1713053 [0] NCCL INFO comm 0xdf6cf90 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 00/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 01/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 02/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 03/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 04/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 05/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 06/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 07/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 08/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 09/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 10/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 11/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 12/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 13/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 14/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 15/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 16/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 17/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 18/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 19/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 20/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 21/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 22/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 23/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 24/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 25/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 26/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 27/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 28/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 29/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 30/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 31/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 32/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 33/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 34/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 35/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 36/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 37/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 38/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 39/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 40/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 41/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 42/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 43/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 44/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 45/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 46/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 47/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 48/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 49/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 50/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 51/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 52/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 53/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 54/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 55/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 56/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 57/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 58/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 59/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 60/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 61/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 62/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Channel 63/64 : 0
lrdn1111:1712948:1713053 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1111:1712948:1713053 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1111:1712948:1713053 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1111:1712948:1713059 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn1111:1712948:1713060 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn1111:1712948:1713053 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1111:1712948:1713053 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1111:1712948:1713053 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1111:1712948:1713053 [0] NCCL INFO ncclCommInitRankConfig comm 0xdf6cf90 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x84173958ac6a471d - Init COMPLETE
lrdn1111:1712948:1713053 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:02,187 | xffl.distributed.distributed |    DEBUG | [Rank 127]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0965:1473702:1473702 [0] NCCL INFO Bootstrap: Using ib0:10.128.21.53<0>
lrdn0965:1473702:1473702 [0] NCCL INFO cudaDriverVersion 12020
lrdn0965:1473702:1473702 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0965:1473702:1473702 [0] NCCL INFO Comm config Blocking set to 1
lrdn0200:1632217:1632217 [0] NCCL INFO Bootstrap: Using ib0:10.128.9.65<0>
lrdn0200:1632217:1632217 [0] NCCL INFO cudaDriverVersion 12020
lrdn0200:1632217:1632217 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0200:1632217:1632217 [0] NCCL INFO Comm config Blocking set to 1
lrdn0696:1664741:1664741 [0] NCCL INFO Bootstrap: Using ib0:10.128.17.1<0>
lrdn0696:1664741:1664741 [0] NCCL INFO cudaDriverVersion 12020
lrdn0696:1664741:1664741 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0696:1664741:1664741 [0] NCCL INFO Comm config Blocking set to 1
lrdn0965:1473702:1473805 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0965:1473702:1473805 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0200:1632217:1632312 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0200:1632217:1632312 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0965:1473702:1473805 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.21.53<0>
lrdn0200:1632217:1632312 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.9.65<0>
lrdn0965:1473702:1473805 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0965:1473702:1473805 [0] NCCL INFO Using network IB
lrdn0200:1632217:1632312 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0200:1632217:1632312 [0] NCCL INFO Using network IB
lrdn0965:1473702:1473805 [0] NCCL INFO ncclCommInitRankConfig comm 0xe416f80 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x315e280a9f8204c9 - Init START
lrdn0200:1632217:1632312 [0] NCCL INFO ncclCommInitRankConfig comm 0xe19f880 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x508ac30e4336bd89 - Init START
lrdn0965:1473702:1473805 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0965:1473702:1473805 [0] NCCL INFO Bootstrap timings total 0.000420 (create 0.000024, send 0.000067, recv 0.000106, ring 0.000001, delay 0.000001)
lrdn0200:1632217:1632312 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0200:1632217:1632312 [0] NCCL INFO Bootstrap timings total 0.000434 (create 0.000022, send 0.000071, recv 0.000112, ring 0.000001, delay 0.000000)
lrdn0200:1632217:1632312 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0200:1632217:1632312 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0965:1473702:1473805 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0965:1473702:1473805 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0200:1632217:1632312 [0] NCCL INFO comm 0xe19f880 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 00/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 01/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 02/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 03/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO comm 0xe416f80 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 00/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 01/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 02/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 03/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 04/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 05/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 06/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 07/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 08/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 09/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 10/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 11/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 12/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 13/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 04/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 05/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 06/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 07/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 08/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 14/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 15/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 16/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 17/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 18/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 19/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 20/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 21/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 22/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 23/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 24/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 25/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 26/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 27/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 28/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 29/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 30/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 31/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 32/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 33/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 34/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 35/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 36/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 37/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 38/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 39/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 40/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 41/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 09/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 10/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 11/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 12/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 13/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 14/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 15/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 16/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 17/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 18/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 19/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 20/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 21/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 22/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 23/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 24/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 25/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 26/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 27/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 28/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 29/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 30/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 31/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 32/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 33/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 34/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 35/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 36/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 37/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 38/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 39/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 40/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 41/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 42/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 43/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 44/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 45/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 46/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 47/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 48/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 49/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 50/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 51/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 52/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 53/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 54/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 55/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 56/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 57/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 58/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 59/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 60/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 61/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 62/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Channel 63/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 42/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 43/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 44/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 45/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 46/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 47/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 48/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 49/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 50/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 51/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 52/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 53/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 54/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 55/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 56/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 57/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 58/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 59/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 60/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 61/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 62/64 : 0
lrdn0200:1632217:1632312 [0] NCCL INFO Channel 63/64 : 0
lrdn0965:1473702:1473805 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0965:1473702:1473805 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0965:1473702:1473805 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0200:1632217:1632312 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0200:1632217:1632312 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0200:1632217:1632312 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0965:1473702:1473811 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0965:1473702:1473812 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0200:1632217:1632319 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1
lrdn0200:1632217:1632318 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn0200:1632217:1632312 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0200:1632217:1632312 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0965:1473702:1473805 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0965:1473702:1473805 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0200:1632217:1632312 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0200:1632217:1632312 [0] NCCL INFO ncclCommInitRankConfig comm 0xe19f880 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x508ac30e4336bd89 - Init COMPLETE
lrdn0200:1632217:1632312 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0965:1473702:1473805 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0965:1473702:1473805 [0] NCCL INFO ncclCommInitRankConfig comm 0xe416f80 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x315e280a9f8204c9 - Init COMPLETE
lrdn0965:1473702:1473805 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.27 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0696:1664741:1664851 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0696:1664741:1664851 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0696:1664741:1664851 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.17.1<0>
lrdn0696:1664741:1664851 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0696:1664741:1664851 [0] NCCL INFO Using network IB
lrdn0696:1664741:1664851 [0] NCCL INFO ncclCommInitRankConfig comm 0x165c2e60 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb39dc6d2e98c7e4e - Init START
lrdn0696:1664741:1664851 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0696:1664741:1664851 [0] NCCL INFO Bootstrap timings total 0.000436 (create 0.000024, send 0.000073, recv 0.000101, ring 0.000001, delay 0.000001)
lrdn0696:1664741:1664851 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0696:1664741:1664851 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0696:1664741:1664851 [0] NCCL INFO comm 0x165c2e60 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 00/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 01/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 02/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 03/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 04/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 05/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 06/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 07/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 08/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 09/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 10/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 11/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 12/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 13/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 14/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 15/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 16/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 17/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 18/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 19/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 20/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 21/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 22/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 23/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 24/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 25/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 26/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 27/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 28/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 29/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 30/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 31/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 32/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 33/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 34/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 35/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 36/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 37/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 38/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 39/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 40/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 41/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 42/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 43/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 44/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 45/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 46/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 47/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 48/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 49/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 50/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 51/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 52/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 53/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 54/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 55/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 56/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 57/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 58/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 59/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 60/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 61/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 62/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Channel 63/64 : 0
lrdn0696:1664741:1664851 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0696:1664741:1664851 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0696:1664741:1664851 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0696:1664741:1664857 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0696:1664741:1664858 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0696:1664741:1664851 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0696:1664741:1664851 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0696:1664741:1664851 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0696:1664741:1664851 [0] NCCL INFO ncclCommInitRankConfig comm 0x165c2e60 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb39dc6d2e98c7e4e - Init COMPLETE
lrdn0696:1664741:1664851 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0746:1587636:1587636 [0] NCCL INFO Bootstrap: Using ib0:10.128.17.201<0>
lrdn0746:1587636:1587636 [0] NCCL INFO cudaDriverVersion 12020
lrdn0746:1587636:1587636 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0746:1587636:1587636 [0] NCCL INFO Comm config Blocking set to 1
lrdn0746:1587636:1587731 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0746:1587636:1587731 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0746:1587636:1587731 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.17.201<0>
lrdn0746:1587636:1587731 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0746:1587636:1587731 [0] NCCL INFO Using network IB
lrdn0746:1587636:1587731 [0] NCCL INFO ncclCommInitRankConfig comm 0xdb93850 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x2e5f1395b0314242 - Init START
lrdn0746:1587636:1587731 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0746:1587636:1587731 [0] NCCL INFO Bootstrap timings total 0.000562 (create 0.000023, send 0.000068, recv 0.000242, ring 0.000001, delay 0.000000)
lrdn0746:1587636:1587731 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0746:1587636:1587731 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0746:1587636:1587731 [0] NCCL INFO comm 0xdb93850 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 00/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 01/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 02/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 03/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 04/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 05/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 06/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 07/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 08/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 09/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 10/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 11/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 12/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 13/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 14/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 15/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 16/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 17/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 18/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 19/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 20/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 21/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 22/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 23/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 24/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 25/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 26/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 27/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 28/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 29/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 30/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 31/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 32/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 33/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 34/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 35/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 36/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 37/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 38/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 39/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 40/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 41/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 42/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 43/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 44/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 45/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 46/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 47/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 48/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 49/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 50/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 51/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 52/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 53/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 54/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 55/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 56/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 57/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 58/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 59/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 60/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 61/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 62/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Channel 63/64 : 0
lrdn0746:1587636:1587731 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0746:1587636:1587731 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0746:1587636:1587731 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0746:1587636:1587737 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn0746:1587636:1587738 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0746:1587636:1587731 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0746:1587636:1587731 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0746:1587636:1587731 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0746:1587636:1587731 [0] NCCL INFO ncclCommInitRankConfig comm 0xdb93850 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x2e5f1395b0314242 - Init COMPLETE
lrdn0746:1587636:1587731 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
[38;5;39m2025-08-02 08:49:03,135 | xffl.distributed.distributed |    DEBUG | [Rank 177]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:03,371 | xffl.distributed.distributed |    DEBUG | [Rank 214]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0981:1717822:1717822 [0] NCCL INFO Bootstrap: Using ib0:10.128.21.117<0>
lrdn0981:1717822:1717822 [0] NCCL INFO cudaDriverVersion 12020
lrdn0981:1717822:1717822 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0981:1717822:1717822 [0] NCCL INFO Comm config Blocking set to 1
lrdn0981:1717822:1717928 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0981:1717822:1717928 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0981:1717822:1717928 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.21.117<0>
lrdn0981:1717822:1717928 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0981:1717822:1717928 [0] NCCL INFO Using network IB
lrdn0981:1717822:1717928 [0] NCCL INFO ncclCommInitRankConfig comm 0xcf6fbe0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc92294e543cc6b8b - Init START
lrdn0981:1717822:1717928 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0981:1717822:1717928 [0] NCCL INFO Bootstrap timings total 0.000440 (create 0.000025, send 0.000065, recv 0.000110, ring 0.000002, delay 0.000000)
lrdn1171:1612854:1612854 [0] NCCL INFO Bootstrap: Using ib0:10.128.24.109<0>
lrdn1171:1612854:1612854 [0] NCCL INFO cudaDriverVersion 12020
lrdn1171:1612854:1612854 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1171:1612854:1612854 [0] NCCL INFO Comm config Blocking set to 1
lrdn0981:1717822:1717928 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0981:1717822:1717928 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0981:1717822:1717928 [0] NCCL INFO comm 0xcf6fbe0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 00/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 01/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 02/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 03/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 04/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 05/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 06/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 07/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 08/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 09/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 10/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 11/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 12/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 13/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 14/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 15/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 16/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 17/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 18/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 19/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 20/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 21/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 22/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 23/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 24/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 25/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 26/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 27/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 28/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 29/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 30/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 31/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 32/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 33/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 34/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 35/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 36/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 37/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 38/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 39/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 40/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 41/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 42/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 43/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 44/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 45/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 46/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 47/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 48/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 49/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 50/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 51/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 52/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 53/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 54/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 55/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 56/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 57/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 58/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 59/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 60/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 61/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 62/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Channel 63/64 : 0
lrdn0981:1717822:1717928 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0981:1717822:1717928 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0981:1717822:1717928 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0981:1717822:1717934 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn0981:1717822:1717935 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 12
lrdn0981:1717822:1717928 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0981:1717822:1717928 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0981:1717822:1717928 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0981:1717822:1717928 [0] NCCL INFO ncclCommInitRankConfig comm 0xcf6fbe0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc92294e543cc6b8b - Init COMPLETE
lrdn0981:1717822:1717928 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:03,822 | xffl.distributed.distributed |    DEBUG | [Rank 218]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1171:1612854:1612951 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1171:1612854:1612951 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:03,923 | xffl.distributed.distributed |    DEBUG | [Rank 121]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1171:1612854:1612951 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.24.109<0>
lrdn1171:1612854:1612951 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1171:1612854:1612951 [0] NCCL INFO Using network IB
lrdn1171:1612854:1612951 [0] NCCL INFO ncclCommInitRankConfig comm 0xd067130 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x251c16e0109b1b69 - Init START
lrdn1171:1612854:1612951 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1171:1612854:1612951 [0] NCCL INFO Bootstrap timings total 0.000444 (create 0.000023, send 0.000068, recv 0.000107, ring 0.000001, delay 0.000000)
lrdn1171:1612854:1612951 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1171:1612854:1612951 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1171:1612854:1612951 [0] NCCL INFO comm 0xd067130 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 00/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 01/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 02/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 03/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 04/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 05/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 06/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 07/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 08/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 09/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 10/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 11/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 12/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 13/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 14/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 15/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 16/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 17/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 18/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 19/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 20/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 21/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 22/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 23/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 24/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 25/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 26/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 27/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 28/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 29/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 30/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 31/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 32/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 33/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 34/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 35/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 36/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 37/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 38/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 39/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 40/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 41/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 42/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 43/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 44/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 45/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 46/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 47/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 48/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 49/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 50/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 51/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 52/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 53/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 54/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 55/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 56/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 57/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 58/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 59/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 60/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 61/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 62/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Channel 63/64 : 0
lrdn1171:1612854:1612951 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1171:1612854:1612951 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1171:1612854:1612951 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1171:1612854:1612957 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1171:1612854:1612958 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1171:1612854:1612951 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1171:1612854:1612951 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1171:1612854:1612951 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1171:1612854:1612951 [0] NCCL INFO ncclCommInitRankConfig comm 0xd067130 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x251c16e0109b1b69 - Init COMPLETE
lrdn1171:1612854:1612951 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1186:2692910:2692910 [0] NCCL INFO Bootstrap: Using ib0:10.128.24.169<0>
lrdn1186:2692910:2692910 [0] NCCL INFO cudaDriverVersion 12020
lrdn1186:2692910:2692910 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1186:2692910:2692910 [0] NCCL INFO Comm config Blocking set to 1
lrdn0716:1747185:1747185 [0] NCCL INFO Bootstrap: Using ib0:10.128.17.81<0>
lrdn0716:1747185:1747185 [0] NCCL INFO cudaDriverVersion 12020
lrdn0716:1747185:1747185 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0716:1747185:1747185 [0] NCCL INFO Comm config Blocking set to 1
lrdn1186:2692910:2693008 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1186:2692910:2693008 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1186:2692910:2693008 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.24.169<0>
lrdn1186:2692910:2693008 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1186:2692910:2693008 [0] NCCL INFO Using network IB
lrdn1186:2692910:2693008 [0] NCCL INFO ncclCommInitRankConfig comm 0xc9c93e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x2515dc099bcf43ba - Init START
lrdn1186:2692910:2693008 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1186:2692910:2693008 [0] NCCL INFO Bootstrap timings total 0.000393 (create 0.000019, send 0.000054, recv 0.000094, ring 0.000001, delay 0.000000)
lrdn1186:2692910:2693008 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1186:2692910:2693008 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1186:2692910:2693008 [0] NCCL INFO comm 0xc9c93e0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 00/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 01/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 02/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 03/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 04/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 05/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 06/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 07/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 08/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 09/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 10/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 11/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 12/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 13/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 14/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 15/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 16/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 17/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 18/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 19/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 20/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 21/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 22/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 23/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 24/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 25/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 26/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 27/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 28/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 29/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 30/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 31/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 32/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 33/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 34/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 35/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 36/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 37/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 38/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 39/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 40/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 41/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 42/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 43/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 44/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 45/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 46/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 47/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 48/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 49/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 50/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 51/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 52/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 53/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 54/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 55/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 56/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 57/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 58/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 59/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 60/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 61/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 62/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Channel 63/64 : 0
lrdn1186:2692910:2693008 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1186:2692910:2693008 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1186:2692910:2693008 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1186:2692910:2693014 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn1186:2692910:2693015 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 15
lrdn1186:2692910:2693008 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1186:2692910:2693008 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1186:2692910:2693008 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1186:2692910:2693008 [0] NCCL INFO ncclCommInitRankConfig comm 0xc9c93e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x2515dc099bcf43ba - Init COMPLETE
lrdn1186:2692910:2693008 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0716:1747185:1747289 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0716:1747185:1747289 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0716:1747185:1747289 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.17.81<0>
lrdn0716:1747185:1747289 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0716:1747185:1747289 [0] NCCL INFO Using network IB
lrdn0716:1747185:1747289 [0] NCCL INFO ncclCommInitRankConfig comm 0xec79250 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9ece56df05f4d13 - Init START
lrdn0716:1747185:1747289 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0716:1747185:1747289 [0] NCCL INFO Bootstrap timings total 0.000495 (create 0.000020, send 0.000063, recv 0.000196, ring 0.000001, delay 0.000000)
lrdn0716:1747185:1747289 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0716:1747185:1747289 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0716:1747185:1747289 [0] NCCL INFO comm 0xec79250 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 00/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 01/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 02/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 03/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 04/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 05/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 06/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 07/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 08/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 09/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 10/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 11/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 12/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 13/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 14/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 15/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 16/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 17/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 18/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 19/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 20/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 21/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 22/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 23/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 24/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 25/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 26/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 27/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 28/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 29/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 30/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 31/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 32/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 33/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 34/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 35/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 36/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 37/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 38/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 39/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 40/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 41/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 42/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 43/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 44/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 45/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 46/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 47/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 48/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 49/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 50/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 51/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 52/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 53/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 54/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 55/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 56/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 57/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 58/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 59/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 60/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 61/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 62/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Channel 63/64 : 0
lrdn0716:1747185:1747289 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0716:1747185:1747289 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0716:1747185:1747289 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0716:1747185:1747295 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0716:1747185:1747296 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 15
lrdn0716:1747185:1747289 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0716:1747185:1747289 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0716:1747185:1747289 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0716:1747185:1747289 [0] NCCL INFO ncclCommInitRankConfig comm 0xec79250 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9ece56df05f4d13 - Init COMPLETE
lrdn0716:1747185:1747289 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:04,798 | xffl.distributed.distributed |    DEBUG | [Rank 22]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0194:3045078:3045078 [0] NCCL INFO Bootstrap: Using ib0:10.128.9.41<0>
lrdn0194:3045078:3045078 [0] NCCL INFO cudaDriverVersion 12020
lrdn0194:3045078:3045078 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0194:3045078:3045078 [0] NCCL INFO Comm config Blocking set to 1
lrdn0194:3045078:3045175 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0194:3045078:3045175 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0194:3045078:3045175 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.9.41<0>
lrdn0194:3045078:3045175 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0194:3045078:3045175 [0] NCCL INFO Using network IB
lrdn0194:3045078:3045175 [0] NCCL INFO ncclCommInitRankConfig comm 0xe13f150 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9748282681249793 - Init START
lrdn0194:3045078:3045175 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0194:3045078:3045175 [0] NCCL INFO Bootstrap timings total 0.000389 (create 0.000019, send 0.000061, recv 0.000091, ring 0.000001, delay 0.000000)
lrdn0194:3045078:3045175 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0194:3045078:3045175 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0194:3045078:3045175 [0] NCCL INFO comm 0xe13f150 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 00/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 01/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 02/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 03/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 04/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 05/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 06/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 07/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 08/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 09/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 10/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 11/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 12/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 13/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 14/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 15/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 16/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 17/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 18/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 19/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 20/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 21/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 22/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 23/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 24/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 25/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 26/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 27/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 28/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 29/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 30/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 31/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 32/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 33/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 34/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 35/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 36/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 37/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 38/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 39/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 40/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 41/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 42/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 43/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 44/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 45/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 46/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 47/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 48/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 49/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 50/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 51/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 52/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 53/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 54/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 55/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 56/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 57/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 58/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 59/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 60/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 61/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 62/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Channel 63/64 : 0
lrdn0194:3045078:3045175 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0194:3045078:3045175 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0194:3045078:3045175 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0194:3045078:3045181 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn0194:3045078:3045182 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0194:3045078:3045175 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0194:3045078:3045175 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0194:3045078:3045175 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0194:3045078:3045175 [0] NCCL INFO ncclCommInitRankConfig comm 0xe13f150 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9748282681249793 - Init COMPLETE
lrdn0194:3045078:3045175 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:05,561 | xffl.distributed.distributed |    DEBUG | [Rank 119]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:05,690 | xffl.distributed.distributed |    DEBUG | [Rank 6]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0705:1603790:1603790 [0] NCCL INFO Bootstrap: Using ib0:10.128.17.37<0>
lrdn0705:1603790:1603790 [0] NCCL INFO cudaDriverVersion 12020
lrdn0705:1603790:1603790 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0705:1603790:1603790 [0] NCCL INFO Comm config Blocking set to 1
lrdn0089:3581058:3581058 [0] NCCL INFO Bootstrap: Using ib0:10.128.7.133<0>
lrdn0089:3581058:3581058 [0] NCCL INFO cudaDriverVersion 12020
lrdn0089:3581058:3581058 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0089:3581058:3581058 [0] NCCL INFO Comm config Blocking set to 1
lrdn0705:1603790:1603888 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0705:1603790:1603888 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0705:1603790:1603888 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.17.37<0>
lrdn0705:1603790:1603888 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0705:1603790:1603888 [0] NCCL INFO Using network IB
lrdn0705:1603790:1603888 [0] NCCL INFO ncclCommInitRankConfig comm 0xeafcd10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x4d48ace43eacb0d6 - Init START
lrdn0705:1603790:1603888 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0705:1603790:1603888 [0] NCCL INFO Bootstrap timings total 0.000404 (create 0.000021, send 0.000060, recv 0.000107, ring 0.000001, delay 0.000000)
lrdn0705:1603790:1603888 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0705:1603790:1603888 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0705:1603790:1603888 [0] NCCL INFO comm 0xeafcd10 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 00/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 01/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 02/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 03/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 04/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 05/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 06/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 07/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 08/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 09/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 10/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 11/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 12/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 13/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 14/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 15/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 16/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 17/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 18/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 19/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 20/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 21/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 22/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 23/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 24/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 25/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 26/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 27/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 28/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 29/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 30/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 31/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 32/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 33/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 34/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 35/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 36/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 37/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 38/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 39/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 40/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 41/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 42/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 43/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 44/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 45/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 46/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 47/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 48/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 49/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 50/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 51/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 52/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 53/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 54/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 55/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 56/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 57/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 58/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 59/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 60/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 61/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 62/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Channel 63/64 : 0
lrdn0705:1603790:1603888 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0705:1603790:1603888 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0705:1603790:1603888 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0705:1603790:1603894 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0705:1603790:1603895 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0705:1603790:1603888 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0705:1603790:1603888 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0705:1603790:1603888 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0705:1603790:1603888 [0] NCCL INFO ncclCommInitRankConfig comm 0xeafcd10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x4d48ace43eacb0d6 - Init COMPLETE
lrdn0705:1603790:1603888 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn0089:3581058:3581155 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0089:3581058:3581155 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:06,217 | xffl.distributed.distributed |    DEBUG | [Rank 183]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0089:3581058:3581155 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.7.133<0>
lrdn0089:3581058:3581155 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0089:3581058:3581155 [0] NCCL INFO Using network IB
lrdn0089:3581058:3581155 [0] NCCL INFO ncclCommInitRankConfig comm 0xe42df50 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8bacb1bfd1ff0e3a - Init START
lrdn0089:3581058:3581155 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0089:3581058:3581155 [0] NCCL INFO Bootstrap timings total 0.000376 (create 0.000019, send 0.000064, recv 0.000078, ring 0.000001, delay 0.000001)
lrdn0089:3581058:3581155 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0089:3581058:3581155 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0089:3581058:3581155 [0] NCCL INFO comm 0xe42df50 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 00/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 01/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 02/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 03/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 04/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 05/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 06/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 07/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 08/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 09/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 10/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 11/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 12/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 13/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 14/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 15/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 16/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 17/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 18/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 19/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 20/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 21/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 22/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 23/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 24/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 25/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 26/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 27/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 28/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 29/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 30/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 31/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 32/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 33/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 34/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 35/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 36/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 37/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 38/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 39/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 40/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 41/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 42/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 43/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 44/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 45/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 46/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 47/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 48/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 49/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 50/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 51/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 52/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 53/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 54/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 55/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 56/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 57/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 58/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 59/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 60/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 61/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 62/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Channel 63/64 : 0
lrdn0089:3581058:3581155 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0089:3581058:3581155 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0089:3581058:3581155 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0089:3581058:3581162 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn0089:3581058:3581161 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0089:3581058:3581155 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0089:3581058:3581155 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0089:3581058:3581155 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0089:3581058:3581155 [0] NCCL INFO ncclCommInitRankConfig comm 0xe42df50 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8bacb1bfd1ff0e3a - Init COMPLETE
lrdn0089:3581058:3581155 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1005:1506067:1506067 [0] NCCL INFO Bootstrap: Using ib0:10.128.21.213<0>
lrdn1005:1506067:1506067 [0] NCCL INFO cudaDriverVersion 12020
lrdn1005:1506067:1506067 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1005:1506067:1506067 [0] NCCL INFO Comm config Blocking set to 1
lrdn1005:1506067:1506164 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1005:1506067:1506164 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1005:1506067:1506164 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.21.213<0>
lrdn1005:1506067:1506164 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1005:1506067:1506164 [0] NCCL INFO Using network IB
lrdn1005:1506067:1506164 [0] NCCL INFO ncclCommInitRankConfig comm 0xef4a470 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x6af21e658d7dd4bd - Init START
lrdn1005:1506067:1506164 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1005:1506067:1506164 [0] NCCL INFO Bootstrap timings total 0.000396 (create 0.000020, send 0.000061, recv 0.000102, ring 0.000001, delay 0.000001)
lrdn1005:1506067:1506164 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1005:1506067:1506164 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1005:1506067:1506164 [0] NCCL INFO comm 0xef4a470 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 00/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 01/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 02/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 03/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 04/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 05/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 06/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 07/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 08/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 09/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 10/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 11/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 12/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 13/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 14/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 15/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 16/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 17/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 18/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 19/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 20/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 21/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 22/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 23/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 24/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 25/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 26/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 27/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 28/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 29/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 30/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 31/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 32/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 33/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 34/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 35/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 36/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 37/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 38/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 39/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 40/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 41/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 42/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 43/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 44/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 45/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 46/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 47/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 48/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 49/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 50/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 51/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 52/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 53/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 54/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 55/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 56/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 57/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 58/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 59/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 60/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 61/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 62/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Channel 63/64 : 0
lrdn1005:1506067:1506164 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1005:1506067:1506164 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1005:1506067:1506164 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1005:1506067:1506171 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn1005:1506067:1506170 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn1005:1506067:1506164 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1005:1506067:1506164 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1005:1506067:1506164 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1005:1506067:1506164 [0] NCCL INFO ncclCommInitRankConfig comm 0xef4a470 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x6af21e658d7dd4bd - Init COMPLETE
lrdn1005:1506067:1506164 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:07,212 | xffl.distributed.distributed |    DEBUG | [Rank 66]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:07,547 | xffl.distributed.distributed |    DEBUG | [Rank 234]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0480:1479637:1479637 [0] NCCL INFO Bootstrap: Using ib0:10.128.13.161<0>
lrdn0480:1479637:1479637 [0] NCCL INFO cudaDriverVersion 12020
lrdn0480:1479637:1479637 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0480:1479637:1479637 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:07,571 | xffl.distributed.distributed |    DEBUG | [Rank 181]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0480:1479637:1479735 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0480:1479637:1479735 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:07,740 | xffl.distributed.distributed |    DEBUG | [Rank 230]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0480:1479637:1479735 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.13.161<0>
lrdn0480:1479637:1479735 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0480:1479637:1479735 [0] NCCL INFO Using network IB
lrdn0480:1479637:1479735 [0] NCCL INFO ncclCommInitRankConfig comm 0xd4b8ef0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc85fd2d1cb401930 - Init START
lrdn0480:1479637:1479735 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0480:1479637:1479735 [0] NCCL INFO Bootstrap timings total 0.000392 (create 0.000021, send 0.000063, recv 0.000084, ring 0.000001, delay 0.000000)
lrdn0480:1479637:1479735 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0480:1479637:1479735 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0480:1479637:1479735 [0] NCCL INFO comm 0xd4b8ef0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 00/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 01/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 02/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 03/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 04/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 05/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 06/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 07/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 08/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 09/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 10/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 11/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 12/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 13/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 14/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 15/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 16/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 17/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 18/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 19/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 20/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 21/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 22/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 23/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 24/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 25/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 26/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 27/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 28/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 29/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 30/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 31/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 32/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 33/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 34/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 35/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 36/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 37/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 38/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 39/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 40/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 41/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 42/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 43/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 44/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 45/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 46/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 47/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 48/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 49/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 50/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 51/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 52/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 53/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 54/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 55/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 56/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 57/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 58/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 59/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 60/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 61/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 62/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Channel 63/64 : 0
lrdn0480:1479637:1479735 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0480:1479637:1479735 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0480:1479637:1479735 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0480:1479637:1479741 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0480:1479637:1479742 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn0480:1479637:1479735 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0480:1479637:1479735 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0480:1479637:1479735 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0480:1479637:1479735 [0] NCCL INFO ncclCommInitRankConfig comm 0xd4b8ef0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc85fd2d1cb401930 - Init COMPLETE
lrdn0480:1479637:1479735 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.16, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1271:1865323:1865323 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.253<0>
lrdn1271:1865323:1865323 [0] NCCL INFO cudaDriverVersion 12020
lrdn1271:1865323:1865323 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1271:1865323:1865323 [0] NCCL INFO Comm config Blocking set to 1
lrdn1001:490760:490760 [0] NCCL INFO Bootstrap: Using ib0:10.128.21.197<0>
lrdn1001:490760:490760 [0] NCCL INFO cudaDriverVersion 12020
lrdn1001:490760:490760 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1001:490760:490760 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:07,994 | xffl.distributed.distributed |    DEBUG | [Rank 223]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1271:1865323:1865429 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1271:1865323:1865429 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1001:490760:490865 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1001:490760:490865 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1271:1865323:1865429 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.253<0>
lrdn1271:1865323:1865429 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1271:1865323:1865429 [0] NCCL INFO Using network IB
lrdn1271:1865323:1865429 [0] NCCL INFO ncclCommInitRankConfig comm 0xd5099b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x4d76f998e0eb4b6d - Init START
lrdn1271:1865323:1865429 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1271:1865323:1865429 [0] NCCL INFO Bootstrap timings total 0.000425 (create 0.000022, send 0.000067, recv 0.000104, ring 0.000001, delay 0.000000)
lrdn1271:1865323:1865429 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1271:1865323:1865429 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1271:1865323:1865429 [0] NCCL INFO comm 0xd5099b0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 00/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 01/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 02/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 03/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 04/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 05/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 06/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 07/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 08/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 09/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 10/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 11/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 12/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 13/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 14/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 15/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 16/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 17/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 18/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 19/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 20/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 21/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 22/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 23/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 24/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 25/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 26/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 27/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 28/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 29/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 30/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 31/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 32/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 33/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 34/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 35/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 36/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.21.197<0>
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 37/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 38/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 39/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 40/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 41/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 42/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 43/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 44/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 45/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 46/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 47/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 48/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 49/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 50/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 51/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 52/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 53/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 54/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 55/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 56/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 57/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 58/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 59/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 60/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 61/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 62/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Channel 63/64 : 0
lrdn1271:1865323:1865429 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1271:1865323:1865429 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1271:1865323:1865429 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1271:1865323:1865435 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn1271:1865323:1865436 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1
lrdn1001:490760:490865 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1001:490760:490865 [0] NCCL INFO Using network IB
lrdn1001:490760:490865 [0] NCCL INFO ncclCommInitRankConfig comm 0xdfaf440 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xdcbe9d07368e2de2 - Init START
lrdn1001:490760:490865 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1001:490760:490865 [0] NCCL INFO Bootstrap timings total 0.000422 (create 0.000022, send 0.000074, recv 0.000095, ring 0.000001, delay 0.000000)
lrdn1271:1865323:1865429 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1271:1865323:1865429 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1245:1634563:1634563 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.149<0>
lrdn1245:1634563:1634563 [0] NCCL INFO cudaDriverVersion 12020
lrdn1245:1634563:1634563 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1245:1634563:1634563 [0] NCCL INFO Comm config Blocking set to 1
lrdn1271:1865323:1865429 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1271:1865323:1865429 [0] NCCL INFO ncclCommInitRankConfig comm 0xd5099b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x4d76f998e0eb4b6d - Init COMPLETE
lrdn1271:1865323:1865429 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1001:490760:490865 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1001:490760:490865 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1001:490760:490865 [0] NCCL INFO comm 0xdfaf440 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 00/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 01/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 02/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 03/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 04/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 05/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 06/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 07/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 08/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 09/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 10/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 11/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 12/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 13/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 14/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 15/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 16/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 17/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 18/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 19/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 20/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 21/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 22/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 23/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 24/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 25/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 26/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 27/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 28/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 29/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 30/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 31/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 32/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 33/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 34/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 35/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 36/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 37/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 38/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 39/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 40/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 41/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 42/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 43/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 44/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 45/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 46/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 47/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 48/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 49/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 50/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 51/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 52/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 53/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 54/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 55/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 56/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 57/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 58/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 59/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 60/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 61/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 62/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Channel 63/64 : 0
lrdn1001:490760:490865 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn1001:490760:490865 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1001:490760:490865 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1001:490760:490871 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn1001:490760:490872 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn1001:490760:490865 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1001:490760:490865 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1001:490760:490865 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1001:490760:490865 [0] NCCL INFO ncclCommInitRankConfig comm 0xdfaf440 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xdcbe9d07368e2de2 - Init COMPLETE
lrdn1001:490760:490865 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1245:1634563:1634668 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1245:1634563:1634668 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1245:1634563:1634668 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.149<0>
lrdn1245:1634563:1634668 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1245:1634563:1634668 [0] NCCL INFO Using network IB
lrdn1245:1634563:1634668 [0] NCCL INFO ncclCommInitRankConfig comm 0xe2aa3b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x58fcfb98bfc4e4cd - Init START
lrdn1245:1634563:1634668 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1245:1634563:1634668 [0] NCCL INFO Bootstrap timings total 0.000402 (create 0.000021, send 0.000058, recv 0.000092, ring 0.000001, delay 0.000001)
lrdn1245:1634563:1634668 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1245:1634563:1634668 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1245:1634563:1634668 [0] NCCL INFO comm 0xe2aa3b0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 00/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 01/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 02/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 03/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 04/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 05/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 06/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 07/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 08/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 09/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 10/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 11/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 12/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 13/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 14/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 15/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 16/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 17/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 18/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 19/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 20/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 21/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 22/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 23/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 24/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 25/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 26/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 27/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 28/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 29/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 30/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 31/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 32/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 33/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 34/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 35/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 36/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 37/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 38/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 39/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 40/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 41/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 42/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 43/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 44/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 45/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 46/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 47/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 48/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 49/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 50/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 51/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 52/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 53/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 54/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 55/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 56/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 57/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 58/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 59/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 60/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 61/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 62/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Channel 63/64 : 0
lrdn1245:1634563:1634668 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1245:1634563:1634668 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1245:1634563:1634668 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1245:1634563:1634674 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1245:1634563:1634675 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1245:1634563:1634668 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1245:1634563:1634668 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1245:1634563:1634668 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1245:1634563:1634668 [0] NCCL INFO ncclCommInitRankConfig comm 0xe2aa3b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x58fcfb98bfc4e4cd - Init COMPLETE
lrdn1245:1634563:1634668 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.04, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1211:2766564:2766564 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.13<0>
lrdn1211:2766564:2766564 [0] NCCL INFO cudaDriverVersion 12020
lrdn1211:2766564:2766564 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1211:2766564:2766564 [0] NCCL INFO Comm config Blocking set to 1
lrdn1211:2766564:2766666 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1211:2766564:2766666 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1211:2766564:2766666 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.13<0>
lrdn1211:2766564:2766666 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1211:2766564:2766666 [0] NCCL INFO Using network IB
lrdn1211:2766564:2766666 [0] NCCL INFO ncclCommInitRankConfig comm 0xd0f6de0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xbec40945ff6aebc3 - Init START
lrdn1211:2766564:2766666 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1211:2766564:2766666 [0] NCCL INFO Bootstrap timings total 0.000435 (create 0.000022, send 0.000070, recv 0.000100, ring 0.000001, delay 0.000001)
lrdn1211:2766564:2766666 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1211:2766564:2766666 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1211:2766564:2766666 [0] NCCL INFO comm 0xd0f6de0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 00/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 01/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 02/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 03/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 04/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 05/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 06/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 07/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 08/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 09/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 10/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 11/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 12/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 13/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 14/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 15/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 16/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 17/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 18/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 19/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 20/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 21/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 22/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 23/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 24/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 25/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 26/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 27/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 28/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 29/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 30/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 31/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 32/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 33/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 34/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 35/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 36/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 37/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 38/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 39/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 40/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 41/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 42/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 43/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 44/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 45/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 46/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 47/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 48/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 49/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 50/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 51/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 52/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 53/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 54/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 55/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 56/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 57/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 58/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 59/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 60/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 61/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 62/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Channel 63/64 : 0
lrdn1211:2766564:2766666 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1211:2766564:2766666 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1211:2766564:2766666 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1211:2766564:2766672 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn1211:2766564:2766673 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn1211:2766564:2766666 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1211:2766564:2766666 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1211:2766564:2766666 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1211:2766564:2766666 [0] NCCL INFO ncclCommInitRankConfig comm 0xd0f6de0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xbec40945ff6aebc3 - Init COMPLETE
lrdn1211:2766564:2766666 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:09,453 | xffl.distributed.distributed |    DEBUG | [Rank 225]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:09,626 | xffl.distributed.distributed |    DEBUG | [Rank 145]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1223:1586205:1586205 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.61<0>
lrdn1223:1586205:1586205 [0] NCCL INFO cudaDriverVersion 12020
lrdn1223:1586205:1586205 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1223:1586205:1586205 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:09,940 | xffl.distributed.distributed |    DEBUG | [Rank 171]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1223:1586205:1586303 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1223:1586205:1586303 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0854:793988:793988 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.121<0>
lrdn0854:793988:793988 [0] NCCL INFO cudaDriverVersion 12020
lrdn0854:793988:793988 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0854:793988:793988 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:09,992 | xffl.distributed.distributed |    DEBUG | [Rank 61]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1223:1586205:1586303 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.61<0>
lrdn1223:1586205:1586303 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1223:1586205:1586303 [0] NCCL INFO Using network IB
lrdn1223:1586205:1586303 [0] NCCL INFO ncclCommInitRankConfig comm 0xe886d60 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x152963fcecb1c15a - Init START
lrdn1223:1586205:1586303 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1223:1586205:1586303 [0] NCCL INFO Bootstrap timings total 0.000421 (create 0.000024, send 0.000069, recv 0.000100, ring 0.000001, delay 0.000000)
lrdn1223:1586205:1586303 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1223:1586205:1586303 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1223:1586205:1586303 [0] NCCL INFO comm 0xe886d60 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 00/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 01/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 02/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 03/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 04/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 05/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 06/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 07/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 08/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 09/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 10/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 11/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 12/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 13/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 14/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 15/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 16/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 17/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 18/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 19/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 20/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 21/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 22/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 23/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 24/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 25/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 26/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 27/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 28/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 29/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 30/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 31/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 32/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 33/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 34/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 35/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 36/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 37/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 38/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 39/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 40/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 41/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 42/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 43/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 44/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 45/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 46/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 47/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 48/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 49/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 50/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 51/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 52/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 53/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 54/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 55/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 56/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 57/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 58/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 59/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 60/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 61/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 62/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Channel 63/64 : 0
lrdn1223:1586205:1586303 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1223:1586205:1586303 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1223:1586205:1586303 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1223:1586205:1586309 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn1223:1586205:1586310 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn1223:1586205:1586303 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1223:1586205:1586303 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1223:1586205:1586303 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1223:1586205:1586303 [0] NCCL INFO ncclCommInitRankConfig comm 0xe886d60 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x152963fcecb1c15a - Init COMPLETE
lrdn1223:1586205:1586303 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0854:793988:794087 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0854:793988:794087 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:10,163 | xffl.distributed.distributed |    DEBUG | [Rank 95]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0854:793988:794087 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.121<0>
lrdn0854:793988:794087 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0854:793988:794087 [0] NCCL INFO Using network IB
lrdn0854:793988:794087 [0] NCCL INFO ncclCommInitRankConfig comm 0xd045f90 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc22836eee5344c64 - Init START
lrdn0854:793988:794087 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0854:793988:794087 [0] NCCL INFO Bootstrap timings total 0.000425 (create 0.000024, send 0.000065, recv 0.000107, ring 0.000001, delay 0.000001)
lrdn0854:793988:794087 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0854:793988:794087 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0854:793988:794087 [0] NCCL INFO comm 0xd045f90 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 00/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 01/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 02/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 03/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 04/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 05/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 06/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 07/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 08/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 09/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 10/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 11/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 12/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 13/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 14/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 15/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 16/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 17/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 18/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 19/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 20/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 21/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 22/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 23/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 24/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 25/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 26/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 27/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 28/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 29/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 30/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 31/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 32/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 33/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 34/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 35/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 36/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 37/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 38/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 39/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 40/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 41/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 42/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 43/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 44/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 45/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 46/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 47/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 48/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 49/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 50/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 51/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 52/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 53/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 54/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 55/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 56/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 57/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 58/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 59/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 60/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 61/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 62/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Channel 63/64 : 0
lrdn0854:793988:794087 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0854:793988:794087 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0854:793988:794087 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0854:793988:794093 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn0854:793988:794094 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn0854:793988:794087 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0854:793988:794087 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0854:793988:794087 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0854:793988:794087 [0] NCCL INFO ncclCommInitRankConfig comm 0xd045f90 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc22836eee5344c64 - Init COMPLETE
lrdn0854:793988:794087 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0963:1608371:1608371 [0] NCCL INFO Bootstrap: Using ib0:10.128.21.45<0>
lrdn0963:1608371:1608371 [0] NCCL INFO cudaDriverVersion 12020
lrdn0963:1608371:1608371 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0963:1608371:1608371 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:10,479 | xffl.distributed.distributed |    DEBUG | [Rank 229]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0447:1574775:1574775 [0] NCCL INFO Bootstrap: Using ib0:10.128.13.29<0>
lrdn0447:1574775:1574775 [0] NCCL INFO cudaDriverVersion 12020
lrdn0447:1574775:1574775 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0447:1574775:1574775 [0] NCCL INFO Comm config Blocking set to 1
lrdn0963:1608371:1608469 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0963:1608371:1608469 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:10,539 | xffl.distributed.distributed |    DEBUG | [Rank 62]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0963:1608371:1608469 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.21.45<0>
lrdn0963:1608371:1608469 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0963:1608371:1608469 [0] NCCL INFO Using network IB
lrdn0963:1608371:1608469 [0] NCCL INFO ncclCommInitRankConfig comm 0xe026760 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe32d3718fee76008 - Init START
lrdn0963:1608371:1608469 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0963:1608371:1608469 [0] NCCL INFO Bootstrap timings total 0.000420 (create 0.000023, send 0.000071, recv 0.000095, ring 0.000001, delay 0.000000)
lrdn0963:1608371:1608469 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0963:1608371:1608469 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0963:1608371:1608469 [0] NCCL INFO comm 0xe026760 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 00/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 01/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 02/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 03/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 04/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 05/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 06/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 07/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 08/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 09/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 10/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 11/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 12/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 13/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 14/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 15/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 16/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 17/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 18/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 19/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 20/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 21/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 22/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 23/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 24/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 25/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 26/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 27/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 28/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 29/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 30/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 31/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 32/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 33/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 34/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 35/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 36/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 37/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 38/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 39/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 40/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 41/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 42/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 43/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 44/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 45/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 46/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 47/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 48/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 49/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 50/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 51/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 52/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 53/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 54/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 55/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 56/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 57/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 58/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 59/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 60/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 61/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 62/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Channel 63/64 : 0
lrdn0963:1608371:1608469 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0963:1608371:1608469 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0963:1608371:1608469 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0963:1608371:1608475 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0963:1608371:1608476 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn0963:1608371:1608469 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0963:1608371:1608469 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0963:1608371:1608469 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0963:1608371:1608469 [0] NCCL INFO ncclCommInitRankConfig comm 0xe026760 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe32d3718fee76008 - Init COMPLETE
lrdn0963:1608371:1608469 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.02, rest 0.00)
lrdn0447:1574775:1574870 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0447:1574775:1574870 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0447:1574775:1574870 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.13.29<0>
lrdn0611:2834843:2834843 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.173<0>
lrdn0611:2834843:2834843 [0] NCCL INFO cudaDriverVersion 12020
lrdn0611:2834843:2834843 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0611:2834843:2834843 [0] NCCL INFO Comm config Blocking set to 1
lrdn0447:1574775:1574870 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0447:1574775:1574870 [0] NCCL INFO Using network IB
lrdn0447:1574775:1574870 [0] NCCL INFO ncclCommInitRankConfig comm 0xccd7090 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xeaa615e58fa50aed - Init START
lrdn0447:1574775:1574870 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0447:1574775:1574870 [0] NCCL INFO Bootstrap timings total 0.000525 (create 0.000020, send 0.000061, recv 0.000221, ring 0.000001, delay 0.000000)
lrdn0447:1574775:1574870 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0447:1574775:1574870 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0447:1574775:1574870 [0] NCCL INFO comm 0xccd7090 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 00/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 01/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 02/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 03/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 04/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 05/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 06/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 07/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 08/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 09/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 10/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 11/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 12/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 13/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 14/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 15/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 16/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 17/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 18/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 19/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 20/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 21/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 22/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 23/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 24/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 25/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 26/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 27/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 28/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 29/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 30/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 31/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 32/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 33/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 34/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 35/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 36/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 37/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 38/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 39/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 40/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 41/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 42/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 43/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 44/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 45/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 46/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 47/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 48/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 49/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 50/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 51/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 52/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 53/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 54/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 55/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 56/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 57/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 58/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 59/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 60/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 61/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 62/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Channel 63/64 : 0
lrdn0447:1574775:1574870 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0447:1574775:1574870 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0447:1574775:1574870 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0447:1574775:1574876 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0447:1574775:1574877 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn0447:1574775:1574870 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0447:1574775:1574870 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0447:1574775:1574870 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0447:1574775:1574870 [0] NCCL INFO ncclCommInitRankConfig comm 0xccd7090 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xeaa615e58fa50aed - Init COMPLETE
lrdn0447:1574775:1574870 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
[38;5;39m2025-08-02 08:49:10,824 | xffl.distributed.distributed |    DEBUG | [Rank 92]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0611:2834843:2834939 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0611:2834843:2834939 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0611:2834843:2834939 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.173<0>
lrdn0611:2834843:2834939 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0611:2834843:2834939 [0] NCCL INFO Using network IB
lrdn0611:2834843:2834939 [0] NCCL INFO ncclCommInitRankConfig comm 0xe8b9440 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x80e396044978d736 - Init START
lrdn0611:2834843:2834939 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0611:2834843:2834939 [0] NCCL INFO Bootstrap timings total 0.000399 (create 0.000020, send 0.000063, recv 0.000100, ring 0.000001, delay 0.000000)
lrdn0611:2834843:2834939 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0611:2834843:2834939 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0611:2834843:2834939 [0] NCCL INFO comm 0xe8b9440 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 00/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 01/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 02/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 03/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 04/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 05/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 06/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 07/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 08/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 09/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 10/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 11/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 12/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 13/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 14/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 15/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 16/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 17/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 18/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 19/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 20/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 21/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 22/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 23/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 24/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 25/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 26/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 27/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 28/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 29/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 30/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 31/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 32/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 33/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 34/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 35/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 36/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 37/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 38/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 39/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 40/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 41/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 42/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 43/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 44/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 45/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 46/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 47/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 48/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 49/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 50/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 51/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 52/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 53/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 54/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 55/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 56/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 57/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 58/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 59/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 60/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 61/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 62/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Channel 63/64 : 0
lrdn0611:2834843:2834939 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0611:2834843:2834939 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0611:2834843:2834939 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0611:2834843:2834945 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn0611:2834843:2834946 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
[38;5;39m2025-08-02 08:49:10,920 | xffl.distributed.distributed |    DEBUG | [Rank 85]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0611:2834843:2834939 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0611:2834843:2834939 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0611:2834843:2834939 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0611:2834843:2834939 [0] NCCL INFO ncclCommInitRankConfig comm 0xe8b9440 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x80e396044978d736 - Init COMPLETE
lrdn0611:2834843:2834939 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1241:1468891:1468891 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.133<0>
lrdn1241:1468891:1468891 [0] NCCL INFO cudaDriverVersion 12020
lrdn1241:1468891:1468891 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1241:1468891:1468891 [0] NCCL INFO Comm config Blocking set to 1
lrdn0458:1951839:1951839 [0] NCCL INFO Bootstrap: Using ib0:10.128.13.73<0>
lrdn0458:1951839:1951839 [0] NCCL INFO cudaDriverVersion 12020
lrdn0458:1951839:1951839 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0458:1951839:1951839 [0] NCCL INFO Comm config Blocking set to 1
lrdn1241:1468891:1468985 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1241:1468891:1468985 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1241:1468891:1468985 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.133<0>
lrdn1241:1468891:1468985 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1241:1468891:1468985 [0] NCCL INFO Using network IB
lrdn1241:1468891:1468985 [0] NCCL INFO ncclCommInitRankConfig comm 0xf88a590 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xa8c2959ac863180 - Init START
lrdn1241:1468891:1468985 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1241:1468891:1468985 [0] NCCL INFO Bootstrap timings total 0.000413 (create 0.000021, send 0.000067, recv 0.000089, ring 0.000001, delay 0.000000)
lrdn1241:1468891:1468985 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1241:1468891:1468985 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1241:1468891:1468985 [0] NCCL INFO comm 0xf88a590 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 00/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 01/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 02/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 03/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 04/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 05/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 06/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 07/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 08/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 09/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 10/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 11/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 12/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 13/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 14/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 15/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 16/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 17/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 18/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 19/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 20/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 21/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 22/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 23/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 24/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 25/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 26/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 27/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 28/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 29/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 30/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 31/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 32/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 33/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 34/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 35/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 36/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 37/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 38/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 39/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 40/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 41/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 42/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 43/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 44/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 45/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 46/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 47/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 48/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 49/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 50/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 51/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 52/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 53/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 54/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 55/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 56/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 57/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 58/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 59/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 60/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 61/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 62/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Channel 63/64 : 0
lrdn1241:1468891:1468985 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1241:1468891:1468985 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1241:1468891:1468985 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1241:1468891:1468992 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 12
lrdn1241:1468891:1468991 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn0458:1951839:1951943 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0458:1951839:1951943 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1241:1468891:1468985 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1241:1468891:1468985 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1241:1468891:1468985 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1241:1468891:1468985 [0] NCCL INFO ncclCommInitRankConfig comm 0xf88a590 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xa8c2959ac863180 - Init COMPLETE
lrdn1241:1468891:1468985 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0458:1951839:1951943 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.13.73<0>
lrdn0458:1951839:1951943 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0458:1951839:1951943 [0] NCCL INFO Using network IB
lrdn0458:1951839:1951943 [0] NCCL INFO ncclCommInitRankConfig comm 0xf352580 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xa12a5725dd43dc6e - Init START
lrdn0458:1951839:1951943 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0458:1951839:1951943 [0] NCCL INFO Bootstrap timings total 0.000604 (create 0.000022, send 0.000071, recv 0.000271, ring 0.000001, delay 0.000001)
lrdn0458:1951839:1951943 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0458:1951839:1951943 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0458:1951839:1951943 [0] NCCL INFO comm 0xf352580 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 00/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 01/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 02/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 03/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 04/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 05/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 06/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 07/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 08/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 09/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 10/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 11/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 12/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 13/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 14/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 15/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 16/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 17/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 18/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 19/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 20/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 21/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 22/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 23/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 24/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 25/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 26/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 27/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 28/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 29/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 30/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 31/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 32/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 33/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 34/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 35/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 36/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 37/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 38/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 39/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 40/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 41/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 42/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 43/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 44/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 45/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 46/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 47/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 48/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 49/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 50/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 51/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 52/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 53/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 54/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 55/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 56/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 57/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 58/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 59/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 60/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 61/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 62/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Channel 63/64 : 0
lrdn0458:1951839:1951943 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0458:1951839:1951943 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0458:1951839:1951943 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0458:1951839:1951950 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0458:1951839:1951949 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
[38;5;39m2025-08-02 08:49:11,397 | xffl.distributed.distributed |    DEBUG | [Rank 180]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0458:1951839:1951943 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0458:1951839:1951943 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0458:1951839:1951943 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0458:1951839:1951943 [0] NCCL INFO ncclCommInitRankConfig comm 0xf352580 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xa12a5725dd43dc6e - Init COMPLETE
lrdn0458:1951839:1951943 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0599:161030:161030 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.125<0>
lrdn0599:161030:161030 [0] NCCL INFO cudaDriverVersion 12020
lrdn0599:161030:161030 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0599:161030:161030 [0] NCCL INFO Comm config Blocking set to 1
lrdn0569:3775528:3775528 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.5<0>
lrdn0569:3775528:3775528 [0] NCCL INFO cudaDriverVersion 12020
lrdn0569:3775528:3775528 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0569:3775528:3775528 [0] NCCL INFO Comm config Blocking set to 1
lrdn0599:161030:161135 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0599:161030:161135 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0599:161030:161135 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.125<0>
lrdn0599:161030:161135 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0599:161030:161135 [0] NCCL INFO Using network IB
lrdn0599:161030:161135 [0] NCCL INFO ncclCommInitRankConfig comm 0xd17f6e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1fa0e63641660e9a - Init START
lrdn0599:161030:161135 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0599:161030:161135 [0] NCCL INFO Bootstrap timings total 0.000389 (create 0.000015, send 0.000056, recv 0.000091, ring 0.000001, delay 0.000000)
lrdn0569:3775528:3775631 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0569:3775528:3775631 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0599:161030:161135 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0599:161030:161135 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0599:161030:161135 [0] NCCL INFO comm 0xd17f6e0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 00/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 01/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 02/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 03/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 04/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 05/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 06/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 07/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 08/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 09/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 10/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 11/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 12/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 13/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 14/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 15/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 16/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 17/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 18/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 19/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 20/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 21/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 22/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 23/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 24/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 25/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 26/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 27/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 28/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 29/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 30/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 31/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 32/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 33/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 34/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 35/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 36/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 37/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 38/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 39/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 40/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 41/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 42/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 43/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 44/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 45/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 46/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 47/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 48/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 49/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 50/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 51/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 52/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 53/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 54/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 55/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 56/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 57/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 58/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 59/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 60/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 61/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 62/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Channel 63/64 : 0
lrdn0599:161030:161135 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0599:161030:161135 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0599:161030:161135 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0599:161030:161142 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn0599:161030:161141 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn0599:161030:161135 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0599:161030:161135 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0599:161030:161135 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0599:161030:161135 [0] NCCL INFO ncclCommInitRankConfig comm 0xd17f6e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1fa0e63641660e9a - Init COMPLETE
lrdn0599:161030:161135 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0569:3775528:3775631 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.5<0>
lrdn0569:3775528:3775631 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0569:3775528:3775631 [0] NCCL INFO Using network IB
lrdn0569:3775528:3775631 [0] NCCL INFO ncclCommInitRankConfig comm 0xe3cd0c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc71241a44dcfff0d - Init START
lrdn0569:3775528:3775631 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0569:3775528:3775631 [0] NCCL INFO Bootstrap timings total 0.000419 (create 0.000018, send 0.000067, recv 0.000097, ring 0.000001, delay 0.000000)
lrdn0569:3775528:3775631 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0569:3775528:3775631 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0569:3775528:3775631 [0] NCCL INFO comm 0xe3cd0c0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 00/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 01/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 02/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 03/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 04/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 05/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 06/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 07/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 08/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 09/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 10/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 11/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 12/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 13/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 14/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 15/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 16/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 17/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 18/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 19/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 20/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 21/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 22/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 23/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 24/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 25/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 26/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 27/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 28/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 29/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 30/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 31/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 32/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 33/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 34/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 35/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 36/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 37/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 38/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 39/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 40/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 41/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 42/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 43/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 44/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 45/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 46/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 47/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 48/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 49/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 50/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 51/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 52/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 53/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 54/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 55/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 56/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 57/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 58/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 59/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 60/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 61/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 62/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Channel 63/64 : 0
lrdn0569:3775528:3775631 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0569:3775528:3775631 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0569:3775528:3775631 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0569:3775528:3775637 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0569:3775528:3775638 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0569:3775528:3775631 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0569:3775528:3775631 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0569:3775528:3775631 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0569:3775528:3775631 [0] NCCL INFO ncclCommInitRankConfig comm 0xe3cd0c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc71241a44dcfff0d - Init COMPLETE
lrdn0569:3775528:3775631 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:11,855 | xffl.distributed.distributed |    DEBUG | [Rank 47]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:11,931 | xffl.distributed.distributed |    DEBUG | [Rank 39]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0992:1403591:1403591 [0] NCCL INFO Bootstrap: Using ib0:10.128.21.161<0>
lrdn0992:1403591:1403591 [0] NCCL INFO cudaDriverVersion 12020
lrdn0992:1403591:1403591 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0992:1403591:1403591 [0] NCCL INFO Comm config Blocking set to 1
lrdn0992:1403591:1403687 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0992:1403591:1403687 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0992:1403591:1403687 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.21.161<0>
lrdn0992:1403591:1403687 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0992:1403591:1403687 [0] NCCL INFO Using network IB
lrdn0992:1403591:1403687 [0] NCCL INFO ncclCommInitRankConfig comm 0xd7ec020 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3a35b6d921a87b7c - Init START
lrdn0992:1403591:1403687 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0992:1403591:1403687 [0] NCCL INFO Bootstrap timings total 0.000409 (create 0.000019, send 0.000060, recv 0.000094, ring 0.000002, delay 0.000001)
lrdn0992:1403591:1403687 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0992:1403591:1403687 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0992:1403591:1403687 [0] NCCL INFO comm 0xd7ec020 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 00/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 01/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 02/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 03/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 04/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 05/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 06/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 07/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 08/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 09/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 10/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 11/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 12/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 13/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 14/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 15/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 16/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 17/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 18/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 19/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 20/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 21/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 22/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 23/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 24/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 25/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 26/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 27/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 28/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 29/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 30/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 31/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 32/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 33/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 34/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 35/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 36/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 37/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 38/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 39/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 40/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 41/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 42/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 43/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 44/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 45/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 46/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 47/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 48/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 49/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 50/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 51/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 52/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 53/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 54/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 55/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 56/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 57/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 58/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 59/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 60/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 61/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 62/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Channel 63/64 : 0
lrdn0992:1403591:1403687 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0992:1403591:1403687 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0992:1403591:1403687 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0992:1403591:1403693 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn0992:1403591:1403694 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0992:1403591:1403687 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0992:1403591:1403687 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0992:1403591:1403687 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0992:1403591:1403687 [0] NCCL INFO ncclCommInitRankConfig comm 0xd7ec020 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3a35b6d921a87b7c - Init COMPLETE
lrdn0992:1403591:1403687 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:12,290 | xffl.distributed.distributed |    DEBUG | [Rank 233]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0326:1692035:1692035 [0] NCCL INFO Bootstrap: Using ib0:10.128.11.57<0>
lrdn0326:1692035:1692035 [0] NCCL INFO cudaDriverVersion 12020
lrdn0326:1692035:1692035 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0326:1692035:1692035 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:12,318 | xffl.distributed.distributed |    DEBUG | [Rank 88]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0289:1476853:1476853 [0] NCCL INFO Bootstrap: Using ib0:10.128.10.165<0>
lrdn0289:1476853:1476853 [0] NCCL INFO cudaDriverVersion 12020
lrdn0289:1476853:1476853 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0289:1476853:1476853 [0] NCCL INFO Comm config Blocking set to 1
lrdn0326:1692035:1692138 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0326:1692035:1692138 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0289:1476853:1476950 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0289:1476853:1476950 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0326:1692035:1692138 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.11.57<0>
lrdn0326:1692035:1692138 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0326:1692035:1692138 [0] NCCL INFO Using network IB
lrdn0326:1692035:1692138 [0] NCCL INFO ncclCommInitRankConfig comm 0xf131b20 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf149e2082003ab39 - Init START
lrdn0326:1692035:1692138 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0326:1692035:1692138 [0] NCCL INFO Bootstrap timings total 0.000394 (create 0.000019, send 0.000057, recv 0.000094, ring 0.000001, delay 0.000001)
lrdn0326:1692035:1692138 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0326:1692035:1692138 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0326:1692035:1692138 [0] NCCL INFO comm 0xf131b20 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 00/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 01/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 02/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 03/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 04/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 05/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 06/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 07/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 08/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 09/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 10/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 11/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 12/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 13/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 14/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 15/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 16/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 17/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 18/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 19/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 20/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 21/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 22/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 23/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 24/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 25/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 26/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 27/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 28/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 29/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 30/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 31/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 32/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 33/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 34/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 35/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 36/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 37/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 38/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 39/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 40/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 41/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 42/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 43/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 44/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 45/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 46/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 47/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 48/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 49/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 50/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 51/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 52/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 53/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 54/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 55/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 56/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 57/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 58/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 59/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 60/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 61/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 62/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Channel 63/64 : 0
lrdn0326:1692035:1692138 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0326:1692035:1692138 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0326:1692035:1692138 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0326:1692035:1692144 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0326:1692035:1692145 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0289:1476853:1476950 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.10.165<0>
lrdn0326:1692035:1692138 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0326:1692035:1692138 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0289:1476853:1476950 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0289:1476853:1476950 [0] NCCL INFO Using network IB
lrdn0326:1692035:1692138 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0326:1692035:1692138 [0] NCCL INFO ncclCommInitRankConfig comm 0xf131b20 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf149e2082003ab39 - Init COMPLETE
lrdn0326:1692035:1692138 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn0289:1476853:1476950 [0] NCCL INFO ncclCommInitRankConfig comm 0x15e52c60 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xec00e8800d7e460 - Init START
lrdn0289:1476853:1476950 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0289:1476853:1476950 [0] NCCL INFO Bootstrap timings total 0.000539 (create 0.000018, send 0.000063, recv 0.000239, ring 0.000001, delay 0.000001)
lrdn0289:1476853:1476950 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0289:1476853:1476950 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0289:1476853:1476950 [0] NCCL INFO comm 0x15e52c60 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 00/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 01/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 02/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 03/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 04/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 05/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 06/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 07/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 08/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 09/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 10/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 11/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 12/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 13/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 14/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 15/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 16/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 17/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 18/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 19/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 20/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 21/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 22/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 23/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 24/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 25/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 26/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 27/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 28/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 29/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 30/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 31/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 32/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 33/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 34/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 35/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 36/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 37/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 38/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 39/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 40/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 41/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 42/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 43/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 44/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 45/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 46/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 47/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 48/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 49/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 50/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 51/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 52/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 53/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 54/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 55/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 56/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 57/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 58/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 59/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 60/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 61/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 62/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Channel 63/64 : 0
lrdn0289:1476853:1476950 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0289:1476853:1476950 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0289:1476853:1476950 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0289:1476853:1476956 [0] NCCL INFO [Proxy Service] Device 0 CPU core 8
lrdn0289:1476853:1476957 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn0289:1476853:1476950 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0289:1476853:1476950 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0289:1476853:1476950 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0289:1476853:1476950 [0] NCCL INFO ncclCommInitRankConfig comm 0x15e52c60 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xec00e8800d7e460 - Init COMPLETE
lrdn0289:1476853:1476950 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:12,682 | xffl.distributed.distributed |    DEBUG | [Rank 203]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1262:1584708:1584708 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.217<0>
lrdn1262:1584708:1584708 [0] NCCL INFO cudaDriverVersion 12020
lrdn1262:1584708:1584708 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1262:1584708:1584708 [0] NCCL INFO Comm config Blocking set to 1
lrdn0590:1647475:1647475 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.89<0>
lrdn0590:1647475:1647475 [0] NCCL INFO cudaDriverVersion 12020
lrdn0590:1647475:1647475 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0590:1647475:1647475 [0] NCCL INFO Comm config Blocking set to 1
lrdn1262:1584708:1584812 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1262:1584708:1584812 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0590:1647475:1647571 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0590:1647475:1647571 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1262:1584708:1584812 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.217<0>
lrdn0590:1647475:1647571 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.89<0>
lrdn1262:1584708:1584812 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1262:1584708:1584812 [0] NCCL INFO Using network IB
lrdn1262:1584708:1584812 [0] NCCL INFO ncclCommInitRankConfig comm 0xd7f6a70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf41db420c6ba73cf - Init START
lrdn1262:1584708:1584812 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1262:1584708:1584812 [0] NCCL INFO Bootstrap timings total 0.000405 (create 0.000020, send 0.000059, recv 0.000103, ring 0.000001, delay 0.000000)
lrdn0590:1647475:1647571 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0590:1647475:1647571 [0] NCCL INFO Using network IB
lrdn0590:1647475:1647571 [0] NCCL INFO ncclCommInitRankConfig comm 0xdf824d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb0e476264af8d038 - Init START
lrdn0590:1647475:1647571 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0590:1647475:1647571 [0] NCCL INFO Bootstrap timings total 0.000384 (create 0.000017, send 0.000058, recv 0.000096, ring 0.000001, delay 0.000000)
lrdn1262:1584708:1584812 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1262:1584708:1584812 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1262:1584708:1584812 [0] NCCL INFO comm 0xd7f6a70 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 00/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 01/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 02/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 03/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 04/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 05/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 06/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 07/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 08/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 09/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 10/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 11/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 12/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 13/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 14/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 15/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 16/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 17/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 18/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 19/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 20/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 21/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 22/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 23/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 24/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 25/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 26/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 27/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 28/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 29/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 30/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 31/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 32/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 33/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 34/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 35/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 36/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 37/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 38/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 39/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 40/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 41/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 42/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 43/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 44/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 45/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 46/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 47/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 48/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 49/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 50/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 51/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 52/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 53/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 54/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 55/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 56/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 57/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 58/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 59/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 60/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 61/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 62/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Channel 63/64 : 0
lrdn1262:1584708:1584812 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1262:1584708:1584812 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1262:1584708:1584812 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1262:1584708:1584819 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn1262:1584708:1584818 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn0590:1647475:1647571 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0590:1647475:1647571 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0590:1647475:1647571 [0] NCCL INFO comm 0xdf824d0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 00/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 01/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 02/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 03/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 04/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 05/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 06/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 07/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 08/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 09/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 10/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 11/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 12/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 13/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 14/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 15/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 16/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 17/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 18/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 19/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 20/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 21/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 22/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 23/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 24/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 25/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 26/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 27/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 28/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 29/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 30/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 31/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 32/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 33/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 34/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 35/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 36/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 37/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 38/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 39/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 40/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 41/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 42/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 43/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 44/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 45/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 46/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 47/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 48/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 49/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 50/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 51/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 52/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 53/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 54/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 55/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 56/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 57/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 58/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 59/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 60/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 61/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 62/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Channel 63/64 : 0
lrdn0590:1647475:1647571 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0590:1647475:1647571 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0590:1647475:1647571 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0590:1647475:1647577 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0590:1647475:1647578 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
[38;5;39m2025-08-02 08:49:13,051 | xffl.distributed.distributed |    DEBUG | [Rank 96]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1262:1584708:1584812 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1262:1584708:1584812 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0590:1647475:1647571 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0590:1647475:1647571 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1262:1584708:1584812 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1262:1584708:1584812 [0] NCCL INFO ncclCommInitRankConfig comm 0xd7f6a70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf41db420c6ba73cf - Init COMPLETE
lrdn1262:1584708:1584812 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0590:1647475:1647571 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0590:1647475:1647571 [0] NCCL INFO ncclCommInitRankConfig comm 0xdf824d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb0e476264af8d038 - Init COMPLETE
lrdn0590:1647475:1647571 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1112:1633009:1633009 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.129<0>
lrdn1112:1633009:1633009 [0] NCCL INFO cudaDriverVersion 12020
lrdn1112:1633009:1633009 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1112:1633009:1633009 [0] NCCL INFO Comm config Blocking set to 1
lrdn1112:1633009:1633105 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1112:1633009:1633105 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1112:1633009:1633105 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.129<0>
lrdn1112:1633009:1633105 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1112:1633009:1633105 [0] NCCL INFO Using network IB
lrdn1112:1633009:1633105 [0] NCCL INFO ncclCommInitRankConfig comm 0xdaa9ae0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xbe4bf80de5f4cde1 - Init START
lrdn1112:1633009:1633105 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1112:1633009:1633105 [0] NCCL INFO Bootstrap timings total 0.000522 (create 0.000020, send 0.000065, recv 0.000221, ring 0.000001, delay 0.000000)
lrdn1112:1633009:1633105 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1112:1633009:1633105 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1112:1633009:1633105 [0] NCCL INFO comm 0xdaa9ae0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 00/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 01/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 02/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 03/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 04/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 05/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 06/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 07/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 08/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 09/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 10/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 11/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 12/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 13/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 14/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 15/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 16/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 17/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 18/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 19/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 20/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 21/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 22/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 23/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 24/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 25/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 26/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 27/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 28/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 29/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 30/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 31/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 32/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 33/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 34/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 35/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 36/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 37/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 38/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 39/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 40/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 41/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 42/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 43/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 44/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 45/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 46/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 47/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 48/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 49/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 50/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 51/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 52/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 53/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 54/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 55/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 56/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 57/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 58/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 59/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 60/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 61/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 62/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Channel 63/64 : 0
lrdn1112:1633009:1633105 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1112:1633009:1633105 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1112:1633009:1633105 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1112:1633009:1633112 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1112:1633009:1633111 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
[38;5;39m2025-08-02 08:49:13,403 | xffl.distributed.distributed |    DEBUG | [Rank 48]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1112:1633009:1633105 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1112:1633009:1633105 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1112:1633009:1633105 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1112:1633009:1633105 [0] NCCL INFO ncclCommInitRankConfig comm 0xdaa9ae0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xbe4bf80de5f4cde1 - Init COMPLETE
lrdn1112:1633009:1633105 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
[38;5;39m2025-08-02 08:49:13,485 | xffl.distributed.distributed |    DEBUG | [Rank 40]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0614:1581089:1581089 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.185<0>
lrdn0614:1581089:1581089 [0] NCCL INFO cudaDriverVersion 12020
lrdn0614:1581089:1581089 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0614:1581089:1581089 [0] NCCL INFO Comm config Blocking set to 1
lrdn0614:1581089:1581196 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0614:1581089:1581196 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:13,687 | xffl.distributed.distributed |    DEBUG | [Rank 89]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:13,718 | xffl.distributed.distributed |    DEBUG | [Rank 4]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0614:1581089:1581196 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.185<0>
lrdn0614:1581089:1581196 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0614:1581089:1581196 [0] NCCL INFO Using network IB
lrdn0614:1581089:1581196 [0] NCCL INFO ncclCommInitRankConfig comm 0xdeef3c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc9cd0e84573a6437 - Init START
lrdn0614:1581089:1581196 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0614:1581089:1581196 [0] NCCL INFO Bootstrap timings total 0.000409 (create 0.000020, send 0.000063, recv 0.000096, ring 0.000002, delay 0.000000)
lrdn0614:1581089:1581196 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0614:1581089:1581196 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0614:1581089:1581196 [0] NCCL INFO comm 0xdeef3c0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 00/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 01/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 02/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 03/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 04/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 05/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 06/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 07/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 08/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 09/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 10/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 11/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 12/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 13/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 14/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 15/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 16/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 17/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 18/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 19/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 20/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 21/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 22/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 23/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 24/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 25/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 26/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 27/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 28/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 29/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 30/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 31/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 32/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 33/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 34/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 35/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 36/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 37/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 38/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 39/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 40/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 41/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 42/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 43/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 44/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 45/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 46/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 47/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 48/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 49/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 50/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 51/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 52/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 53/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 54/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 55/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 56/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 57/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 58/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 59/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 60/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 61/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 62/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Channel 63/64 : 0
lrdn0614:1581089:1581196 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0614:1581089:1581196 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0614:1581089:1581196 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0614:1581089:1581202 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0614:1581089:1581203 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0614:1581089:1581196 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0614:1581089:1581196 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0614:1581089:1581196 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0614:1581089:1581196 [0] NCCL INFO ncclCommInitRankConfig comm 0xdeef3c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc9cd0e84573a6437 - Init COMPLETE
lrdn0614:1581089:1581196 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0329:1724546:1724546 [0] NCCL INFO Bootstrap: Using ib0:10.128.11.69<0>
lrdn0329:1724546:1724546 [0] NCCL INFO cudaDriverVersion 12020
lrdn0329:1724546:1724546 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0329:1724546:1724546 [0] NCCL INFO Comm config Blocking set to 1
lrdn0295:1874970:1874970 [0] NCCL INFO Bootstrap: Using ib0:10.128.10.189<0>
lrdn0295:1874970:1874970 [0] NCCL INFO cudaDriverVersion 12020
lrdn0295:1874970:1874970 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0295:1874970:1874970 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:13,942 | xffl.distributed.distributed |    DEBUG | [Rank 45]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0329:1724546:1724644 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0329:1724546:1724644 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0329:1724546:1724644 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.11.69<0>
lrdn0329:1724546:1724644 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0329:1724546:1724644 [0] NCCL INFO Using network IB
lrdn0329:1724546:1724644 [0] NCCL INFO ncclCommInitRankConfig comm 0xe49d0b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x88bed71eff1e55f9 - Init START
lrdn0329:1724546:1724644 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0329:1724546:1724644 [0] NCCL INFO Bootstrap timings total 0.000426 (create 0.000023, send 0.000075, recv 0.000102, ring 0.000001, delay 0.000000)
lrdn0295:1874970:1875132 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0295:1874970:1875132 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0329:1724546:1724644 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0329:1724546:1724644 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0329:1724546:1724644 [0] NCCL INFO comm 0xe49d0b0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 00/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 01/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 02/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 03/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 04/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 05/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 06/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 07/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 08/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 09/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 10/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 11/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 12/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 13/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 14/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 15/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 16/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 17/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 18/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 19/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 20/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 21/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 22/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 23/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 24/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 25/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 26/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 27/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 28/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 29/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 30/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 31/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 32/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 33/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 34/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 35/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 36/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 37/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 38/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 39/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 40/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 41/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 42/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 43/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 44/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 45/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 46/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 47/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 48/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 49/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 50/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 51/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 52/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 53/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 54/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 55/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 56/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 57/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 58/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 59/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 60/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 61/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 62/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Channel 63/64 : 0
lrdn0329:1724546:1724644 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0329:1724546:1724644 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0329:1724546:1724644 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0329:1724546:1724650 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn0329:1724546:1724651 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0329:1724546:1724644 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0329:1724546:1724644 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0329:1724546:1724644 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0329:1724546:1724644 [0] NCCL INFO ncclCommInitRankConfig comm 0xe49d0b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x88bed71eff1e55f9 - Init COMPLETE
lrdn0329:1724546:1724644 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn0295:1874970:1875132 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.10.189<0>
lrdn0295:1874970:1875132 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0295:1874970:1875132 [0] NCCL INFO Using network IB
lrdn0295:1874970:1875132 [0] NCCL INFO ncclCommInitRankConfig comm 0x174ee850 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x312b7250e5babd57 - Init START
lrdn0295:1874970:1875132 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0295:1874970:1875132 [0] NCCL INFO Bootstrap timings total 0.000401 (create 0.000019, send 0.000063, recv 0.000093, ring 0.000001, delay 0.000001)
lrdn0295:1874970:1875132 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0295:1874970:1875132 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0295:1874970:1875132 [0] NCCL INFO comm 0x174ee850 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 00/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 01/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 02/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 03/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 04/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 05/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 06/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 07/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 08/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 09/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 10/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 11/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 12/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 13/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 14/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 15/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 16/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 17/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 18/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 19/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 20/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 21/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 22/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 23/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 24/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 25/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 26/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 27/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 28/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 29/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 30/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 31/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 32/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 33/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 34/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 35/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 36/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 37/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 38/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 39/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 40/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 41/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 42/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 43/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 44/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 45/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 46/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 47/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 48/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 49/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 50/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 51/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 52/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 53/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 54/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 55/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 56/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 57/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 58/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 59/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 60/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 61/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 62/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Channel 63/64 : 0
lrdn0295:1874970:1875132 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0295:1874970:1875132 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0295:1874970:1875132 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0295:1874970:1875139 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn0295:1874970:1875138 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn0295:1874970:1875132 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0295:1874970:1875132 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0295:1874970:1875132 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0295:1874970:1875132 [0] NCCL INFO ncclCommInitRankConfig comm 0x174ee850 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x312b7250e5babd57 - Init COMPLETE
lrdn0295:1874970:1875132 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:14,215 | xffl.distributed.distributed |    DEBUG | [Rank 74]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0596:1736202:1736202 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.113<0>
lrdn0596:1736202:1736202 [0] NCCL INFO cudaDriverVersion 12020
lrdn0596:1736202:1736202 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0596:1736202:1736202 [0] NCCL INFO Comm config Blocking set to 1
lrdn0060:925357:925357 [0] NCCL INFO Bootstrap: Using ib0:10.128.7.17<0>
lrdn0060:925357:925357 [0] NCCL INFO cudaDriverVersion 12020
lrdn0060:925357:925357 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0060:925357:925357 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:14,327 | xffl.distributed.distributed |    DEBUG | [Rank 83]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:14,366 | xffl.distributed.distributed |    DEBUG | [Rank 196]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:14,376 | xffl.distributed.distributed |    DEBUG | [Rank 123]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0596:1736202:1736298 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0596:1736202:1736298 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:14,435 | xffl.distributed.distributed |    DEBUG | [Rank 30]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0596:1736202:1736298 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.113<0>
lrdn0596:1736202:1736298 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0596:1736202:1736298 [0] NCCL INFO Using network IB
lrdn0060:925357:925451 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0060:925357:925451 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0596:1736202:1736298 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc8e520 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9afa2a5012afce47 - Init START
lrdn0596:1736202:1736298 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0596:1736202:1736298 [0] NCCL INFO Bootstrap timings total 0.000401 (create 0.000018, send 0.000062, recv 0.000103, ring 0.000001, delay 0.000000)
lrdn0596:1736202:1736298 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0596:1736202:1736298 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0596:1736202:1736298 [0] NCCL INFO comm 0xdc8e520 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 00/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 01/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 02/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 03/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 04/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 05/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 06/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 07/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 08/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 09/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 10/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 11/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 12/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 13/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 14/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 15/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 16/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 17/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 18/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 19/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 20/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 21/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 22/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 23/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 24/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 25/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 26/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 27/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 28/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 29/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 30/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 31/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 32/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 33/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 34/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 35/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 36/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 37/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 38/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 39/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 40/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 41/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 42/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 43/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 44/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 45/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 46/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 47/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 48/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 49/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 50/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 51/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 52/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 53/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 54/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 55/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 56/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 57/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 58/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 59/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 60/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 61/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 62/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Channel 63/64 : 0
lrdn0596:1736202:1736298 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0596:1736202:1736298 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0596:1736202:1736298 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0596:1736202:1736305 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1
lrdn0596:1736202:1736304 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn0596:1736202:1736298 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0596:1736202:1736298 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0596:1736202:1736298 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0596:1736202:1736298 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc8e520 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9afa2a5012afce47 - Init COMPLETE
lrdn0596:1736202:1736298 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0060:925357:925451 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.7.17<0>
lrdn0060:925357:925451 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0060:925357:925451 [0] NCCL INFO Using network IB
lrdn0060:925357:925451 [0] NCCL INFO ncclCommInitRankConfig comm 0xe7ed880 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x26859bdf37269803 - Init START
lrdn0060:925357:925451 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0060:925357:925451 [0] NCCL INFO Bootstrap timings total 0.000410 (create 0.000021, send 0.000068, recv 0.000099, ring 0.000001, delay 0.000000)
lrdn0060:925357:925451 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0060:925357:925451 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0060:925357:925451 [0] NCCL INFO comm 0xe7ed880 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 00/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 01/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 02/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 03/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 04/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 05/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 06/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 07/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 08/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 09/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 10/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 11/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 12/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 13/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 14/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 15/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 16/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 17/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 18/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 19/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 20/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 21/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 22/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 23/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 24/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 25/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 26/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 27/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 28/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 29/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 30/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 31/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 32/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 33/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 34/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 35/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 36/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 37/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 38/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 39/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 40/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 41/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 42/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 43/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 44/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 45/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 46/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 47/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 48/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 49/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 50/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 51/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 52/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 53/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 54/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 55/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 56/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 57/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 58/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 59/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 60/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 61/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 62/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Channel 63/64 : 0
lrdn0060:925357:925451 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0060:925357:925451 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0060:925357:925451 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0060:925357:925457 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn0060:925357:925458 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 15
lrdn0060:925357:925451 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0060:925357:925451 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0060:925357:925451 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0060:925357:925451 [0] NCCL INFO ncclCommInitRankConfig comm 0xe7ed880 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x26859bdf37269803 - Init COMPLETE
lrdn0060:925357:925451 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0307:1873275:1873275 [0] NCCL INFO Bootstrap: Using ib0:10.128.10.237<0>
lrdn0307:1873275:1873275 [0] NCCL INFO cudaDriverVersion 12020
lrdn0307:1873275:1873275 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0307:1873275:1873275 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:14,595 | xffl.distributed.distributed |    DEBUG | [Rank 25]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0307:1873275:1873371 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0307:1873275:1873371 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0307:1873275:1873371 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.10.237<0>
lrdn0307:1873275:1873371 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0307:1873275:1873371 [0] NCCL INFO Using network IB
lrdn0307:1873275:1873371 [0] NCCL INFO ncclCommInitRankConfig comm 0xf076d90 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8252b0102f91a3e2 - Init START
lrdn0307:1873275:1873371 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0307:1873275:1873371 [0] NCCL INFO Bootstrap timings total 0.000410 (create 0.000022, send 0.000062, recv 0.000096, ring 0.000001, delay 0.000000)
[38;5;39m2025-08-02 08:49:14,789 | xffl.distributed.distributed |    DEBUG | [Rank 33]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0307:1873275:1873371 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0307:1873275:1873371 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0307:1873275:1873371 [0] NCCL INFO comm 0xf076d90 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 00/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 01/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 02/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 03/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 04/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 05/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 06/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 07/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 08/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 09/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 10/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 11/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 12/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 13/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 14/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 15/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 16/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 17/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 18/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 19/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 20/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 21/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 22/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 23/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 24/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 25/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 26/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 27/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 28/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 29/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 30/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 31/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 32/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 33/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 34/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 35/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 36/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 37/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 38/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 39/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 40/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 41/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 42/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 43/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 44/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 45/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 46/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 47/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 48/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 49/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 50/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 51/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 52/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 53/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 54/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 55/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 56/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 57/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 58/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 59/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 60/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 61/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 62/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Channel 63/64 : 0
lrdn0307:1873275:1873371 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0307:1873275:1873371 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0307:1873275:1873371 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0307:1873275:1873377 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn0307:1873275:1873378 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0307:1873275:1873371 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0307:1873275:1873371 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0307:1873275:1873371 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0307:1873275:1873371 [0] NCCL INFO ncclCommInitRankConfig comm 0xf076d90 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8252b0102f91a3e2 - Init COMPLETE
lrdn0307:1873275:1873371 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:14,838 | xffl.distributed.distributed |    DEBUG | [Rank 77]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:14,961 | xffl.distributed.distributed |    DEBUG | [Rank 174]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0523:1525899:1525899 [0] NCCL INFO Bootstrap: Using ib0:10.128.14.77<0>
lrdn0523:1525899:1525899 [0] NCCL INFO cudaDriverVersion 12020
lrdn0523:1525899:1525899 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0523:1525899:1525899 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:15,081 | xffl.distributed.distributed |    DEBUG | [Rank 27]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0523:1525899:1526003 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0523:1525899:1526003 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:15,241 | xffl.distributed.distributed |    DEBUG | [Rank 191]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0565:3828397:3828397 [0] NCCL INFO Bootstrap: Using ib0:10.128.14.245<0>
lrdn0565:3828397:3828397 [0] NCCL INFO cudaDriverVersion 12020
lrdn0565:3828397:3828397 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0565:3828397:3828397 [0] NCCL INFO Comm config Blocking set to 1
lrdn0523:1525899:1526003 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.14.77<0>
lrdn0523:1525899:1526003 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0523:1525899:1526003 [0] NCCL INFO Using network IB
lrdn0523:1525899:1526003 [0] NCCL INFO ncclCommInitRankConfig comm 0xd021440 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x4b177446209f7726 - Init START
lrdn0523:1525899:1526003 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0523:1525899:1526003 [0] NCCL INFO Bootstrap timings total 0.000424 (create 0.000022, send 0.000072, recv 0.000104, ring 0.000001, delay 0.000001)
lrdn0523:1525899:1526003 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0523:1525899:1526003 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0523:1525899:1526003 [0] NCCL INFO comm 0xd021440 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 00/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 01/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 02/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 03/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 04/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 05/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 06/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 07/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 08/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 09/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 10/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 11/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 12/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 13/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 14/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 15/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 16/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 17/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 18/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 19/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 20/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 21/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 22/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 23/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 24/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 25/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 26/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 27/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 28/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 29/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 30/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 31/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 32/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 33/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 34/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 35/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 36/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 37/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 38/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 39/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 40/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 41/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 42/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 43/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 44/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 45/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 46/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 47/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 48/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 49/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 50/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 51/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 52/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 53/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 54/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 55/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 56/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 57/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 58/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 59/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 60/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 61/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 62/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Channel 63/64 : 0
lrdn0523:1525899:1526003 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0523:1525899:1526003 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0523:1525899:1526003 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0523:1525899:1526009 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0523:1525899:1526010 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0523:1525899:1526003 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0523:1525899:1526003 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0523:1525899:1526003 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0523:1525899:1526003 [0] NCCL INFO ncclCommInitRankConfig comm 0xd021440 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x4b177446209f7726 - Init COMPLETE
lrdn0523:1525899:1526003 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1082:1968917:1968917 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.9<0>
lrdn1082:1968917:1968917 [0] NCCL INFO cudaDriverVersion 12020
lrdn1082:1968917:1968917 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1082:1968917:1968917 [0] NCCL INFO Comm config Blocking set to 1
lrdn0721:1259508:1259508 [0] NCCL INFO Bootstrap: Using ib0:10.128.17.101<0>
lrdn0721:1259508:1259508 [0] NCCL INFO cudaDriverVersion 12020
lrdn0721:1259508:1259508 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0721:1259508:1259508 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:15,368 | xffl.distributed.distributed |    DEBUG | [Rank 124]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:15,405 | xffl.distributed.distributed |    DEBUG | [Rank 37]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0565:3828397:3828492 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0565:3828397:3828492 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0238:1761357:1761357 [0] NCCL INFO Bootstrap: Using ib0:10.128.9.217<0>
lrdn0238:1761357:1761357 [0] NCCL INFO cudaDriverVersion 12020
lrdn0238:1761357:1761357 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0238:1761357:1761357 [0] NCCL INFO Comm config Blocking set to 1
lrdn0565:3828397:3828492 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.14.245<0>
lrdn0565:3828397:3828492 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0565:3828397:3828492 [0] NCCL INFO Using network IB
lrdn0565:3828397:3828492 [0] NCCL INFO ncclCommInitRankConfig comm 0xe216cc0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc30a78c9fec1497f - Init START
lrdn0565:3828397:3828492 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0565:3828397:3828492 [0] NCCL INFO Bootstrap timings total 0.000412 (create 0.000021, send 0.000066, recv 0.000098, ring 0.000001, delay 0.000000)
lrdn1082:1968917:1969025 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1082:1968917:1969025 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0565:3828397:3828492 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0565:3828397:3828492 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0565:3828397:3828492 [0] NCCL INFO comm 0xe216cc0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 00/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 01/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 02/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 03/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 04/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 05/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 06/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 07/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 08/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 09/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 10/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 11/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 12/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 13/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 14/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 15/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 16/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 17/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 18/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 19/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 20/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 21/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 22/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 23/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 24/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 25/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 26/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 27/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 28/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 29/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 30/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 31/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 32/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 33/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 34/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 35/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 36/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 37/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 38/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 39/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 40/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 41/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 42/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 43/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 44/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 45/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 46/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 47/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 48/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 49/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 50/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 51/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 52/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 53/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 54/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 55/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 56/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 57/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 58/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 59/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 60/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 61/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 62/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Channel 63/64 : 0
lrdn0565:3828397:3828492 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0565:3828397:3828492 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0565:3828397:3828492 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0565:3828397:3828498 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn0565:3828397:3828499 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0565:3828397:3828492 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0565:3828397:3828492 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0565:3828397:3828492 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0565:3828397:3828492 [0] NCCL INFO ncclCommInitRankConfig comm 0xe216cc0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc30a78c9fec1497f - Init COMPLETE
lrdn0565:3828397:3828492 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0721:1259508:1259605 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0721:1259508:1259605 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:15,508 | xffl.distributed.distributed |    DEBUG | [Rank 205]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:15,512 | xffl.distributed.distributed |    DEBUG | [Rank 21]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1082:1968917:1969025 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.9<0>
lrdn1082:1968917:1969025 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1082:1968917:1969025 [0] NCCL INFO Using network IB
lrdn1082:1968917:1969025 [0] NCCL INFO ncclCommInitRankConfig comm 0xcc66f20 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3e5972ed4b0ce454 - Init START
lrdn1082:1968917:1969025 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1082:1968917:1969025 [0] NCCL INFO Bootstrap timings total 0.000388 (create 0.000019, send 0.000060, recv 0.000090, ring 0.000001, delay 0.000000)
lrdn1082:1968917:1969025 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1082:1968917:1969025 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1082:1968917:1969025 [0] NCCL INFO comm 0xcc66f20 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 00/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 01/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 02/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 03/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 04/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 05/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 06/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 07/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 08/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 09/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 10/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 11/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 12/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 13/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 14/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 15/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 16/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 17/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 18/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 19/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 20/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 21/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 22/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 23/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 24/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 25/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 26/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 27/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 28/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 29/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 30/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 31/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 32/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 33/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 34/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 35/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 36/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 37/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 38/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 39/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 40/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 41/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 42/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 43/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 44/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 45/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 46/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 47/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 48/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 49/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 50/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 51/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 52/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 53/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 54/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 55/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 56/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 57/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 58/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 59/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 60/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 61/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 62/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Channel 63/64 : 0
lrdn1082:1968917:1969025 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1082:1968917:1969025 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1082:1968917:1969025 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
[38;5;39m2025-08-02 08:49:15,559 | xffl.distributed.distributed |    DEBUG | [Rank 44]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1082:1968917:1969031 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn1082:1968917:1969032 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn0238:1761357:1761454 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0238:1761357:1761454 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0721:1259508:1259605 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.17.101<0>
lrdn1082:1968917:1969025 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1082:1968917:1969025 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0721:1259508:1259605 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0721:1259508:1259605 [0] NCCL INFO Using network IB
lrdn0721:1259508:1259605 [0] NCCL INFO ncclCommInitRankConfig comm 0xe174080 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x4a8d31802b6fe724 - Init START
lrdn0721:1259508:1259605 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0721:1259508:1259605 [0] NCCL INFO Bootstrap timings total 0.000391 (create 0.000018, send 0.000063, recv 0.000099, ring 0.000001, delay 0.000001)
lrdn1082:1968917:1969025 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1082:1968917:1969025 [0] NCCL INFO ncclCommInitRankConfig comm 0xcc66f20 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3e5972ed4b0ce454 - Init COMPLETE
lrdn1082:1968917:1969025 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0721:1259508:1259605 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0721:1259508:1259605 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0721:1259508:1259605 [0] NCCL INFO comm 0xe174080 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 00/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 01/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 02/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 03/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 04/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 05/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 06/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 07/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 08/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 09/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 10/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 11/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 12/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 13/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 14/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 15/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 16/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 17/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 18/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 19/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 20/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 21/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 22/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 23/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 24/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 25/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 26/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 27/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 28/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 29/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 30/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 31/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 32/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 33/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 34/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 35/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 36/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 37/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 38/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 39/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 40/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 41/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 42/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 43/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 44/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 45/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 46/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 47/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 48/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 49/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 50/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 51/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 52/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 53/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 54/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 55/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 56/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 57/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 58/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 59/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 60/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 61/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 62/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Channel 63/64 : 0
lrdn0721:1259508:1259605 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0721:1259508:1259605 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0721:1259508:1259605 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0721:1259508:1259612 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0721:1259508:1259611 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn0238:1761357:1761454 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.9.217<0>
lrdn0721:1259508:1259605 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0721:1259508:1259605 [0] NCCL INFO CC Off, workFifoBytes 1048576
[38;5;39m2025-08-02 08:49:15,610 | xffl.distributed.distributed |    DEBUG | [Rank 32]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:15,613 | xffl.distributed.distributed |    DEBUG | [Rank 81]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0238:1761357:1761454 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0238:1761357:1761454 [0] NCCL INFO Using network IB
lrdn0721:1259508:1259605 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0721:1259508:1259605 [0] NCCL INFO ncclCommInitRankConfig comm 0xe174080 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x4a8d31802b6fe724 - Init COMPLETE
lrdn0721:1259508:1259605 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.27 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn0238:1761357:1761454 [0] NCCL INFO ncclCommInitRankConfig comm 0xca5a730 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xec760aa69076fc94 - Init START
lrdn0238:1761357:1761454 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0238:1761357:1761454 [0] NCCL INFO Bootstrap timings total 0.000511 (create 0.000020, send 0.000059, recv 0.000217, ring 0.000001, delay 0.000000)
lrdn0238:1761357:1761454 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0238:1761357:1761454 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0238:1761357:1761454 [0] NCCL INFO comm 0xca5a730 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 00/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 01/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 02/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 03/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 04/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 05/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 06/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 07/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 08/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 09/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 10/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 11/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 12/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 13/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 14/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 15/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 16/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 17/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 18/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 19/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 20/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 21/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 22/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 23/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 24/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 25/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 26/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 27/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 28/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 29/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 30/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 31/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 32/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 33/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 34/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 35/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 36/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 37/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 38/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 39/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 40/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 41/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 42/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 43/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 44/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 45/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 46/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 47/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 48/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 49/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 50/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 51/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 52/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 53/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 54/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 55/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 56/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 57/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 58/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 59/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 60/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 61/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 62/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Channel 63/64 : 0
lrdn0238:1761357:1761454 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0238:1761357:1761454 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0238:1761357:1761454 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0238:1761357:1761460 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0238:1761357:1761461 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn0205:2793889:2793889 [0] NCCL INFO Bootstrap: Using ib0:10.128.9.85<0>
lrdn0205:2793889:2793889 [0] NCCL INFO cudaDriverVersion 12020
lrdn0205:2793889:2793889 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0205:2793889:2793889 [0] NCCL INFO Comm config Blocking set to 1
lrdn0238:1761357:1761454 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0238:1761357:1761454 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0238:1761357:1761454 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0238:1761357:1761454 [0] NCCL INFO ncclCommInitRankConfig comm 0xca5a730 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xec760aa69076fc94 - Init COMPLETE
lrdn0238:1761357:1761454 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
[38;5;39m2025-08-02 08:49:15,745 | xffl.distributed.distributed |    DEBUG | [Rank 2]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0205:2793889:2793984 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0205:2793889:2793984 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0205:2793889:2793984 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.9.85<0>
[38;5;39m2025-08-02 08:49:15,851 | xffl.distributed.distributed |    DEBUG | [Rank 172]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0205:2793889:2793984 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0205:2793889:2793984 [0] NCCL INFO Using network IB
lrdn0205:2793889:2793984 [0] NCCL INFO ncclCommInitRankConfig comm 0xcf7d0c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xae346724cfe50603 - Init START
lrdn0205:2793889:2793984 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0205:2793889:2793984 [0] NCCL INFO Bootstrap timings total 0.000479 (create 0.000018, send 0.000062, recv 0.000191, ring 0.000001, delay 0.000000)
lrdn0205:2793889:2793984 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0205:2793889:2793984 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0205:2793889:2793984 [0] NCCL INFO comm 0xcf7d0c0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 00/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 01/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 02/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 03/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 04/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 05/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 06/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 07/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 08/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 09/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 10/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 11/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 12/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 13/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 14/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 15/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 16/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 17/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 18/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 19/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 20/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 21/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 22/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 23/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 24/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 25/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 26/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 27/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 28/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 29/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 30/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 31/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 32/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 33/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 34/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 35/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 36/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 37/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 38/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 39/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 40/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 41/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 42/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 43/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 44/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 45/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 46/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 47/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 48/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 49/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 50/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 51/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 52/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 53/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 54/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 55/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 56/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 57/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 58/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 59/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 60/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 61/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 62/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Channel 63/64 : 0
lrdn0205:2793889:2793984 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0205:2793889:2793984 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0205:2793889:2793984 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0205:2793889:2793990 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0205:2793889:2793991 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn0205:2793889:2793984 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0205:2793889:2793984 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0205:2793889:2793984 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0205:2793889:2793984 [0] NCCL INFO ncclCommInitRankConfig comm 0xcf7d0c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xae346724cfe50603 - Init COMPLETE
lrdn0205:2793889:2793984 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn0266:1530030:1530030 [0] NCCL INFO Bootstrap: Using ib0:10.128.10.73<0>
lrdn0266:1530030:1530030 [0] NCCL INFO cudaDriverVersion 12020
lrdn0266:1530030:1530030 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0266:1530030:1530030 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:16,074 | xffl.distributed.distributed |    DEBUG | [Rank 1]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0535:3448215:3448215 [0] NCCL INFO Bootstrap: Using ib0:10.128.14.125<0>
lrdn0535:3448215:3448215 [0] NCCL INFO cudaDriverVersion 12020
lrdn0535:3448215:3448215 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0535:3448215:3448215 [0] NCCL INFO Comm config Blocking set to 1
lrdn0266:1530030:1530125 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0266:1530030:1530125 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0266:1530030:1530125 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.10.73<0>
lrdn0266:1530030:1530125 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0266:1530030:1530125 [0] NCCL INFO Using network IB
lrdn0266:1530030:1530125 [0] NCCL INFO ncclCommInitRankConfig comm 0xce7cc30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc10944ad34a3d848 - Init START
lrdn0266:1530030:1530125 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0266:1530030:1530125 [0] NCCL INFO Bootstrap timings total 0.000374 (create 0.000019, send 0.000058, recv 0.000085, ring 0.000001, delay 0.000000)
lrdn0266:1530030:1530125 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0266:1530030:1530125 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0266:1530030:1530125 [0] NCCL INFO comm 0xce7cc30 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 00/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 01/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 02/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 03/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 04/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 05/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 06/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 07/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 08/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 09/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 10/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 11/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 12/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 13/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 14/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 15/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 16/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 17/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 18/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 19/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 20/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 21/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 22/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 23/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 24/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 25/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 26/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 27/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 28/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 29/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 30/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 31/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 32/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 33/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 34/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 35/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 36/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 37/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 38/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 39/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 40/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 41/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 42/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 43/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 44/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 45/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 46/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 47/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 48/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 49/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 50/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 51/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 52/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 53/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 54/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 55/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 56/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 57/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 58/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 59/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 60/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 61/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 62/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Channel 63/64 : 0
lrdn0266:1530030:1530125 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0266:1530030:1530125 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0266:1530030:1530125 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0266:1530030:1530131 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0266:1530030:1530132 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0266:1530030:1530125 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0266:1530030:1530125 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0266:1530030:1530125 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0266:1530030:1530125 [0] NCCL INFO ncclCommInitRankConfig comm 0xce7cc30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc10944ad34a3d848 - Init COMPLETE
lrdn0266:1530030:1530125 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0535:3448215:3448310 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0535:3448215:3448310 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:16,287 | xffl.distributed.distributed |    DEBUG | [Rank 117]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0535:3448215:3448310 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.14.125<0>
lrdn0535:3448215:3448310 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0535:3448215:3448310 [0] NCCL INFO Using network IB
lrdn0535:3448215:3448310 [0] NCCL INFO ncclCommInitRankConfig comm 0xe5531e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xac0756ea3992cdce - Init START
lrdn0535:3448215:3448310 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0535:3448215:3448310 [0] NCCL INFO Bootstrap timings total 0.000371 (create 0.000019, send 0.000062, recv 0.000077, ring 0.000001, delay 0.000000)
lrdn0535:3448215:3448310 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0535:3448215:3448310 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0535:3448215:3448310 [0] NCCL INFO comm 0xe5531e0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 00/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 01/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 02/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 03/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 04/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 05/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 06/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 07/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 08/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 09/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 10/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 11/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 12/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 13/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 14/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 15/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 16/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 17/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 18/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 19/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 20/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 21/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 22/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 23/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 24/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 25/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 26/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 27/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 28/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 29/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 30/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 31/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 32/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 33/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 34/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 35/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 36/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 37/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 38/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 39/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 40/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 41/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 42/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 43/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 44/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 45/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 46/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 47/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 48/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 49/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 50/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 51/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 52/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 53/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 54/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 55/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 56/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 57/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 58/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 59/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 60/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 61/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 62/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Channel 63/64 : 0
lrdn0535:3448215:3448310 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0535:3448215:3448310 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0535:3448215:3448310 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0535:3448215:3448316 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn0535:3448215:3448317 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
[38;5;39m2025-08-02 08:49:16,321 | xffl.distributed.distributed |    DEBUG | [Rank 65]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0535:3448215:3448310 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0535:3448215:3448310 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0535:3448215:3448310 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0535:3448215:3448310 [0] NCCL INFO ncclCommInitRankConfig comm 0xe5531e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xac0756ea3992cdce - Init COMPLETE
lrdn0535:3448215:3448310 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn0970:779132:779132 [0] NCCL INFO Bootstrap: Using ib0:10.128.21.73<0>
lrdn0970:779132:779132 [0] NCCL INFO cudaDriverVersion 12020
lrdn0970:779132:779132 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0970:779132:779132 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:16,463 | xffl.distributed.distributed |    DEBUG | [Rank 133]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0970:779132:779229 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0970:779132:779229 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:16,549 | xffl.distributed.distributed |    DEBUG | [Rank 188]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0970:779132:779229 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.21.73<0>
lrdn0970:779132:779229 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0970:779132:779229 [0] NCCL INFO Using network IB
lrdn0970:779132:779229 [0] NCCL INFO ncclCommInitRankConfig comm 0xe344b10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1755fc2c12958052 - Init START
lrdn0970:779132:779229 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0970:779132:779229 [0] NCCL INFO Bootstrap timings total 0.000402 (create 0.000020, send 0.000063, recv 0.000104, ring 0.000001, delay 0.000000)
lrdn0970:779132:779229 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0970:779132:779229 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0970:779132:779229 [0] NCCL INFO comm 0xe344b10 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 00/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 01/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 02/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 03/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 04/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 05/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 06/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 07/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 08/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 09/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 10/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 11/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 12/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 13/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 14/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 15/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 16/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 17/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 18/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 19/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 20/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 21/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 22/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 23/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 24/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 25/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 26/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 27/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 28/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 29/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 30/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 31/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 32/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 33/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 34/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 35/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 36/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 37/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 38/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 39/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 40/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 41/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 42/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 43/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 44/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 45/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 46/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 47/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 48/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 49/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 50/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 51/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 52/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 53/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 54/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 55/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 56/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 57/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 58/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 59/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 60/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 61/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 62/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Channel 63/64 : 0
lrdn0970:779132:779229 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0970:779132:779229 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0970:779132:779229 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0970:779132:779236 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0970:779132:779235 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn0970:779132:779229 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0970:779132:779229 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0970:779132:779229 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0970:779132:779229 [0] NCCL INFO ncclCommInitRankConfig comm 0xe344b10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1755fc2c12958052 - Init COMPLETE
lrdn0970:779132:779229 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0223:1701386:1701386 [0] NCCL INFO Bootstrap: Using ib0:10.128.9.157<0>
lrdn0223:1701386:1701386 [0] NCCL INFO cudaDriverVersion 12020
lrdn0223:1701386:1701386 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0223:1701386:1701386 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:16,647 | xffl.distributed.distributed |    DEBUG | [Rank 130]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:16,745 | xffl.distributed.distributed |    DEBUG | [Rank 63]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:16,751 | xffl.distributed.distributed |    DEBUG | [Rank 128]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0223:1701386:1701481 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0223:1701386:1701481 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:16,777 | xffl.distributed.distributed |    DEBUG | [Rank 138]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0223:1701386:1701481 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.9.157<0>
lrdn0223:1701386:1701481 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0223:1701386:1701481 [0] NCCL INFO Using network IB
lrdn0223:1701386:1701481 [0] NCCL INFO ncclCommInitRankConfig comm 0xdce5e80 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe141f226b4a772ed - Init START
lrdn0223:1701386:1701481 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0223:1701386:1701481 [0] NCCL INFO Bootstrap timings total 0.000384 (create 0.000019, send 0.000058, recv 0.000099, ring 0.000001, delay 0.000000)
lrdn0223:1701386:1701481 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0223:1701386:1701481 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0223:1701386:1701481 [0] NCCL INFO comm 0xdce5e80 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 00/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 01/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 02/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 03/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 04/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 05/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 06/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 07/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 08/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 09/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 10/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 11/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 12/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 13/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 14/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 15/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 16/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 17/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 18/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 19/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 20/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 21/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 22/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 23/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 24/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 25/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 26/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 27/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 28/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 29/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 30/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 31/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 32/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 33/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 34/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 35/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 36/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 37/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 38/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 39/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 40/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 41/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 42/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 43/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 44/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 45/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 46/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 47/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 48/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 49/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 50/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 51/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 52/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 53/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 54/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 55/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 56/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 57/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 58/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 59/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 60/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 61/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 62/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Channel 63/64 : 0
lrdn0223:1701386:1701481 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0223:1701386:1701481 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0223:1701386:1701481 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0223:1701386:1701487 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0223:1701386:1701488 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn0223:1701386:1701481 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0223:1701386:1701481 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0223:1701386:1701481 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0223:1701386:1701481 [0] NCCL INFO ncclCommInitRankConfig comm 0xdce5e80 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe141f226b4a772ed - Init COMPLETE
lrdn0223:1701386:1701481 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.27 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
[38;5;39m2025-08-02 08:49:16,871 | xffl.distributed.distributed |    DEBUG | [Rank 136]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:16,888 | xffl.distributed.distributed |    DEBUG | [Rank 71]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1054:163779:163779 [0] NCCL INFO Bootstrap: Using ib0:10.128.22.153<0>
[38;5;39m2025-08-02 08:49:16,895 | xffl.distributed.distributed |    DEBUG | [Rank 3]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1054:163779:163779 [0] NCCL INFO cudaDriverVersion 12020
lrdn1054:163779:163779 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1054:163779:163779 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:16,971 | xffl.distributed.distributed |    DEBUG | [Rank 250]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:17,005 | xffl.distributed.distributed |    DEBUG | [Rank 36]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1054:163779:163883 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1054:163779:163883 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1054:163779:163883 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.22.153<0>
lrdn1054:163779:163883 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1054:163779:163883 [0] NCCL INFO Using network IB
lrdn1054:163779:163883 [0] NCCL INFO ncclCommInitRankConfig comm 0xd85b200 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5173045c3dc7c50e - Init START
lrdn1054:163779:163883 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1054:163779:163883 [0] NCCL INFO Bootstrap timings total 0.000372 (create 0.000018, send 0.000054, recv 0.000088, ring 0.000001, delay 0.000000)
lrdn1054:163779:163883 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1054:163779:163883 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1054:163779:163883 [0] NCCL INFO comm 0xd85b200 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 00/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 01/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 02/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 03/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 04/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 05/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 06/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 07/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 08/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 09/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 10/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 11/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 12/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 13/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 14/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 15/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 16/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 17/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 18/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 19/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 20/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 21/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 22/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 23/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 24/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 25/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 26/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 27/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 28/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 29/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 30/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 31/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 32/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 33/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 34/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 35/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 36/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 37/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 38/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 39/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 40/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 41/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 42/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 43/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 44/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 45/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 46/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 47/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 48/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 49/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 50/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 51/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 52/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 53/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 54/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 55/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 56/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 57/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 58/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 59/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 60/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 61/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 62/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Channel 63/64 : 0
lrdn1054:163779:163883 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn1054:163779:163883 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1054:163779:163883 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1054:163779:163889 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn1054:163779:163890 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
[38;5;39m2025-08-02 08:49:17,119 | xffl.distributed.distributed |    DEBUG | [Rank 184]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1054:163779:163883 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1054:163779:163883 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1054:163779:163883 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1054:163779:163883 [0] NCCL INFO ncclCommInitRankConfig comm 0xd85b200 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5173045c3dc7c50e - Init COMPLETE
lrdn1054:163779:163883 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:17,152 | xffl.distributed.distributed |    DEBUG | [Rank 248]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:17,178 | xffl.distributed.distributed |    DEBUG | [Rank 140]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0284:1876255:1876255 [0] NCCL INFO Bootstrap: Using ib0:10.128.10.145<0>
lrdn0284:1876255:1876255 [0] NCCL INFO cudaDriverVersion 12020
lrdn0284:1876255:1876255 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0284:1876255:1876255 [0] NCCL INFO Comm config Blocking set to 1
lrdn0727:2905734:2905734 [0] NCCL INFO Bootstrap: Using ib0:10.128.17.125<0>
lrdn0727:2905734:2905734 [0] NCCL INFO cudaDriverVersion 12020
lrdn0727:2905734:2905734 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0727:2905734:2905734 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:17,345 | xffl.distributed.distributed |    DEBUG | [Rank 187]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:17,357 | xffl.distributed.distributed |    DEBUG | [Rank 242]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0284:1876255:1876349 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0284:1876255:1876349 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0284:1876255:1876349 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.10.145<0>
lrdn0727:2905734:2905830 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0727:2905734:2905830 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0284:1876255:1876349 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0284:1876255:1876349 [0] NCCL INFO Using network IB
lrdn0284:1876255:1876349 [0] NCCL INFO ncclCommInitRankConfig comm 0xd9ca010 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xbc787f2f1553c4b8 - Init START
lrdn0284:1876255:1876349 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0284:1876255:1876349 [0] NCCL INFO Bootstrap timings total 0.000402 (create 0.000021, send 0.000065, recv 0.000102, ring 0.000001, delay 0.000000)
lrdn1121:3096513:3096513 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.165<0>
lrdn1121:3096513:3096513 [0] NCCL INFO cudaDriverVersion 12020
lrdn1121:3096513:3096513 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1121:3096513:3096513 [0] NCCL INFO Comm config Blocking set to 1
lrdn0284:1876255:1876349 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0284:1876255:1876349 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0284:1876255:1876349 [0] NCCL INFO comm 0xd9ca010 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 00/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 01/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 02/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 03/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 04/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 05/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 06/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 07/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 08/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 09/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 10/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 11/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 12/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 13/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 14/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 15/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 16/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 17/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 18/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 19/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 20/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 21/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 22/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 23/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 24/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 25/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 26/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 27/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 28/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 29/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 30/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 31/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 32/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 33/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 34/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 35/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 36/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 37/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 38/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 39/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 40/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 41/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 42/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 43/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 44/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 45/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 46/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 47/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 48/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 49/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 50/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 51/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 52/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 53/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 54/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 55/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 56/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 57/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 58/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 59/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 60/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 61/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 62/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Channel 63/64 : 0
lrdn0284:1876255:1876349 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0284:1876255:1876349 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0284:1876255:1876349 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0284:1876255:1876356 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn0284:1876255:1876355 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0284:1876255:1876349 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0284:1876255:1876349 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0727:2905734:2905830 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.17.125<0>
lrdn0284:1876255:1876349 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0284:1876255:1876349 [0] NCCL INFO ncclCommInitRankConfig comm 0xd9ca010 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xbc787f2f1553c4b8 - Init COMPLETE
lrdn0284:1876255:1876349 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0727:2905734:2905830 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0727:2905734:2905830 [0] NCCL INFO Using network IB
lrdn0727:2905734:2905830 [0] NCCL INFO ncclCommInitRankConfig comm 0xd34b440 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x440fdc01e2a70a69 - Init START
lrdn0727:2905734:2905830 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0727:2905734:2905830 [0] NCCL INFO Bootstrap timings total 0.000402 (create 0.000022, send 0.000060, recv 0.000095, ring 0.000001, delay 0.000000)
lrdn0727:2905734:2905830 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0727:2905734:2905830 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0727:2905734:2905830 [0] NCCL INFO comm 0xd34b440 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 00/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 01/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 02/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 03/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 04/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 05/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 06/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 07/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 08/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 09/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 10/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 11/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 12/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 13/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 14/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 15/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 16/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 17/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 18/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 19/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 20/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 21/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 22/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 23/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 24/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 25/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 26/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 27/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 28/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 29/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 30/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 31/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 32/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 33/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 34/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 35/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 36/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 37/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 38/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 39/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 40/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 41/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 42/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 43/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 44/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 45/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 46/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 47/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 48/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 49/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 50/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 51/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 52/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 53/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 54/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 55/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 56/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 57/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 58/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 59/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 60/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 61/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 62/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Channel 63/64 : 0
lrdn0727:2905734:2905830 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0727:2905734:2905830 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0727:2905734:2905830 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0727:2905734:2905836 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0727:2905734:2905837 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn0727:2905734:2905830 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0727:2905734:2905830 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0727:2905734:2905830 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0727:2905734:2905830 [0] NCCL INFO ncclCommInitRankConfig comm 0xd34b440 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x440fdc01e2a70a69 - Init COMPLETE
lrdn0727:2905734:2905830 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.02, rest 0.00)
lrdn0305:1129623:1129623 [0] NCCL INFO Bootstrap: Using ib0:10.128.10.229<0>
lrdn0305:1129623:1129623 [0] NCCL INFO cudaDriverVersion 12020
lrdn0305:1129623:1129623 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0305:1129623:1129623 [0] NCCL INFO Comm config Blocking set to 1
lrdn1121:3096513:3096618 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1121:3096513:3096618 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1121:3096513:3096618 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.165<0>
lrdn1121:3096513:3096618 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1121:3096513:3096618 [0] NCCL INFO Using network IB
lrdn1121:3096513:3096618 [0] NCCL INFO ncclCommInitRankConfig comm 0xea8cf60 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x458cf35aac713a9e - Init START
lrdn1121:3096513:3096618 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1121:3096513:3096618 [0] NCCL INFO Bootstrap timings total 0.000470 (create 0.000019, send 0.000058, recv 0.000186, ring 0.000001, delay 0.000000)
[38;5;39m2025-08-02 08:49:17,631 | xffl.distributed.distributed |    DEBUG | [Rank 10]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1121:3096513:3096618 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1121:3096513:3096618 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1121:3096513:3096618 [0] NCCL INFO comm 0xea8cf60 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 00/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 01/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 02/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 03/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 04/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 05/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 06/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 07/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 08/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 09/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 10/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 11/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 12/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 13/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 14/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 15/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 16/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 17/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 18/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 19/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 20/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 21/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 22/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 23/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 24/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 25/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 26/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 27/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 28/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 29/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 30/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 31/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 32/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 33/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 34/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 35/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 36/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 37/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 38/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 39/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 40/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 41/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 42/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 43/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 44/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 45/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 46/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 47/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 48/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 49/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 50/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 51/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 52/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 53/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 54/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 55/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 56/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 57/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 58/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 59/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 60/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 61/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 62/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Channel 63/64 : 0
lrdn1121:3096513:3096618 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1121:3096513:3096618 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1121:3096513:3096618 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1121:3096513:3096624 [0] NCCL INFO [Proxy Service] Device 0 CPU core 8
lrdn1121:3096513:3096625 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn1121:3096513:3096618 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1121:3096513:3096618 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1121:3096513:3096618 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1121:3096513:3096618 [0] NCCL INFO ncclCommInitRankConfig comm 0xea8cf60 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x458cf35aac713a9e - Init COMPLETE
lrdn1121:3096513:3096618 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0305:1129623:1129719 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0305:1129623:1129719 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:17,707 | xffl.distributed.distributed |    DEBUG | [Rank 9]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0544:1567247:1567247 [0] NCCL INFO Bootstrap: Using ib0:10.128.14.161<0>
lrdn0544:1567247:1567247 [0] NCCL INFO cudaDriverVersion 12020
lrdn0544:1567247:1567247 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0544:1567247:1567247 [0] NCCL INFO Comm config Blocking set to 1
lrdn0305:1129623:1129719 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.10.229<0>
lrdn0305:1129623:1129719 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0305:1129623:1129719 [0] NCCL INFO Using network IB
lrdn0305:1129623:1129719 [0] NCCL INFO ncclCommInitRankConfig comm 0xe35a410 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8c59955ecdf0b30e - Init START
lrdn0305:1129623:1129719 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0305:1129623:1129719 [0] NCCL INFO Bootstrap timings total 0.000377 (create 0.000019, send 0.000062, recv 0.000085, ring 0.000001, delay 0.000000)
lrdn0305:1129623:1129719 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0305:1129623:1129719 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0305:1129623:1129719 [0] NCCL INFO comm 0xe35a410 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 00/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 01/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 02/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 03/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 04/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 05/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 06/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 07/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 08/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 09/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 10/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 11/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 12/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 13/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 14/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 15/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 16/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 17/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 18/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 19/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 20/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 21/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 22/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 23/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 24/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 25/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 26/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 27/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 28/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 29/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 30/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 31/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 32/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 33/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 34/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 35/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 36/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 37/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 38/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 39/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 40/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 41/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 42/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 43/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 44/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 45/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 46/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 47/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 48/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 49/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 50/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 51/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 52/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 53/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 54/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 55/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 56/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 57/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 58/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 59/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 60/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 61/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 62/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Channel 63/64 : 0
lrdn0305:1129623:1129719 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0305:1129623:1129719 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0305:1129623:1129719 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0305:1129623:1129725 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn0305:1129623:1129726 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 12
lrdn0305:1129623:1129719 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0305:1129623:1129719 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0305:1129623:1129719 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0305:1129623:1129719 [0] NCCL INFO ncclCommInitRankConfig comm 0xe35a410 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8c59955ecdf0b30e - Init COMPLETE
lrdn0305:1129623:1129719 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0193:1561269:1561269 [0] NCCL INFO Bootstrap: Using ib0:10.128.9.37<0>
lrdn0193:1561269:1561269 [0] NCCL INFO cudaDriverVersion 12020
lrdn0193:1561269:1561269 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0193:1561269:1561269 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:17,873 | xffl.distributed.distributed |    DEBUG | [Rank 78]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0544:1567247:1567352 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0544:1567247:1567352 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:17,913 | xffl.distributed.distributed |    DEBUG | [Rank 150]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0544:1567247:1567352 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.14.161<0>
[38;5;39m2025-08-02 08:49:17,939 | xffl.distributed.distributed |    DEBUG | [Rank 125]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0544:1567247:1567352 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0544:1567247:1567352 [0] NCCL INFO Using network IB
lrdn0544:1567247:1567352 [0] NCCL INFO ncclCommInitRankConfig comm 0x100d6340 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9fc545b6ba825400 - Init START
lrdn0544:1567247:1567352 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0544:1567247:1567352 [0] NCCL INFO Bootstrap timings total 0.000370 (create 0.000019, send 0.000062, recv 0.000082, ring 0.000001, delay 0.000000)
lrdn0544:1567247:1567352 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0544:1567247:1567352 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0544:1567247:1567352 [0] NCCL INFO comm 0x100d6340 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 00/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 01/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 02/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 03/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 04/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 05/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 06/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 07/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 08/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 09/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 10/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 11/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 12/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 13/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 14/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 15/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 16/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 17/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 18/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 19/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 20/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 21/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 22/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 23/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 24/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 25/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 26/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 27/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 28/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 29/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 30/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 31/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 32/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 33/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 34/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 35/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 36/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 37/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 38/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 39/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 40/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 41/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 42/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 43/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 44/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 45/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 46/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 47/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 48/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 49/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 50/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 51/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 52/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 53/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 54/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 55/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 56/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 57/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 58/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 59/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 60/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 61/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 62/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Channel 63/64 : 0
lrdn0544:1567247:1567352 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0544:1567247:1567352 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0544:1567247:1567352 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0544:1567247:1567359 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn0544:1567247:1567358 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
[38;5;39m2025-08-02 08:49:17,966 | xffl.distributed.distributed |    DEBUG | [Rank 82]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0544:1567247:1567352 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0544:1567247:1567352 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0544:1567247:1567352 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0544:1567247:1567352 [0] NCCL INFO ncclCommInitRankConfig comm 0x100d6340 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9fc545b6ba825400 - Init COMPLETE
lrdn0544:1567247:1567352 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0193:1561269:1561364 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0193:1561269:1561364 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0029:1561975:1561975 [0] NCCL INFO Bootstrap: Using ib0:10.128.6.149<0>
lrdn0029:1561975:1561975 [0] NCCL INFO cudaDriverVersion 12020
lrdn0029:1561975:1561975 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0029:1561975:1561975 [0] NCCL INFO Comm config Blocking set to 1
lrdn0254:1202236:1202236 [0] NCCL INFO Bootstrap: Using ib0:10.128.10.25<0>
lrdn0254:1202236:1202236 [0] NCCL INFO cudaDriverVersion 12020
lrdn0254:1202236:1202236 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0254:1202236:1202236 [0] NCCL INFO Comm config Blocking set to 1
lrdn0193:1561269:1561364 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.9.37<0>
lrdn0193:1561269:1561364 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0193:1561269:1561364 [0] NCCL INFO Using network IB
lrdn0193:1561269:1561364 [0] NCCL INFO ncclCommInitRankConfig comm 0xd6795f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x895522676db06124 - Init START
lrdn0193:1561269:1561364 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0193:1561269:1561364 [0] NCCL INFO Bootstrap timings total 0.000385 (create 0.000017, send 0.000058, recv 0.000103, ring 0.000001, delay 0.000000)
lrdn0193:1561269:1561364 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0193:1561269:1561364 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0193:1561269:1561364 [0] NCCL INFO comm 0xd6795f0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 00/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 01/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 02/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 03/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 04/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 05/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 06/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 07/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 08/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 09/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 10/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 11/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 12/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 13/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 14/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 15/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 16/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 17/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 18/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 19/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 20/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 21/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 22/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 23/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 24/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 25/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 26/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 27/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 28/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 29/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 30/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 31/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 32/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 33/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 34/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 35/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 36/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 37/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 38/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 39/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 40/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 41/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 42/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 43/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 44/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 45/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 46/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 47/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 48/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 49/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 50/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 51/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 52/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 53/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 54/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 55/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 56/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 57/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 58/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 59/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 60/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 61/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 62/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Channel 63/64 : 0
lrdn0193:1561269:1561364 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0193:1561269:1561364 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0193:1561269:1561364 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0193:1561269:1561371 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0193:1561269:1561370 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn0193:1561269:1561364 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0193:1561269:1561364 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0193:1561269:1561364 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0193:1561269:1561364 [0] NCCL INFO ncclCommInitRankConfig comm 0xd6795f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x895522676db06124 - Init COMPLETE
lrdn0193:1561269:1561364 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:18,132 | xffl.distributed.distributed |    DEBUG | [Rank 134]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0029:1561975:1562070 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0029:1561975:1562070 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0254:1202236:1202342 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0254:1202236:1202342 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0029:1561975:1562070 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.6.149<0>
lrdn0029:1561975:1562070 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0029:1561975:1562070 [0] NCCL INFO Using network IB
lrdn0029:1561975:1562070 [0] NCCL INFO ncclCommInitRankConfig comm 0xcbe92b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd6d32425972ef049 - Init START
lrdn0029:1561975:1562070 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0029:1561975:1562070 [0] NCCL INFO Bootstrap timings total 0.000386 (create 0.000018, send 0.000061, recv 0.000098, ring 0.000001, delay 0.000001)
lrdn0254:1202236:1202342 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.10.25<0>
lrdn0029:1561975:1562070 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0029:1561975:1562070 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0029:1561975:1562070 [0] NCCL INFO comm 0xcbe92b0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 00/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 01/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 02/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 03/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 04/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 05/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 06/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 07/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 08/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 09/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 10/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 11/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 12/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 13/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 14/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 15/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 16/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 17/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 18/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 19/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 20/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 21/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 22/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 23/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 24/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 25/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 26/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 27/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 28/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 29/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 30/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 31/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 32/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 33/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 34/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 35/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 36/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 37/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 38/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 39/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 40/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 41/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 42/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 43/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 44/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 45/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 46/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 47/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 48/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 49/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 50/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 51/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 52/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 53/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 54/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 55/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 56/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 57/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 58/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 59/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 60/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 61/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 62/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Channel 63/64 : 0
lrdn0029:1561975:1562070 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0029:1561975:1562070 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0029:1561975:1562070 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0029:1561975:1562076 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0029:1561975:1562077 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0254:1202236:1202342 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0254:1202236:1202342 [0] NCCL INFO Using network IB
lrdn0254:1202236:1202342 [0] NCCL INFO ncclCommInitRankConfig comm 0xcf264f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xbc69acf4ba370b9e - Init START
lrdn0254:1202236:1202342 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0254:1202236:1202342 [0] NCCL INFO Bootstrap timings total 0.000362 (create 0.000019, send 0.000054, recv 0.000090, ring 0.000001, delay 0.000000)
lrdn0029:1561975:1562070 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0029:1561975:1562070 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0029:1561975:1562070 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0029:1561975:1562070 [0] NCCL INFO ncclCommInitRankConfig comm 0xcbe92b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd6d32425972ef049 - Init COMPLETE
lrdn0029:1561975:1562070 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0254:1202236:1202342 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0254:1202236:1202342 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0254:1202236:1202342 [0] NCCL INFO comm 0xcf264f0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 00/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 01/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 02/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 03/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 04/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 05/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 06/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 07/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 08/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 09/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 10/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 11/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 12/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 13/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 14/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 15/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 16/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 17/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 18/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 19/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 20/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 21/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 22/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 23/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 24/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 25/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 26/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 27/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 28/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 29/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 30/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 31/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 32/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 33/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 34/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 35/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 36/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 37/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 38/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 39/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 40/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 41/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 42/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 43/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 44/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 45/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 46/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 47/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 48/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 49/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 50/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 51/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 52/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 53/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 54/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 55/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 56/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 57/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 58/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 59/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 60/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 61/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 62/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Channel 63/64 : 0
lrdn0254:1202236:1202342 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0254:1202236:1202342 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0254:1202236:1202342 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0254:1202236:1202348 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0254:1202236:1202349 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1
[38;5;39m2025-08-02 08:49:18,242 | xffl.distributed.distributed |    DEBUG | [Rank 8]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0254:1202236:1202342 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0254:1202236:1202342 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0254:1202236:1202342 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0254:1202236:1202342 [0] NCCL INFO ncclCommInitRankConfig comm 0xcf264f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xbc69acf4ba370b9e - Init COMPLETE
lrdn0254:1202236:1202342 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:18,268 | xffl.distributed.distributed |    DEBUG | [Rank 76]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:18,321 | xffl.distributed.distributed |    DEBUG | [Rank 120]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:18,361 | xffl.distributed.distributed |    DEBUG | [Rank 86]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:18,427 | xffl.distributed.distributed |    DEBUG | [Rank 149]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:18,434 | xffl.distributed.distributed |    DEBUG | [Rank 129]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:18,454 | xffl.distributed.distributed |    DEBUG | [Rank 178]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0964:1708887:1708887 [0] NCCL INFO Bootstrap: Using ib0:10.128.21.49<0>
lrdn0964:1708887:1708887 [0] NCCL INFO cudaDriverVersion 12020
lrdn0964:1708887:1708887 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0964:1708887:1708887 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:18,636 | xffl.distributed.distributed |    DEBUG | [Rank 201]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:18,658 | xffl.distributed.distributed |    DEBUG | [Rank 236]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:18,676 | xffl.distributed.distributed |    DEBUG | [Rank 198]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0964:1708887:1708983 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0964:1708887:1708983 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:18,717 | xffl.distributed.distributed |    DEBUG | [Rank 235]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:18,731 | xffl.distributed.distributed |    DEBUG | [Rank 15]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:18,733 | xffl.distributed.distributed |    DEBUG | [Rank 13]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0964:1708887:1708983 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.21.49<0>
lrdn0964:1708887:1708983 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0964:1708887:1708983 [0] NCCL INFO Using network IB
lrdn0964:1708887:1708983 [0] NCCL INFO ncclCommInitRankConfig comm 0xc4d3630 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7969ebfe45a027b1 - Init START
lrdn0964:1708887:1708983 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0964:1708887:1708983 [0] NCCL INFO Bootstrap timings total 0.000587 (create 0.000022, send 0.000066, recv 0.000272, ring 0.000001, delay 0.000000)
lrdn0964:1708887:1708983 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0964:1708887:1708983 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0964:1708887:1708983 [0] NCCL INFO comm 0xc4d3630 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 00/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 01/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 02/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 03/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 04/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 05/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 06/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 07/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 08/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 09/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 10/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 11/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 12/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 13/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 14/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 15/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 16/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 17/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 18/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 19/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 20/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 21/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 22/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 23/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 24/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 25/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 26/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 27/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 28/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 29/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 30/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 31/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 32/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 33/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 34/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 35/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 36/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 37/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 38/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 39/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 40/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 41/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 42/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 43/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 44/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 45/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 46/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 47/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 48/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 49/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 50/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 51/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 52/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 53/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 54/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 55/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 56/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 57/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 58/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 59/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 60/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 61/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 62/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Channel 63/64 : 0
lrdn0964:1708887:1708983 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0964:1708887:1708983 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0964:1708887:1708983 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0964:1708887:1708989 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn0964:1708887:1708990 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn0964:1708887:1708983 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0964:1708887:1708983 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0964:1708887:1708983 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0964:1708887:1708983 [0] NCCL INFO ncclCommInitRankConfig comm 0xc4d3630 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7969ebfe45a027b1 - Init COMPLETE
lrdn0964:1708887:1708983 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.14, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0020:1671373:1671373 [0] NCCL INFO Bootstrap: Using ib0:10.128.6.113<0>
lrdn0020:1671373:1671373 [0] NCCL INFO cudaDriverVersion 12020
lrdn0020:1671373:1671373 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0020:1671373:1671373 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:18,832 | xffl.distributed.distributed |    DEBUG | [Rank 11]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:18,833 | xffl.distributed.distributed |    DEBUG | [Rank 31]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:18,854 | xffl.distributed.distributed |    DEBUG | [Rank 227]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0020:1671373:1671468 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0020:1671373:1671468 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0479:600014:600014 [0] NCCL INFO Bootstrap: Using ib0:10.128.13.157<0>
lrdn0479:600014:600014 [0] NCCL INFO cudaDriverVersion 12020
lrdn0479:600014:600014 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0479:600014:600014 [0] NCCL INFO Comm config Blocking set to 1
lrdn0020:1671373:1671468 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.6.113<0>
lrdn0020:1671373:1671468 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0020:1671373:1671468 [0] NCCL INFO Using network IB
lrdn0020:1671373:1671468 [0] NCCL INFO ncclCommInitRankConfig comm 0xe062f80 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd19469853ce598bd - Init START
lrdn0020:1671373:1671468 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0020:1671373:1671468 [0] NCCL INFO Bootstrap timings total 0.000389 (create 0.000017, send 0.000056, recv 0.000101, ring 0.000001, delay 0.000000)
lrdn0020:1671373:1671468 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0020:1671373:1671468 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0020:1671373:1671468 [0] NCCL INFO comm 0xe062f80 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 00/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 01/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 02/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 03/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 04/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 05/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 06/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 07/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 08/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 09/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 10/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 11/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 12/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 13/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 14/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 15/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 16/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 17/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 18/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 19/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 20/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 21/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 22/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 23/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 24/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 25/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 26/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 27/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 28/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 29/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 30/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 31/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 32/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 33/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 34/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 35/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 36/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 37/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 38/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 39/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 40/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 41/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 42/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 43/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 44/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 45/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 46/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 47/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 48/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 49/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 50/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 51/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 52/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 53/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 54/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 55/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 56/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 57/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 58/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 59/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 60/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 61/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 62/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Channel 63/64 : 0
lrdn0020:1671373:1671468 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0020:1671373:1671468 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0020:1671373:1671468 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0020:1671373:1671474 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn0020:1671373:1671475 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0020:1671373:1671468 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0020:1671373:1671468 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0020:1671373:1671468 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0020:1671373:1671468 [0] NCCL INFO ncclCommInitRankConfig comm 0xe062f80 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd19469853ce598bd - Init COMPLETE
lrdn0020:1671373:1671468 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:19,115 | xffl.distributed.distributed |    DEBUG | [Rank 185]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0479:600014:600108 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0479:600014:600108 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0479:600014:600108 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.13.157<0>
lrdn0479:600014:600108 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0479:600014:600108 [0] NCCL INFO Using network IB
lrdn0479:600014:600108 [0] NCCL INFO ncclCommInitRankConfig comm 0xc5bee20 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe4d4c6b909e78166 - Init START
lrdn0479:600014:600108 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0479:600014:600108 [0] NCCL INFO Bootstrap timings total 0.000378 (create 0.000019, send 0.000059, recv 0.000094, ring 0.000001, delay 0.000000)
lrdn0479:600014:600108 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0479:600014:600108 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0479:600014:600108 [0] NCCL INFO comm 0xc5bee20 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 00/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 01/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 02/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 03/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 04/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 05/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 06/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 07/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 08/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 09/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 10/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 11/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 12/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 13/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 14/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 15/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 16/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 17/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 18/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 19/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 20/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 21/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 22/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 23/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 24/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 25/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 26/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 27/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 28/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 29/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 30/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 31/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 32/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 33/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 34/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 35/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 36/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 37/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 38/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 39/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 40/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 41/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 42/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 43/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 44/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 45/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 46/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 47/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 48/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 49/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 50/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 51/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 52/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 53/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 54/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 55/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 56/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 57/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 58/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 59/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 60/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 61/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 62/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Channel 63/64 : 0
lrdn0479:600014:600108 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0479:600014:600108 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0479:600014:600108 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0479:600014:600114 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn0479:600014:600115 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0479:600014:600108 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0479:600014:600108 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0479:600014:600108 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0479:600014:600108 [0] NCCL INFO ncclCommInitRankConfig comm 0xc5bee20 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe4d4c6b909e78166 - Init COMPLETE
lrdn0479:600014:600108 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:19,278 | xffl.distributed.distributed |    DEBUG | [Rank 222]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:19,294 | xffl.distributed.distributed |    DEBUG | [Rank 238]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:19,295 | xffl.distributed.distributed |    DEBUG | [Rank 16]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:19,336 | xffl.distributed.distributed |    DEBUG | [Rank 79]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:19,342 | xffl.distributed.distributed |    DEBUG | [Rank 69]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:19,367 | xffl.distributed.distributed |    DEBUG | [Rank 146]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:19,380 | xffl.distributed.distributed |    DEBUG | [Rank 253]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:19,401 | xffl.distributed.distributed |    DEBUG | [Rank 46]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0803:1970378:1970378 [0] NCCL INFO Bootstrap: Using ib0:10.128.18.173<0>
lrdn0803:1970378:1970378 [0] NCCL INFO cudaDriverVersion 12020
lrdn0803:1970378:1970378 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0803:1970378:1970378 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:19,480 | xffl.distributed.distributed |    DEBUG | [Rank 98]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:19,513 | xffl.distributed.distributed |    DEBUG | [Rank 142]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:19,517 | xffl.distributed.distributed |    DEBUG | [Rank 94]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:19,525 | xffl.distributed.distributed |    DEBUG | [Rank 241]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:19,551 | xffl.distributed.distributed |    DEBUG | [Rank 239]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:19,559 | xffl.distributed.distributed |    DEBUG | [Rank 137]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0803:1970378:1970537 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0803:1970378:1970537 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:19,627 | xffl.distributed.distributed |    DEBUG | [Rank 162]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0803:1970378:1970537 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.18.173<0>
[38;5;39m2025-08-02 08:49:19,632 | xffl.distributed.distributed |    DEBUG | [Rank 42]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0803:1970378:1970537 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0803:1970378:1970537 [0] NCCL INFO Using network IB
lrdn0803:1970378:1970537 [0] NCCL INFO ncclCommInitRankConfig comm 0x2fe44fe0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x57b331037edf598b - Init START
lrdn0803:1970378:1970537 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0803:1970378:1970537 [0] NCCL INFO Bootstrap timings total 0.000380 (create 0.000018, send 0.000058, recv 0.000089, ring 0.000001, delay 0.000000)
lrdn0803:1970378:1970537 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0803:1970378:1970537 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0803:1970378:1970537 [0] NCCL INFO comm 0x2fe44fe0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 00/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 01/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 02/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 03/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 04/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 05/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 06/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 07/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 08/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 09/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 10/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 11/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 12/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 13/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 14/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 15/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 16/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 17/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 18/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 19/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 20/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 21/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 22/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 23/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 24/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 25/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 26/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 27/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 28/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 29/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 30/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 31/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 32/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 33/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 34/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 35/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 36/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 37/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 38/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 39/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 40/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 41/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 42/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 43/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 44/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 45/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 46/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 47/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 48/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 49/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 50/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 51/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 52/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 53/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 54/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 55/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 56/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 57/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 58/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 59/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 60/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 61/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 62/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Channel 63/64 : 0
lrdn0803:1970378:1970537 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0803:1970378:1970537 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0803:1970378:1970537 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0803:1970378:1970543 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn0803:1970378:1970544 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
[38;5;39m2025-08-02 08:49:19,659 | xffl.distributed.distributed |    DEBUG | [Rank 58]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0803:1970378:1970537 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0803:1970378:1970537 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0803:1970378:1970537 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0803:1970378:1970537 [0] NCCL INFO ncclCommInitRankConfig comm 0x2fe44fe0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x57b331037edf598b - Init COMPLETE
lrdn0803:1970378:1970537 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0773:1742369:1742369 [0] NCCL INFO Bootstrap: Using ib0:10.128.18.53<0>
lrdn0773:1742369:1742369 [0] NCCL INFO cudaDriverVersion 12020
lrdn0773:1742369:1742369 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0773:1742369:1742369 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:19,766 | xffl.distributed.distributed |    DEBUG | [Rank 195]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:19,786 | xffl.distributed.distributed |    DEBUG | [Rank 132]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0773:1742369:1742465 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0773:1742369:1742465 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0773:1742369:1742465 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.18.53<0>
lrdn0773:1742369:1742465 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0773:1742369:1742465 [0] NCCL INFO Using network IB
[38;5;39m2025-08-02 08:49:19,877 | xffl.distributed.distributed |    DEBUG | [Rank 200]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0773:1742369:1742465 [0] NCCL INFO ncclCommInitRankConfig comm 0xd5a3e50 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7eb3b3471ef48471 - Init START
lrdn0773:1742369:1742465 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0773:1742369:1742465 [0] NCCL INFO Bootstrap timings total 0.000389 (create 0.000020, send 0.000068, recv 0.000086, ring 0.000001, delay 0.000001)
[38;5;39m2025-08-02 08:49:19,887 | xffl.distributed.distributed |    DEBUG | [Rank 217]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0773:1742369:1742465 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0773:1742369:1742465 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0773:1742369:1742465 [0] NCCL INFO comm 0xd5a3e50 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 00/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 01/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 02/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 03/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 04/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 05/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 06/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 07/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 08/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 09/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 10/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 11/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 12/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 13/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 14/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 15/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 16/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 17/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 18/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 19/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 20/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 21/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 22/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 23/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 24/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 25/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 26/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 27/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 28/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 29/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 30/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 31/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 32/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 33/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 34/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 35/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 36/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 37/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 38/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 39/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 40/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 41/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 42/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 43/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 44/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 45/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 46/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 47/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 48/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 49/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 50/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 51/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 52/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 53/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 54/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 55/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 56/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 57/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 58/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 59/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 60/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 61/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 62/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Channel 63/64 : 0
lrdn0773:1742369:1742465 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0773:1742369:1742465 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0773:1742369:1742465 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0773:1742369:1742471 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn0773:1742369:1742472 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0773:1742369:1742465 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0773:1742369:1742465 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0773:1742369:1742465 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0773:1742369:1742465 [0] NCCL INFO ncclCommInitRankConfig comm 0xd5a3e50 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7eb3b3471ef48471 - Init COMPLETE
lrdn0773:1742369:1742465 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:19,972 | xffl.distributed.distributed |    DEBUG | [Rank 90]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:19,984 | xffl.distributed.distributed |    DEBUG | [Rank 179]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,037 | xffl.distributed.distributed |    DEBUG | [Rank 244]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0763:1858216:1858216 [0] NCCL INFO Bootstrap: Using ib0:10.128.18.13<0>
lrdn0763:1858216:1858216 [0] NCCL INFO cudaDriverVersion 12020
lrdn0763:1858216:1858216 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0763:1858216:1858216 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:20,109 | xffl.distributed.distributed |    DEBUG | [Rank 80]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,122 | xffl.distributed.distributed |    DEBUG | [Rank 206]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,123 | xffl.distributed.distributed |    DEBUG | [Rank 189]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0830:1770477:1770477 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.25<0>
lrdn0830:1770477:1770477 [0] NCCL INFO cudaDriverVersion 12020
lrdn0830:1770477:1770477 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
[38;5;39m2025-08-02 08:49:20,136 | xffl.distributed.distributed |    DEBUG | [Rank 7]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0830:1770477:1770477 [0] NCCL INFO Comm config Blocking set to 1
lrdn1042:1873399:1873399 [0] NCCL INFO Bootstrap: Using ib0:10.128.22.105<0>
lrdn1042:1873399:1873399 [0] NCCL INFO cudaDriverVersion 12020
lrdn1042:1873399:1873399 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
[38;5;39m2025-08-02 08:49:20,167 | xffl.distributed.distributed |    DEBUG | [Rank 64]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1042:1873399:1873399 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:20,182 | xffl.distributed.distributed |    DEBUG | [Rank 240]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0763:1858216:1858310 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0763:1858216:1858310 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:20,231 | xffl.distributed.distributed |    DEBUG | [Rank 232]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,268 | xffl.distributed.distributed |    DEBUG | [Rank 228]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0763:1858216:1858310 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.18.13<0>
lrdn0763:1858216:1858310 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0763:1858216:1858310 [0] NCCL INFO Using network IB
lrdn0763:1858216:1858310 [0] NCCL INFO ncclCommInitRankConfig comm 0xf87b030 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x891e0e4e79c25850 - Init START
lrdn0763:1858216:1858310 [0] NCCL INFO RAS client listening socket at ::1<28028>
[38;5;39m2025-08-02 08:49:20,280 | xffl.distributed.distributed |    DEBUG | [Rank 197]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0763:1858216:1858310 [0] NCCL INFO Bootstrap timings total 0.000459 (create 0.000019, send 0.000062, recv 0.000177, ring 0.000001, delay 0.000000)
lrdn0830:1770477:1770583 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0830:1770477:1770583 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0763:1858216:1858310 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0763:1858216:1858310 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0763:1858216:1858310 [0] NCCL INFO comm 0xf87b030 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 00/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 01/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 02/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 03/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 04/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 05/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 06/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 07/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 08/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 09/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 10/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 11/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 12/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 13/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 14/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 15/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 16/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 17/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 18/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 19/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 20/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 21/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 22/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 23/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 24/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 25/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 26/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 27/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 28/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 29/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 30/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 31/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 32/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 33/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 34/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 35/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 36/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 37/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 38/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 39/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 40/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 41/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 42/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 43/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 44/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 45/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 46/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 47/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 48/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 49/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 50/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 51/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 52/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 53/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 54/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 55/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 56/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 57/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 58/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 59/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 60/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 61/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 62/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Channel 63/64 : 0
lrdn0763:1858216:1858310 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0763:1858216:1858310 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0763:1858216:1858310 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0763:1858216:1858316 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0763:1858216:1858317 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0763:1858216:1858310 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0763:1858216:1858310 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0763:1858216:1858310 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0763:1858216:1858310 [0] NCCL INFO ncclCommInitRankConfig comm 0xf87b030 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x891e0e4e79c25850 - Init COMPLETE
lrdn0763:1858216:1858310 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn1042:1873399:1873503 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1042:1873399:1873503 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:20,324 | xffl.distributed.distributed |    DEBUG | [Rank 139]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0830:1770477:1770583 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.25<0>
lrdn0830:1770477:1770583 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0830:1770477:1770583 [0] NCCL INFO Using network IB
lrdn0830:1770477:1770583 [0] NCCL INFO ncclCommInitRankConfig comm 0xe271520 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xa44e7f050c304479 - Init START
lrdn0830:1770477:1770583 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0830:1770477:1770583 [0] NCCL INFO Bootstrap timings total 0.000582 (create 0.000016, send 0.000053, recv 0.000314, ring 0.000001, delay 0.000001)
lrdn0830:1770477:1770583 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0830:1770477:1770583 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0830:1770477:1770583 [0] NCCL INFO comm 0xe271520 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 00/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 01/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 02/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 03/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 04/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 05/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 06/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 07/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 08/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 09/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 10/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 11/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 12/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 13/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 14/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 15/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 16/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 17/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 18/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 19/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 20/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 21/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 22/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 23/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 24/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 25/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 26/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 27/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 28/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 29/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 30/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 31/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 32/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 33/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 34/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 35/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 36/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 37/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 38/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 39/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 40/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 41/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 42/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 43/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 44/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 45/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 46/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 47/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 48/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 49/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 50/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 51/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 52/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 53/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 54/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 55/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 56/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 57/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 58/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 59/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 60/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 61/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 62/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Channel 63/64 : 0
lrdn0830:1770477:1770583 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0830:1770477:1770583 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0830:1770477:1770583 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0830:1770477:1770589 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn0830:1770477:1770590 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn1042:1873399:1873503 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.22.105<0>
lrdn1042:1873399:1873503 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1042:1873399:1873503 [0] NCCL INFO Using network IB
lrdn0830:1770477:1770583 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0830:1770477:1770583 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1042:1873399:1873503 [0] NCCL INFO ncclCommInitRankConfig comm 0xccb9920 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x92a9582b1cfbd592 - Init START
lrdn1042:1873399:1873503 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1042:1873399:1873503 [0] NCCL INFO Bootstrap timings total 0.000367 (create 0.000018, send 0.000068, recv 0.000089, ring 0.000001, delay 0.000000)
lrdn0830:1770477:1770583 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0830:1770477:1770583 [0] NCCL INFO ncclCommInitRankConfig comm 0xe271520 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xa44e7f050c304479 - Init COMPLETE
lrdn0830:1770477:1770583 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:20,373 | xffl.distributed.distributed |    DEBUG | [Rank 243]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1042:1873399:1873503 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1042:1873399:1873503 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1042:1873399:1873503 [0] NCCL INFO comm 0xccb9920 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 00/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 01/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 02/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 03/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 04/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 05/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 06/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 07/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 08/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 09/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 10/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 11/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 12/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 13/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 14/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 15/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 16/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 17/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 18/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 19/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 20/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 21/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 22/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 23/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 24/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 25/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 26/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 27/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 28/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 29/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 30/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 31/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 32/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 33/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 34/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 35/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 36/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 37/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 38/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 39/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 40/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 41/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 42/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 43/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 44/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 45/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 46/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 47/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 48/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 49/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 50/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 51/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 52/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 53/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 54/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 55/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 56/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 57/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 58/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 59/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 60/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 61/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 62/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Channel 63/64 : 0
lrdn1042:1873399:1873503 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1042:1873399:1873503 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1042:1873399:1873503 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1042:1873399:1873511 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn1042:1873399:1873512 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1042:1873399:1873503 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1042:1873399:1873503 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1042:1873399:1873503 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1042:1873399:1873503 [0] NCCL INFO ncclCommInitRankConfig comm 0xccb9920 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x92a9582b1cfbd592 - Init COMPLETE
lrdn1042:1873399:1873503 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:20,475 | xffl.distributed.distributed |    DEBUG | [Rank 101]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,484 | xffl.distributed.distributed |    DEBUG | [Rank 60]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,484 | xffl.distributed.distributed |    DEBUG | [Rank 50]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,494 | xffl.distributed.distributed |    DEBUG | [Rank 165]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1397:2438823:2438823 [0] NCCL INFO Bootstrap: Using ib0:10.128.27.245<0>
lrdn1397:2438823:2438823 [0] NCCL INFO cudaDriverVersion 12020
lrdn1397:2438823:2438823 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1397:2438823:2438823 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:20,518 | xffl.distributed.distributed |    DEBUG | [Rank 254]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,525 | xffl.distributed.distributed |    DEBUG | [Rank 147]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,557 | xffl.distributed.distributed |    DEBUG | [Rank 115]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,563 | xffl.distributed.distributed |    DEBUG | [Rank 104]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,565 | xffl.distributed.distributed |    DEBUG | [Rank 164]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,593 | xffl.distributed.distributed |    DEBUG | [Rank 168]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,597 | xffl.distributed.distributed |    DEBUG | [Rank 155]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,625 | xffl.distributed.distributed |    DEBUG | [Rank 156]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,627 | xffl.distributed.distributed |    DEBUG | [Rank 70]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0506:2407304:2407304 [0] NCCL INFO Bootstrap: Using ib0:10.128.14.9<0>
[38;5;39m2025-08-02 08:49:20,628 | xffl.distributed.distributed |    DEBUG | [Rank 97]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0506:2407304:2407304 [0] NCCL INFO cudaDriverVersion 12020
lrdn0506:2407304:2407304 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0506:2407304:2407304 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:20,631 | xffl.distributed.distributed |    DEBUG | [Rank 110]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,633 | xffl.distributed.distributed |    DEBUG | [Rank 153]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,637 | xffl.distributed.distributed |    DEBUG | [Rank 99]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,655 | xffl.distributed.distributed |    DEBUG | [Rank 249]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,656 | xffl.distributed.distributed |    DEBUG | [Rank 246]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,662 | xffl.distributed.distributed |    DEBUG | [Rank 192]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1397:2438823:2438918 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1397:2438823:2438918 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:20,690 | xffl.distributed.distributed |    DEBUG | [Rank 141]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1397:2438823:2438918 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.27.245<0>
lrdn1397:2438823:2438918 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1397:2438823:2438918 [0] NCCL INFO Using network IB
lrdn1397:2438823:2438918 [0] NCCL INFO ncclCommInitRankConfig comm 0xdb1ef90 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1b610ec11aad89db - Init START
lrdn1397:2438823:2438918 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1397:2438823:2438918 [0] NCCL INFO Bootstrap timings total 0.000383 (create 0.000016, send 0.000055, recv 0.000092, ring 0.000001, delay 0.000000)
[38;5;39m2025-08-02 08:49:20,726 | xffl.distributed.distributed |    DEBUG | [Rank 56]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1397:2438823:2438918 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1397:2438823:2438918 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1397:2438823:2438918 [0] NCCL INFO comm 0xdb1ef90 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 00/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 01/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 02/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 03/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 04/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 05/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 06/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 07/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 08/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 09/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 10/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 11/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 12/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 13/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 14/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 15/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 16/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 17/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 18/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 19/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 20/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 21/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 22/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 23/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 24/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 25/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 26/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 27/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 28/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 29/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 30/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 31/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 32/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 33/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 34/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 35/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 36/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 37/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 38/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 39/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 40/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 41/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 42/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 43/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 44/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 45/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 46/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 47/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 48/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 49/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 50/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 51/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 52/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 53/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 54/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 55/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 56/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 57/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 58/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 59/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 60/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 61/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 62/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Channel 63/64 : 0
lrdn1397:2438823:2438918 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1397:2438823:2438918 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1397:2438823:2438918 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1397:2438823:2438925 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn1397:2438823:2438924 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
[38;5;39m2025-08-02 08:49:20,736 | xffl.distributed.distributed |    DEBUG | [Rank 157]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,740 | xffl.distributed.distributed |    DEBUG | [Rank 18]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1397:2438823:2438918 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1397:2438823:2438918 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1397:2438823:2438918 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1397:2438823:2438918 [0] NCCL INFO ncclCommInitRankConfig comm 0xdb1ef90 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1b610ec11aad89db - Init COMPLETE
lrdn1397:2438823:2438918 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1376:1889406:1889406 [0] NCCL INFO Bootstrap: Using ib0:10.128.27.161<0>
lrdn1376:1889406:1889406 [0] NCCL INFO cudaDriverVersion 12020
lrdn1376:1889406:1889406 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1376:1889406:1889406 [0] NCCL INFO Comm config Blocking set to 1
lrdn0506:2407304:2407399 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0506:2407304:2407399 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:20,788 | xffl.distributed.distributed |    DEBUG | [Rank 55]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,806 | xffl.distributed.distributed |    DEBUG | [Rank 111]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,818 | xffl.distributed.distributed |    DEBUG | [Rank 231]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0506:2407304:2407399 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.14.9<0>
lrdn0506:2407304:2407399 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0506:2407304:2407399 [0] NCCL INFO Using network IB
lrdn0506:2407304:2407399 [0] NCCL INFO ncclCommInitRankConfig comm 0xd4fb120 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x6189bb2d6f14d08d - Init START
lrdn0506:2407304:2407399 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0506:2407304:2407399 [0] NCCL INFO Bootstrap timings total 0.000357 (create 0.000017, send 0.000064, recv 0.000077, ring 0.000001, delay 0.000001)
[38;5;39m2025-08-02 08:49:20,827 | xffl.distributed.distributed |    DEBUG | [Rank 113]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,827 | xffl.distributed.distributed |    DEBUG | [Rank 167]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,837 | xffl.distributed.distributed |    DEBUG | [Rank 152]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,838 | xffl.distributed.distributed |    DEBUG | [Rank 53]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0506:2407304:2407399 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0506:2407304:2407399 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0506:2407304:2407399 [0] NCCL INFO comm 0xd4fb120 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 00/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 01/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 02/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 03/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 04/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 05/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 06/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 07/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 08/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 09/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 10/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 11/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 12/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 13/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 14/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 15/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 16/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 17/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 18/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 19/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 20/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 21/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 22/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 23/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 24/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 25/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 26/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 27/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 28/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 29/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 30/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 31/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 32/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 33/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 34/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 35/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 36/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 37/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 38/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 39/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 40/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 41/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 42/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 43/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 44/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 45/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 46/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 47/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 48/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 49/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 50/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 51/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 52/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 53/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 54/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 55/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 56/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 57/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 58/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 59/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 60/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 61/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 62/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Channel 63/64 : 0
lrdn0506:2407304:2407399 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0506:2407304:2407399 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0506:2407304:2407399 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0506:2407304:2407406 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0506:2407304:2407405 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0506:2407304:2407399 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0506:2407304:2407399 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0506:2407304:2407399 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0506:2407304:2407399 [0] NCCL INFO ncclCommInitRankConfig comm 0xd4fb120 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x6189bb2d6f14d08d - Init COMPLETE
lrdn0506:2407304:2407399 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:20,869 | xffl.distributed.distributed |    DEBUG | [Rank 211]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,876 | xffl.distributed.distributed |    DEBUG | [Rank 213]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,877 | xffl.distributed.distributed |    DEBUG | [Rank 20]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,893 | xffl.distributed.distributed |    DEBUG | [Rank 215]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,914 | xffl.distributed.distributed |    DEBUG | [Rank 54]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1376:1889406:1889512 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1376:1889406:1889512 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[38;5;39m2025-08-02 08:49:20,920 | xffl.distributed.distributed |    DEBUG | [Rank 221]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,944 | xffl.distributed.distributed |    DEBUG | [Rank 12]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,950 | xffl.distributed.distributed |    DEBUG | [Rank 106]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1376:1889406:1889512 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.27.161<0>
lrdn1376:1889406:1889512 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1376:1889406:1889512 [0] NCCL INFO Using network IB
lrdn1376:1889406:1889512 [0] NCCL INFO ncclCommInitRankConfig comm 0xd55d060 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8df83b1f551f162 - Init START
lrdn1376:1889406:1889512 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1376:1889406:1889512 [0] NCCL INFO Bootstrap timings total 0.000378 (create 0.000019, send 0.000063, recv 0.000090, ring 0.000001, delay 0.000001)
[38;5;39m2025-08-02 08:49:20,971 | xffl.distributed.distributed |    DEBUG | [Rank 159]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,972 | xffl.distributed.distributed |    DEBUG | [Rank 169]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1376:1889406:1889512 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1376:1889406:1889512 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1376:1889406:1889512 [0] NCCL INFO comm 0xd55d060 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 00/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 01/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 02/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 03/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 04/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 05/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 06/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 07/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 08/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 09/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 10/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 11/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 12/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 13/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 14/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 15/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 16/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 17/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 18/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 19/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 20/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 21/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 22/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 23/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 24/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 25/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 26/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 27/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 28/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 29/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 30/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 31/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 32/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 33/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 34/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 35/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 36/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 37/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 38/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 39/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 40/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 41/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 42/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 43/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 44/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 45/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 46/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 47/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 48/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 49/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 50/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 51/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 52/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 53/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 54/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 55/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 56/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 57/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 58/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 59/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 60/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 61/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 62/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Channel 63/64 : 0
lrdn1376:1889406:1889512 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1376:1889406:1889512 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1376:1889406:1889512 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1376:1889406:1889518 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1376:1889406:1889519 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0842:3750746:3750746 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.73<0>
lrdn1376:1889406:1889512 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1376:1889406:1889512 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0842:3750746:3750746 [0] NCCL INFO cudaDriverVersion 12020
lrdn0842:3750746:3750746 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
[38;5;39m2025-08-02 08:49:20,993 | xffl.distributed.distributed |    DEBUG | [Rank 210]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0842:3750746:3750746 [0] NCCL INFO Comm config Blocking set to 1
[38;5;39m2025-08-02 08:49:20,994 | xffl.distributed.distributed |    DEBUG | [Rank 252]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:20,995 | xffl.distributed.distributed |    DEBUG | [Rank 108]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn1376:1889406:1889512 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1376:1889406:1889512 [0] NCCL INFO ncclCommInitRankConfig comm 0xd55d060 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8df83b1f551f162 - Init COMPLETE
lrdn1376:1889406:1889512 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
[38;5;39m2025-08-02 08:49:21,005 | xffl.distributed.distributed |    DEBUG | [Rank 166]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:21,007 | xffl.distributed.distributed |    DEBUG | [Rank 251]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:21,019 | xffl.distributed.distributed |    DEBUG | [Rank 107]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:21,020 | xffl.distributed.distributed |    DEBUG | [Rank 219]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:21,035 | xffl.distributed.distributed |    DEBUG | [Rank 114]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:21,043 | xffl.distributed.distributed |    DEBUG | [Rank 144]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:21,043 | xffl.distributed.distributed |    DEBUG | [Rank 154]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:21,058 | xffl.distributed.distributed |    DEBUG | [Rank 216]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:21,062 | xffl.distributed.distributed |    DEBUG | [Rank 163]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:21,064 | xffl.distributed.distributed |    DEBUG | [Rank 209]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
[38;5;39m2025-08-02 08:49:21,068 | xffl.distributed.distributed |    DEBUG | [Rank 59]: Activating "ShardingStrategy.FULL_SHARD" sharding strategy[0m
lrdn0842:3750746:3750842 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0842:3750746:3750842 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0842:3750746:3750842 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.73<0>
lrdn0842:3750746:3750842 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0842:3750746:3750842 [0] NCCL INFO Using network IB
lrdn0842:3750746:3750842 [0] NCCL INFO ncclCommInitRankConfig comm 0xe8dd2a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8e9885646f037e2d - Init START
lrdn0842:3750746:3750842 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0842:3750746:3750842 [0] NCCL INFO Bootstrap timings total 0.000382 (create 0.000016, send 0.000057, recv 0.000096, ring 0.000001, delay 0.000001)
lrdn0842:3750746:3750842 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0842:3750746:3750842 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0842:3750746:3750842 [0] NCCL INFO comm 0xe8dd2a0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 00/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 01/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 02/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 03/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 04/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 05/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 06/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 07/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 08/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 09/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 10/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 11/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 12/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 13/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 14/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 15/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 16/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 17/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 18/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 19/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 20/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 21/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 22/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 23/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 24/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 25/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 26/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 27/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 28/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 29/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 30/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 31/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 32/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 33/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 34/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 35/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 36/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 37/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 38/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 39/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 40/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 41/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 42/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 43/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 44/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 45/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 46/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 47/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 48/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 49/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 50/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 51/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 52/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 53/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 54/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 55/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 56/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 57/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 58/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 59/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 60/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 61/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 62/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Channel 63/64 : 0
lrdn0842:3750746:3750842 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0842:3750746:3750842 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0842:3750746:3750842 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0842:3750746:3750848 [0] NCCL INFO [Proxy Service] Device 0 CPU core 8
lrdn0842:3750746:3750849 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn0842:3750746:3750842 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0842:3750746:3750842 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0842:3750746:3750842 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0842:3750746:3750842 [0] NCCL INFO ncclCommInitRankConfig comm 0xe8dd2a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8e9885646f037e2d - Init COMPLETE
lrdn0842:3750746:3750842 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0283:1744962:1744962 [0] NCCL INFO Bootstrap: Using ib0:10.128.10.141<0>
lrdn0283:1744962:1744962 [0] NCCL INFO cudaDriverVersion 12020
lrdn0283:1744962:1744962 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0283:1744962:1744962 [0] NCCL INFO Comm config Blocking set to 1
lrdn0283:1744962:1745057 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0283:1744962:1745057 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0283:1744962:1745057 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.10.141<0>
lrdn0283:1744962:1745057 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0283:1744962:1745057 [0] NCCL INFO Using network IB
lrdn0283:1744962:1745057 [0] NCCL INFO ncclCommInitRankConfig comm 0xda61230 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x238e820f0d02d99c - Init START
lrdn0283:1744962:1745057 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0283:1744962:1745057 [0] NCCL INFO Bootstrap timings total 0.000385 (create 0.000017, send 0.000058, recv 0.000094, ring 0.000001, delay 0.000000)
lrdn0283:1744962:1745057 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0283:1744962:1745057 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0283:1744962:1745057 [0] NCCL INFO comm 0xda61230 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 00/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 01/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 02/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 03/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 04/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 05/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 06/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 07/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 08/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 09/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 10/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 11/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 12/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 13/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 14/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 15/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 16/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 17/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 18/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 19/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 20/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 21/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 22/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 23/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 24/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 25/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 26/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 27/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 28/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 29/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 30/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 31/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 32/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 33/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 34/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 35/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 36/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 37/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 38/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 39/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 40/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 41/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 42/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 43/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 44/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 45/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 46/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 47/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 48/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 49/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 50/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 51/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 52/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 53/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 54/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 55/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 56/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 57/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 58/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 59/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 60/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 61/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 62/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Channel 63/64 : 0
lrdn0283:1744962:1745057 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0283:1744962:1745057 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0283:1744962:1745057 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0283:1744962:1745063 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0283:1744962:1745064 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0283:1744962:1745057 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0283:1744962:1745057 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0283:1744962:1745057 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0283:1744962:1745057 [0] NCCL INFO ncclCommInitRankConfig comm 0xda61230 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x238e820f0d02d99c - Init COMPLETE
lrdn0283:1744962:1745057 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.14, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0475:1717583:1717583 [0] NCCL INFO Bootstrap: Using ib0:10.128.13.141<0>
lrdn0475:1717583:1717583 [0] NCCL INFO cudaDriverVersion 12020
lrdn0475:1717583:1717583 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0475:1717583:1717583 [0] NCCL INFO Comm config Blocking set to 1
lrdn0697:2705141:2705141 [0] NCCL INFO Bootstrap: Using ib0:10.128.17.5<0>
lrdn0697:2705141:2705141 [0] NCCL INFO cudaDriverVersion 12020
lrdn0697:2705141:2705141 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0697:2705141:2705141 [0] NCCL INFO Comm config Blocking set to 1
lrdn0475:1717583:1717678 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0475:1717583:1717678 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0697:2705141:2705308 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0697:2705141:2705308 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0475:1717583:1717678 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.13.141<0>
lrdn0475:1717583:1717678 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0475:1717583:1717678 [0] NCCL INFO Using network IB
lrdn0475:1717583:1717678 [0] NCCL INFO ncclCommInitRankConfig comm 0xdf02bc0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xbec606f5350baa3f - Init START
lrdn0475:1717583:1717678 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0475:1717583:1717678 [0] NCCL INFO Bootstrap timings total 0.000496 (create 0.000015, send 0.000062, recv 0.000196, ring 0.000001, delay 0.000000)
lrdn0697:2705141:2705308 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.17.5<0>
lrdn0697:2705141:2705308 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0697:2705141:2705308 [0] NCCL INFO Using network IB
lrdn0697:2705141:2705308 [0] NCCL INFO ncclCommInitRankConfig comm 0xdb43540 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1fedc3840d7fbd02 - Init START
lrdn0697:2705141:2705308 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0697:2705141:2705308 [0] NCCL INFO Bootstrap timings total 0.000376 (create 0.000015, send 0.000059, recv 0.000094, ring 0.000001, delay 0.000000)
lrdn0475:1717583:1717678 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0475:1717583:1717678 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0475:1717583:1717678 [0] NCCL INFO comm 0xdf02bc0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 00/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 01/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 02/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 03/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 04/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 05/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 06/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 07/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 08/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 09/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 10/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 11/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 12/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 13/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 14/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 15/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 16/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 17/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 18/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 19/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 20/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 21/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 22/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 23/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 24/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 25/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 26/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 27/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 28/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 29/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 30/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 31/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 32/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 33/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 34/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 35/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 36/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 37/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 38/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 39/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 40/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 41/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 42/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 43/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 44/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 45/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 46/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 47/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 48/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 49/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 50/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 51/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 52/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 53/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 54/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 55/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 56/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 57/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 58/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 59/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 60/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 61/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 62/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Channel 63/64 : 0
lrdn0475:1717583:1717678 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0475:1717583:1717678 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0475:1717583:1717678 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0475:1717583:1717684 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn0475:1717583:1717685 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0697:2705141:2705308 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0697:2705141:2705308 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0697:2705141:2705308 [0] NCCL INFO comm 0xdb43540 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 00/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 01/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 02/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 03/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 04/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 05/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 06/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 07/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 08/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 09/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 10/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 11/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 12/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 13/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 14/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 15/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 16/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 17/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 18/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 19/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 20/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 21/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 22/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 23/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 24/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 25/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 26/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 27/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 28/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 29/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 30/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 31/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 32/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 33/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 34/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 35/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 36/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 37/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 38/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 39/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 40/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 41/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 42/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 43/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 44/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 45/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 46/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 47/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 48/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 49/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 50/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 51/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 52/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 53/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 54/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 55/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 56/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 57/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 58/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 59/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 60/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 61/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 62/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Channel 63/64 : 0
lrdn0697:2705141:2705308 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0697:2705141:2705308 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0697:2705141:2705308 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0697:2705141:2705315 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn0697:2705141:2705314 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn0475:1717583:1717678 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0475:1717583:1717678 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0475:1717583:1717678 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0475:1717583:1717678 [0] NCCL INFO ncclCommInitRankConfig comm 0xdf02bc0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xbec606f5350baa3f - Init COMPLETE
lrdn0475:1717583:1717678 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0697:2705141:2705308 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0697:2705141:2705308 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0697:2705141:2705308 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0697:2705141:2705308 [0] NCCL INFO ncclCommInitRankConfig comm 0xdb43540 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1fedc3840d7fbd02 - Init COMPLETE
lrdn0697:2705141:2705308 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0875:123346:123346 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.205<0>
lrdn0875:123346:123346 [0] NCCL INFO cudaDriverVersion 12020
lrdn0875:123346:123346 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0875:123346:123346 [0] NCCL INFO Comm config Blocking set to 1
lrdn0875:123346:123451 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0875:123346:123451 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0875:123346:123451 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.205<0>
lrdn0875:123346:123451 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0875:123346:123451 [0] NCCL INFO Using network IB
lrdn0875:123346:123451 [0] NCCL INFO ncclCommInitRankConfig comm 0xd81dbc0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf46d2d6a4be8ba04 - Init START
lrdn0875:123346:123451 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0875:123346:123451 [0] NCCL INFO Bootstrap timings total 0.000383 (create 0.000016, send 0.000057, recv 0.000095, ring 0.000001, delay 0.000000)
lrdn0875:123346:123451 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0875:123346:123451 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0875:123346:123451 [0] NCCL INFO comm 0xd81dbc0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 00/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 01/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 02/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 03/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 04/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 05/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 06/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 07/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 08/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 09/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 10/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 11/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 12/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 13/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 14/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 15/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 16/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 17/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 18/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 19/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 20/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 21/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 22/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 23/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 24/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 25/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 26/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 27/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 28/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 29/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 30/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 31/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 32/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 33/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 34/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 35/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 36/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 37/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 38/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 39/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 40/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 41/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 42/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 43/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 44/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 45/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 46/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 47/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 48/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 49/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 50/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 51/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 52/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 53/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 54/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 55/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 56/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 57/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 58/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 59/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 60/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 61/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 62/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Channel 63/64 : 0
lrdn0875:123346:123451 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0875:123346:123451 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0875:123346:123451 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0875:123346:123457 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0875:123346:123458 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0875:123346:123451 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0875:123346:123451 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0875:123346:123451 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0875:123346:123451 [0] NCCL INFO ncclCommInitRankConfig comm 0xd81dbc0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf46d2d6a4be8ba04 - Init COMPLETE
lrdn0875:123346:123451 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0117:3684502:3684502 [0] NCCL INFO Bootstrap: Using ib0:10.128.7.245<0>
lrdn0117:3684502:3684502 [0] NCCL INFO cudaDriverVersion 12020
lrdn0117:3684502:3684502 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0117:3684502:3684502 [0] NCCL INFO Comm config Blocking set to 1
lrdn0117:3684502:3684598 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0117:3684502:3684598 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0823:1648443:1648443 [0] NCCL INFO Bootstrap: Using ib0:10.128.18.253<0>
lrdn0117:3684502:3684598 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.7.245<0>
lrdn0823:1648443:1648443 [0] NCCL INFO cudaDriverVersion 12020
lrdn0823:1648443:1648443 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0823:1648443:1648443 [0] NCCL INFO Comm config Blocking set to 1
lrdn0117:3684502:3684598 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0117:3684502:3684598 [0] NCCL INFO Using network IB
lrdn0117:3684502:3684598 [0] NCCL INFO ncclCommInitRankConfig comm 0xd4cb400 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x59524d761d50d417 - Init START
lrdn0117:3684502:3684598 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0117:3684502:3684598 [0] NCCL INFO Bootstrap timings total 0.000385 (create 0.000019, send 0.000061, recv 0.000099, ring 0.000001, delay 0.000000)
lrdn0117:3684502:3684598 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0117:3684502:3684598 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0117:3684502:3684598 [0] NCCL INFO comm 0xd4cb400 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 00/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 01/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 02/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 03/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 04/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 05/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 06/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 07/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 08/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 09/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 10/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 11/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 12/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 13/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 14/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 15/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 16/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 17/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 18/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 19/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 20/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 21/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 22/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 23/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 24/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 25/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 26/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 27/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 28/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 29/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 30/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 31/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 32/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 33/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 34/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 35/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 36/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 37/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 38/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 39/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 40/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 41/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 42/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 43/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 44/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 45/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 46/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 47/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 48/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 49/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 50/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 51/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 52/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 53/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 54/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 55/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 56/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 57/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 58/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 59/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 60/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 61/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 62/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Channel 63/64 : 0
lrdn0117:3684502:3684598 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0117:3684502:3684598 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0117:3684502:3684598 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0117:3684502:3684604 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0117:3684502:3684605 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0117:3684502:3684598 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0117:3684502:3684598 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0117:3684502:3684598 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0117:3684502:3684598 [0] NCCL INFO ncclCommInitRankConfig comm 0xd4cb400 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x59524d761d50d417 - Init COMPLETE
lrdn0117:3684502:3684598 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0823:1648443:1648548 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0823:1648443:1648548 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0110:4136466:4136466 [0] NCCL INFO Bootstrap: Using ib0:10.128.7.217<0>
lrdn0110:4136466:4136466 [0] NCCL INFO cudaDriverVersion 12020
lrdn0110:4136466:4136466 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0110:4136466:4136466 [0] NCCL INFO Comm config Blocking set to 1
lrdn0823:1648443:1648548 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.18.253<0>
lrdn0823:1648443:1648548 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0823:1648443:1648548 [0] NCCL INFO Using network IB
lrdn0823:1648443:1648548 [0] NCCL INFO ncclCommInitRankConfig comm 0xd6f08d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x64291c913fb09479 - Init START
lrdn0823:1648443:1648548 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0823:1648443:1648548 [0] NCCL INFO Bootstrap timings total 0.000365 (create 0.000017, send 0.000057, recv 0.000088, ring 0.000001, delay 0.000000)
lrdn0823:1648443:1648548 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0823:1648443:1648548 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0823:1648443:1648548 [0] NCCL INFO comm 0xd6f08d0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 00/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 01/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 02/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 03/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 04/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 05/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 06/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 07/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 08/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 09/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 10/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 11/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 12/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 13/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 14/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 15/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 16/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 17/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 18/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 19/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 20/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 21/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 22/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 23/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 24/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 25/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 26/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 27/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 28/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 29/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 30/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 31/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 32/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 33/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 34/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 35/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 36/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 37/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 38/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 39/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 40/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 41/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 42/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 43/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 44/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 45/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 46/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 47/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 48/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 49/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 50/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 51/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 52/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 53/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 54/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 55/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 56/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 57/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 58/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 59/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 60/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 61/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 62/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Channel 63/64 : 0
lrdn0823:1648443:1648548 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0823:1648443:1648548 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0823:1648443:1648548 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0823:1648443:1648554 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0823:1648443:1648555 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0823:1648443:1648548 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0823:1648443:1648548 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0823:1648443:1648548 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0823:1648443:1648548 [0] NCCL INFO ncclCommInitRankConfig comm 0xd6f08d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x64291c913fb09479 - Init COMPLETE
lrdn0823:1648443:1648548 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.14, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0110:4136466:4136570 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0110:4136466:4136570 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0872:536285:536285 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.193<0>
lrdn0872:536285:536285 [0] NCCL INFO cudaDriverVersion 12020
lrdn0872:536285:536285 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0872:536285:536285 [0] NCCL INFO Comm config Blocking set to 1
lrdn0110:4136466:4136570 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.7.217<0>
lrdn0110:4136466:4136570 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0110:4136466:4136570 [0] NCCL INFO Using network IB
lrdn0110:4136466:4136570 [0] NCCL INFO ncclCommInitRankConfig comm 0xe563300 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7af19110ea109853 - Init START
lrdn0110:4136466:4136570 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0110:4136466:4136570 [0] NCCL INFO Bootstrap timings total 0.000393 (create 0.000022, send 0.000064, recv 0.000093, ring 0.000001, delay 0.000000)
lrdn0110:4136466:4136570 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0110:4136466:4136570 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0110:4136466:4136570 [0] NCCL INFO comm 0xe563300 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 00/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 01/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 02/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 03/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 04/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 05/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 06/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 07/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 08/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 09/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 10/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 11/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 12/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 13/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 14/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 15/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 16/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 17/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 18/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 19/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 20/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 21/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 22/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 23/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 24/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 25/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 26/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 27/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 28/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 29/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 30/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 31/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 32/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 33/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 34/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 35/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 36/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 37/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 38/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 39/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 40/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 41/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 42/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 43/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 44/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 45/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 46/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 47/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 48/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 49/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 50/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 51/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 52/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 53/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 54/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 55/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 56/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 57/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 58/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 59/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 60/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 61/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 62/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Channel 63/64 : 0
lrdn0110:4136466:4136570 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0110:4136466:4136570 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0110:4136466:4136570 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0110:4136466:4136577 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn0110:4136466:4136578 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 12
lrdn0110:4136466:4136570 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0110:4136466:4136570 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0110:4136466:4136570 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0110:4136466:4136570 [0] NCCL INFO ncclCommInitRankConfig comm 0xe563300 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7af19110ea109853 - Init COMPLETE
lrdn0110:4136466:4136570 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0872:536285:536407 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0872:536285:536407 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0872:536285:536407 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.193<0>
lrdn0872:536285:536407 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0872:536285:536407 [0] NCCL INFO Using network IB
lrdn0872:536285:536407 [0] NCCL INFO ncclCommInitRankConfig comm 0x160b06d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x39288277f461bfe4 - Init START
lrdn0872:536285:536407 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0872:536285:536407 [0] NCCL INFO Bootstrap timings total 0.000368 (create 0.000015, send 0.000053, recv 0.000093, ring 0.000001, delay 0.000001)
lrdn0872:536285:536407 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0872:536285:536407 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0872:536285:536407 [0] NCCL INFO comm 0x160b06d0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 00/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 01/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 02/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 03/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 04/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 05/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 06/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 07/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 08/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 09/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 10/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 11/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 12/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 13/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 14/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 15/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 16/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 17/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 18/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 19/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 20/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 21/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 22/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 23/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 24/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 25/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 26/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 27/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 28/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 29/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 30/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 31/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 32/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 33/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 34/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 35/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 36/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 37/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 38/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 39/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 40/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 41/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 42/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 43/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 44/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 45/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 46/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 47/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 48/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 49/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 50/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 51/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 52/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 53/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 54/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 55/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 56/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 57/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 58/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 59/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 60/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 61/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 62/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Channel 63/64 : 0
lrdn0872:536285:536407 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0872:536285:536407 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0872:536285:536407 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0872:536285:536413 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn0872:536285:536414 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0872:536285:536407 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0872:536285:536407 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0872:536285:536407 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0872:536285:536407 [0] NCCL INFO ncclCommInitRankConfig comm 0x160b06d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x39288277f461bfe4 - Init COMPLETE
lrdn0872:536285:536407 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.22 (kernels 0.14, alloc 0.04, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0114:1700246:1700246 [0] NCCL INFO Bootstrap: Using ib0:10.128.7.233<0>
lrdn0114:1700246:1700246 [0] NCCL INFO cudaDriverVersion 12020
lrdn0114:1700246:1700246 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0114:1700246:1700246 [0] NCCL INFO Comm config Blocking set to 1
lrdn0114:1700246:1700340 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0114:1700246:1700340 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0114:1700246:1700340 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.7.233<0>
lrdn0114:1700246:1700340 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0114:1700246:1700340 [0] NCCL INFO Using network IB
lrdn0114:1700246:1700340 [0] NCCL INFO ncclCommInitRankConfig comm 0xe79bfc0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb843f57be4af4b93 - Init START
lrdn0114:1700246:1700340 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0114:1700246:1700340 [0] NCCL INFO Bootstrap timings total 0.000380 (create 0.000019, send 0.000056, recv 0.000094, ring 0.000001, delay 0.000000)
lrdn0114:1700246:1700340 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0114:1700246:1700340 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0114:1700246:1700340 [0] NCCL INFO comm 0xe79bfc0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 00/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 01/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 02/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 03/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 04/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 05/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 06/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 07/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 08/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 09/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 10/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 11/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 12/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 13/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 14/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 15/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 16/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 17/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 18/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 19/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 20/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 21/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 22/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 23/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 24/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 25/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 26/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 27/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 28/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 29/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 30/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 31/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 32/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 33/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 34/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 35/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 36/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 37/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 38/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 39/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 40/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 41/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 42/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 43/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 44/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 45/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 46/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 47/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 48/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 49/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 50/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 51/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 52/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 53/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 54/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 55/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 56/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 57/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 58/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 59/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 60/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 61/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 62/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Channel 63/64 : 0
lrdn0114:1700246:1700340 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0114:1700246:1700340 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0114:1700246:1700340 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0114:1700246:1700347 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0114:1700246:1700346 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0114:1700246:1700340 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0114:1700246:1700340 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0114:1700246:1700340 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0114:1700246:1700340 [0] NCCL INFO ncclCommInitRankConfig comm 0xe79bfc0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb843f57be4af4b93 - Init COMPLETE
lrdn0114:1700246:1700340 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0570:1729371:1729371 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.9<0>
lrdn0570:1729371:1729371 [0] NCCL INFO cudaDriverVersion 12020
lrdn0570:1729371:1729371 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0570:1729371:1729371 [0] NCCL INFO Comm config Blocking set to 1
lrdn0038:667426:667426 [0] NCCL INFO Bootstrap: Using ib0:10.128.6.185<0>
lrdn0038:667426:667426 [0] NCCL INFO cudaDriverVersion 12020
lrdn0038:667426:667426 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0038:667426:667426 [0] NCCL INFO Comm config Blocking set to 1
lrdn0570:1729371:1729484 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0570:1729371:1729484 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0038:667426:667522 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0038:667426:667522 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0570:1729371:1729484 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.9<0>
lrdn0570:1729371:1729484 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0570:1729371:1729484 [0] NCCL INFO Using network IB
lrdn0570:1729371:1729484 [0] NCCL INFO ncclCommInitRankConfig comm 0xc84aed0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd86c55dc89f1e38d - Init START
lrdn0570:1729371:1729484 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0570:1729371:1729484 [0] NCCL INFO Bootstrap timings total 0.000379 (create 0.000018, send 0.000061, recv 0.000090, ring 0.000001, delay 0.000000)
lrdn0570:1729371:1729484 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0570:1729371:1729484 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0570:1729371:1729484 [0] NCCL INFO comm 0xc84aed0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 00/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 01/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 02/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 03/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 04/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 05/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 06/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 07/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 08/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 09/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 10/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 11/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 12/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 13/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 14/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 15/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 16/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 17/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 18/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 19/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 20/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 21/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 22/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 23/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 24/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 25/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 26/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 27/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 28/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 29/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 30/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 31/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 32/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 33/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 34/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 35/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 36/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 37/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 38/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 39/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 40/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 41/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 42/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 43/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 44/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 45/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 46/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 47/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 48/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 49/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 50/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 51/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 52/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 53/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 54/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 55/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 56/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 57/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 58/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 59/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 60/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 61/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 62/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Channel 63/64 : 0
lrdn0570:1729371:1729484 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0570:1729371:1729484 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0570:1729371:1729484 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0570:1729371:1729490 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn0570:1729371:1729491 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0038:667426:667522 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.6.185<0>
lrdn0570:1729371:1729484 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0570:1729371:1729484 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0038:667426:667522 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0038:667426:667522 [0] NCCL INFO Using network IB
lrdn0570:1729371:1729484 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0570:1729371:1729484 [0] NCCL INFO ncclCommInitRankConfig comm 0xc84aed0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd86c55dc89f1e38d - Init COMPLETE
lrdn0570:1729371:1729484 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0038:667426:667522 [0] NCCL INFO ncclCommInitRankConfig comm 0xcf625e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x68e4f23b7090d620 - Init START
lrdn0038:667426:667522 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0038:667426:667522 [0] NCCL INFO Bootstrap timings total 0.000368 (create 0.000016, send 0.000056, recv 0.000090, ring 0.000001, delay 0.000001)
lrdn0038:667426:667522 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0038:667426:667522 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0038:667426:667522 [0] NCCL INFO comm 0xcf625e0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 00/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 01/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 02/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 03/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 04/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 05/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 06/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 07/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 08/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 09/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 10/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 11/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 12/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 13/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 14/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 15/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 16/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 17/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 18/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 19/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 20/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 21/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 22/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 23/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 24/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 25/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 26/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 27/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 28/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 29/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 30/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 31/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 32/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 33/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 34/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 35/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 36/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 37/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 38/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 39/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 40/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 41/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 42/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 43/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 44/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 45/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 46/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 47/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 48/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 49/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 50/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 51/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 52/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 53/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 54/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 55/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 56/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 57/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 58/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 59/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 60/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 61/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 62/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Channel 63/64 : 0
lrdn0038:667426:667522 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0038:667426:667522 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0038:667426:667522 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0038:667426:667528 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0038:667426:667529 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn0038:667426:667522 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0038:667426:667522 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0038:667426:667522 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0038:667426:667522 [0] NCCL INFO ncclCommInitRankConfig comm 0xcf625e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x68e4f23b7090d620 - Init COMPLETE
lrdn0038:667426:667522 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0150:2596900:2596900 [0] NCCL INFO Bootstrap: Using ib0:10.128.8.121<0>
lrdn0150:2596900:2596900 [0] NCCL INFO cudaDriverVersion 12020
lrdn0150:2596900:2596900 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0150:2596900:2596900 [0] NCCL INFO Comm config Blocking set to 1
lrdn0150:2596900:2596996 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0150:2596900:2596996 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0150:2596900:2596996 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.8.121<0>
lrdn0150:2596900:2596996 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0150:2596900:2596996 [0] NCCL INFO Using network IB
lrdn0150:2596900:2596996 [0] NCCL INFO ncclCommInitRankConfig comm 0xe5c0360 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x90471de4be86f281 - Init START
lrdn0150:2596900:2596996 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0150:2596900:2596996 [0] NCCL INFO Bootstrap timings total 0.000479 (create 0.000017, send 0.000063, recv 0.000193, ring 0.000001, delay 0.000000)
lrdn0150:2596900:2596996 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0150:2596900:2596996 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0150:2596900:2596996 [0] NCCL INFO comm 0xe5c0360 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 00/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 01/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 02/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 03/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 04/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 05/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 06/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 07/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 08/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 09/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 10/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 11/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 12/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 13/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 14/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 15/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 16/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 17/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 18/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 19/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 20/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 21/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 22/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 23/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 24/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 25/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 26/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 27/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 28/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 29/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 30/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 31/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 32/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 33/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 34/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 35/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 36/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 37/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 38/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 39/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 40/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 41/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 42/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 43/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 44/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 45/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 46/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 47/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 48/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 49/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 50/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 51/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 52/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 53/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 54/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 55/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 56/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 57/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 58/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 59/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 60/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 61/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 62/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Channel 63/64 : 0
lrdn0150:2596900:2596996 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0150:2596900:2596996 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0150:2596900:2596996 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0150:2596900:2597002 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0150:2596900:2597003 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0150:2596900:2596996 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0150:2596900:2596996 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0150:2596900:2596996 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0150:2596900:2596996 [0] NCCL INFO ncclCommInitRankConfig comm 0xe5c0360 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x90471de4be86f281 - Init COMPLETE
lrdn0150:2596900:2596996 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0739:2688192:2688192 [0] NCCL INFO Bootstrap: Using ib0:10.128.17.173<0>
lrdn0739:2688192:2688192 [0] NCCL INFO cudaDriverVersion 12020
lrdn0739:2688192:2688192 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0739:2688192:2688192 [0] NCCL INFO Comm config Blocking set to 1
lrdn0164:1702171:1702171 [0] NCCL INFO Bootstrap: Using ib0:10.128.8.177<0>
lrdn0164:1702171:1702171 [0] NCCL INFO cudaDriverVersion 12020
lrdn0164:1702171:1702171 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0164:1702171:1702171 [0] NCCL INFO Comm config Blocking set to 1
lrdn0739:2688192:2688288 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0739:2688192:2688288 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0739:2688192:2688288 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.17.173<0>
lrdn0739:2688192:2688288 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0739:2688192:2688288 [0] NCCL INFO Using network IB
lrdn0739:2688192:2688288 [0] NCCL INFO ncclCommInitRankConfig comm 0xe717830 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xcc61f3a4fd746b29 - Init START
lrdn0739:2688192:2688288 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0739:2688192:2688288 [0] NCCL INFO Bootstrap timings total 0.000461 (create 0.000019, send 0.000063, recv 0.000176, ring 0.000001, delay 0.000000)
lrdn0739:2688192:2688288 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0739:2688192:2688288 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0739:2688192:2688288 [0] NCCL INFO comm 0xe717830 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 00/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 01/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 02/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 03/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 04/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 05/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 06/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 07/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 08/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 09/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 10/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 11/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 12/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 13/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 14/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 15/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 16/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 17/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 18/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 19/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 20/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 21/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 22/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 23/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 24/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 25/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 26/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 27/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 28/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 29/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 30/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 31/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 32/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 33/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 34/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 35/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 36/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 37/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 38/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 39/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 40/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 41/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 42/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 43/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 44/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 45/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 46/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 47/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 48/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 49/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 50/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 51/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 52/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 53/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 54/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 55/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 56/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 57/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 58/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 59/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 60/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 61/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 62/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Channel 63/64 : 0
lrdn0739:2688192:2688288 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0739:2688192:2688288 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0739:2688192:2688288 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0739:2688192:2688295 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0739:2688192:2688294 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0739:2688192:2688288 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0739:2688192:2688288 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0739:2688192:2688288 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0739:2688192:2688288 [0] NCCL INFO ncclCommInitRankConfig comm 0xe717830 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xcc61f3a4fd746b29 - Init COMPLETE
lrdn0739:2688192:2688288 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.15, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn0164:1702171:1702266 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0164:1702171:1702266 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1103:1654006:1654006 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.93<0>
lrdn1103:1654006:1654006 [0] NCCL INFO cudaDriverVersion 12020
lrdn1103:1654006:1654006 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1103:1654006:1654006 [0] NCCL INFO Comm config Blocking set to 1
lrdn0164:1702171:1702266 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.8.177<0>
lrdn0164:1702171:1702266 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0164:1702171:1702266 [0] NCCL INFO Using network IB
lrdn0164:1702171:1702266 [0] NCCL INFO ncclCommInitRankConfig comm 0xe3fc9e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9f1639ac66f1d01b - Init START
lrdn0164:1702171:1702266 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0164:1702171:1702266 [0] NCCL INFO Bootstrap timings total 0.000507 (create 0.000016, send 0.000060, recv 0.000215, ring 0.000001, delay 0.000000)
lrdn0164:1702171:1702266 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0164:1702171:1702266 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0164:1702171:1702266 [0] NCCL INFO comm 0xe3fc9e0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 00/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 01/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 02/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 03/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 04/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 05/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 06/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 07/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 08/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 09/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 10/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 11/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 12/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 13/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 14/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 15/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 16/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 17/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 18/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 19/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 20/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 21/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 22/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 23/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 24/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 25/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 26/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 27/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 28/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 29/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 30/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 31/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 32/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 33/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 34/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 35/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 36/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 37/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 38/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 39/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 40/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 41/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 42/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 43/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 44/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 45/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 46/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 47/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 48/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 49/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 50/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 51/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 52/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 53/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 54/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 55/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 56/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 57/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 58/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 59/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 60/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 61/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 62/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Channel 63/64 : 0
lrdn0164:1702171:1702266 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0164:1702171:1702266 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0164:1702171:1702266 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0164:1702171:1702273 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0164:1702171:1702272 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0164:1702171:1702266 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0164:1702171:1702266 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0164:1702171:1702266 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0164:1702171:1702266 [0] NCCL INFO ncclCommInitRankConfig comm 0xe3fc9e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9f1639ac66f1d01b - Init COMPLETE
lrdn0164:1702171:1702266 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn1103:1654006:1654102 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1103:1654006:1654102 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1103:1654006:1654102 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.93<0>
lrdn1103:1654006:1654102 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1103:1654006:1654102 [0] NCCL INFO Using network IB
lrdn1103:1654006:1654102 [0] NCCL INFO ncclCommInitRankConfig comm 0xe8a7690 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x74efc7f56bbfab9b - Init START
lrdn1103:1654006:1654102 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1103:1654006:1654102 [0] NCCL INFO Bootstrap timings total 0.000467 (create 0.000016, send 0.000052, recv 0.000200, ring 0.000001, delay 0.000000)
lrdn1103:1654006:1654102 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1103:1654006:1654102 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1103:1654006:1654102 [0] NCCL INFO comm 0xe8a7690 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 00/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 01/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 02/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 03/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 04/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 05/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 06/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 07/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 08/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 09/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 10/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 11/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 12/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 13/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 14/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 15/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 16/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 17/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 18/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 19/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 20/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 21/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 22/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 23/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 24/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 25/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 26/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 27/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 28/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 29/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 30/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 31/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 32/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 33/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 34/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 35/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 36/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 37/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 38/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 39/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 40/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 41/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 42/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 43/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 44/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 45/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 46/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 47/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 48/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 49/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 50/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 51/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 52/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 53/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 54/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 55/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 56/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 57/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 58/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 59/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 60/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 61/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 62/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Channel 63/64 : 0
lrdn1103:1654006:1654102 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1103:1654006:1654102 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1103:1654006:1654102 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1103:1654006:1654109 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 12
lrdn1103:1654006:1654108 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn1103:1654006:1654102 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1103:1654006:1654102 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1103:1654006:1654102 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1103:1654006:1654102 [0] NCCL INFO ncclCommInitRankConfig comm 0xe8a7690 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x74efc7f56bbfab9b - Init COMPLETE
lrdn1103:1654006:1654102 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.14, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0766:1565834:1565834 [0] NCCL INFO Bootstrap: Using ib0:10.128.18.25<0>
lrdn0766:1565834:1565834 [0] NCCL INFO cudaDriverVersion 12020
lrdn0766:1565834:1565834 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0766:1565834:1565834 [0] NCCL INFO Comm config Blocking set to 1
lrdn1006:1697051:1697051 [0] NCCL INFO Bootstrap: Using ib0:10.128.21.217<0>
lrdn1006:1697051:1697051 [0] NCCL INFO cudaDriverVersion 12020
lrdn1006:1697051:1697051 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1006:1697051:1697051 [0] NCCL INFO Comm config Blocking set to 1
lrdn0766:1565834:1565947 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0766:1565834:1565947 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0766:1565834:1565947 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.18.25<0>
lrdn0766:1565834:1565947 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0766:1565834:1565947 [0] NCCL INFO Using network IB
lrdn0766:1565834:1565947 [0] NCCL INFO ncclCommInitRankConfig comm 0xe0255d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd964fa7647a8dea1 - Init START
lrdn0766:1565834:1565947 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0766:1565834:1565947 [0] NCCL INFO Bootstrap timings total 0.000376 (create 0.000017, send 0.000056, recv 0.000095, ring 0.000001, delay 0.000000)
lrdn1030:1505262:1505262 [0] NCCL INFO Bootstrap: Using ib0:10.128.22.57<0>
lrdn1030:1505262:1505262 [0] NCCL INFO cudaDriverVersion 12020
lrdn1030:1505262:1505262 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1030:1505262:1505262 [0] NCCL INFO Comm config Blocking set to 1
lrdn0766:1565834:1565947 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0766:1565834:1565947 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0766:1565834:1565947 [0] NCCL INFO comm 0xe0255d0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 00/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 01/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 02/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 03/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 04/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 05/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 06/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 07/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 08/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 09/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 10/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 11/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 12/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 13/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 14/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 15/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 16/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 17/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 18/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 19/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 20/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 21/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 22/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 23/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 24/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 25/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 26/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 27/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 28/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 29/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 30/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 31/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 32/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 33/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 34/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 35/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 36/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 37/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 38/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 39/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 40/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 41/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 42/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 43/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 44/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 45/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 46/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 47/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 48/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 49/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 50/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 51/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 52/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 53/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 54/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 55/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 56/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 57/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 58/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 59/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 60/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 61/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 62/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Channel 63/64 : 0
lrdn0766:1565834:1565947 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0766:1565834:1565947 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0766:1565834:1565947 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0766:1565834:1565954 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0766:1565834:1565953 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn1318:2081362:2081362 [0] NCCL INFO Bootstrap: Using ib0:10.128.26.185<0>
lrdn1318:2081362:2081362 [0] NCCL INFO cudaDriverVersion 12020
lrdn1318:2081362:2081362 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1318:2081362:2081362 [0] NCCL INFO Comm config Blocking set to 1
lrdn0766:1565834:1565947 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0766:1565834:1565947 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0766:1565834:1565947 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0766:1565834:1565947 [0] NCCL INFO ncclCommInitRankConfig comm 0xe0255d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd964fa7647a8dea1 - Init COMPLETE
lrdn0766:1565834:1565947 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.14, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1006:1697051:1697147 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1006:1697051:1697147 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1006:1697051:1697147 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.21.217<0>
lrdn1006:1697051:1697147 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1006:1697051:1697147 [0] NCCL INFO Using network IB
lrdn1006:1697051:1697147 [0] NCCL INFO ncclCommInitRankConfig comm 0xe48ae60 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x17167ceca23263cb - Init START
lrdn1006:1697051:1697147 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1006:1697051:1697147 [0] NCCL INFO Bootstrap timings total 0.000377 (create 0.000017, send 0.000059, recv 0.000096, ring 0.000001, delay 0.000001)
lrdn1006:1697051:1697147 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1006:1697051:1697147 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1006:1697051:1697147 [0] NCCL INFO comm 0xe48ae60 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 00/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 01/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 02/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 03/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 04/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 05/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 06/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 07/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 08/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 09/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 10/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 11/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 12/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 13/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 14/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 15/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 16/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 17/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 18/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 19/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 20/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 21/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 22/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 23/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 24/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 25/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 26/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 27/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 28/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 29/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 30/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 31/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 32/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 33/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 34/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 35/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 36/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 37/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 38/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 39/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 40/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 41/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 42/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 43/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 44/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 45/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 46/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 47/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 48/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 49/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 50/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 51/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 52/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 53/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 54/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 55/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 56/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 57/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 58/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 59/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 60/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 61/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 62/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Channel 63/64 : 0
lrdn1006:1697051:1697147 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1006:1697051:1697147 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1006:1697051:1697147 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1006:1697051:1697154 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 7
lrdn1006:1697051:1697153 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1006:1697051:1697147 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1006:1697051:1697147 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1006:1697051:1697147 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1006:1697051:1697147 [0] NCCL INFO ncclCommInitRankConfig comm 0xe48ae60 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x17167ceca23263cb - Init COMPLETE
lrdn1006:1697051:1697147 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1030:1505262:1505383 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1030:1505262:1505383 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1318:2081362:2081468 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1318:2081362:2081468 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1030:1505262:1505383 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.22.57<0>
lrdn1030:1505262:1505383 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1030:1505262:1505383 [0] NCCL INFO Using network IB
lrdn1030:1505262:1505383 [0] NCCL INFO ncclCommInitRankConfig comm 0xd07ba40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9209774e395d32db - Init START
lrdn1030:1505262:1505383 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1030:1505262:1505383 [0] NCCL INFO Bootstrap timings total 0.000379 (create 0.000019, send 0.000055, recv 0.000097, ring 0.000001, delay 0.000000)
lrdn1318:2081362:2081468 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.26.185<0>
lrdn1104:2727035:2727035 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.97<0>
lrdn1104:2727035:2727035 [0] NCCL INFO cudaDriverVersion 12020
lrdn1104:2727035:2727035 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1104:2727035:2727035 [0] NCCL INFO Comm config Blocking set to 1
lrdn1318:2081362:2081468 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1318:2081362:2081468 [0] NCCL INFO Using network IB
lrdn1318:2081362:2081468 [0] NCCL INFO ncclCommInitRankConfig comm 0xd02fcd0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe6551e018baeb722 - Init START
lrdn1318:2081362:2081468 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1318:2081362:2081468 [0] NCCL INFO Bootstrap timings total 0.000352 (create 0.000015, send 0.000054, recv 0.000090, ring 0.000001, delay 0.000001)
lrdn1030:1505262:1505383 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1030:1505262:1505383 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1030:1505262:1505383 [0] NCCL INFO comm 0xd07ba40 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 00/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 01/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 02/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 03/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 04/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 05/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 06/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 07/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 08/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 09/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 10/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 11/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 12/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 13/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 14/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 15/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 16/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 17/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 18/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 19/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 20/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 21/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 22/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 23/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 24/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 25/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 26/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 27/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 28/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 29/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 30/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 31/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 32/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 33/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 34/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 35/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 36/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 37/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 38/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 39/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 40/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 41/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 42/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 43/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 44/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 45/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 46/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 47/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 48/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 49/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 50/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 51/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 52/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 53/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 54/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 55/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 56/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 57/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 58/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 59/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 60/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 61/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 62/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Channel 63/64 : 0
lrdn1030:1505262:1505383 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1030:1505262:1505383 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1030:1505262:1505383 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1030:1505262:1505389 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn1030:1505262:1505390 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn1318:2081362:2081468 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1318:2081362:2081468 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1318:2081362:2081468 [0] NCCL INFO comm 0xd02fcd0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 00/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 01/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 02/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 03/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 04/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 05/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 06/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 07/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 08/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 09/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 10/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 11/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 12/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 13/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 14/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 15/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 16/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 17/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 18/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 19/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 20/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 21/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 22/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 23/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 24/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 25/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 26/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 27/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 28/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 29/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 30/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 31/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 32/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 33/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 34/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 35/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 36/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 37/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 38/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 39/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 40/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 41/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 42/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 43/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 44/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 45/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 46/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 47/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 48/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 49/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 50/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 51/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 52/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 53/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 54/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 55/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 56/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 57/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 58/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 59/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 60/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 61/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 62/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Channel 63/64 : 0
lrdn1318:2081362:2081468 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1318:2081362:2081468 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1318:2081362:2081468 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1318:2081362:2081474 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn1318:2081362:2081475 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn1030:1505262:1505383 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1030:1505262:1505383 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1030:1505262:1505383 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1030:1505262:1505383 [0] NCCL INFO ncclCommInitRankConfig comm 0xd07ba40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9209774e395d32db - Init COMPLETE
lrdn1030:1505262:1505383 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1318:2081362:2081468 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1318:2081362:2081468 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1318:2081362:2081468 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1318:2081362:2081468 [0] NCCL INFO ncclCommInitRankConfig comm 0xd02fcd0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe6551e018baeb722 - Init COMPLETE
lrdn1318:2081362:2081468 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1104:2727035:2727132 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1104:2727035:2727132 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1104:2727035:2727132 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.97<0>
lrdn1104:2727035:2727132 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1104:2727035:2727132 [0] NCCL INFO Using network IB
lrdn1104:2727035:2727132 [0] NCCL INFO ncclCommInitRankConfig comm 0xe71c8a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xeed52139846632e9 - Init START
lrdn1104:2727035:2727132 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1104:2727035:2727132 [0] NCCL INFO Bootstrap timings total 0.000474 (create 0.000015, send 0.000058, recv 0.000195, ring 0.000001, delay 0.000000)
lrdn1104:2727035:2727132 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1104:2727035:2727132 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1104:2727035:2727132 [0] NCCL INFO comm 0xe71c8a0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 00/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 01/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 02/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 03/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 04/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 05/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 06/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 07/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 08/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 09/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 10/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 11/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 12/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 13/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 14/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 15/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 16/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 17/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 18/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 19/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 20/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 21/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 22/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 23/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 24/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 25/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 26/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 27/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 28/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 29/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 30/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 31/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 32/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 33/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 34/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 35/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 36/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 37/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 38/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 39/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 40/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 41/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 42/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 43/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 44/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 45/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 46/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 47/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 48/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 49/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 50/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 51/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 52/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 53/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 54/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 55/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 56/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 57/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 58/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 59/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 60/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 61/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 62/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Channel 63/64 : 0
lrdn1104:2727035:2727132 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1104:2727035:2727132 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1104:2727035:2727132 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1104:2727035:2727139 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1104:2727035:2727138 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1104:2727035:2727132 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1104:2727035:2727132 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1104:2727035:2727132 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1104:2727035:2727132 [0] NCCL INFO ncclCommInitRankConfig comm 0xe71c8a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xeed52139846632e9 - Init COMPLETE
lrdn1104:2727035:2727132 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1089:1671209:1671209 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.37<0>
lrdn1089:1671209:1671209 [0] NCCL INFO cudaDriverVersion 12020
lrdn1089:1671209:1671209 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1089:1671209:1671209 [0] NCCL INFO Comm config Blocking set to 1
lrdn0553:3116443:3116443 [0] NCCL INFO Bootstrap: Using ib0:10.128.14.197<0>
lrdn0553:3116443:3116443 [0] NCCL INFO cudaDriverVersion 12020
lrdn0553:3116443:3116443 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0553:3116443:3116443 [0] NCCL INFO Comm config Blocking set to 1
lrdn1089:1671209:1671305 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1089:1671209:1671305 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0553:3116443:3116565 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0553:3116443:3116565 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1089:1671209:1671305 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.37<0>
lrdn1089:1671209:1671305 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1089:1671209:1671305 [0] NCCL INFO Using network IB
lrdn1089:1671209:1671305 [0] NCCL INFO ncclCommInitRankConfig comm 0xd1a7ba0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe3c2c1a95897db8d - Init START
lrdn1089:1671209:1671305 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1089:1671209:1671305 [0] NCCL INFO Bootstrap timings total 0.000382 (create 0.000016, send 0.000060, recv 0.000092, ring 0.000001, delay 0.000000)
lrdn1089:1671209:1671305 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1089:1671209:1671305 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1089:1671209:1671305 [0] NCCL INFO comm 0xd1a7ba0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 00/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 01/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 02/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 03/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 04/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 05/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 06/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 07/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 08/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 09/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 10/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 11/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 12/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 13/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 14/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 15/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 16/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 17/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 18/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 19/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 20/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 21/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 22/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 23/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 24/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 25/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 26/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 27/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 28/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 29/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 30/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 31/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 32/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 33/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 34/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 35/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 36/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 37/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 38/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 39/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 40/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 41/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 42/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 43/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 44/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 45/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 46/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 47/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 48/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 49/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 50/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 51/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 52/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 53/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 54/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 55/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 56/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 57/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 58/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 59/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 60/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 61/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 62/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Channel 63/64 : 0
lrdn1089:1671209:1671305 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1089:1671209:1671305 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1089:1671209:1671305 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1089:1671209:1671311 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn1089:1671209:1671312 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn1089:1671209:1671305 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1089:1671209:1671305 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0553:3116443:3116565 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.14.197<0>
lrdn1089:1671209:1671305 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1089:1671209:1671305 [0] NCCL INFO ncclCommInitRankConfig comm 0xd1a7ba0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe3c2c1a95897db8d - Init COMPLETE
lrdn1089:1671209:1671305 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0553:3116443:3116565 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0553:3116443:3116565 [0] NCCL INFO Using network IB
lrdn0553:3116443:3116565 [0] NCCL INFO ncclCommInitRankConfig comm 0xe74b2d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x585db3142959081 - Init START
lrdn0553:3116443:3116565 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0553:3116443:3116565 [0] NCCL INFO Bootstrap timings total 0.000375 (create 0.000017, send 0.000058, recv 0.000086, ring 0.000001, delay 0.000000)
lrdn0553:3116443:3116565 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0553:3116443:3116565 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0553:3116443:3116565 [0] NCCL INFO comm 0xe74b2d0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 00/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 01/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 02/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 03/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 04/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 05/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 06/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 07/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 08/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 09/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 10/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 11/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 12/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 13/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 14/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 15/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 16/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 17/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 18/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 19/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 20/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 21/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 22/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 23/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 24/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 25/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 26/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 27/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 28/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 29/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 30/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 31/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 32/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 33/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 34/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 35/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 36/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 37/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 38/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 39/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 40/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 41/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 42/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 43/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 44/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 45/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 46/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 47/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 48/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 49/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 50/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 51/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 52/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 53/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 54/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 55/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 56/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 57/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 58/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 59/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 60/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 61/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 62/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Channel 63/64 : 0
lrdn0553:3116443:3116565 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0553:3116443:3116565 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0553:3116443:3116565 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0553:3116443:3116571 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0553:3116443:3116572 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0553:3116443:3116565 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0553:3116443:3116565 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0553:3116443:3116565 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0553:3116443:3116565 [0] NCCL INFO ncclCommInitRankConfig comm 0xe74b2d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x585db3142959081 - Init COMPLETE
lrdn0553:3116443:3116565 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0624:1835012:1835012 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.225<0>
lrdn0624:1835012:1835012 [0] NCCL INFO cudaDriverVersion 12020
lrdn0624:1835012:1835012 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0624:1835012:1835012 [0] NCCL INFO Comm config Blocking set to 1
lrdn0529:3299727:3299727 [0] NCCL INFO Bootstrap: Using ib0:10.128.14.101<0>
lrdn0529:3299727:3299727 [0] NCCL INFO cudaDriverVersion 12020
lrdn0529:3299727:3299727 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0529:3299727:3299727 [0] NCCL INFO Comm config Blocking set to 1
lrdn0606:1640320:1640320 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.153<0>
lrdn0606:1640320:1640320 [0] NCCL INFO cudaDriverVersion 12020
lrdn0606:1640320:1640320 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0606:1640320:1640320 [0] NCCL INFO Comm config Blocking set to 1
lrdn0624:1835012:1835137 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0624:1835012:1835137 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0624:1835012:1835137 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.225<0>
lrdn0624:1835012:1835137 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0624:1835012:1835137 [0] NCCL INFO Using network IB
lrdn0624:1835012:1835137 [0] NCCL INFO ncclCommInitRankConfig comm 0xd010a50 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe872ccd712bb7aca - Init START
lrdn0624:1835012:1835137 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0624:1835012:1835137 [0] NCCL INFO Bootstrap timings total 0.000373 (create 0.000015, send 0.000057, recv 0.000093, ring 0.000001, delay 0.000000)
lrdn1027:1745942:1745942 [0] NCCL INFO Bootstrap: Using ib0:10.128.22.45<0>
lrdn1027:1745942:1745942 [0] NCCL INFO cudaDriverVersion 12020
lrdn1027:1745942:1745942 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1027:1745942:1745942 [0] NCCL INFO Comm config Blocking set to 1
lrdn0529:3299727:3299823 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0529:3299727:3299823 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0624:1835012:1835137 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0624:1835012:1835137 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0624:1835012:1835137 [0] NCCL INFO comm 0xd010a50 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 00/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 01/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 02/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 03/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 04/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 05/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 06/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 07/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 08/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 09/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 10/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 11/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 12/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 13/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 14/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 15/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 16/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 17/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 18/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 19/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 20/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 21/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 22/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 23/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 24/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 25/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 26/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 27/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 28/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 29/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 30/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 31/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 32/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 33/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 34/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 35/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 36/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 37/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 38/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 39/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 40/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 41/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 42/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 43/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 44/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 45/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 46/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 47/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 48/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 49/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 50/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 51/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 52/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 53/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 54/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 55/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 56/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 57/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 58/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 59/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 60/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 61/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 62/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Channel 63/64 : 0
lrdn0624:1835012:1835137 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0624:1835012:1835137 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0624:1835012:1835137 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0624:1835012:1835143 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn0624:1835012:1835144 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0624:1835012:1835137 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0624:1835012:1835137 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0624:1835012:1835137 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0624:1835012:1835137 [0] NCCL INFO ncclCommInitRankConfig comm 0xd010a50 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe872ccd712bb7aca - Init COMPLETE
lrdn0624:1835012:1835137 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0529:3299727:3299823 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.14.101<0>
lrdn0606:1640320:1640416 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0606:1640320:1640416 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0529:3299727:3299823 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0529:3299727:3299823 [0] NCCL INFO Using network IB
lrdn0529:3299727:3299823 [0] NCCL INFO ncclCommInitRankConfig comm 0xf407010 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd5106d104d07d909 - Init START
lrdn0529:3299727:3299823 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0529:3299727:3299823 [0] NCCL INFO Bootstrap timings total 0.000379 (create 0.000018, send 0.000061, recv 0.000089, ring 0.000001, delay 0.000000)
lrdn0529:3299727:3299823 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0529:3299727:3299823 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0529:3299727:3299823 [0] NCCL INFO comm 0xf407010 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 00/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 01/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 02/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 03/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 04/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 05/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 06/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 07/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 08/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 09/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 10/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 11/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 12/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 13/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 14/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 15/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 16/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 17/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 18/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 19/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 20/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 21/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 22/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 23/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 24/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 25/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 26/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 27/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 28/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 29/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 30/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 31/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 32/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 33/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 34/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 35/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 36/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 37/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 38/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 39/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 40/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 41/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 42/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 43/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 44/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 45/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 46/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 47/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 48/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 49/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 50/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 51/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 52/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 53/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 54/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 55/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 56/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 57/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 58/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 59/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 60/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 61/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 62/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Channel 63/64 : 0
lrdn0529:3299727:3299823 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0529:3299727:3299823 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0529:3299727:3299823 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0529:3299727:3299829 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn0529:3299727:3299830 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 15
lrdn0529:3299727:3299823 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0529:3299727:3299823 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0529:3299727:3299823 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0529:3299727:3299823 [0] NCCL INFO ncclCommInitRankConfig comm 0xf407010 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd5106d104d07d909 - Init COMPLETE
lrdn0529:3299727:3299823 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0606:1640320:1640416 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.153<0>
lrdn0606:1640320:1640416 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0606:1640320:1640416 [0] NCCL INFO Using network IB
lrdn0606:1640320:1640416 [0] NCCL INFO ncclCommInitRankConfig comm 0xe226700 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xabf5db3c8087ea2c - Init START
lrdn0606:1640320:1640416 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0606:1640320:1640416 [0] NCCL INFO Bootstrap timings total 0.000357 (create 0.000014, send 0.000054, recv 0.000089, ring 0.000001, delay 0.000001)
lrdn0606:1640320:1640416 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0606:1640320:1640416 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0606:1640320:1640416 [0] NCCL INFO comm 0xe226700 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 00/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 01/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 02/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 03/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 04/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 05/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 06/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 07/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 08/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 09/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 10/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 11/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 12/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 13/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 14/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 15/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 16/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 17/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 18/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 19/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 20/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 21/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 22/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 23/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 24/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 25/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 26/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 27/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 28/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 29/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 30/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 31/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 32/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 33/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 34/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 35/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 36/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 37/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 38/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 39/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 40/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 41/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 42/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 43/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 44/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 45/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 46/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 47/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 48/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 49/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 50/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 51/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 52/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 53/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 54/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 55/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 56/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 57/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 58/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 59/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 60/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 61/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 62/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Channel 63/64 : 0
lrdn0606:1640320:1640416 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0606:1640320:1640416 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0606:1640320:1640416 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0606:1640320:1640423 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0606:1640320:1640422 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0606:1640320:1640416 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0606:1640320:1640416 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0606:1640320:1640416 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0606:1640320:1640416 [0] NCCL INFO ncclCommInitRankConfig comm 0xe226700 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xabf5db3c8087ea2c - Init COMPLETE
lrdn0606:1640320:1640416 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1027:1745942:1746048 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1027:1745942:1746048 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1027:1745942:1746048 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.22.45<0>
lrdn1027:1745942:1746048 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1027:1745942:1746048 [0] NCCL INFO Using network IB
lrdn1027:1745942:1746048 [0] NCCL INFO ncclCommInitRankConfig comm 0xe03f700 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x14c84f179e584636 - Init START
lrdn1027:1745942:1746048 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1027:1745942:1746048 [0] NCCL INFO Bootstrap timings total 0.000461 (create 0.000017, send 0.000061, recv 0.000179, ring 0.000001, delay 0.000001)
lrdn1027:1745942:1746048 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1027:1745942:1746048 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1027:1745942:1746048 [0] NCCL INFO comm 0xe03f700 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 00/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 01/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 02/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 03/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 04/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 05/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 06/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 07/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 08/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 09/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 10/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 11/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 12/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 13/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 14/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 15/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 16/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 17/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 18/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 19/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 20/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 21/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 22/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 23/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 24/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 25/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 26/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 27/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 28/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 29/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 30/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 31/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 32/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 33/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 34/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 35/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 36/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 37/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 38/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 39/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 40/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 41/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 42/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 43/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 44/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 45/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 46/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 47/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 48/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 49/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 50/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 51/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 52/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 53/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 54/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 55/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 56/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 57/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 58/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 59/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 60/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 61/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 62/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Channel 63/64 : 0
lrdn1027:1745942:1746048 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1027:1745942:1746048 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1027:1745942:1746048 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1027:1745942:1746054 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn1027:1745942:1746055 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1
lrdn1027:1745942:1746048 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1027:1745942:1746048 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1027:1745942:1746048 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1027:1745942:1746048 [0] NCCL INFO ncclCommInitRankConfig comm 0xe03f700 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x14c84f179e584636 - Init COMPLETE
lrdn1027:1745942:1746048 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0982:1374856:1374856 [0] NCCL INFO Bootstrap: Using ib0:10.128.21.121<0>
lrdn0982:1374856:1374856 [0] NCCL INFO cudaDriverVersion 12020
lrdn0982:1374856:1374856 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0982:1374856:1374856 [0] NCCL INFO Comm config Blocking set to 1
lrdn0923:1505817:1505817 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.141<0>
lrdn0923:1505817:1505817 [0] NCCL INFO cudaDriverVersion 12020
lrdn0923:1505817:1505817 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0923:1505817:1505817 [0] NCCL INFO Comm config Blocking set to 1
lrdn0982:1374856:1374954 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0982:1374856:1374954 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0170:1950768:1950768 [0] NCCL INFO Bootstrap: Using ib0:10.128.8.201<0>
lrdn0170:1950768:1950768 [0] NCCL INFO cudaDriverVersion 12020
lrdn0170:1950768:1950768 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0170:1950768:1950768 [0] NCCL INFO Comm config Blocking set to 1
lrdn0982:1374856:1374954 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.21.121<0>
lrdn0982:1374856:1374954 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0982:1374856:1374954 [0] NCCL INFO Using network IB
lrdn0982:1374856:1374954 [0] NCCL INFO ncclCommInitRankConfig comm 0xd6b16d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5063fc6ec54d8f8a - Init START
lrdn0982:1374856:1374954 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0982:1374856:1374954 [0] NCCL INFO Bootstrap timings total 0.000370 (create 0.000016, send 0.000060, recv 0.000089, ring 0.000001, delay 0.000001)
lrdn0982:1374856:1374954 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0982:1374856:1374954 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0982:1374856:1374954 [0] NCCL INFO comm 0xd6b16d0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 00/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 01/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 02/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 03/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 04/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 05/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 06/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 07/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 08/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 09/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 10/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 11/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 12/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 13/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 14/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 15/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 16/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 17/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 18/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 19/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 20/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 21/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 22/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 23/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 24/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 25/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 26/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 27/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 28/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 29/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 30/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 31/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 32/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 33/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 34/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 35/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 36/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 37/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 38/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 39/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 40/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 41/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 42/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 43/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 44/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 45/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 46/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 47/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 48/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 49/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 50/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 51/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 52/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 53/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 54/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 55/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 56/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 57/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 58/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 59/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 60/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 61/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 62/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Channel 63/64 : 0
lrdn0982:1374856:1374954 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0982:1374856:1374954 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0982:1374856:1374954 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0982:1374856:1374961 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0982:1374856:1374960 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn0982:1374856:1374954 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0982:1374856:1374954 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0923:1505817:1505915 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0923:1505817:1505915 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0982:1374856:1374954 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0982:1374856:1374954 [0] NCCL INFO ncclCommInitRankConfig comm 0xd6b16d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5063fc6ec54d8f8a - Init COMPLETE
lrdn0982:1374856:1374954 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn0923:1505817:1505915 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.141<0>
lrdn0923:1505817:1505915 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0923:1505817:1505915 [0] NCCL INFO Using network IB
lrdn0923:1505817:1505915 [0] NCCL INFO ncclCommInitRankConfig comm 0xcbb72d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc53b23812f86be46 - Init START
lrdn0923:1505817:1505915 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0923:1505817:1505915 [0] NCCL INFO Bootstrap timings total 0.000380 (create 0.000019, send 0.000059, recv 0.000093, ring 0.000001, delay 0.000000)
lrdn0923:1505817:1505915 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0923:1505817:1505915 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0923:1505817:1505915 [0] NCCL INFO comm 0xcbb72d0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 00/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 01/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 02/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 03/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 04/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 05/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 06/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 07/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 08/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 09/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 10/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 11/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 12/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 13/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 14/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 15/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 16/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 17/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 18/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 19/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 20/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 21/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 22/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 23/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 24/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 25/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 26/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 27/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 28/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 29/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 30/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 31/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 32/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 33/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 34/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 35/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 36/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 37/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 38/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 39/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 40/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 41/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 42/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 43/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 44/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 45/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 46/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 47/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 48/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 49/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 50/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 51/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 52/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 53/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 54/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 55/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 56/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 57/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 58/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 59/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 60/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 61/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 62/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Channel 63/64 : 0
lrdn0923:1505817:1505915 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0923:1505817:1505915 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0923:1505817:1505915 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0923:1505817:1505922 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0923:1505817:1505921 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn0923:1505817:1505915 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0923:1505817:1505915 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0923:1505817:1505915 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0923:1505817:1505915 [0] NCCL INFO ncclCommInitRankConfig comm 0xcbb72d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc53b23812f86be46 - Init COMPLETE
lrdn0923:1505817:1505915 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0170:1950768:1950943 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0170:1950768:1950943 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0170:1950768:1950943 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.8.201<0>
lrdn0170:1950768:1950943 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0170:1950768:1950943 [0] NCCL INFO Using network IB
lrdn0170:1950768:1950943 [0] NCCL INFO ncclCommInitRankConfig comm 0xd78ba10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5d8c3784e2f6cc16 - Init START
lrdn0170:1950768:1950943 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0170:1950768:1950943 [0] NCCL INFO Bootstrap timings total 0.000479 (create 0.000015, send 0.000060, recv 0.000201, ring 0.000001, delay 0.000000)
lrdn0170:1950768:1950943 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0170:1950768:1950943 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0170:1950768:1950943 [0] NCCL INFO comm 0xd78ba10 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 00/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 01/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 02/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 03/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 04/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 05/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 06/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 07/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 08/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 09/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 10/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 11/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 12/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 13/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 14/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 15/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 16/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 17/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 18/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 19/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 20/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 21/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 22/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 23/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 24/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 25/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 26/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 27/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 28/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 29/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 30/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 31/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 32/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 33/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 34/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 35/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 36/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 37/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 38/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 39/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 40/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 41/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 42/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 43/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 44/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 45/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 46/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 47/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 48/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 49/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 50/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 51/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 52/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 53/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 54/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 55/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 56/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 57/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 58/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 59/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 60/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 61/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 62/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Channel 63/64 : 0
lrdn0170:1950768:1950943 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0170:1950768:1950943 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0170:1950768:1950943 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0170:1950768:1950950 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0170:1950768:1950949 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0170:1950768:1950943 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0170:1950768:1950943 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0170:1950768:1950943 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0170:1950768:1950943 [0] NCCL INFO ncclCommInitRankConfig comm 0xd78ba10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5d8c3784e2f6cc16 - Init COMPLETE
lrdn0170:1950768:1950943 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1276:1691254:1691254 [0] NCCL INFO Bootstrap: Using ib0:10.128.26.17<0>
lrdn1276:1691254:1691254 [0] NCCL INFO cudaDriverVersion 12020
lrdn1276:1691254:1691254 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1276:1691254:1691254 [0] NCCL INFO Comm config Blocking set to 1
lrdn1276:1691254:1691359 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1276:1691254:1691359 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1276:1691254:1691359 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.26.17<0>
lrdn1276:1691254:1691359 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1276:1691254:1691359 [0] NCCL INFO Using network IB
lrdn1276:1691254:1691359 [0] NCCL INFO ncclCommInitRankConfig comm 0xd3c9ae0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe6f7960079bcbc0c - Init START
lrdn1276:1691254:1691359 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1276:1691254:1691359 [0] NCCL INFO Bootstrap timings total 0.000384 (create 0.000017, send 0.000058, recv 0.000098, ring 0.000001, delay 0.000001)
lrdn1276:1691254:1691359 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1276:1691254:1691359 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1276:1691254:1691359 [0] NCCL INFO comm 0xd3c9ae0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 00/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 01/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 02/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 03/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 04/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 05/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 06/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 07/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 08/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 09/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 10/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 11/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 12/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 13/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 14/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 15/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 16/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 17/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 18/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 19/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 20/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 21/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 22/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 23/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 24/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 25/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 26/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 27/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 28/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 29/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 30/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 31/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 32/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 33/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 34/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 35/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 36/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 37/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 38/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 39/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 40/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 41/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 42/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 43/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 44/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 45/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 46/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 47/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 48/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 49/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 50/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 51/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 52/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 53/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 54/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 55/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 56/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 57/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 58/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 59/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 60/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 61/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 62/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Channel 63/64 : 0
lrdn1276:1691254:1691359 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1276:1691254:1691359 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1276:1691254:1691359 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1276:1691254:1691365 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn1276:1691254:1691366 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn1276:1691254:1691359 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1276:1691254:1691359 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1276:1691254:1691359 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1276:1691254:1691359 [0] NCCL INFO ncclCommInitRankConfig comm 0xd3c9ae0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe6f7960079bcbc0c - Init COMPLETE
lrdn1276:1691254:1691359 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0539:3885339:3885339 [0] NCCL INFO Bootstrap: Using ib0:10.128.14.141<0>
lrdn0539:3885339:3885339 [0] NCCL INFO cudaDriverVersion 12020
lrdn0539:3885339:3885339 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0539:3885339:3885339 [0] NCCL INFO Comm config Blocking set to 1
lrdn0134:2712778:2712778 [0] NCCL INFO Bootstrap: Using ib0:10.128.8.57<0>
lrdn0134:2712778:2712778 [0] NCCL INFO cudaDriverVersion 12020
lrdn0134:2712778:2712778 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0134:2712778:2712778 [0] NCCL INFO Comm config Blocking set to 1
lrdn0298:1697225:1697225 [0] NCCL INFO Bootstrap: Using ib0:10.128.10.201<0>
lrdn0298:1697225:1697225 [0] NCCL INFO cudaDriverVersion 12020
lrdn0298:1697225:1697225 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0298:1697225:1697225 [0] NCCL INFO Comm config Blocking set to 1
lrdn0539:3885339:3885436 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0539:3885339:3885436 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0134:2712778:2712953 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0134:2712778:2712953 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0539:3885339:3885436 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.14.141<0>
lrdn0539:3885339:3885436 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0539:3885339:3885436 [0] NCCL INFO Using network IB
lrdn0539:3885339:3885436 [0] NCCL INFO ncclCommInitRankConfig comm 0xf764f90 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x37cb49b75cf4f3bd - Init START
lrdn0539:3885339:3885436 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0539:3885339:3885436 [0] NCCL INFO Bootstrap timings total 0.000378 (create 0.000015, send 0.000051, recv 0.000096, ring 0.000001, delay 0.000000)
lrdn0539:3885339:3885436 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0539:3885339:3885436 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0539:3885339:3885436 [0] NCCL INFO comm 0xf764f90 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 00/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 01/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 02/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 03/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 04/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 05/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 06/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 07/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 08/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 09/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 10/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 11/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 12/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 13/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 14/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 15/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 16/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 17/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 18/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 19/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 20/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 21/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 22/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 23/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 24/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 25/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 26/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 27/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 28/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 29/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 30/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 31/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 32/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 33/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 34/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 35/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 36/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 37/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 38/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 39/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 40/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 41/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 42/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 43/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 44/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 45/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 46/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 47/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 48/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 49/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 50/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 51/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 52/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 53/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 54/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 55/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 56/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 57/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 58/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 59/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 60/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 61/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 62/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Channel 63/64 : 0
lrdn0539:3885339:3885436 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0539:3885339:3885436 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0539:3885339:3885436 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0539:3885339:3885442 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn0539:3885339:3885443 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0134:2712778:2712953 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.8.57<0>
lrdn0134:2712778:2712953 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0134:2712778:2712953 [0] NCCL INFO Using network IB
lrdn0134:2712778:2712953 [0] NCCL INFO ncclCommInitRankConfig comm 0xc5f9020 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5995fe4fbdb53044 - Init START
lrdn0134:2712778:2712953 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0134:2712778:2712953 [0] NCCL INFO Bootstrap timings total 0.000381 (create 0.000018, send 0.000059, recv 0.000091, ring 0.000001, delay 0.000000)
lrdn0539:3885339:3885436 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0539:3885339:3885436 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0539:3885339:3885436 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0539:3885339:3885436 [0] NCCL INFO ncclCommInitRankConfig comm 0xf764f90 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x37cb49b75cf4f3bd - Init COMPLETE
lrdn0539:3885339:3885436 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.02, rest 0.00)
lrdn0134:2712778:2712953 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0134:2712778:2712953 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0134:2712778:2712953 [0] NCCL INFO comm 0xc5f9020 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 00/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 01/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 02/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 03/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 04/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 05/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 06/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 07/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 08/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 09/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 10/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 11/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 12/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 13/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 14/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 15/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 16/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 17/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 18/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 19/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 20/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 21/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 22/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 23/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 24/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 25/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 26/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 27/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 28/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 29/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 30/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 31/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 32/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 33/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 34/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 35/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 36/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 37/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 38/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 39/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 40/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 41/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 42/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 43/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 44/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 45/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 46/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 47/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 48/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 49/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 50/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 51/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 52/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 53/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 54/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 55/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 56/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 57/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 58/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 59/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 60/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 61/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 62/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Channel 63/64 : 0
lrdn0134:2712778:2712953 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0134:2712778:2712953 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0134:2712778:2712953 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0134:2712778:2712959 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn0134:2712778:2712960 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn0134:2712778:2712953 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0134:2712778:2712953 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0134:2712778:2712953 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0134:2712778:2712953 [0] NCCL INFO ncclCommInitRankConfig comm 0xc5f9020 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5995fe4fbdb53044 - Init COMPLETE
lrdn0134:2712778:2712953 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0298:1697225:1697320 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0298:1697225:1697320 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0493:1513831:1513831 [0] NCCL INFO Bootstrap: Using ib0:10.128.13.213<0>
lrdn0493:1513831:1513831 [0] NCCL INFO cudaDriverVersion 12020
lrdn0493:1513831:1513831 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0493:1513831:1513831 [0] NCCL INFO Comm config Blocking set to 1
lrdn0298:1697225:1697320 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.10.201<0>
lrdn0298:1697225:1697320 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0298:1697225:1697320 [0] NCCL INFO Using network IB
lrdn0298:1697225:1697320 [0] NCCL INFO ncclCommInitRankConfig comm 0xcf404a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xcae90cb4eaf92345 - Init START
lrdn0298:1697225:1697320 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0298:1697225:1697320 [0] NCCL INFO Bootstrap timings total 0.000497 (create 0.000016, send 0.000054, recv 0.000211, ring 0.000001, delay 0.000000)
lrdn0298:1697225:1697320 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0298:1697225:1697320 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0298:1697225:1697320 [0] NCCL INFO comm 0xcf404a0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 00/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 01/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 02/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 03/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 04/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 05/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 06/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 07/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 08/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 09/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 10/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 11/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 12/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 13/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 14/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 15/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 16/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 17/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 18/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 19/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 20/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 21/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 22/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 23/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 24/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 25/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 26/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 27/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 28/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 29/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 30/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 31/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 32/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 33/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 34/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 35/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 36/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 37/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 38/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 39/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 40/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 41/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 42/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 43/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 44/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 45/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 46/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 47/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 48/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 49/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 50/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 51/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 52/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 53/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 54/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 55/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 56/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 57/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 58/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 59/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 60/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 61/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 62/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Channel 63/64 : 0
lrdn0298:1697225:1697320 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0298:1697225:1697320 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0298:1697225:1697320 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0298:1697225:1697326 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn0298:1697225:1697327 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1
lrdn0298:1697225:1697320 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0298:1697225:1697320 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0298:1697225:1697320 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0298:1697225:1697320 [0] NCCL INFO ncclCommInitRankConfig comm 0xcf404a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xcae90cb4eaf92345 - Init COMPLETE
lrdn0298:1697225:1697320 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0478:1546843:1546843 [0] NCCL INFO Bootstrap: Using ib0:10.128.13.153<0>
lrdn0478:1546843:1546843 [0] NCCL INFO cudaDriverVersion 12020
lrdn0478:1546843:1546843 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0478:1546843:1546843 [0] NCCL INFO Comm config Blocking set to 1
lrdn0493:1513831:1513927 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0493:1513831:1513927 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0493:1513831:1513927 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.13.213<0>
lrdn0493:1513831:1513927 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0493:1513831:1513927 [0] NCCL INFO Using network IB
lrdn0493:1513831:1513927 [0] NCCL INFO ncclCommInitRankConfig comm 0xdac9670 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9f1c38b6b596675e - Init START
lrdn0493:1513831:1513927 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0493:1513831:1513927 [0] NCCL INFO Bootstrap timings total 0.000372 (create 0.000017, send 0.000057, recv 0.000095, ring 0.000001, delay 0.000000)
lrdn0493:1513831:1513927 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0493:1513831:1513927 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0493:1513831:1513927 [0] NCCL INFO comm 0xdac9670 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 00/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 01/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 02/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 03/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 04/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 05/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 06/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 07/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 08/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 09/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 10/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 11/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 12/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 13/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 14/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 15/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 16/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 17/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 18/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 19/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 20/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 21/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 22/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 23/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 24/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 25/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 26/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 27/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 28/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 29/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 30/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 31/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 32/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 33/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 34/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 35/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 36/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 37/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 38/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 39/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 40/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 41/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 42/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 43/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 44/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 45/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 46/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 47/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 48/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 49/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 50/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 51/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 52/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 53/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 54/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 55/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 56/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 57/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 58/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 59/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 60/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 61/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 62/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Channel 63/64 : 0
lrdn0493:1513831:1513927 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0493:1513831:1513927 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0493:1513831:1513927 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0493:1513831:1513933 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0493:1513831:1513934 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0493:1513831:1513927 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0493:1513831:1513927 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0493:1513831:1513927 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0493:1513831:1513927 [0] NCCL INFO ncclCommInitRankConfig comm 0xdac9670 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9f1c38b6b596675e - Init COMPLETE
lrdn0493:1513831:1513927 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.14, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0478:1546843:1546950 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0478:1546843:1546950 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1301:1723839:1723839 [0] NCCL INFO Bootstrap: Using ib0:10.128.26.117<0>
lrdn1301:1723839:1723839 [0] NCCL INFO cudaDriverVersion 12020
lrdn1301:1723839:1723839 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1301:1723839:1723839 [0] NCCL INFO Comm config Blocking set to 1
lrdn1295:3087500:3087500 [0] NCCL INFO Bootstrap: Using ib0:10.128.26.93<0>
lrdn1295:3087500:3087500 [0] NCCL INFO cudaDriverVersion 12020
lrdn1295:3087500:3087500 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1295:3087500:3087500 [0] NCCL INFO Comm config Blocking set to 1
lrdn0478:1546843:1546950 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.13.153<0>
lrdn0478:1546843:1546950 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0478:1546843:1546950 [0] NCCL INFO Using network IB
lrdn0478:1546843:1546950 [0] NCCL INFO ncclCommInitRankConfig comm 0xdea5d40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf0cfe91ab8ac09f - Init START
lrdn0478:1546843:1546950 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0478:1546843:1546950 [0] NCCL INFO Bootstrap timings total 0.000520 (create 0.000018, send 0.000062, recv 0.000225, ring 0.000001, delay 0.000001)
lrdn0478:1546843:1546950 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0478:1546843:1546950 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0478:1546843:1546950 [0] NCCL INFO comm 0xdea5d40 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 00/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 01/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 02/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 03/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 04/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 05/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 06/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 07/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 08/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 09/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 10/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 11/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 12/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 13/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 14/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 15/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 16/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 17/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 18/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 19/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 20/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 21/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 22/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 23/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 24/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 25/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 26/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 27/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 28/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 29/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 30/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 31/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 32/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 33/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 34/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 35/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 36/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 37/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 38/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 39/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 40/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 41/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 42/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 43/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 44/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 45/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 46/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 47/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 48/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 49/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 50/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 51/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 52/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 53/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 54/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 55/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 56/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 57/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 58/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 59/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 60/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 61/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 62/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Channel 63/64 : 0
lrdn0478:1546843:1546950 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0478:1546843:1546950 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0478:1546843:1546950 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0478:1546843:1546957 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0478:1546843:1546956 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0478:1546843:1546950 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0478:1546843:1546950 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0478:1546843:1546950 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0478:1546843:1546950 [0] NCCL INFO ncclCommInitRankConfig comm 0xdea5d40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf0cfe91ab8ac09f - Init COMPLETE
lrdn0478:1546843:1546950 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0632:1818398:1818398 [0] NCCL INFO Bootstrap: Using ib0:10.128.16.1<0>
lrdn0413:1991619:1991619 [0] NCCL INFO Bootstrap: Using ib0:10.128.12.149<0>
lrdn0632:1818398:1818398 [0] NCCL INFO cudaDriverVersion 12020
lrdn0413:1991619:1991619 [0] NCCL INFO cudaDriverVersion 12020
lrdn0632:1818398:1818398 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0413:1991619:1991619 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0632:1818398:1818398 [0] NCCL INFO Comm config Blocking set to 1
lrdn0413:1991619:1991619 [0] NCCL INFO Comm config Blocking set to 1
lrdn1301:1723839:1723960 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1301:1723839:1723960 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1295:3087500:3087606 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1295:3087500:3087606 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1294:2066796:2066796 [0] NCCL INFO Bootstrap: Using ib0:10.128.26.89<0>
lrdn1294:2066796:2066796 [0] NCCL INFO cudaDriverVersion 12020
lrdn1294:2066796:2066796 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1294:2066796:2066796 [0] NCCL INFO Comm config Blocking set to 1
lrdn1295:3087500:3087606 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.26.93<0>
lrdn1301:1723839:1723960 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.26.117<0>
lrdn1295:3087500:3087606 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1295:3087500:3087606 [0] NCCL INFO Using network IB
lrdn1295:3087500:3087606 [0] NCCL INFO ncclCommInitRankConfig comm 0xe0f92f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x31c750f8704a9e8f - Init START
lrdn1301:1723839:1723960 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1301:1723839:1723960 [0] NCCL INFO Using network IB
lrdn1295:3087500:3087606 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1295:3087500:3087606 [0] NCCL INFO Bootstrap timings total 0.000393 (create 0.000017, send 0.000061, recv 0.000098, ring 0.000001, delay 0.000001)
lrdn1301:1723839:1723960 [0] NCCL INFO ncclCommInitRankConfig comm 0xe3ed680 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8b75c1a8bfeab4cf - Init START
lrdn1301:1723839:1723960 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1301:1723839:1723960 [0] NCCL INFO Bootstrap timings total 0.000373 (create 0.000019, send 0.000060, recv 0.000088, ring 0.000001, delay 0.000000)
lrdn1424:3084189:3084189 [0] NCCL INFO Bootstrap: Using ib0:10.128.28.97<0>
lrdn1424:3084189:3084189 [0] NCCL INFO cudaDriverVersion 12020
lrdn1424:3084189:3084189 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1424:3084189:3084189 [0] NCCL INFO Comm config Blocking set to 1
lrdn1295:3087500:3087606 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1295:3087500:3087606 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1295:3087500:3087606 [0] NCCL INFO comm 0xe0f92f0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 00/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 01/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 02/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 03/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 04/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 05/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 06/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 07/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 08/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 09/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 10/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 11/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 12/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 13/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 14/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 15/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 16/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 17/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 18/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 19/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 20/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 21/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 22/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 23/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 24/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 25/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 26/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 27/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 28/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 29/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 30/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 31/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 32/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 33/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 34/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 35/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 36/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 37/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 38/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 39/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 40/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 41/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 42/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 43/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 44/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 45/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 46/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 47/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 48/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 49/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 50/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 51/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 52/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 53/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 54/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 55/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 56/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 57/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 58/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 59/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 60/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 61/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 62/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Channel 63/64 : 0
lrdn1295:3087500:3087606 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1295:3087500:3087606 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1295:3087500:3087606 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1295:3087500:3087612 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn1295:3087500:3087613 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn1301:1723839:1723960 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1301:1723839:1723960 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1301:1723839:1723960 [0] NCCL INFO comm 0xe3ed680 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 00/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 01/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 02/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 03/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 04/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 05/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 06/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 07/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 08/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 09/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 10/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 11/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 12/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 13/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 14/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 15/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 16/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 17/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 18/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 19/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 20/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 21/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 22/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 23/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 24/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 25/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 26/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 27/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 28/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 29/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 30/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 31/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 32/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 33/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 34/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 35/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 36/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 37/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 38/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 39/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 40/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 41/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 42/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 43/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 44/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 45/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 46/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 47/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 48/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 49/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 50/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 51/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 52/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 53/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 54/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 55/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 56/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 57/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 58/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 59/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 60/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 61/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 62/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Channel 63/64 : 0
lrdn1301:1723839:1723960 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1301:1723839:1723960 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1301:1723839:1723960 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1301:1723839:1723966 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1301:1723839:1723967 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0632:1818398:1818505 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0632:1818398:1818505 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0413:1991619:1991714 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0413:1991619:1991714 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1295:3087500:3087606 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1295:3087500:3087606 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1301:1723839:1723960 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1301:1723839:1723960 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1295:3087500:3087606 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1295:3087500:3087606 [0] NCCL INFO ncclCommInitRankConfig comm 0xe0f92f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x31c750f8704a9e8f - Init COMPLETE
lrdn1295:3087500:3087606 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1301:1723839:1723960 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1301:1723839:1723960 [0] NCCL INFO ncclCommInitRankConfig comm 0xe3ed680 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8b75c1a8bfeab4cf - Init COMPLETE
lrdn1301:1723839:1723960 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.14, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0632:1818398:1818505 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.16.1<0>
lrdn0413:1991619:1991714 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.12.149<0>
lrdn0632:1818398:1818505 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0632:1818398:1818505 [0] NCCL INFO Using network IB
lrdn0632:1818398:1818505 [0] NCCL INFO ncclCommInitRankConfig comm 0xd128900 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe0cc37d1ac701e46 - Init START
lrdn0632:1818398:1818505 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0632:1818398:1818505 [0] NCCL INFO Bootstrap timings total 0.000368 (create 0.000015, send 0.000052, recv 0.000092, ring 0.000001, delay 0.000001)
lrdn0413:1991619:1991714 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0413:1991619:1991714 [0] NCCL INFO Using network IB
lrdn0413:1991619:1991714 [0] NCCL INFO ncclCommInitRankConfig comm 0xe651320 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x20d9743f88c8a511 - Init START
lrdn0413:1991619:1991714 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0413:1991619:1991714 [0] NCCL INFO Bootstrap timings total 0.000383 (create 0.000016, send 0.000062, recv 0.000093, ring 0.000001, delay 0.000000)
lrdn0709:1450873:1450873 [0] NCCL INFO Bootstrap: Using ib0:10.128.17.53<0>
lrdn0709:1450873:1450873 [0] NCCL INFO cudaDriverVersion 12020
lrdn0709:1450873:1450873 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0709:1450873:1450873 [0] NCCL INFO Comm config Blocking set to 1
lrdn0632:1818398:1818505 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0632:1818398:1818505 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0632:1818398:1818505 [0] NCCL INFO comm 0xd128900 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 00/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 01/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 02/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 03/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 04/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 05/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 06/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 07/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 08/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 09/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 10/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 11/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 12/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 13/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 14/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 15/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 16/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 17/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 18/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 19/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 20/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 21/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 22/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 23/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 24/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 25/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 26/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 27/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 28/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 29/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 30/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 31/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 32/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 33/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 34/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 35/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 36/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 37/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 38/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 39/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 40/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 41/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 42/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 43/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 44/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 45/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 46/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 47/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 48/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 49/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 50/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 51/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 52/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 53/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 54/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 55/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 56/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 57/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 58/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 59/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 60/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 61/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 62/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Channel 63/64 : 0
lrdn0632:1818398:1818505 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0632:1818398:1818505 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0632:1818398:1818505 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0632:1818398:1818511 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0632:1818398:1818512 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn0413:1991619:1991714 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0413:1991619:1991714 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0413:1991619:1991714 [0] NCCL INFO comm 0xe651320 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 00/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 01/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 02/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 03/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 04/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 05/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 06/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 07/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 08/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 09/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 10/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 11/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 12/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 13/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 14/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 15/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 16/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 17/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 18/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 19/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 20/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 21/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 22/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 23/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 24/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 25/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 26/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 27/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 28/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 29/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 30/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 31/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 32/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 33/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 34/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 35/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 36/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 37/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 38/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 39/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 40/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 41/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 42/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 43/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 44/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 45/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 46/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 47/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 48/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 49/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 50/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 51/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 52/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 53/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 54/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 55/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 56/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 57/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 58/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 59/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 60/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 61/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 62/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Channel 63/64 : 0
lrdn0413:1991619:1991714 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0413:1991619:1991714 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0413:1991619:1991714 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0413:1991619:1991721 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0413:1991619:1991720 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0632:1818398:1818505 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0632:1818398:1818505 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0413:1991619:1991714 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0413:1991619:1991714 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0632:1818398:1818505 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0632:1818398:1818505 [0] NCCL INFO ncclCommInitRankConfig comm 0xd128900 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe0cc37d1ac701e46 - Init COMPLETE
lrdn0632:1818398:1818505 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1294:2066796:2066901 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1294:2066796:2066901 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0413:1991619:1991714 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0413:1991619:1991714 [0] NCCL INFO ncclCommInitRankConfig comm 0xe651320 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x20d9743f88c8a511 - Init COMPLETE
lrdn0413:1991619:1991714 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1294:2066796:2066901 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.26.89<0>
lrdn1294:2066796:2066901 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1294:2066796:2066901 [0] NCCL INFO Using network IB
lrdn1294:2066796:2066901 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc26c10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xcc3d89c4d81b0685 - Init START
lrdn1294:2066796:2066901 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1294:2066796:2066901 [0] NCCL INFO Bootstrap timings total 0.000375 (create 0.000019, send 0.000061, recv 0.000082, ring 0.000001, delay 0.000000)
lrdn1424:3084189:3084284 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1424:3084189:3084284 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1294:2066796:2066901 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1294:2066796:2066901 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1294:2066796:2066901 [0] NCCL INFO comm 0xdc26c10 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 00/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 01/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 02/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 03/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 04/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 05/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 06/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 07/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 08/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 09/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 10/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 11/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 12/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 13/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 14/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 15/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 16/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 17/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 18/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 19/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 20/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 21/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 22/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 23/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 24/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 25/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 26/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 27/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 28/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 29/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 30/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 31/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 32/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 33/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 34/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 35/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 36/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 37/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 38/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 39/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 40/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 41/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 42/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 43/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 44/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 45/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 46/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 47/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 48/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 49/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 50/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 51/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 52/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 53/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 54/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 55/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 56/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 57/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 58/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 59/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 60/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 61/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 62/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Channel 63/64 : 0
lrdn1294:2066796:2066901 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1294:2066796:2066901 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1294:2066796:2066901 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1294:2066796:2066908 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn1294:2066796:2066907 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn1294:2066796:2066901 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1294:2066796:2066901 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1294:2066796:2066901 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1294:2066796:2066901 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc26c10 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xcc3d89c4d81b0685 - Init COMPLETE
lrdn1294:2066796:2066901 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1424:3084189:3084284 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.28.97<0>
lrdn1424:3084189:3084284 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1424:3084189:3084284 [0] NCCL INFO Using network IB
lrdn1424:3084189:3084284 [0] NCCL INFO ncclCommInitRankConfig comm 0x17d69df0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xdc070ad86701c829 - Init START
lrdn1424:3084189:3084284 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1424:3084189:3084284 [0] NCCL INFO Bootstrap timings total 0.000372 (create 0.000016, send 0.000061, recv 0.000087, ring 0.000001, delay 0.000000)
lrdn1424:3084189:3084284 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1424:3084189:3084284 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1424:3084189:3084284 [0] NCCL INFO comm 0x17d69df0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 00/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 01/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 02/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 03/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 04/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 05/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 06/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 07/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 08/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 09/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 10/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 11/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 12/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 13/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 14/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 15/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 16/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 17/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 18/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 19/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 20/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 21/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 22/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 23/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 24/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 25/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 26/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 27/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 28/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 29/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 30/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 31/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 32/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 33/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 34/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 35/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 36/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 37/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 38/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 39/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 40/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 41/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 42/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 43/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 44/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 45/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 46/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 47/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 48/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 49/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 50/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 51/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 52/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 53/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 54/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 55/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 56/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 57/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 58/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 59/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 60/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 61/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 62/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Channel 63/64 : 0
lrdn1424:3084189:3084284 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1424:3084189:3084284 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1424:3084189:3084284 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1424:3084189:3084290 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn1424:3084189:3084291 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn0709:1450873:1450969 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0709:1450873:1450969 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1424:3084189:3084284 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1424:3084189:3084284 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1424:3084189:3084284 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1424:3084189:3084284 [0] NCCL INFO ncclCommInitRankConfig comm 0x17d69df0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xdc070ad86701c829 - Init COMPLETE
lrdn1424:3084189:3084284 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0709:1450873:1450969 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.17.53<0>
lrdn0709:1450873:1450969 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0709:1450873:1450969 [0] NCCL INFO Using network IB
lrdn0709:1450873:1450969 [0] NCCL INFO ncclCommInitRankConfig comm 0xd8e0220 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x893ec550e54a5f9f - Init START
lrdn0709:1450873:1450969 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0709:1450873:1450969 [0] NCCL INFO Bootstrap timings total 0.000357 (create 0.000014, send 0.000052, recv 0.000098, ring 0.000001, delay 0.000000)
lrdn0709:1450873:1450969 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0709:1450873:1450969 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0709:1450873:1450969 [0] NCCL INFO comm 0xd8e0220 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 00/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 01/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 02/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 03/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 04/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 05/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 06/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 07/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 08/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 09/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 10/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 11/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 12/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 13/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 14/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 15/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 16/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 17/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 18/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 19/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 20/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 21/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 22/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 23/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 24/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 25/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 26/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 27/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 28/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 29/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 30/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 31/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 32/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 33/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 34/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 35/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 36/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 37/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 38/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 39/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 40/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 41/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 42/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 43/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 44/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 45/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 46/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 47/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 48/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 49/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 50/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 51/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 52/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 53/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 54/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 55/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 56/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 57/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 58/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 59/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 60/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 61/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 62/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Channel 63/64 : 0
lrdn0709:1450873:1450969 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0709:1450873:1450969 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0709:1450873:1450969 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0709:1450873:1450975 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn0709:1450873:1450976 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn0090:1669266:1669266 [0] NCCL INFO Bootstrap: Using ib0:10.128.7.137<0>
lrdn0090:1669266:1669266 [0] NCCL INFO cudaDriverVersion 12020
lrdn0090:1669266:1669266 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0090:1669266:1669266 [0] NCCL INFO Comm config Blocking set to 1
lrdn0709:1450873:1450969 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0709:1450873:1450969 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0709:1450873:1450969 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0709:1450873:1450969 [0] NCCL INFO ncclCommInitRankConfig comm 0xd8e0220 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x893ec550e54a5f9f - Init COMPLETE
lrdn0709:1450873:1450969 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.14, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0318:1611974:1611974 [0] NCCL INFO Bootstrap: Using ib0:10.128.11.25<0>
lrdn0318:1611974:1611974 [0] NCCL INFO cudaDriverVersion 12020
lrdn0318:1611974:1611974 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0318:1611974:1611974 [0] NCCL INFO Comm config Blocking set to 1
lrdn0090:1669266:1669362 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0090:1669266:1669362 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1234:1664788:1664788 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.105<0>
lrdn1234:1664788:1664788 [0] NCCL INFO cudaDriverVersion 12020
lrdn1234:1664788:1664788 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1234:1664788:1664788 [0] NCCL INFO Comm config Blocking set to 1
lrdn0090:1669266:1669362 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.7.137<0>
lrdn0090:1669266:1669362 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0090:1669266:1669362 [0] NCCL INFO Using network IB
lrdn0090:1669266:1669362 [0] NCCL INFO ncclCommInitRankConfig comm 0xe8e16c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x2c3735f839c91647 - Init START
lrdn0090:1669266:1669362 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0090:1669266:1669362 [0] NCCL INFO Bootstrap timings total 0.000363 (create 0.000015, send 0.000058, recv 0.000087, ring 0.000001, delay 0.000000)
lrdn0090:1669266:1669362 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0090:1669266:1669362 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0090:1669266:1669362 [0] NCCL INFO comm 0xe8e16c0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 00/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 01/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 02/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 03/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 04/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 05/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 06/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 07/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 08/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 09/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 10/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 11/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 12/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 13/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 14/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 15/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 16/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 17/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 18/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 19/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 20/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 21/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 22/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 23/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 24/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 25/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 26/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 27/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 28/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 29/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 30/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 31/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 32/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 33/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 34/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 35/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 36/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 37/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 38/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 39/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 40/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 41/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 42/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 43/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 44/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 45/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 46/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 47/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 48/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 49/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 50/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 51/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 52/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 53/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 54/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 55/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 56/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 57/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 58/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 59/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 60/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 61/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 62/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Channel 63/64 : 0
lrdn0090:1669266:1669362 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0090:1669266:1669362 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0090:1669266:1669362 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0090:1669266:1669368 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn0090:1669266:1669369 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn0090:1669266:1669362 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0090:1669266:1669362 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0090:1669266:1669362 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0090:1669266:1669362 [0] NCCL INFO ncclCommInitRankConfig comm 0xe8e16c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x2c3735f839c91647 - Init COMPLETE
lrdn0090:1669266:1669362 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0318:1611974:1612069 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0318:1611974:1612069 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0318:1611974:1612069 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.11.25<0>
lrdn0536:1614437:1614437 [0] NCCL INFO Bootstrap: Using ib0:10.128.14.129<0>
lrdn0536:1614437:1614437 [0] NCCL INFO cudaDriverVersion 12020
lrdn0536:1614437:1614437 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0536:1614437:1614437 [0] NCCL INFO Comm config Blocking set to 1
lrdn0318:1611974:1612069 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0318:1611974:1612069 [0] NCCL INFO Using network IB
lrdn1234:1664788:1664895 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1234:1664788:1664895 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0318:1611974:1612069 [0] NCCL INFO ncclCommInitRankConfig comm 0xcb85fb0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8b3f6a196931173e - Init START
lrdn0318:1611974:1612069 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0318:1611974:1612069 [0] NCCL INFO Bootstrap timings total 0.000371 (create 0.000017, send 0.000059, recv 0.000094, ring 0.000001, delay 0.000001)
lrdn0893:1602515:1602515 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.21<0>
lrdn0893:1602515:1602515 [0] NCCL INFO cudaDriverVersion 12020
lrdn0893:1602515:1602515 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0893:1602515:1602515 [0] NCCL INFO Comm config Blocking set to 1
lrdn0318:1611974:1612069 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0318:1611974:1612069 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0318:1611974:1612069 [0] NCCL INFO comm 0xcb85fb0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 00/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 01/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 02/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 03/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 04/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 05/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 06/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 07/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 08/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 09/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 10/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 11/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 12/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 13/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 14/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 15/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 16/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 17/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 18/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 19/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 20/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 21/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 22/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 23/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 24/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 25/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 26/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 27/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 28/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 29/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 30/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 31/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 32/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 33/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 34/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 35/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 36/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 37/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 38/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 39/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 40/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 41/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 42/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 43/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 44/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 45/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 46/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 47/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 48/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 49/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 50/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 51/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 52/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 53/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 54/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 55/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 56/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 57/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 58/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 59/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 60/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 61/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 62/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Channel 63/64 : 0
lrdn0318:1611974:1612069 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0318:1611974:1612069 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0318:1611974:1612069 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0318:1611974:1612075 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0318:1611974:1612076 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0318:1611974:1612069 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0318:1611974:1612069 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0318:1611974:1612069 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0318:1611974:1612069 [0] NCCL INFO ncclCommInitRankConfig comm 0xcb85fb0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8b3f6a196931173e - Init COMPLETE
lrdn0318:1611974:1612069 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.14, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1234:1664788:1664895 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.105<0>
lrdn1234:1664788:1664895 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1234:1664788:1664895 [0] NCCL INFO Using network IB
lrdn1234:1664788:1664895 [0] NCCL INFO ncclCommInitRankConfig comm 0xd3120a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x76b283833ddc42ab - Init START
lrdn1234:1664788:1664895 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1234:1664788:1664895 [0] NCCL INFO Bootstrap timings total 0.000505 (create 0.000019, send 0.000059, recv 0.000219, ring 0.000001, delay 0.000001)
lrdn1234:1664788:1664895 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1234:1664788:1664895 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1234:1664788:1664895 [0] NCCL INFO comm 0xd3120a0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 00/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 01/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 02/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 03/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 04/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 05/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 06/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 07/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 08/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 09/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 10/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 11/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 12/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 13/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 14/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 15/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 16/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 17/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 18/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 19/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 20/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 21/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 22/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 23/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 24/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 25/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 26/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 27/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 28/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 29/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 30/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 31/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 32/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 33/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 34/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 35/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 36/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 37/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 38/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 39/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 40/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 41/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 42/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 43/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 44/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 45/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 46/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 47/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 48/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 49/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 50/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 51/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 52/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 53/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 54/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 55/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 56/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 57/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 58/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 59/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 60/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 61/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 62/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Channel 63/64 : 0
lrdn1234:1664788:1664895 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1234:1664788:1664895 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1234:1664788:1664895 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1234:1664788:1664901 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn1234:1664788:1664902 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn1234:1664788:1664895 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1234:1664788:1664895 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1234:1664788:1664895 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1234:1664788:1664895 [0] NCCL INFO ncclCommInitRankConfig comm 0xd3120a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x76b283833ddc42ab - Init COMPLETE
lrdn1234:1664788:1664895 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0536:1614437:1614549 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0536:1614437:1614549 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0893:1602515:1602611 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0893:1602515:1602611 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0536:1614437:1614549 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.14.129<0>
lrdn0536:1614437:1614549 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0536:1614437:1614549 [0] NCCL INFO Using network IB
lrdn0536:1614437:1614549 [0] NCCL INFO ncclCommInitRankConfig comm 0xeb04a30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x372466dd4503f49 - Init START
lrdn0536:1614437:1614549 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0536:1614437:1614549 [0] NCCL INFO Bootstrap timings total 0.000381 (create 0.000018, send 0.000059, recv 0.000097, ring 0.000001, delay 0.000000)
lrdn0855:172542:172542 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.125<0>
lrdn0855:172542:172542 [0] NCCL INFO cudaDriverVersion 12020
lrdn0855:172542:172542 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0855:172542:172542 [0] NCCL INFO Comm config Blocking set to 1
lrdn0815:1752480:1752480 [0] NCCL INFO Bootstrap: Using ib0:10.128.18.221<0>
lrdn0815:1752480:1752480 [0] NCCL INFO cudaDriverVersion 12020
lrdn0815:1752480:1752480 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0815:1752480:1752480 [0] NCCL INFO Comm config Blocking set to 1
lrdn0893:1602515:1602611 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.21<0>
lrdn0536:1614437:1614549 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0536:1614437:1614549 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1075:2205415:2205415 [0] NCCL INFO Bootstrap: Using ib0:10.128.22.237<0>
lrdn0536:1614437:1614549 [0] NCCL INFO comm 0xeb04a30 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 00/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 01/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 02/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 03/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 04/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 05/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 06/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 07/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 08/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 09/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 10/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 11/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 12/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 13/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 14/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 15/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 16/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 17/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 18/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 19/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 20/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 21/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 22/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 23/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 24/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 25/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 26/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 27/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 28/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 29/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 30/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 31/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 32/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 33/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 34/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 35/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 36/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 37/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 38/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 39/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 40/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 41/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 42/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 43/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 44/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 45/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 46/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 47/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 48/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 49/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 50/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 51/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 52/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 53/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 54/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 55/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 56/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 57/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 58/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 59/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 60/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 61/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 62/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Channel 63/64 : 0
lrdn0536:1614437:1614549 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0536:1614437:1614549 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0536:1614437:1614549 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0536:1614437:1614555 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn0536:1614437:1614556 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn1075:2205415:2205415 [0] NCCL INFO cudaDriverVersion 12020
lrdn1075:2205415:2205415 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0893:1602515:1602611 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0893:1602515:1602611 [0] NCCL INFO Using network IB
lrdn1075:2205415:2205415 [0] NCCL INFO Comm config Blocking set to 1
lrdn0893:1602515:1602611 [0] NCCL INFO ncclCommInitRankConfig comm 0xc8175d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x543edc71f7d22726 - Init START
lrdn0893:1602515:1602611 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0893:1602515:1602611 [0] NCCL INFO Bootstrap timings total 0.000446 (create 0.000019, send 0.000060, recv 0.000170, ring 0.000001, delay 0.000000)
lrdn0536:1614437:1614549 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0536:1614437:1614549 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0536:1614437:1614549 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0536:1614437:1614549 [0] NCCL INFO ncclCommInitRankConfig comm 0xeb04a30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x372466dd4503f49 - Init COMPLETE
lrdn0536:1614437:1614549 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0893:1602515:1602611 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0893:1602515:1602611 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0893:1602515:1602611 [0] NCCL INFO comm 0xc8175d0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 00/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 01/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 02/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 03/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 04/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 05/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 06/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 07/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 08/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 09/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 10/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 11/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 12/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 13/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 14/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 15/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 16/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 17/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 18/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 19/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 20/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 21/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 22/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 23/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 24/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 25/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 26/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 27/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 28/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 29/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 30/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 31/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 32/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 33/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 34/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 35/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 36/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 37/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 38/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 39/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 40/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 41/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 42/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 43/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 44/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 45/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 46/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 47/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 48/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 49/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 50/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 51/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 52/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 53/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 54/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 55/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 56/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 57/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 58/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 59/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 60/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 61/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 62/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Channel 63/64 : 0
lrdn0893:1602515:1602611 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0893:1602515:1602611 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0893:1602515:1602611 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0893:1602515:1602618 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0893:1602515:1602617 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn0893:1602515:1602611 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0893:1602515:1602611 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0893:1602515:1602611 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0893:1602515:1602611 [0] NCCL INFO ncclCommInitRankConfig comm 0xc8175d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x543edc71f7d22726 - Init COMPLETE
lrdn0893:1602515:1602611 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.14, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0855:172542:172665 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0855:172542:172665 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0815:1752480:1752589 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0815:1752480:1752589 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1075:2205415:2205521 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1075:2205415:2205521 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1131:3988070:3988070 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.205<0>
lrdn1131:3988070:3988070 [0] NCCL INFO cudaDriverVersion 12020
lrdn1131:3988070:3988070 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1131:3988070:3988070 [0] NCCL INFO Comm config Blocking set to 1
lrdn0855:172542:172665 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.125<0>
lrdn0815:1752480:1752589 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.18.221<0>
lrdn0855:172542:172665 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0855:172542:172665 [0] NCCL INFO Using network IB
lrdn0855:172542:172665 [0] NCCL INFO ncclCommInitRankConfig comm 0xe617910 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x518f932d8dcc80fc - Init START
lrdn0855:172542:172665 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0855:172542:172665 [0] NCCL INFO Bootstrap timings total 0.000507 (create 0.000016, send 0.000055, recv 0.000229, ring 0.000001, delay 0.000000)
lrdn0815:1752480:1752589 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0815:1752480:1752589 [0] NCCL INFO Using network IB
lrdn0815:1752480:1752589 [0] NCCL INFO ncclCommInitRankConfig comm 0xd2acd70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xa2b6464b25fe977f - Init START
lrdn0815:1752480:1752589 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0815:1752480:1752589 [0] NCCL INFO Bootstrap timings total 0.000373 (create 0.000018, send 0.000051, recv 0.000099, ring 0.000001, delay 0.000000)
lrdn1075:2205415:2205521 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.22.237<0>
lrdn1075:2205415:2205521 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1075:2205415:2205521 [0] NCCL INFO Using network IB
lrdn1075:2205415:2205521 [0] NCCL INFO ncclCommInitRankConfig comm 0xc986d70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x829b4aef65267f25 - Init START
lrdn1075:2205415:2205521 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1075:2205415:2205521 [0] NCCL INFO Bootstrap timings total 0.000520 (create 0.000015, send 0.000056, recv 0.000231, ring 0.000001, delay 0.000000)
lrdn0855:172542:172665 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0855:172542:172665 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0855:172542:172665 [0] NCCL INFO comm 0xe617910 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 00/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 01/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 02/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 03/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 04/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 05/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 06/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 07/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 08/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 09/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 10/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 11/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 12/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 13/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 14/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 15/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 16/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 17/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 18/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 19/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 20/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 21/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 22/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 23/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 24/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 25/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 26/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 27/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 28/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 29/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 30/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 31/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 32/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 33/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 34/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 35/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 36/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 37/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 38/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 39/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 40/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 41/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 42/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 43/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 44/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 45/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 46/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 47/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 48/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 49/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 50/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 51/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 52/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 53/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 54/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 55/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 56/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 57/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 58/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 59/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 60/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 61/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 62/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Channel 63/64 : 0
lrdn0855:172542:172665 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0855:172542:172665 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0855:172542:172665 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0855:172542:172671 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0855:172542:172672 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0815:1752480:1752589 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0815:1752480:1752589 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0815:1752480:1752589 [0] NCCL INFO comm 0xd2acd70 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 00/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 01/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 02/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 03/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 04/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 05/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 06/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 07/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 08/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 09/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 10/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 11/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 12/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 13/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 14/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 15/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 16/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 17/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 18/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 19/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 20/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 21/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 22/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 23/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 24/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 25/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 26/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 27/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 28/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 29/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 30/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 31/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 32/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 33/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 34/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 35/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 36/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 37/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 38/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 39/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 40/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 41/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 42/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 43/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 44/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 45/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 46/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 47/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 48/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 49/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 50/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 51/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 52/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 53/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 54/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 55/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 56/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 57/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 58/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 59/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 60/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 61/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 62/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Channel 63/64 : 0
lrdn0815:1752480:1752589 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0815:1752480:1752589 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0815:1752480:1752589 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0815:1752480:1752595 [0] NCCL INFO [Proxy Service] Device 0 CPU core 6
lrdn0815:1752480:1752596 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 12
lrdn1075:2205415:2205521 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1075:2205415:2205521 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1075:2205415:2205521 [0] NCCL INFO comm 0xc986d70 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 00/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 01/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 02/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 03/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 04/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 05/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 06/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 07/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 08/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 09/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 10/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 11/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 12/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 13/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 14/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 15/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 16/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 17/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 18/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 19/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 20/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 21/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 22/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 23/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 24/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 25/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 26/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 27/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 28/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 29/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 30/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 31/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 32/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 33/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 34/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 35/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 36/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 37/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 38/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 39/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 40/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 41/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 42/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 43/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 44/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 45/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 46/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 47/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 48/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 49/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 50/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 51/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 52/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 53/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 54/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 55/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 56/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 57/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 58/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 59/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 60/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 61/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 62/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Channel 63/64 : 0
lrdn1075:2205415:2205521 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1075:2205415:2205521 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1075:2205415:2205521 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1075:2205415:2205527 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1075:2205415:2205528 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0855:172542:172665 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0855:172542:172665 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0815:1752480:1752589 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0815:1752480:1752589 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0855:172542:172665 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0855:172542:172665 [0] NCCL INFO ncclCommInitRankConfig comm 0xe617910 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x518f932d8dcc80fc - Init COMPLETE
lrdn0855:172542:172665 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0815:1752480:1752589 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0815:1752480:1752589 [0] NCCL INFO ncclCommInitRankConfig comm 0xd2acd70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xa2b6464b25fe977f - Init COMPLETE
lrdn0815:1752480:1752589 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1075:2205415:2205521 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1075:2205415:2205521 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1075:2205415:2205521 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1075:2205415:2205521 [0] NCCL INFO ncclCommInitRankConfig comm 0xc986d70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x829b4aef65267f25 - Init COMPLETE
lrdn1075:2205415:2205521 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1131:3988070:3988165 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1131:3988070:3988165 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1131:3988070:3988165 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.205<0>
lrdn1131:3988070:3988165 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1131:3988070:3988165 [0] NCCL INFO Using network IB
lrdn1131:3988070:3988165 [0] NCCL INFO ncclCommInitRankConfig comm 0xd1c0580 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xfc782d6ab7eb4b9c - Init START
lrdn1131:3988070:3988165 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1131:3988070:3988165 [0] NCCL INFO Bootstrap timings total 0.000370 (create 0.000019, send 0.000061, recv 0.000088, ring 0.000001, delay 0.000000)
lrdn1131:3988070:3988165 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1131:3988070:3988165 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1131:3988070:3988165 [0] NCCL INFO comm 0xd1c0580 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 00/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 01/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 02/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 03/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 04/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 05/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 06/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 07/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 08/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 09/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 10/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 11/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 12/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 13/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 14/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 15/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 16/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 17/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 18/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 19/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 20/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 21/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 22/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 23/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 24/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 25/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 26/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 27/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 28/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 29/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 30/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 31/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 32/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 33/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 34/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 35/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 36/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 37/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 38/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 39/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 40/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 41/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 42/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 43/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 44/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 45/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 46/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 47/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 48/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 49/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 50/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 51/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 52/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 53/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 54/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 55/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 56/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 57/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 58/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 59/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 60/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 61/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 62/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Channel 63/64 : 0
lrdn1131:3988070:3988165 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1131:3988070:3988165 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1131:3988070:3988165 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1131:3988070:3988172 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn1131:3988070:3988171 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn1210:161042:161042 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.9<0>
lrdn0849:2195906:2195906 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.101<0>
lrdn1131:3988070:3988165 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1131:3988070:3988165 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1210:161042:161042 [0] NCCL INFO cudaDriverVersion 12020
lrdn1210:161042:161042 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0849:2195906:2195906 [0] NCCL INFO cudaDriverVersion 12020
lrdn0849:2195906:2195906 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1210:161042:161042 [0] NCCL INFO Comm config Blocking set to 1
lrdn0849:2195906:2195906 [0] NCCL INFO Comm config Blocking set to 1
lrdn1131:3988070:3988165 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1131:3988070:3988165 [0] NCCL INFO ncclCommInitRankConfig comm 0xd1c0580 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xfc782d6ab7eb4b9c - Init COMPLETE
lrdn1131:3988070:3988165 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0390:1401765:1401765 [0] NCCL INFO Bootstrap: Using ib0:10.128.12.57<0>
lrdn0390:1401765:1401765 [0] NCCL INFO cudaDriverVersion 12020
lrdn0390:1401765:1401765 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0390:1401765:1401765 [0] NCCL INFO Comm config Blocking set to 1
lrdn1344:1924538:1924538 [0] NCCL INFO Bootstrap: Using ib0:10.128.27.33<0>
lrdn1344:1924538:1924538 [0] NCCL INFO cudaDriverVersion 12020
lrdn1344:1924538:1924538 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1344:1924538:1924538 [0] NCCL INFO Comm config Blocking set to 1
lrdn0849:2195906:2196003 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0849:2195906:2196003 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1210:161042:161148 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1210:161042:161148 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0390:1401765:1401860 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0390:1401765:1401860 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0843:1593394:1593394 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.77<0>
lrdn0843:1593394:1593394 [0] NCCL INFO cudaDriverVersion 12020
lrdn0843:1593394:1593394 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0843:1593394:1593394 [0] NCCL INFO Comm config Blocking set to 1
lrdn0849:2195906:2196003 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.101<0>
lrdn0390:1401765:1401860 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.12.57<0>
lrdn0390:1401765:1401860 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0390:1401765:1401860 [0] NCCL INFO Using network IB
lrdn0849:2195906:2196003 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0849:2195906:2196003 [0] NCCL INFO Using network IB
lrdn0849:2195906:2196003 [0] NCCL INFO ncclCommInitRankConfig comm 0xdf719e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x58254c8b56e01769 - Init START
lrdn0390:1401765:1401860 [0] NCCL INFO ncclCommInitRankConfig comm 0xe2bd560 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xfa56fb6daf42ea22 - Init START
lrdn0849:2195906:2196003 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0849:2195906:2196003 [0] NCCL INFO Bootstrap timings total 0.000381 (create 0.000016, send 0.000055, recv 0.000101, ring 0.000001, delay 0.000000)
lrdn0390:1401765:1401860 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0390:1401765:1401860 [0] NCCL INFO Bootstrap timings total 0.000363 (create 0.000014, send 0.000052, recv 0.000085, ring 0.000001, delay 0.000001)
lrdn1210:161042:161148 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.9<0>
lrdn1210:161042:161148 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1210:161042:161148 [0] NCCL INFO Using network IB
lrdn1210:161042:161148 [0] NCCL INFO ncclCommInitRankConfig comm 0xd4f8850 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1f354c3f28039ff7 - Init START
lrdn1210:161042:161148 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1210:161042:161148 [0] NCCL INFO Bootstrap timings total 0.000374 (create 0.000016, send 0.000063, recv 0.000089, ring 0.000001, delay 0.000000)
lrdn0390:1401765:1401860 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0390:1401765:1401860 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0849:2195906:2196003 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0849:2195906:2196003 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0849:2195906:2196003 [0] NCCL INFO comm 0xdf719e0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 00/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 01/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 02/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 03/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 04/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 05/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 06/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 07/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 08/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 09/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 10/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 11/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 12/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 13/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 14/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 15/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 16/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 17/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 18/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 19/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 20/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 21/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 22/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 23/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 24/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 25/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 26/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 27/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 28/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 29/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 30/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 31/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 32/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 33/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 34/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 35/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 36/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 37/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 38/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 39/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 40/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 41/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 42/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 43/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 44/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 45/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 46/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO comm 0xe2bd560 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 47/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 48/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 49/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 50/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 51/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 52/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 53/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 54/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 55/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 56/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 57/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 58/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 59/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 60/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 61/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 62/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Channel 63/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0849:2195906:2196003 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 00/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 01/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 02/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 03/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 04/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 05/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 06/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 07/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 08/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 09/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 10/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 11/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 12/64 : 0
lrdn0849:2195906:2196003 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 13/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 14/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 15/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 16/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 17/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 18/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 19/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 20/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 21/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 22/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 23/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 24/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 25/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 26/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 27/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 28/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 29/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 30/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 31/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 32/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 33/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 34/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 35/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 36/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 37/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 38/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 39/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 40/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 41/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 42/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 43/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 44/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 45/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 46/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 47/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 48/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 49/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 50/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 51/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 52/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 53/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 54/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 55/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 56/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 57/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 58/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 59/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 60/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 61/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 62/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Channel 63/64 : 0
lrdn0390:1401765:1401860 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0390:1401765:1401860 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0390:1401765:1401860 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0849:2195906:2196009 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0849:2195906:2196010 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0390:1401765:1401867 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0390:1401765:1401866 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn1210:161042:161148 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1210:161042:161148 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1210:161042:161148 [0] NCCL INFO comm 0xd4f8850 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 00/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 01/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 02/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 03/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 04/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 05/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 06/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 07/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 08/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 09/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 10/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 11/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 12/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 13/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 14/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 15/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 16/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 17/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 18/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 19/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 20/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 21/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 22/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 23/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 24/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 25/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 26/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 27/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 28/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 29/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 30/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 31/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 32/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 33/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 34/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 35/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 36/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 37/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 38/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 39/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 40/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 41/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 42/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 43/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 44/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 45/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 46/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 47/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 48/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 49/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 50/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 51/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 52/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 53/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 54/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 55/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 56/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 57/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 58/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 59/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 60/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 61/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 62/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Channel 63/64 : 0
lrdn1210:161042:161148 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn1210:161042:161148 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1210:161042:161148 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1210:161042:161154 [0] NCCL INFO [Proxy Service] Device 0 CPU core 10
lrdn1210:161042:161155 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0849:2195906:2196003 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0849:2195906:2196003 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0390:1401765:1401860 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0390:1401765:1401860 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0390:1401765:1401860 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0390:1401765:1401860 [0] NCCL INFO ncclCommInitRankConfig comm 0xe2bd560 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xfa56fb6daf42ea22 - Init COMPLETE
lrdn0390:1401765:1401860 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0849:2195906:2196003 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0849:2195906:2196003 [0] NCCL INFO ncclCommInitRankConfig comm 0xdf719e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x58254c8b56e01769 - Init COMPLETE
lrdn0849:2195906:2196003 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1210:161042:161148 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1210:161042:161148 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1210:161042:161148 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1210:161042:161148 [0] NCCL INFO ncclCommInitRankConfig comm 0xd4f8850 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x1f354c3f28039ff7 - Init COMPLETE
lrdn1210:161042:161148 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1344:1924538:1924635 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1344:1924538:1924635 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1344:1924538:1924635 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.27.33<0>
lrdn1344:1924538:1924635 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1344:1924538:1924635 [0] NCCL INFO Using network IB
lrdn1344:1924538:1924635 [0] NCCL INFO ncclCommInitRankConfig comm 0xc734ff0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x58a5ed2de0d9c096 - Init START
lrdn1344:1924538:1924635 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1344:1924538:1924635 [0] NCCL INFO Bootstrap timings total 0.000369 (create 0.000016, send 0.000056, recv 0.000086, ring 0.000001, delay 0.000000)
lrdn0843:1593394:1593502 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0843:1593394:1593502 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1344:1924538:1924635 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1344:1924538:1924635 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1344:1924538:1924635 [0] NCCL INFO comm 0xc734ff0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 00/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 01/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 02/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 03/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 04/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 05/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 06/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 07/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 08/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 09/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 10/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 11/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 12/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 13/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 14/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 15/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 16/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 17/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 18/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 19/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 20/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 21/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 22/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 23/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 24/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 25/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 26/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 27/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 28/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 29/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 30/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 31/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 32/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 33/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 34/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 35/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 36/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 37/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 38/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 39/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 40/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 41/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 42/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 43/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 44/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 45/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 46/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 47/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 48/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 49/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 50/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 51/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 52/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 53/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 54/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 55/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 56/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 57/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 58/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 59/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 60/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 61/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 62/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Channel 63/64 : 0
lrdn1344:1924538:1924635 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1344:1924538:1924635 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1344:1924538:1924635 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1344:1924538:1924641 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn1344:1924538:1924642 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn1344:1924538:1924635 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1344:1924538:1924635 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1344:1924538:1924635 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1344:1924538:1924635 [0] NCCL INFO ncclCommInitRankConfig comm 0xc734ff0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x58a5ed2de0d9c096 - Init COMPLETE
lrdn1344:1924538:1924635 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0843:1593394:1593502 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.77<0>
lrdn0843:1593394:1593502 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0843:1593394:1593502 [0] NCCL INFO Using network IB
lrdn0843:1593394:1593502 [0] NCCL INFO ncclCommInitRankConfig comm 0xf8b16f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x23f038502e1289c0 - Init START
lrdn0843:1593394:1593502 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0843:1593394:1593502 [0] NCCL INFO Bootstrap timings total 0.000452 (create 0.000017, send 0.000062, recv 0.000172, ring 0.000001, delay 0.000000)
lrdn0843:1593394:1593502 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0843:1593394:1593502 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0843:1593394:1593502 [0] NCCL INFO comm 0xf8b16f0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 00/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 01/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 02/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 03/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 04/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 05/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 06/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 07/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 08/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 09/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 10/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 11/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 12/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 13/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 14/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 15/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 16/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 17/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 18/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 19/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 20/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 21/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 22/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 23/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 24/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 25/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 26/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 27/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 28/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 29/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 30/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 31/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 32/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 33/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 34/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 35/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 36/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 37/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 38/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 39/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 40/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 41/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 42/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 43/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 44/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 45/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 46/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 47/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 48/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 49/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 50/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 51/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 52/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 53/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 54/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 55/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 56/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 57/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 58/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 59/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 60/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 61/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 62/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Channel 63/64 : 0
lrdn0843:1593394:1593502 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0843:1593394:1593502 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0843:1593394:1593502 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0843:1593394:1593508 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn0843:1593394:1593509 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0932:1700071:1700071 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.177<0>
lrdn0932:1700071:1700071 [0] NCCL INFO cudaDriverVersion 12020
lrdn0932:1700071:1700071 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0843:1593394:1593502 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0843:1593394:1593502 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0932:1700071:1700071 [0] NCCL INFO Comm config Blocking set to 1
lrdn0843:1593394:1593502 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0843:1593394:1593502 [0] NCCL INFO ncclCommInitRankConfig comm 0xf8b16f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x23f038502e1289c0 - Init COMPLETE
lrdn0843:1593394:1593502 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1331:1964040:1964040 [0] NCCL INFO Bootstrap: Using ib0:10.128.26.237<0>
lrdn1283:2320097:2320097 [0] NCCL INFO Bootstrap: Using ib0:10.128.26.45<0>
lrdn1331:1964040:1964040 [0] NCCL INFO cudaDriverVersion 12020
lrdn1283:2320097:2320097 [0] NCCL INFO cudaDriverVersion 12020
lrdn1331:1964040:1964040 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1283:2320097:2320097 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1331:1964040:1964040 [0] NCCL INFO Comm config Blocking set to 1
lrdn1283:2320097:2320097 [0] NCCL INFO Comm config Blocking set to 1
lrdn0499:1654785:1654785 [0] NCCL INFO Bootstrap: Using ib0:10.128.13.237<0>
lrdn0499:1654785:1654785 [0] NCCL INFO cudaDriverVersion 12020
lrdn0499:1654785:1654785 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0499:1654785:1654785 [0] NCCL INFO Comm config Blocking set to 1
lrdn0791:3812973:3812973 [0] NCCL INFO Bootstrap: Using ib0:10.128.18.125<0>
lrdn0791:3812973:3812973 [0] NCCL INFO cudaDriverVersion 12020
lrdn0791:3812973:3812973 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0791:3812973:3812973 [0] NCCL INFO Comm config Blocking set to 1
lrdn1240:1764188:1764188 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.129<0>
lrdn1240:1764188:1764188 [0] NCCL INFO cudaDriverVersion 12020
lrdn1240:1764188:1764188 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1240:1764188:1764188 [0] NCCL INFO Comm config Blocking set to 1
lrdn0933:2108154:2108154 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.181<0>
lrdn0933:2108154:2108154 [0] NCCL INFO cudaDriverVersion 12020
lrdn0933:2108154:2108154 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0933:2108154:2108154 [0] NCCL INFO Comm config Blocking set to 1
lrdn0932:1700071:1700177 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0932:1700071:1700177 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0932:1700071:1700177 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.177<0>
lrdn0932:1700071:1700177 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0932:1700071:1700177 [0] NCCL INFO Using network IB
lrdn0932:1700071:1700177 [0] NCCL INFO ncclCommInitRankConfig comm 0xd608b00 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd8a6aee80c9253cf - Init START
lrdn0932:1700071:1700177 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0932:1700071:1700177 [0] NCCL INFO Bootstrap timings total 0.000382 (create 0.000018, send 0.000060, recv 0.000096, ring 0.000001, delay 0.000000)
lrdn0499:1654785:1654880 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0499:1654785:1654880 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1331:1964040:1964146 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1331:1964040:1964146 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1283:2320097:2320203 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1283:2320097:2320203 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0932:1700071:1700177 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0932:1700071:1700177 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0932:1700071:1700177 [0] NCCL INFO comm 0xd608b00 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 00/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 01/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 02/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 03/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 04/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 05/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 06/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 07/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 08/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 09/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 10/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 11/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 12/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 13/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 14/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 15/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 16/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 17/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 18/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 19/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 20/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 21/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 22/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 23/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 24/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 25/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 26/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 27/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 28/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 29/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 30/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 31/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 32/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 33/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 34/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 35/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 36/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 37/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 38/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 39/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 40/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 41/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 42/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 43/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 44/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 45/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 46/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 47/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 48/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 49/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 50/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 51/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 52/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 53/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 54/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 55/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 56/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 57/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 58/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 59/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 60/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 61/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 62/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Channel 63/64 : 0
lrdn0932:1700071:1700177 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0932:1700071:1700177 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0932:1700071:1700177 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0932:1700071:1700183 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0932:1700071:1700184 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0791:3812973:3813078 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0791:3812973:3813078 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0932:1700071:1700177 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0932:1700071:1700177 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0932:1700071:1700177 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0932:1700071:1700177 [0] NCCL INFO ncclCommInitRankConfig comm 0xd608b00 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd8a6aee80c9253cf - Init COMPLETE
lrdn0932:1700071:1700177 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1240:1764188:1764294 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1240:1764188:1764294 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0499:1654785:1654880 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.13.237<0>
lrdn0499:1654785:1654880 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0499:1654785:1654880 [0] NCCL INFO Using network IB
lrdn0791:3812973:3813078 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.18.125<0>
lrdn0499:1654785:1654880 [0] NCCL INFO ncclCommInitRankConfig comm 0xfa31ee0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7689270d3940a707 - Init START
lrdn0499:1654785:1654880 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0499:1654785:1654880 [0] NCCL INFO Bootstrap timings total 0.000379 (create 0.000017, send 0.000060, recv 0.000096, ring 0.000001, delay 0.000001)
lrdn0791:3812973:3813078 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0791:3812973:3813078 [0] NCCL INFO Using network IB
lrdn0791:3812973:3813078 [0] NCCL INFO ncclCommInitRankConfig comm 0xe441d30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x27c3454763229814 - Init START
lrdn0791:3812973:3813078 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0791:3812973:3813078 [0] NCCL INFO Bootstrap timings total 0.000378 (create 0.000018, send 0.000061, recv 0.000092, ring 0.000001, delay 0.000001)
lrdn1283:2320097:2320203 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.26.45<0>
lrdn1283:2320097:2320203 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1283:2320097:2320203 [0] NCCL INFO Using network IB
lrdn0499:1654785:1654880 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0499:1654785:1654880 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0499:1654785:1654880 [0] NCCL INFO comm 0xfa31ee0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 00/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 01/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 02/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO ncclCommInitRankConfig comm 0xe608de0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xcb4d9566687de327 - Init START
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 03/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 04/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 05/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 06/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 07/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 08/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 09/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 10/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 11/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 12/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 13/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 14/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 15/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 16/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 17/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 18/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 19/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 20/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 21/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 22/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 23/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 24/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 25/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 26/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 27/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 28/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 29/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 30/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 31/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 32/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 33/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 34/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 35/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 36/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 37/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 38/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 39/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 40/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 41/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 42/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 43/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 44/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 45/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 46/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 47/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 48/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 49/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 50/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 51/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 52/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 53/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 54/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 55/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 56/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 57/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 58/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 59/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 60/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 61/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 62/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Channel 63/64 : 0
lrdn0499:1654785:1654880 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0499:1654785:1654880 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0499:1654785:1654880 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0499:1654785:1654886 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0499:1654785:1654887 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn1283:2320097:2320203 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1283:2320097:2320203 [0] NCCL INFO Bootstrap timings total 0.000368 (create 0.000015, send 0.000054, recv 0.000093, ring 0.000001, delay 0.000001)
lrdn1240:1764188:1764294 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.129<0>
lrdn1331:1964040:1964146 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.26.237<0>
lrdn0791:3812973:3813078 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0791:3812973:3813078 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0791:3812973:3813078 [0] NCCL INFO comm 0xe441d30 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 00/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 01/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 02/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 03/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 04/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 05/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 06/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 07/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 08/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 09/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 10/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 11/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 12/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 13/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 14/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 15/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 16/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 17/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 18/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 19/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 20/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 21/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 22/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 23/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 24/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 25/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 26/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 27/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 28/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 29/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 30/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 31/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 32/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 33/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 34/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 35/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 36/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 37/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 38/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 39/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 40/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 41/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 42/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 43/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 44/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 45/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 46/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 47/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 48/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 49/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 50/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 51/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 52/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 53/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 54/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 55/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 56/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 57/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 58/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 59/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 60/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 61/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 62/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Channel 63/64 : 0
lrdn0791:3812973:3813078 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0791:3812973:3813078 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0791:3812973:3813078 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0791:3812973:3813084 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0791:3812973:3813085 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn1240:1764188:1764294 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1240:1764188:1764294 [0] NCCL INFO Using network IB
lrdn1331:1964040:1964146 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1331:1964040:1964146 [0] NCCL INFO Using network IB
lrdn1240:1764188:1764294 [0] NCCL INFO ncclCommInitRankConfig comm 0xe1b8d30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf6e078a290eac1c - Init START
lrdn1331:1964040:1964146 [0] NCCL INFO ncclCommInitRankConfig comm 0xde7c790 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb1a4384f2f93b728 - Init START
lrdn1240:1764188:1764294 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1240:1764188:1764294 [0] NCCL INFO Bootstrap timings total 0.000396 (create 0.000018, send 0.000060, recv 0.000105, ring 0.000002, delay 0.000001)
lrdn1331:1964040:1964146 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1331:1964040:1964146 [0] NCCL INFO Bootstrap timings total 0.000361 (create 0.000013, send 0.000054, recv 0.000096, ring 0.000001, delay 0.000000)
lrdn0499:1654785:1654880 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0499:1654785:1654880 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0991:2352931:2352931 [0] NCCL INFO Bootstrap: Using ib0:10.128.21.157<0>
lrdn0991:2352931:2352931 [0] NCCL INFO cudaDriverVersion 12020
lrdn0991:2352931:2352931 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0991:2352931:2352931 [0] NCCL INFO Comm config Blocking set to 1
lrdn1283:2320097:2320203 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1283:2320097:2320203 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1283:2320097:2320203 [0] NCCL INFO comm 0xe608de0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 00/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 01/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 02/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 03/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 04/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 05/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 06/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 07/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 08/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 09/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 10/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 11/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 12/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 13/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 14/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 15/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 16/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 17/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 18/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 19/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 20/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 21/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 22/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 23/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 24/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 25/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 26/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 27/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 28/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 29/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 30/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 31/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 32/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 33/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 34/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 35/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 36/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 37/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 38/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 39/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 40/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 41/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 42/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 43/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 44/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 45/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 46/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 47/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 48/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 49/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 50/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 51/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 52/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 53/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 54/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 55/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 56/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 57/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 58/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 59/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 60/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 61/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 62/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Channel 63/64 : 0
lrdn1283:2320097:2320203 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1283:2320097:2320203 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1283:2320097:2320203 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1283:2320097:2320209 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1283:2320097:2320210 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0791:3812973:3813078 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0791:3812973:3813078 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0499:1654785:1654880 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0499:1654785:1654880 [0] NCCL INFO ncclCommInitRankConfig comm 0xfa31ee0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7689270d3940a707 - Init COMPLETE
lrdn0499:1654785:1654880 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0791:3812973:3813078 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0791:3812973:3813078 [0] NCCL INFO ncclCommInitRankConfig comm 0xe441d30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x27c3454763229814 - Init COMPLETE
lrdn0791:3812973:3813078 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0933:2108154:2108259 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0933:2108154:2108259 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1240:1764188:1764294 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1240:1764188:1764294 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1240:1764188:1764294 [0] NCCL INFO comm 0xe1b8d30 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 00/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 01/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 02/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 03/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 04/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 05/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 06/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 07/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 08/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 09/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 10/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 11/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 12/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 13/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 14/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 15/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 16/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 17/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 18/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 19/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 20/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 21/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 22/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 23/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 24/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 25/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 26/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 27/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 28/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 29/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 30/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 31/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 32/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 33/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 34/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 35/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 36/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 37/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 38/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 39/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 40/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 41/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 42/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 43/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 44/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 45/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 46/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 47/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 48/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 49/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 50/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 51/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 52/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 53/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 54/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 55/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 56/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 57/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 58/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 59/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 60/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 61/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 62/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Channel 63/64 : 0
lrdn1240:1764188:1764294 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1240:1764188:1764294 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1240:1764188:1764294 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1240:1764188:1764300 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn1240:1764188:1764301 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1331:1964040:1964146 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1331:1964040:1964146 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1331:1964040:1964146 [0] NCCL INFO comm 0xde7c790 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 00/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 01/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 02/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 03/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 04/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 05/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 06/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 07/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 08/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 09/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 10/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 11/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 12/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 13/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 14/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 15/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 16/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 17/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 18/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 19/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 20/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 21/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 22/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 23/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 24/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 25/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 26/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 27/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 28/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 29/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 30/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 31/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 32/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 33/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 34/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 35/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 36/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 37/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 38/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 39/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 40/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 41/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 42/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 43/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 44/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 45/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 46/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 47/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 48/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 49/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 50/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 51/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 52/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 53/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 54/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 55/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 56/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 57/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 58/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 59/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 60/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 61/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 62/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Channel 63/64 : 0
lrdn1331:1964040:1964146 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1331:1964040:1964146 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1331:1964040:1964146 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1331:1964040:1964153 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn1331:1964040:1964152 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn1283:2320097:2320203 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1283:2320097:2320203 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1283:2320097:2320203 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1283:2320097:2320203 [0] NCCL INFO ncclCommInitRankConfig comm 0xe608de0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xcb4d9566687de327 - Init COMPLETE
lrdn1283:2320097:2320203 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.14, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1240:1764188:1764294 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1240:1764188:1764294 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1331:1964040:1964146 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1331:1964040:1964146 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1240:1764188:1764294 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1240:1764188:1764294 [0] NCCL INFO ncclCommInitRankConfig comm 0xe1b8d30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf6e078a290eac1c - Init COMPLETE
lrdn1240:1764188:1764294 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1331:1964040:1964146 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1331:1964040:1964146 [0] NCCL INFO ncclCommInitRankConfig comm 0xde7c790 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb1a4384f2f93b728 - Init COMPLETE
lrdn1331:1964040:1964146 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.14, alloc 0.08, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0933:2108154:2108259 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.181<0>
lrdn0933:2108154:2108259 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0933:2108154:2108259 [0] NCCL INFO Using network IB
lrdn0933:2108154:2108259 [0] NCCL INFO ncclCommInitRankConfig comm 0xc567620 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3df226c9b6bcbdb3 - Init START
lrdn0933:2108154:2108259 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0933:2108154:2108259 [0] NCCL INFO Bootstrap timings total 0.000469 (create 0.000019, send 0.000059, recv 0.000185, ring 0.000001, delay 0.000000)
lrdn0933:2108154:2108259 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0933:2108154:2108259 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0933:2108154:2108259 [0] NCCL INFO comm 0xc567620 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 00/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 01/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 02/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 03/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 04/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 05/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 06/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 07/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 08/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 09/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 10/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 11/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 12/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 13/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 14/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 15/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 16/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 17/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 18/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 19/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 20/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 21/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 22/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 23/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 24/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 25/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 26/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 27/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 28/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 29/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 30/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 31/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 32/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 33/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 34/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 35/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 36/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 37/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 38/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 39/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 40/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 41/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 42/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 43/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 44/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 45/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 46/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 47/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 48/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 49/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 50/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 51/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 52/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 53/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 54/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 55/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 56/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 57/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 58/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 59/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 60/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 61/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 62/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Channel 63/64 : 0
lrdn0933:2108154:2108259 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0933:2108154:2108259 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0933:2108154:2108259 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0933:2108154:2108266 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0933:2108154:2108265 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn0933:2108154:2108259 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0933:2108154:2108259 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0933:2108154:2108259 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0933:2108154:2108259 [0] NCCL INFO ncclCommInitRankConfig comm 0xc567620 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3df226c9b6bcbdb3 - Init COMPLETE
lrdn0933:2108154:2108259 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0991:2352931:2353037 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0991:2352931:2353037 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0991:2352931:2353037 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.21.157<0>
lrdn0991:2352931:2353037 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0991:2352931:2353037 [0] NCCL INFO Using network IB
lrdn0991:2352931:2353037 [0] NCCL INFO ncclCommInitRankConfig comm 0xdb8e190 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9e60a37b347e64aa - Init START
lrdn0991:2352931:2353037 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0991:2352931:2353037 [0] NCCL INFO Bootstrap timings total 0.000381 (create 0.000016, send 0.000060, recv 0.000095, ring 0.000001, delay 0.000000)
lrdn1181:21073:21073 [0] NCCL INFO Bootstrap: Using ib0:10.128.24.149<0>
lrdn1181:21073:21073 [0] NCCL INFO cudaDriverVersion 12020
lrdn1181:21073:21073 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1181:21073:21073 [0] NCCL INFO Comm config Blocking set to 1
lrdn0991:2352931:2353037 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0991:2352931:2353037 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0991:2352931:2353037 [0] NCCL INFO comm 0xdb8e190 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 00/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 01/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 02/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 03/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 04/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 05/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 06/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 07/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 08/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 09/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 10/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 11/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 12/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 13/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 14/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 15/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 16/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 17/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 18/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 19/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 20/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 21/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 22/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 23/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 24/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 25/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 26/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 27/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 28/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 29/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 30/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 31/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 32/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 33/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 34/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 35/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 36/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 37/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 38/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 39/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 40/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 41/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 42/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 43/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 44/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 45/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 46/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 47/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 48/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 49/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 50/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 51/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 52/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 53/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 54/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 55/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 56/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 57/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 58/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 59/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 60/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 61/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 62/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Channel 63/64 : 0
lrdn0991:2352931:2353037 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0991:2352931:2353037 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0991:2352931:2353037 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0991:2352931:2353043 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0991:2352931:2353044 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0177:1599647:1599647 [0] NCCL INFO Bootstrap: Using ib0:10.128.8.229<0>
lrdn0177:1599647:1599647 [0] NCCL INFO cudaDriverVersion 12020
lrdn0177:1599647:1599647 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0991:2352931:2353037 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0991:2352931:2353037 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0177:1599647:1599647 [0] NCCL INFO Comm config Blocking set to 1
lrdn0991:2352931:2353037 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0991:2352931:2353037 [0] NCCL INFO ncclCommInitRankConfig comm 0xdb8e190 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9e60a37b347e64aa - Init COMPLETE
lrdn0991:2352931:2353037 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1261:2025781:2025781 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.213<0>
lrdn1261:2025781:2025781 [0] NCCL INFO cudaDriverVersion 12020
lrdn1261:2025781:2025781 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1261:2025781:2025781 [0] NCCL INFO Comm config Blocking set to 1
lrdn1324:788381:788381 [0] NCCL INFO Bootstrap: Using ib0:10.128.26.209<0>
lrdn1324:788381:788381 [0] NCCL INFO cudaDriverVersion 12020
lrdn1324:788381:788381 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1324:788381:788381 [0] NCCL INFO Comm config Blocking set to 1
lrdn0942:2880356:2880356 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.217<0>
lrdn0942:2880356:2880356 [0] NCCL INFO cudaDriverVersion 12020
lrdn0942:2880356:2880356 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0942:2880356:2880356 [0] NCCL INFO Comm config Blocking set to 1
lrdn1181:21073:21169 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1181:21073:21169 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0177:1599647:1599752 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0177:1599647:1599752 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1181:21073:21169 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.24.149<0>
lrdn1181:21073:21169 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1181:21073:21169 [0] NCCL INFO Using network IB
lrdn1181:21073:21169 [0] NCCL INFO ncclCommInitRankConfig comm 0xdbaf310 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf68648cea3055d6 - Init START
lrdn1181:21073:21169 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1181:21073:21169 [0] NCCL INFO Bootstrap timings total 0.000373 (create 0.000016, send 0.000058, recv 0.000090, ring 0.000001, delay 0.000000)
lrdn1261:2025781:2025878 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1261:2025781:2025878 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1428:1483363:1483363 [0] NCCL INFO Bootstrap: Using ib0:10.128.28.113<0>
lrdn0177:1599647:1599752 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.8.229<0>
lrdn1428:1483363:1483363 [0] NCCL INFO cudaDriverVersion 12020
lrdn1428:1483363:1483363 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1428:1483363:1483363 [0] NCCL INFO Comm config Blocking set to 1
lrdn0177:1599647:1599752 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0177:1599647:1599752 [0] NCCL INFO Using network IB
lrdn1181:21073:21169 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1181:21073:21169 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1181:21073:21169 [0] NCCL INFO comm 0xdbaf310 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 00/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 01/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 02/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 03/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 04/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 05/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 06/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 07/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 08/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 09/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 10/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 11/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 12/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 13/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 14/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 15/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 16/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 17/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 18/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 19/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 20/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 21/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 22/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 23/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 24/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 25/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 26/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 27/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 28/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 29/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 30/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 31/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 32/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 33/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 34/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 35/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 36/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 37/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 38/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 39/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 40/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 41/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 42/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 43/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 44/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 45/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 46/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 47/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 48/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 49/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 50/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 51/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 52/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 53/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 54/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 55/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 56/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 57/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 58/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 59/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 60/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 61/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 62/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Channel 63/64 : 0
lrdn1181:21073:21169 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47] 
lrdn1181:21073:21169 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1181:21073:21169 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1181:21073:21176 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1181:21073:21175 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0177:1599647:1599752 [0] NCCL INFO ncclCommInitRankConfig comm 0xcd8b6b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9037d191a73cabab - Init START
lrdn0177:1599647:1599752 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0177:1599647:1599752 [0] NCCL INFO Bootstrap timings total 0.000379 (create 0.000018, send 0.000059, recv 0.000089, ring 0.000001, delay 0.000000)
lrdn1181:21073:21169 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1181:21073:21169 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1324:788381:788478 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1324:788381:788478 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0177:1599647:1599752 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0177:1599647:1599752 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0177:1599647:1599752 [0] NCCL INFO comm 0xcd8b6b0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 00/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 01/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 02/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 03/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 04/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 05/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 06/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 07/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 08/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 09/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 10/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 11/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 12/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 13/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 14/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 15/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 16/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 17/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 18/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 19/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 20/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 21/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 22/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 23/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 24/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 25/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 26/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 27/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 28/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 29/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 30/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 31/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 32/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 33/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 34/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 35/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 36/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 37/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 38/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 39/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 40/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 41/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 42/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 43/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 44/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 45/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 46/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 47/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 48/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 49/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 50/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 51/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 52/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 53/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 54/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 55/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 56/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 57/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 58/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 59/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 60/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 61/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 62/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Channel 63/64 : 0
lrdn0177:1599647:1599752 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0177:1599647:1599752 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0177:1599647:1599752 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0177:1599647:1599758 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0177:1599647:1599759 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn1181:21073:21169 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1181:21073:21169 [0] NCCL INFO ncclCommInitRankConfig comm 0xdbaf310 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf68648cea3055d6 - Init COMPLETE
lrdn1181:21073:21169 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1261:2025781:2025878 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.213<0>
lrdn1261:2025781:2025878 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1261:2025781:2025878 [0] NCCL INFO Using network IB
lrdn1261:2025781:2025878 [0] NCCL INFO ncclCommInitRankConfig comm 0x160ea610 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc9d7d3a16aad437 - Init START
lrdn1261:2025781:2025878 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1261:2025781:2025878 [0] NCCL INFO Bootstrap timings total 0.000472 (create 0.000018, send 0.000061, recv 0.000188, ring 0.000001, delay 0.000000)
lrdn0942:2880356:2880451 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0942:2880356:2880451 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0177:1599647:1599752 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0177:1599647:1599752 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0177:1599647:1599752 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0177:1599647:1599752 [0] NCCL INFO ncclCommInitRankConfig comm 0xcd8b6b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9037d191a73cabab - Init COMPLETE
lrdn0177:1599647:1599752 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1261:2025781:2025878 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1261:2025781:2025878 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1261:2025781:2025878 [0] NCCL INFO comm 0x160ea610 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 00/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 01/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 02/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 03/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 04/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 05/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 06/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 07/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 08/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 09/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 10/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 11/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 12/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 13/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 14/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 15/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 16/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 17/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 18/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 19/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 20/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 21/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 22/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 23/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 24/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 25/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 26/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 27/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 28/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 29/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 30/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 31/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 32/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 33/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 34/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 35/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 36/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 37/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 38/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 39/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 40/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 41/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 42/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 43/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 44/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 45/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 46/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 47/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 48/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 49/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 50/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 51/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 52/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 53/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 54/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 55/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 56/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 57/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 58/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 59/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 60/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 61/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 62/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Channel 63/64 : 0
lrdn1261:2025781:2025878 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1261:2025781:2025878 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1261:2025781:2025878 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1261:2025781:2025884 [0] NCCL INFO [Proxy Service] Device 0 CPU core 8
lrdn1261:2025781:2025885 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn1261:2025781:2025878 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1261:2025781:2025878 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1261:2025781:2025878 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1261:2025781:2025878 [0] NCCL INFO ncclCommInitRankConfig comm 0x160ea610 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc9d7d3a16aad437 - Init COMPLETE
lrdn1261:2025781:2025878 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.22 (kernels 0.15, alloc 0.04, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1324:788381:788478 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.26.209<0>
lrdn0942:2880356:2880451 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.217<0>
lrdn1324:788381:788478 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1324:788381:788478 [0] NCCL INFO Using network IB
lrdn0942:2880356:2880451 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0942:2880356:2880451 [0] NCCL INFO Using network IB
lrdn1324:788381:788478 [0] NCCL INFO ncclCommInitRankConfig comm 0xd6f6d00 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x61a69ba9bb9ec44d - Init START
lrdn0942:2880356:2880451 [0] NCCL INFO ncclCommInitRankConfig comm 0xd88a170 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf4d41a56f4f7e8f2 - Init START
lrdn1324:788381:788478 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1324:788381:788478 [0] NCCL INFO Bootstrap timings total 0.000380 (create 0.000017, send 0.000060, recv 0.000089, ring 0.000001, delay 0.000001)
lrdn0942:2880356:2880451 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0942:2880356:2880451 [0] NCCL INFO Bootstrap timings total 0.000382 (create 0.000019, send 0.000062, recv 0.000087, ring 0.000001, delay 0.000000)
lrdn1324:788381:788478 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1324:788381:788478 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1324:788381:788478 [0] NCCL INFO comm 0xd6f6d00 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 00/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 01/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 02/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 03/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 04/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 05/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 06/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 07/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 08/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 09/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 10/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 11/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 12/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 13/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 14/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 15/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 16/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 17/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 18/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 19/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 20/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 21/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 22/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 23/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 24/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 25/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 26/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 27/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 28/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 29/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 30/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 31/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 32/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 33/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 34/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 35/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 36/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 37/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 38/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 39/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 40/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 41/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 42/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 43/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 44/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 45/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 46/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 47/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 48/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 49/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 50/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 51/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 52/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 53/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 54/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 55/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 56/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 57/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 58/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 59/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 60/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 61/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 62/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Channel 63/64 : 0
lrdn1324:788381:788478 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn1324:788381:788478 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1324:788381:788478 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1324:788381:788484 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn1324:788381:788485 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0942:2880356:2880451 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0942:2880356:2880451 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0942:2880356:2880451 [0] NCCL INFO comm 0xd88a170 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 00/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 01/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 02/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 03/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 04/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 05/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 06/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 07/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 08/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 09/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 10/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 11/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 12/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 13/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 14/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 15/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 16/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 17/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 18/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 19/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 20/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 21/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 22/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 23/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 24/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 25/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 26/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 27/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 28/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 29/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 30/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 31/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 32/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 33/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 34/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 35/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 36/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 37/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 38/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 39/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 40/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 41/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 42/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 43/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 44/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 45/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 46/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 47/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 48/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 49/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 50/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 51/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 52/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 53/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 54/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 55/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 56/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 57/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 58/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 59/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 60/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 61/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 62/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Channel 63/64 : 0
lrdn0942:2880356:2880451 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0942:2880356:2880451 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0942:2880356:2880451 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0942:2880356:2880457 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn0942:2880356:2880458 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0248:1992477:1992477 [0] NCCL INFO Bootstrap: Using ib0:10.128.10.1<0>
lrdn0248:1992477:1992477 [0] NCCL INFO cudaDriverVersion 12020
lrdn0248:1992477:1992477 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0248:1992477:1992477 [0] NCCL INFO Comm config Blocking set to 1
lrdn0942:2880356:2880451 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0942:2880356:2880451 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1324:788381:788478 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1324:788381:788478 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0942:2880356:2880451 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0942:2880356:2880451 [0] NCCL INFO ncclCommInitRankConfig comm 0xd88a170 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf4d41a56f4f7e8f2 - Init COMPLETE
lrdn0942:2880356:2880451 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1324:788381:788478 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1324:788381:788478 [0] NCCL INFO ncclCommInitRankConfig comm 0xd6f6d00 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x61a69ba9bb9ec44d - Init COMPLETE
lrdn1324:788381:788478 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.14, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0642:1597688:1597688 [0] NCCL INFO Bootstrap: Using ib0:10.128.16.41<0>
lrdn0642:1597688:1597688 [0] NCCL INFO cudaDriverVersion 12020
lrdn0642:1597688:1597688 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0642:1597688:1597688 [0] NCCL INFO Comm config Blocking set to 1
lrdn0693:1636112:1636112 [0] NCCL INFO Bootstrap: Using ib0:10.128.16.245<0>
lrdn0693:1636112:1636112 [0] NCCL INFO cudaDriverVersion 12020
lrdn0693:1636112:1636112 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0693:1636112:1636112 [0] NCCL INFO Comm config Blocking set to 1
lrdn0681:1653353:1653353 [0] NCCL INFO Bootstrap: Using ib0:10.128.16.197<0>
lrdn0681:1653353:1653353 [0] NCCL INFO cudaDriverVersion 12020
lrdn0681:1653353:1653353 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0681:1653353:1653353 [0] NCCL INFO Comm config Blocking set to 1
lrdn1428:1483363:1483459 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1428:1483363:1483459 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1428:1483363:1483459 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.28.113<0>
lrdn1428:1483363:1483459 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1428:1483363:1483459 [0] NCCL INFO Using network IB
lrdn1428:1483363:1483459 [0] NCCL INFO ncclCommInitRankConfig comm 0xdd88060 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc81a2b9819c398da - Init START
lrdn1428:1483363:1483459 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1428:1483363:1483459 [0] NCCL INFO Bootstrap timings total 0.000376 (create 0.000015, send 0.000059, recv 0.000095, ring 0.000001, delay 0.000000)
lrdn0936:1846638:1846638 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.193<0>
lrdn0936:1846638:1846638 [0] NCCL INFO cudaDriverVersion 12020
lrdn0936:1846638:1846638 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0936:1846638:1846638 [0] NCCL INFO Comm config Blocking set to 1
lrdn1428:1483363:1483459 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1428:1483363:1483459 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1428:1483363:1483459 [0] NCCL INFO comm 0xdd88060 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 00/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 01/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 02/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 03/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 04/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 05/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 06/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 07/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 08/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 09/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 10/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 11/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 12/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 13/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 14/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 15/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 16/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 17/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 18/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 19/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 20/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 21/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 22/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 23/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 24/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 25/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 26/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 27/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 28/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 29/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 30/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 31/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 32/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 33/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 34/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 35/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 36/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 37/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 38/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 39/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 40/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 41/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 42/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 43/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 44/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 45/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 46/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 47/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 48/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 49/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 50/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 51/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 52/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 53/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 54/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 55/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 56/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 57/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 58/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 59/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 60/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 61/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 62/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Channel 63/64 : 0
lrdn1428:1483363:1483459 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1428:1483363:1483459 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1428:1483363:1483459 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1428:1483363:1483466 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn1428:1483363:1483465 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1428:1483363:1483459 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1428:1483363:1483459 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1428:1483363:1483459 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1428:1483363:1483459 [0] NCCL INFO ncclCommInitRankConfig comm 0xdd88060 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc81a2b9819c398da - Init COMPLETE
lrdn1428:1483363:1483459 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0248:1992477:1992573 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0248:1992477:1992573 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0248:1992477:1992573 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.10.1<0>
lrdn0248:1992477:1992573 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0248:1992477:1992573 [0] NCCL INFO Using network IB
lrdn0642:1597688:1597784 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0642:1597688:1597784 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0248:1992477:1992573 [0] NCCL INFO ncclCommInitRankConfig comm 0xceea680 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x731098785c9a914a - Init START
lrdn0248:1992477:1992573 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0248:1992477:1992573 [0] NCCL INFO Bootstrap timings total 0.000382 (create 0.000016, send 0.000059, recv 0.000095, ring 0.000001, delay 0.000000)
lrdn0693:1636112:1636208 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0693:1636112:1636208 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0681:1653353:1653458 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0681:1653353:1653458 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0248:1992477:1992573 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0248:1992477:1992573 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0248:1992477:1992573 [0] NCCL INFO comm 0xceea680 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 00/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 01/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 02/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 03/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 04/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 05/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 06/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 07/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 08/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 09/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 10/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 11/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 12/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 13/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 14/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 15/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 16/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 17/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 18/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 19/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 20/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 21/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 22/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 23/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 24/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 25/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 26/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 27/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 28/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 29/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 30/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 31/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 32/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 33/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 34/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 35/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 36/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 37/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 38/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 39/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 40/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 41/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 42/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 43/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 44/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 45/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 46/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 47/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 48/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 49/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 50/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 51/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 52/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 53/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 54/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 55/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 56/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 57/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 58/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 59/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 60/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 61/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 62/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Channel 63/64 : 0
lrdn0248:1992477:1992573 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0248:1992477:1992573 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0248:1992477:1992573 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0248:1992477:1992580 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn0248:1992477:1992579 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn1313:368130:368130 [0] NCCL INFO Bootstrap: Using ib0:10.128.26.165<0>
lrdn1313:368130:368130 [0] NCCL INFO cudaDriverVersion 12020
lrdn1313:368130:368130 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1313:368130:368130 [0] NCCL INFO Comm config Blocking set to 1
lrdn0248:1992477:1992573 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0248:1992477:1992573 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0248:1992477:1992573 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0248:1992477:1992573 [0] NCCL INFO ncclCommInitRankConfig comm 0xceea680 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x731098785c9a914a - Init COMPLETE
lrdn0248:1992477:1992573 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn0642:1597688:1597784 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.16.41<0>
lrdn0681:1653353:1653458 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.16.197<0>
lrdn0642:1597688:1597784 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0642:1597688:1597784 [0] NCCL INFO Using network IB
lrdn0642:1597688:1597784 [0] NCCL INFO ncclCommInitRankConfig comm 0xe755260 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xbb0b2a176d4f2f17 - Init START
lrdn0642:1597688:1597784 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0642:1597688:1597784 [0] NCCL INFO Bootstrap timings total 0.000361 (create 0.000015, send 0.000053, recv 0.000087, ring 0.000001, delay 0.000000)
lrdn0681:1653353:1653458 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0681:1653353:1653458 [0] NCCL INFO Using network IB
lrdn0681:1653353:1653458 [0] NCCL INFO ncclCommInitRankConfig comm 0xde8f940 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xfa77e6e55f832cb3 - Init START
lrdn0681:1653353:1653458 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0681:1653353:1653458 [0] NCCL INFO Bootstrap timings total 0.000493 (create 0.000019, send 0.000062, recv 0.000197, ring 0.000001, delay 0.000000)
lrdn0693:1636112:1636208 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.16.245<0>
lrdn0941:1513798:1513798 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.213<0>
lrdn0941:1513798:1513798 [0] NCCL INFO cudaDriverVersion 12020
lrdn0941:1513798:1513798 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0941:1513798:1513798 [0] NCCL INFO Comm config Blocking set to 1
lrdn0693:1636112:1636208 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0693:1636112:1636208 [0] NCCL INFO Using network IB
lrdn0693:1636112:1636208 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc9c2e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe3bc784cedc0004c - Init START
lrdn0693:1636112:1636208 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0693:1636112:1636208 [0] NCCL INFO Bootstrap timings total 0.000376 (create 0.000016, send 0.000059, recv 0.000096, ring 0.000001, delay 0.000001)
lrdn0642:1597688:1597784 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0642:1597688:1597784 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0642:1597688:1597784 [0] NCCL INFO comm 0xe755260 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 00/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 01/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 02/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 03/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 04/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 05/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 06/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 07/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 08/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 09/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 10/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 11/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 12/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 13/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 14/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 15/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 16/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 17/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 18/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 19/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 20/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 21/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 22/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 23/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 24/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 25/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 26/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 27/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 28/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 29/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 30/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 31/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 32/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 33/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 34/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 35/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 36/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 37/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 38/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 39/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 40/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 41/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 42/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 43/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 44/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 45/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 46/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 47/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 48/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 49/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 50/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 51/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 52/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 53/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 54/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 55/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 56/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 57/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 58/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 59/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 60/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 61/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 62/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Channel 63/64 : 0
lrdn0642:1597688:1597784 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0642:1597688:1597784 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0642:1597688:1597784 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0642:1597688:1597790 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0642:1597688:1597791 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0681:1653353:1653458 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0681:1653353:1653458 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0681:1653353:1653458 [0] NCCL INFO comm 0xde8f940 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 00/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 01/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 02/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 03/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 04/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 05/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 06/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 07/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 08/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 09/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 10/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 11/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 12/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 13/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 14/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 15/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 16/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 17/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 18/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 19/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 20/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 21/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 22/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 23/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 24/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 25/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 26/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 27/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 28/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 29/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 30/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 31/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 32/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 33/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 34/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 35/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 36/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 37/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 38/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 39/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 40/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 41/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 42/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 43/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 44/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 45/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 46/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 47/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 48/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 49/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 50/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 51/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 52/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 53/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 54/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 55/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 56/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 57/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 58/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 59/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 60/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 61/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 62/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Channel 63/64 : 0
lrdn0681:1653353:1653458 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0681:1653353:1653458 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0681:1653353:1653458 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0681:1653353:1653464 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0681:1653353:1653465 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0672:2798667:2798667 [0] NCCL INFO Bootstrap: Using ib0:10.128.16.161<0>
lrdn0672:2798667:2798667 [0] NCCL INFO cudaDriverVersion 12020
lrdn0672:2798667:2798667 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0672:2798667:2798667 [0] NCCL INFO Comm config Blocking set to 1
lrdn0936:1846638:1846745 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0936:1846638:1846745 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0888:298444:298444 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.1<0>
lrdn0888:298444:298444 [0] NCCL INFO cudaDriverVersion 12020
lrdn0888:298444:298444 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0888:298444:298444 [0] NCCL INFO Comm config Blocking set to 1
lrdn0693:1636112:1636208 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0693:1636112:1636208 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0693:1636112:1636208 [0] NCCL INFO comm 0xdc9c2e0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 00/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 01/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 02/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 03/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 04/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 05/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 06/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 07/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 08/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 09/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 10/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 11/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 12/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 13/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 14/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 15/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 16/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 17/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 18/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 19/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 20/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 21/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 22/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 23/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 24/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 25/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 26/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 27/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 28/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 29/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 30/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 31/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 32/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 33/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 34/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 35/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 36/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 37/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 38/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 39/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 40/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 41/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 42/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 43/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 44/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 45/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 46/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 47/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 48/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 49/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 50/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 51/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 52/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 53/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 54/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 55/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 56/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 57/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 58/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 59/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 60/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 61/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 62/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Channel 63/64 : 0
lrdn0693:1636112:1636208 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0693:1636112:1636208 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0693:1636112:1636208 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0693:1636112:1636214 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0693:1636112:1636215 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn0642:1597688:1597784 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0642:1597688:1597784 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0681:1653353:1653458 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0681:1653353:1653458 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0642:1597688:1597784 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0642:1597688:1597784 [0] NCCL INFO ncclCommInitRankConfig comm 0xe755260 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xbb0b2a176d4f2f17 - Init COMPLETE
lrdn0642:1597688:1597784 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.14, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0681:1653353:1653458 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0681:1653353:1653458 [0] NCCL INFO ncclCommInitRankConfig comm 0xde8f940 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xfa77e6e55f832cb3 - Init COMPLETE
lrdn0681:1653353:1653458 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0693:1636112:1636208 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0693:1636112:1636208 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0693:1636112:1636208 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0693:1636112:1636208 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc9c2e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe3bc784cedc0004c - Init COMPLETE
lrdn0693:1636112:1636208 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0936:1846638:1846745 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.193<0>
lrdn0936:1846638:1846745 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0936:1846638:1846745 [0] NCCL INFO Using network IB
lrdn0936:1846638:1846745 [0] NCCL INFO ncclCommInitRankConfig comm 0xd2640e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3af8854c194bc217 - Init START
lrdn0936:1846638:1846745 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0936:1846638:1846745 [0] NCCL INFO Bootstrap timings total 0.000382 (create 0.000018, send 0.000055, recv 0.000104, ring 0.000001, delay 0.000001)
lrdn0936:1846638:1846745 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0936:1846638:1846745 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0936:1846638:1846745 [0] NCCL INFO comm 0xd2640e0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 00/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 01/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 02/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 03/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 04/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 05/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 06/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 07/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 08/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 09/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 10/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 11/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 12/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 13/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 14/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 15/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 16/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 17/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 18/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 19/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 20/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 21/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 22/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 23/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 24/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 25/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 26/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 27/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 28/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 29/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 30/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 31/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 32/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 33/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 34/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 35/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 36/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 37/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 38/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 39/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 40/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 41/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 42/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 43/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 44/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 45/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 46/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 47/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 48/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 49/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 50/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 51/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 52/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 53/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 54/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 55/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 56/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 57/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 58/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 59/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 60/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 61/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 62/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Channel 63/64 : 0
lrdn0936:1846638:1846745 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0936:1846638:1846745 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0936:1846638:1846745 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0936:1846638:1846752 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0936:1846638:1846751 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0936:1846638:1846745 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0936:1846638:1846745 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0936:1846638:1846745 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0936:1846638:1846745 [0] NCCL INFO ncclCommInitRankConfig comm 0xd2640e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3af8854c194bc217 - Init COMPLETE
lrdn0936:1846638:1846745 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1313:368130:368238 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1313:368130:368238 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0941:1513798:1513906 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0941:1513798:1513906 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0672:2798667:2798773 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0672:2798667:2798773 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1313:368130:368238 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.26.165<0>
lrdn0888:298444:298541 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0888:298444:298541 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1313:368130:368238 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1313:368130:368238 [0] NCCL INFO Using network IB
lrdn1313:368130:368238 [0] NCCL INFO ncclCommInitRankConfig comm 0xccd3200 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x595cad19b39a738d - Init START
lrdn1313:368130:368238 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1313:368130:368238 [0] NCCL INFO Bootstrap timings total 0.000485 (create 0.000016, send 0.000061, recv 0.000191, ring 0.000001, delay 0.000000)
lrdn1313:368130:368238 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1313:368130:368238 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1313:368130:368238 [0] NCCL INFO comm 0xccd3200 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 00/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 01/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 02/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 03/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 04/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 05/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 06/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 07/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 08/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 09/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 10/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 11/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 12/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 13/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 14/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 15/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 16/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 17/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 18/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 19/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 20/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 21/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 22/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 23/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 24/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 25/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 26/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 27/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 28/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 29/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 30/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 31/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 32/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 33/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 34/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 35/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 36/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 37/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 38/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 39/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 40/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 41/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 42/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 43/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 44/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 45/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 46/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 47/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 48/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 49/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 50/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 51/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 52/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 53/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 54/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 55/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 56/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 57/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 58/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 59/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 60/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 61/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 62/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Channel 63/64 : 0
lrdn1313:368130:368238 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn1313:368130:368238 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1313:368130:368238 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1313:368130:368245 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn1313:368130:368244 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn0941:1513798:1513906 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.213<0>
lrdn0941:1513798:1513906 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0941:1513798:1513906 [0] NCCL INFO Using network IB
lrdn0941:1513798:1513906 [0] NCCL INFO ncclCommInitRankConfig comm 0xd502f20 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x18f08143c89ab7e2 - Init START
lrdn0941:1513798:1513906 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0941:1513798:1513906 [0] NCCL INFO Bootstrap timings total 0.000547 (create 0.000018, send 0.000065, recv 0.000251, ring 0.000001, delay 0.000000)
lrdn1313:368130:368238 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1313:368130:368238 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0672:2798667:2798773 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.16.161<0>
lrdn1313:368130:368238 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1313:368130:368238 [0] NCCL INFO ncclCommInitRankConfig comm 0xccd3200 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x595cad19b39a738d - Init COMPLETE
lrdn1313:368130:368238 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0672:2798667:2798773 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0672:2798667:2798773 [0] NCCL INFO Using network IB
lrdn0888:298444:298541 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.1<0>
lrdn0672:2798667:2798773 [0] NCCL INFO ncclCommInitRankConfig comm 0xec37590 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x864c60cb75f32949 - Init START
lrdn0672:2798667:2798773 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0672:2798667:2798773 [0] NCCL INFO Bootstrap timings total 0.000385 (create 0.000019, send 0.000064, recv 0.000088, ring 0.000001, delay 0.000000)
lrdn0888:298444:298541 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0888:298444:298541 [0] NCCL INFO Using network IB
lrdn0888:298444:298541 [0] NCCL INFO ncclCommInitRankConfig comm 0xef570c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd08364dcd225f205 - Init START
lrdn0888:298444:298541 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0888:298444:298541 [0] NCCL INFO Bootstrap timings total 0.000378 (create 0.000018, send 0.000065, recv 0.000082, ring 0.000001, delay 0.000000)
lrdn0941:1513798:1513906 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0941:1513798:1513906 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0941:1513798:1513906 [0] NCCL INFO comm 0xd502f20 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 00/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 01/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 02/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 03/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 04/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 05/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 06/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 07/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 08/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 09/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 10/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 11/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 12/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 13/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 14/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 15/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 16/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 17/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 18/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 19/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 20/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 21/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 22/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 23/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 24/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 25/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 26/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 27/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 28/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 29/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 30/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 31/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 32/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 33/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 34/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 35/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 36/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 37/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 38/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 39/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 40/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 41/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 42/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 43/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 44/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 45/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 46/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 47/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 48/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 49/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 50/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 51/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 52/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 53/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 54/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 55/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 56/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 57/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 58/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 59/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 60/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 61/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 62/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Channel 63/64 : 0
lrdn0941:1513798:1513906 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0941:1513798:1513906 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0941:1513798:1513906 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0941:1513798:1513913 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0941:1513798:1513912 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0672:2798667:2798773 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0672:2798667:2798773 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0672:2798667:2798773 [0] NCCL INFO comm 0xec37590 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 00/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 01/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 02/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 03/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 04/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 05/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 06/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 07/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 08/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 09/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 10/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 11/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 12/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 13/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 14/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 15/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 16/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 17/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 18/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 19/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 20/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 21/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 22/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 23/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 24/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 25/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 26/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 27/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 28/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 29/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 30/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 31/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 32/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 33/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 34/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 35/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 36/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 37/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 38/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 39/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 40/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 41/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 42/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 43/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 44/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 45/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 46/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 47/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 48/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 49/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 50/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 51/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 52/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 53/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 54/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 55/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 56/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 57/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 58/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 59/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 60/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 61/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 62/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Channel 63/64 : 0
lrdn0672:2798667:2798773 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0672:2798667:2798773 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0672:2798667:2798773 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0672:2798667:2798779 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn0672:2798667:2798780 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn0941:1513798:1513906 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0941:1513798:1513906 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0888:298444:298541 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0888:298444:298541 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0888:298444:298541 [0] NCCL INFO comm 0xef570c0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 00/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 01/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 02/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 03/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 04/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 05/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 06/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 07/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 08/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 09/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 10/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 11/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 12/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 13/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 14/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 15/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 16/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 17/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 18/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 19/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 20/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 21/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 22/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 23/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 24/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 25/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 26/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 27/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 28/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 29/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 30/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 31/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 32/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 33/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 34/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 35/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 36/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 37/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 38/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 39/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 40/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 41/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 42/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 43/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 44/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 45/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 46/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 47/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 48/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 49/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 50/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 51/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 52/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 53/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 54/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 55/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 56/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 57/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 58/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 59/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 60/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 61/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 62/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Channel 63/64 : 0
lrdn0888:298444:298541 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0888:298444:298541 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0888:298444:298541 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0888:298444:298547 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn0888:298444:298548 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn0941:1513798:1513906 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0941:1513798:1513906 [0] NCCL INFO ncclCommInitRankConfig comm 0xd502f20 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x18f08143c89ab7e2 - Init COMPLETE
lrdn0941:1513798:1513906 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0672:2798667:2798773 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0672:2798667:2798773 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0828:457320:457320 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.17<0>
lrdn0828:457320:457320 [0] NCCL INFO cudaDriverVersion 12020
lrdn0828:457320:457320 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0888:298444:298541 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0888:298444:298541 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0672:2798667:2798773 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0672:2798667:2798773 [0] NCCL INFO ncclCommInitRankConfig comm 0xec37590 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x864c60cb75f32949 - Init COMPLETE
lrdn0672:2798667:2798773 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0828:457320:457320 [0] NCCL INFO Comm config Blocking set to 1
lrdn0888:298444:298541 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0888:298444:298541 [0] NCCL INFO ncclCommInitRankConfig comm 0xef570c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd08364dcd225f205 - Init COMPLETE
lrdn0888:298444:298541 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0828:457320:457430 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0828:457320:457430 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1152:3008174:3008174 [0] NCCL INFO Bootstrap: Using ib0:10.128.24.33<0>
lrdn1152:3008174:3008174 [0] NCCL INFO cudaDriverVersion 12020
lrdn1152:3008174:3008174 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0828:457320:457430 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.17<0>
lrdn1152:3008174:3008174 [0] NCCL INFO Comm config Blocking set to 1
lrdn0828:457320:457430 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0828:457320:457430 [0] NCCL INFO Using network IB
lrdn0828:457320:457430 [0] NCCL INFO ncclCommInitRankConfig comm 0xd57edd0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x37a7646ec5764e4d - Init START
lrdn0828:457320:457430 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0828:457320:457430 [0] NCCL INFO Bootstrap timings total 0.000505 (create 0.000018, send 0.000064, recv 0.000212, ring 0.000001, delay 0.000000)
lrdn0828:457320:457430 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0828:457320:457430 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0828:457320:457430 [0] NCCL INFO comm 0xd57edd0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 00/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 01/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 02/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 03/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 04/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 05/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 06/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 07/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 08/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 09/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 10/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 11/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 12/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 13/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 14/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 15/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 16/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 17/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 18/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 19/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 20/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 21/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 22/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 23/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 24/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 25/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 26/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 27/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 28/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 29/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 30/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 31/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 32/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 33/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 34/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 35/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 36/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 37/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 38/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 39/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 40/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 41/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 42/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 43/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 44/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 45/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 46/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 47/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 48/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 49/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 50/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 51/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 52/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 53/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 54/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 55/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 56/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 57/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 58/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 59/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 60/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 61/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 62/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Channel 63/64 : 0
lrdn0828:457320:457430 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0828:457320:457430 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0828:457320:457430 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0828:457320:457436 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0828:457320:457437 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn0912:2074724:2074724 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.97<0>
lrdn0912:2074724:2074724 [0] NCCL INFO cudaDriverVersion 12020
lrdn0912:2074724:2074724 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0912:2074724:2074724 [0] NCCL INFO Comm config Blocking set to 1
lrdn0828:457320:457430 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0828:457320:457430 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0828:457320:457430 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0828:457320:457430 [0] NCCL INFO ncclCommInitRankConfig comm 0xd57edd0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x37a7646ec5764e4d - Init COMPLETE
lrdn0828:457320:457430 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0542:1578195:1578195 [0] NCCL INFO Bootstrap: Using ib0:10.128.14.153<0>
lrdn0542:1578195:1578195 [0] NCCL INFO cudaDriverVersion 12020
lrdn0542:1578195:1578195 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0542:1578195:1578195 [0] NCCL INFO Comm config Blocking set to 1
lrdn1152:3008174:3008271 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1152:3008174:3008271 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0912:2074724:2074831 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0912:2074724:2074831 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1152:3008174:3008271 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.24.33<0>
lrdn1152:3008174:3008271 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1152:3008174:3008271 [0] NCCL INFO Using network IB
lrdn1152:3008174:3008271 [0] NCCL INFO ncclCommInitRankConfig comm 0x16a5ae60 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9455256420710175 - Init START
lrdn1152:3008174:3008271 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1152:3008174:3008271 [0] NCCL INFO Bootstrap timings total 0.000358 (create 0.000015, send 0.000055, recv 0.000087, ring 0.000001, delay 0.000000)
lrdn1152:3008174:3008271 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1152:3008174:3008271 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1152:3008174:3008271 [0] NCCL INFO comm 0x16a5ae60 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 00/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 01/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 02/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 03/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 04/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 05/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 06/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 07/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 08/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 09/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 10/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 11/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 12/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 13/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 14/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 15/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 16/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 17/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 18/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 19/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 20/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 21/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 22/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 23/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 24/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 25/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 26/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 27/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 28/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 29/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 30/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 31/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 32/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 33/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 34/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 35/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 36/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 37/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 38/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 39/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 40/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 41/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 42/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 43/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 44/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 45/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 46/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 47/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 48/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 49/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 50/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 51/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 52/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 53/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 54/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 55/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 56/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 57/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 58/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 59/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 60/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 61/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 62/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Channel 63/64 : 0
lrdn1152:3008174:3008271 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1152:3008174:3008271 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1152:3008174:3008271 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1152:3008174:3008277 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn1152:3008174:3008278 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 15
lrdn1152:3008174:3008271 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1152:3008174:3008271 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1152:3008174:3008271 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1152:3008174:3008271 [0] NCCL INFO ncclCommInitRankConfig comm 0x16a5ae60 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9455256420710175 - Init COMPLETE
lrdn1152:3008174:3008271 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.22 (kernels 0.14, alloc 0.04, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0542:1578195:1578292 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0542:1578195:1578292 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0912:2074724:2074831 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.97<0>
lrdn0912:2074724:2074831 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0912:2074724:2074831 [0] NCCL INFO Using network IB
lrdn0912:2074724:2074831 [0] NCCL INFO ncclCommInitRankConfig comm 0xf1beb50 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x49538112bba271c4 - Init START
lrdn0912:2074724:2074831 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0912:2074724:2074831 [0] NCCL INFO Bootstrap timings total 0.000362 (create 0.000015, send 0.000055, recv 0.000091, ring 0.000001, delay 0.000000)
lrdn0912:2074724:2074831 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0912:2074724:2074831 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0912:2074724:2074831 [0] NCCL INFO comm 0xf1beb50 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 00/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 01/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 02/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 03/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 04/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 05/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 06/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 07/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 08/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 09/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 10/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 11/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 12/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 13/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 14/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 15/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 16/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 17/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 18/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 19/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 20/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 21/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 22/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 23/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 24/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 25/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 26/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 27/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 28/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 29/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 30/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 31/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 32/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 33/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 34/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 35/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 36/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 37/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 38/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 39/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 40/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 41/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 42/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 43/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 44/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 45/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 46/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 47/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 48/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 49/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 50/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 51/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 52/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 53/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 54/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 55/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 56/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 57/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 58/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 59/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 60/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 61/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 62/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Channel 63/64 : 0
lrdn0912:2074724:2074831 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0912:2074724:2074831 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0912:2074724:2074831 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0912:2074724:2074838 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0912:2074724:2074837 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn0542:1578195:1578292 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.14.153<0>
lrdn0912:2074724:2074831 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0912:2074724:2074831 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0542:1578195:1578292 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0542:1578195:1578292 [0] NCCL INFO Using network IB
lrdn0542:1578195:1578292 [0] NCCL INFO ncclCommInitRankConfig comm 0xc94c1a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xded5d130c2fb58c4 - Init START
lrdn0542:1578195:1578292 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0542:1578195:1578292 [0] NCCL INFO Bootstrap timings total 0.000355 (create 0.000015, send 0.000053, recv 0.000090, ring 0.000001, delay 0.000000)
lrdn0912:2074724:2074831 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0912:2074724:2074831 [0] NCCL INFO ncclCommInitRankConfig comm 0xf1beb50 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x49538112bba271c4 - Init COMPLETE
lrdn0912:2074724:2074831 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.14, alloc 0.07, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0542:1578195:1578292 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0542:1578195:1578292 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0542:1578195:1578292 [0] NCCL INFO comm 0xc94c1a0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 00/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 01/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 02/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 03/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 04/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 05/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 06/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 07/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 08/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 09/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 10/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 11/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 12/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 13/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 14/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 15/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 16/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 17/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 18/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 19/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 20/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 21/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 22/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 23/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 24/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 25/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 26/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 27/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 28/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 29/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 30/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 31/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 32/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 33/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 34/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 35/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 36/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 37/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 38/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 39/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 40/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 41/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 42/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 43/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 44/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 45/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 46/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 47/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 48/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 49/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 50/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 51/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 52/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 53/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 54/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 55/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 56/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 57/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 58/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 59/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 60/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 61/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 62/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Channel 63/64 : 0
lrdn0542:1578195:1578292 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0542:1578195:1578292 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0542:1578195:1578292 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0542:1578195:1578299 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn0542:1578195:1578298 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn0542:1578195:1578292 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0542:1578195:1578292 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0542:1578195:1578292 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0542:1578195:1578292 [0] NCCL INFO ncclCommInitRankConfig comm 0xc94c1a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xded5d130c2fb58c4 - Init COMPLETE
lrdn0542:1578195:1578292 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0387:1604830:1604830 [0] NCCL INFO Bootstrap: Using ib0:10.128.12.45<0>
lrdn0387:1604830:1604830 [0] NCCL INFO cudaDriverVersion 12020
lrdn0387:1604830:1604830 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0387:1604830:1604830 [0] NCCL INFO Comm config Blocking set to 1
lrdn0387:1604830:1604928 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0387:1604830:1604928 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0387:1604830:1604928 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.12.45<0>
lrdn0387:1604830:1604928 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0387:1604830:1604928 [0] NCCL INFO Using network IB
lrdn0387:1604830:1604928 [0] NCCL INFO ncclCommInitRankConfig comm 0xcc8bc40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x30a2abf03d1d7abb - Init START
lrdn0387:1604830:1604928 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0387:1604830:1604928 [0] NCCL INFO Bootstrap timings total 0.000379 (create 0.000017, send 0.000060, recv 0.000097, ring 0.000001, delay 0.000000)
lrdn0387:1604830:1604928 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0387:1604830:1604928 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0387:1604830:1604928 [0] NCCL INFO comm 0xcc8bc40 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 00/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 01/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 02/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 03/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 04/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 05/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 06/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 07/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 08/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 09/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 10/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 11/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 12/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 13/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 14/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 15/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 16/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 17/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 18/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 19/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 20/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 21/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 22/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 23/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 24/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 25/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 26/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 27/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 28/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 29/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 30/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 31/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 32/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 33/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 34/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 35/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 36/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 37/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 38/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 39/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 40/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 41/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 42/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 43/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 44/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 45/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 46/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 47/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 48/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 49/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 50/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 51/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 52/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 53/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 54/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 55/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 56/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 57/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 58/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 59/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 60/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 61/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 62/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Channel 63/64 : 0
lrdn0387:1604830:1604928 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0387:1604830:1604928 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0387:1604830:1604928 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0387:1604830:1604934 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0387:1604830:1604935 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn0387:1604830:1604928 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0387:1604830:1604928 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0387:1604830:1604928 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0387:1604830:1604928 [0] NCCL INFO ncclCommInitRankConfig comm 0xcc8bc40 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x30a2abf03d1d7abb - Init COMPLETE
lrdn0387:1604830:1604928 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0434:838683:838683 [0] NCCL INFO Bootstrap: Using ib0:10.128.12.233<0>
lrdn0434:838683:838683 [0] NCCL INFO cudaDriverVersion 12020
lrdn0434:838683:838683 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0434:838683:838683 [0] NCCL INFO Comm config Blocking set to 1
lrdn0597:1687014:1687014 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.117<0>
lrdn0597:1687014:1687014 [0] NCCL INFO cudaDriverVersion 12020
lrdn0597:1687014:1687014 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0597:1687014:1687014 [0] NCCL INFO Comm config Blocking set to 1
lrdn0434:838683:838859 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0434:838683:838859 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0434:838683:838859 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.12.233<0>
lrdn0434:838683:838859 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0434:838683:838859 [0] NCCL INFO Using network IB
lrdn0434:838683:838859 [0] NCCL INFO ncclCommInitRankConfig comm 0x32946410 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9254e6f91a47a414 - Init START
lrdn0434:838683:838859 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0434:838683:838859 [0] NCCL INFO Bootstrap timings total 0.000360 (create 0.000015, send 0.000052, recv 0.000085, ring 0.000001, delay 0.000000)
lrdn0434:838683:838859 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0434:838683:838859 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0434:838683:838859 [0] NCCL INFO comm 0x32946410 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 00/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 01/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 02/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 03/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 04/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 05/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 06/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 07/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 08/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 09/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 10/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 11/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 12/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 13/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 14/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 15/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 16/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 17/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 18/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 19/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 20/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 21/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 22/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 23/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 24/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 25/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 26/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 27/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 28/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 29/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 30/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 31/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 32/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 33/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 34/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 35/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 36/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 37/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 38/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 39/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 40/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 41/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 42/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 43/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 44/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 45/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 46/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 47/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 48/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 49/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 50/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 51/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 52/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 53/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 54/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 55/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 56/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 57/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 58/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 59/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 60/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 61/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 62/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Channel 63/64 : 0
lrdn0434:838683:838859 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [47
lrdn0434:838683:838859 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0434:838683:838859 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0434:838683:838866 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0434:838683:838865 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn0434:838683:838859 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0434:838683:838859 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0434:838683:838859 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0434:838683:838859 [0] NCCL INFO ncclCommInitRankConfig comm 0x32946410 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9254e6f91a47a414 - Init COMPLETE
lrdn0434:838683:838859 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.14, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0668:1708195:1708195 [0] NCCL INFO Bootstrap: Using ib0:10.128.16.145<0>
lrdn0668:1708195:1708195 [0] NCCL INFO cudaDriverVersion 12020
lrdn0668:1708195:1708195 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0668:1708195:1708195 [0] NCCL INFO Comm config Blocking set to 1
lrdn0597:1687014:1687121 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0597:1687014:1687121 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0597:1687014:1687121 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.117<0>
lrdn0597:1687014:1687121 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0597:1687014:1687121 [0] NCCL INFO Using network IB
lrdn0597:1687014:1687121 [0] NCCL INFO ncclCommInitRankConfig comm 0xd40f2a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xbf32854e8c0b993c - Init START
lrdn0597:1687014:1687121 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0597:1687014:1687121 [0] NCCL INFO Bootstrap timings total 0.000381 (create 0.000017, send 0.000062, recv 0.000095, ring 0.000001, delay 0.000001)
lrdn0597:1687014:1687121 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0597:1687014:1687121 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0597:1687014:1687121 [0] NCCL INFO comm 0xd40f2a0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 00/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 01/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 02/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 03/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 04/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 05/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 06/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 07/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 08/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 09/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 10/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 11/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 12/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 13/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 14/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 15/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 16/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 17/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 18/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 19/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 20/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 21/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 22/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 23/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 24/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 25/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 26/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 27/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 28/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 29/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 30/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 31/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 32/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 33/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 34/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 35/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 36/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 37/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 38/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 39/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 40/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 41/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 42/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 43/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 44/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 45/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 46/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 47/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 48/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 49/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 50/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 51/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 52/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 53/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 54/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 55/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 56/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 57/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 58/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 59/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 60/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 61/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 62/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Channel 63/64 : 0
lrdn0597:1687014:1687121 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0597:1687014:1687121 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0597:1687014:1687121 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0597:1687014:1687127 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0597:1687014:1687128 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0597:1687014:1687121 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0597:1687014:1687121 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0597:1687014:1687121 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0597:1687014:1687121 [0] NCCL INFO ncclCommInitRankConfig comm 0xd40f2a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xbf32854e8c0b993c - Init COMPLETE
lrdn0597:1687014:1687121 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1043:3601459:3601459 [0] NCCL INFO Bootstrap: Using ib0:10.128.22.109<0>
lrdn1043:3601459:3601459 [0] NCCL INFO cudaDriverVersion 12020
lrdn1043:3601459:3601459 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1043:3601459:3601459 [0] NCCL INFO Comm config Blocking set to 1
lrdn0668:1708195:1708300 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0668:1708195:1708300 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0668:1708195:1708300 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.16.145<0>
lrdn0668:1708195:1708300 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0668:1708195:1708300 [0] NCCL INFO Using network IB
lrdn0668:1708195:1708300 [0] NCCL INFO ncclCommInitRankConfig comm 0xccdd220 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x52298ff062acdae3 - Init START
lrdn0668:1708195:1708300 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0668:1708195:1708300 [0] NCCL INFO Bootstrap timings total 0.000378 (create 0.000018, send 0.000056, recv 0.000095, ring 0.000001, delay 0.000000)
lrdn0668:1708195:1708300 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0668:1708195:1708300 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0668:1708195:1708300 [0] NCCL INFO comm 0xccdd220 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 00/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 01/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 02/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 03/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 04/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 05/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 06/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 07/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 08/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 09/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 10/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 11/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 12/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 13/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 14/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 15/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 16/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 17/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 18/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 19/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 20/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 21/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 22/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 23/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 24/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 25/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 26/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 27/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 28/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 29/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 30/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 31/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 32/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 33/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 34/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 35/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 36/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 37/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 38/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 39/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 40/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 41/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 42/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 43/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 44/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 45/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 46/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 47/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 48/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 49/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 50/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 51/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 52/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 53/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 54/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 55/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 56/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 57/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 58/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 59/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 60/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 61/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 62/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Channel 63/64 : 0
lrdn0668:1708195:1708300 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0668:1708195:1708300 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0668:1708195:1708300 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0668:1708195:1708306 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0668:1708195:1708307 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn0668:1708195:1708300 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0668:1708195:1708300 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0668:1708195:1708300 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0668:1708195:1708300 [0] NCCL INFO ncclCommInitRankConfig comm 0xccdd220 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x52298ff062acdae3 - Init COMPLETE
lrdn0668:1708195:1708300 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0147:1522198:1522198 [0] NCCL INFO Bootstrap: Using ib0:10.128.8.109<0>
lrdn0147:1522198:1522198 [0] NCCL INFO cudaDriverVersion 12020
lrdn0147:1522198:1522198 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0147:1522198:1522198 [0] NCCL INFO Comm config Blocking set to 1
lrdn1043:3601459:3601564 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1043:3601459:3601564 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1203:1551578:1551578 [0] NCCL INFO Bootstrap: Using ib0:10.128.24.237<0>
lrdn1203:1551578:1551578 [0] NCCL INFO cudaDriverVersion 12020
lrdn1203:1551578:1551578 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1203:1551578:1551578 [0] NCCL INFO Comm config Blocking set to 1
lrdn0905:2134816:2134816 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.69<0>
lrdn0905:2134816:2134816 [0] NCCL INFO cudaDriverVersion 12020
lrdn0905:2134816:2134816 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0905:2134816:2134816 [0] NCCL INFO Comm config Blocking set to 1
lrdn0896:1508101:1508101 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.33<0>
lrdn0896:1508101:1508101 [0] NCCL INFO cudaDriverVersion 12020
lrdn0896:1508101:1508101 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0896:1508101:1508101 [0] NCCL INFO Comm config Blocking set to 1
lrdn1043:3601459:3601564 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.22.109<0>
lrdn1043:3601459:3601564 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1043:3601459:3601564 [0] NCCL INFO Using network IB
lrdn1043:3601459:3601564 [0] NCCL INFO ncclCommInitRankConfig comm 0xf9b73b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5a45ca628a150e11 - Init START
lrdn1043:3601459:3601564 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1043:3601459:3601564 [0] NCCL INFO Bootstrap timings total 0.000373 (create 0.000018, send 0.000061, recv 0.000084, ring 0.000001, delay 0.000000)
lrdn1043:3601459:3601564 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1043:3601459:3601564 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1043:3601459:3601564 [0] NCCL INFO comm 0xf9b73b0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 00/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 01/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 02/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 03/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 04/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 05/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 06/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 07/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 08/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 09/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 10/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 11/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 12/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 13/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 14/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 15/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 16/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 17/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 18/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 19/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 20/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 21/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 22/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 23/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 24/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 25/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 26/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 27/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 28/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 29/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 30/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 31/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 32/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 33/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 34/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 35/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 36/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 37/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 38/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 39/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 40/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 41/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 42/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 43/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 44/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 45/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 46/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 47/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 48/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 49/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 50/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 51/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 52/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 53/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 54/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 55/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 56/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 57/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 58/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 59/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 60/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 61/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 62/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Channel 63/64 : 0
lrdn1043:3601459:3601564 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1043:3601459:3601564 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1043:3601459:3601564 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1043:3601459:3601570 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn1043:3601459:3601571 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1043:3601459:3601564 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1043:3601459:3601564 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0890:2121848:2121848 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.9<0>
lrdn0890:2121848:2121848 [0] NCCL INFO cudaDriverVersion 12020
lrdn0890:2121848:2121848 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0890:2121848:2121848 [0] NCCL INFO Comm config Blocking set to 1
lrdn1043:3601459:3601564 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1043:3601459:3601564 [0] NCCL INFO ncclCommInitRankConfig comm 0xf9b73b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5a45ca628a150e11 - Init COMPLETE
lrdn1043:3601459:3601564 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0147:1522198:1522294 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0147:1522198:1522294 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0147:1522198:1522294 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.8.109<0>
lrdn0147:1522198:1522294 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0147:1522198:1522294 [0] NCCL INFO Using network IB
lrdn0147:1522198:1522294 [0] NCCL INFO ncclCommInitRankConfig comm 0xd1b1a20 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe3f30bf52d515f14 - Init START
lrdn0147:1522198:1522294 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0147:1522198:1522294 [0] NCCL INFO Bootstrap timings total 0.000507 (create 0.000019, send 0.000061, recv 0.000218, ring 0.000001, delay 0.000000)
lrdn0147:1522198:1522294 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0147:1522198:1522294 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0147:1522198:1522294 [0] NCCL INFO comm 0xd1b1a20 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 00/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 01/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 02/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 03/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 04/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 05/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 06/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 07/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 08/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 09/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 10/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 11/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 12/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 13/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 14/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 15/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 16/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 17/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 18/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 19/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 20/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 21/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 22/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 23/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 24/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 25/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 26/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 27/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 28/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 29/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 30/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 31/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 32/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 33/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 34/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 35/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 36/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 37/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 38/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 39/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 40/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 41/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 42/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 43/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 44/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 45/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 46/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 47/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 48/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 49/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 50/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 51/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 52/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 53/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 54/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 55/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 56/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 57/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 58/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 59/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 60/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 61/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 62/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Channel 63/64 : 0
lrdn0147:1522198:1522294 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0147:1522198:1522294 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0147:1522198:1522294 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0147:1522198:1522300 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn0147:1522198:1522301 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn1203:1551578:1551685 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1203:1551578:1551685 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0905:2134816:2134913 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0905:2134816:2134913 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0147:1522198:1522294 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0147:1522198:1522294 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0147:1522198:1522294 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0147:1522198:1522294 [0] NCCL INFO ncclCommInitRankConfig comm 0xd1b1a20 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe3f30bf52d515f14 - Init COMPLETE
lrdn0147:1522198:1522294 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0896:1508101:1508197 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0896:1508101:1508197 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0905:2134816:2134913 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.69<0>
lrdn0905:2134816:2134913 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0905:2134816:2134913 [0] NCCL INFO Using network IB
lrdn1203:1551578:1551685 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.24.237<0>
lrdn0905:2134816:2134913 [0] NCCL INFO ncclCommInitRankConfig comm 0xd433080 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x57067077c87e9209 - Init START
lrdn0905:2134816:2134913 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0905:2134816:2134913 [0] NCCL INFO Bootstrap timings total 0.000374 (create 0.000018, send 0.000062, recv 0.000090, ring 0.000001, delay 0.000000)
lrdn1203:1551578:1551685 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1203:1551578:1551685 [0] NCCL INFO Using network IB
lrdn1203:1551578:1551685 [0] NCCL INFO ncclCommInitRankConfig comm 0xc733af0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x752969a8d006c653 - Init START
lrdn1203:1551578:1551685 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1203:1551578:1551685 [0] NCCL INFO Bootstrap timings total 0.000501 (create 0.000019, send 0.000063, recv 0.000209, ring 0.000001, delay 0.000000)
lrdn0404:1476320:1476320 [0] NCCL INFO Bootstrap: Using ib0:10.128.12.113<0>
lrdn0404:1476320:1476320 [0] NCCL INFO cudaDriverVersion 12020
lrdn0404:1476320:1476320 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0404:1476320:1476320 [0] NCCL INFO Comm config Blocking set to 1
lrdn0905:2134816:2134913 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0905:2134816:2134913 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0905:2134816:2134913 [0] NCCL INFO comm 0xd433080 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 00/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 01/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 02/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 03/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 04/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 05/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 06/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 07/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 08/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 09/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 10/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 11/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 12/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 13/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 14/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 15/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 16/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 17/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 18/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 19/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 20/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 21/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 22/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 23/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 24/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 25/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 26/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 27/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 28/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 29/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 30/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 31/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 32/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 33/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 34/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 35/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 36/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 37/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 38/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 39/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 40/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 41/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 42/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 43/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 44/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 45/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 46/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 47/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 48/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 49/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 50/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 51/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 52/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 53/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 54/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 55/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 56/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 57/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 58/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 59/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 60/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 61/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 62/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Channel 63/64 : 0
lrdn0905:2134816:2134913 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0905:2134816:2134913 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0905:2134816:2134913 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0905:2134816:2134920 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1
lrdn0905:2134816:2134919 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn0896:1508101:1508197 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.33<0>
lrdn1203:1551578:1551685 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1203:1551578:1551685 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1203:1551578:1551685 [0] NCCL INFO comm 0xc733af0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 00/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 01/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 02/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 03/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 04/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 05/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 06/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 07/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 08/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 09/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 10/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 11/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 12/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 13/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 14/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 15/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 16/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 17/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 18/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 19/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 20/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 21/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 22/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 23/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 24/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 25/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 26/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 27/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 28/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 29/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 30/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 31/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 32/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 33/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 34/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 35/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 36/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 37/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 38/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 39/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 40/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 41/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 42/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 43/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 44/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 45/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 46/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 47/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 48/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 49/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 50/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 51/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 52/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 53/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 54/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 55/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 56/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 57/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 58/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 59/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 60/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 61/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 62/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Channel 63/64 : 0
lrdn1203:1551578:1551685 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1203:1551578:1551685 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1203:1551578:1551685 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1203:1551578:1551692 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1203:1551578:1551691 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0890:2121848:2122025 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0890:2121848:2122025 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0896:1508101:1508197 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0896:1508101:1508197 [0] NCCL INFO Using network IB
lrdn0896:1508101:1508197 [0] NCCL INFO ncclCommInitRankConfig comm 0xde7dd20 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf164a7c402809b4b - Init START
lrdn0896:1508101:1508197 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0896:1508101:1508197 [0] NCCL INFO Bootstrap timings total 0.000378 (create 0.000018, send 0.000056, recv 0.000095, ring 0.000001, delay 0.000001)
lrdn0905:2134816:2134913 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0905:2134816:2134913 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1203:1551578:1551685 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1203:1551578:1551685 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0905:2134816:2134913 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0905:2134816:2134913 [0] NCCL INFO ncclCommInitRankConfig comm 0xd433080 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x57067077c87e9209 - Init COMPLETE
lrdn0905:2134816:2134913 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.22 (kernels 0.15, alloc 0.04, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1203:1551578:1551685 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1203:1551578:1551685 [0] NCCL INFO ncclCommInitRankConfig comm 0xc733af0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x752969a8d006c653 - Init COMPLETE
lrdn1203:1551578:1551685 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0896:1508101:1508197 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0896:1508101:1508197 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0896:1508101:1508197 [0] NCCL INFO comm 0xde7dd20 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 00/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 01/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 02/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 03/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 04/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 05/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 06/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 07/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 08/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 09/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 10/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 11/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 12/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 13/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 14/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 15/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 16/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 17/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 18/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 19/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 20/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 21/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 22/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 23/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 24/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 25/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 26/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 27/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 28/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 29/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 30/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 31/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 32/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 33/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 34/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 35/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 36/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 37/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 38/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 39/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 40/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 41/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 42/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 43/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 44/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 45/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 46/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 47/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 48/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 49/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 50/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 51/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 52/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 53/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 54/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 55/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 56/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 57/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 58/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 59/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 60/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 61/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 62/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Channel 63/64 : 0
lrdn0896:1508101:1508197 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0896:1508101:1508197 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0896:1508101:1508197 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0896:1508101:1508203 [0] NCCL INFO [Proxy Service] Device 0 CPU core 15
lrdn0896:1508101:1508204 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn0951:1448128:1448128 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.253<0>
lrdn0951:1448128:1448128 [0] NCCL INFO cudaDriverVersion 12020
lrdn0951:1448128:1448128 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0951:1448128:1448128 [0] NCCL INFO Comm config Blocking set to 1
lrdn0896:1508101:1508197 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0896:1508101:1508197 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0896:1508101:1508197 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0896:1508101:1508197 [0] NCCL INFO ncclCommInitRankConfig comm 0xde7dd20 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xf164a7c402809b4b - Init COMPLETE
lrdn0896:1508101:1508197 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0890:2121848:2122025 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.9<0>
lrdn0890:2121848:2122025 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0890:2121848:2122025 [0] NCCL INFO Using network IB
lrdn0890:2121848:2122025 [0] NCCL INFO ncclCommInitRankConfig comm 0xe5bff00 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x80924980d7ffa375 - Init START
lrdn0890:2121848:2122025 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0890:2121848:2122025 [0] NCCL INFO Bootstrap timings total 0.000403 (create 0.000030, send 0.000059, recv 0.000098, ring 0.000001, delay 0.000000)
lrdn1157:1572120:1572120 [0] NCCL INFO Bootstrap: Using ib0:10.128.24.53<0>
lrdn1157:1572120:1572120 [0] NCCL INFO cudaDriverVersion 12020
lrdn1157:1572120:1572120 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1157:1572120:1572120 [0] NCCL INFO Comm config Blocking set to 1
lrdn0890:2121848:2122025 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0890:2121848:2122025 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0890:2121848:2122025 [0] NCCL INFO comm 0xe5bff00 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 00/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 01/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 02/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 03/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 04/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 05/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 06/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 07/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 08/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 09/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 10/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 11/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 12/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 13/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 14/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 15/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 16/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 17/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 18/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 19/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 20/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 21/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 22/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 23/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 24/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 25/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 26/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 27/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 28/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 29/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 30/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 31/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 32/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 33/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 34/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 35/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 36/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 37/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 38/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 39/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 40/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 41/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 42/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 43/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 44/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 45/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 46/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 47/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 48/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 49/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 50/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 51/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 52/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 53/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 54/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 55/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 56/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 57/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 58/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 59/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 60/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 61/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 62/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Channel 63/64 : 0
lrdn0890:2121848:2122025 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0890:2121848:2122025 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0890:2121848:2122025 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0890:2121848:2122031 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0890:2121848:2122032 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn0416:1594081:1594081 [0] NCCL INFO Bootstrap: Using ib0:10.128.12.161<0>
lrdn0416:1594081:1594081 [0] NCCL INFO cudaDriverVersion 12020
lrdn0416:1594081:1594081 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0416:1594081:1594081 [0] NCCL INFO Comm config Blocking set to 1
lrdn0890:2121848:2122025 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0890:2121848:2122025 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0890:2121848:2122025 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0890:2121848:2122025 [0] NCCL INFO ncclCommInitRankConfig comm 0xe5bff00 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x80924980d7ffa375 - Init COMPLETE
lrdn0890:2121848:2122025 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1061:1436280:1436280 [0] NCCL INFO Bootstrap: Using ib0:10.128.22.181<0>
lrdn0179:2350380:2350380 [0] NCCL INFO Bootstrap: Using ib0:10.128.8.237<0>
lrdn1061:1436280:1436280 [0] NCCL INFO cudaDriverVersion 12020
lrdn1061:1436280:1436280 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0179:2350380:2350380 [0] NCCL INFO cudaDriverVersion 12020
lrdn0179:2350380:2350380 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1061:1436280:1436280 [0] NCCL INFO Comm config Blocking set to 1
lrdn0179:2350380:2350380 [0] NCCL INFO Comm config Blocking set to 1
lrdn0404:1476320:1476415 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0404:1476320:1476415 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0951:1448128:1448224 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0951:1448128:1448224 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0404:1476320:1476415 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.12.113<0>
lrdn0404:1476320:1476415 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0404:1476320:1476415 [0] NCCL INFO Using network IB
lrdn0404:1476320:1476415 [0] NCCL INFO ncclCommInitRankConfig comm 0xd919fa0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8a944b89e518177b - Init START
lrdn0404:1476320:1476415 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0404:1476320:1476415 [0] NCCL INFO Bootstrap timings total 0.000488 (create 0.000016, send 0.000061, recv 0.000204, ring 0.000001, delay 0.000000)
lrdn1157:1572120:1572216 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1157:1572120:1572216 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0404:1476320:1476415 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0404:1476320:1476415 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0404:1476320:1476415 [0] NCCL INFO comm 0xd919fa0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 00/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 01/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 02/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 03/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 04/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 05/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 06/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 07/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 08/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 09/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 10/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 11/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 12/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 13/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 14/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 15/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 16/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 17/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 18/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 19/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 20/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 21/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 22/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 23/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 24/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 25/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 26/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 27/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 28/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 29/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 30/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 31/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 32/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 33/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 34/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 35/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 36/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 37/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 38/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 39/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 40/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 41/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 42/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 43/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 44/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 45/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 46/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 47/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 48/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 49/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 50/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 51/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 52/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 53/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 54/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 55/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 56/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 57/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 58/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 59/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 60/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 61/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 62/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Channel 63/64 : 0
lrdn0404:1476320:1476415 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0404:1476320:1476415 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0404:1476320:1476415 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0404:1476320:1476421 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn0404:1476320:1476422 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn0416:1594081:1594258 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0416:1594081:1594258 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0404:1476320:1476415 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0404:1476320:1476415 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0951:1448128:1448224 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.253<0>
lrdn0951:1448128:1448224 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0951:1448128:1448224 [0] NCCL INFO Using network IB
lrdn0404:1476320:1476415 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0404:1476320:1476415 [0] NCCL INFO ncclCommInitRankConfig comm 0xd919fa0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8a944b89e518177b - Init COMPLETE
lrdn0404:1476320:1476415 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0951:1448128:1448224 [0] NCCL INFO ncclCommInitRankConfig comm 0xd7ef280 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x453e308f1b794ce5 - Init START
lrdn0951:1448128:1448224 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0951:1448128:1448224 [0] NCCL INFO Bootstrap timings total 0.000381 (create 0.000019, send 0.000058, recv 0.000100, ring 0.000001, delay 0.000000)
lrdn1172:1854740:1854740 [0] NCCL INFO Bootstrap: Using ib0:10.128.24.113<0>
lrdn1172:1854740:1854740 [0] NCCL INFO cudaDriverVersion 12020
lrdn1172:1854740:1854740 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1172:1854740:1854740 [0] NCCL INFO Comm config Blocking set to 1
lrdn0951:1448128:1448224 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0951:1448128:1448224 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0951:1448128:1448224 [0] NCCL INFO comm 0xd7ef280 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 00/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 01/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 02/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 03/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 04/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 05/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 06/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 07/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 08/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 09/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 10/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 11/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 12/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 13/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 14/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 15/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 16/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 17/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 18/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 19/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 20/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 21/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 22/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 23/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 24/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 25/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 26/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 27/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 28/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 29/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 30/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 31/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 32/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 33/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 34/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 35/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 36/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 37/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 38/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 39/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 40/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 41/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 42/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 43/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 44/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 45/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 46/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 47/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 48/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 49/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 50/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 51/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 52/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 53/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 54/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 55/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 56/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 57/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 58/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 59/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 60/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 61/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 62/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Channel 63/64 : 0
lrdn0951:1448128:1448224 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0951:1448128:1448224 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0951:1448128:1448224 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0951:1448128:1448231 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 14
lrdn0951:1448128:1448230 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn1061:1436280:1436375 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1061:1436280:1436375 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0179:2350380:2350477 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0179:2350380:2350477 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1157:1572120:1572216 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.24.53<0>
lrdn1157:1572120:1572216 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1157:1572120:1572216 [0] NCCL INFO Using network IB
lrdn0951:1448128:1448224 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0951:1448128:1448224 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1157:1572120:1572216 [0] NCCL INFO ncclCommInitRankConfig comm 0x2f34cfc0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe4e6dbd256fc7c98 - Init START
lrdn1157:1572120:1572216 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1157:1572120:1572216 [0] NCCL INFO Bootstrap timings total 0.000464 (create 0.000016, send 0.000057, recv 0.000180, ring 0.000001, delay 0.000000)
lrdn0416:1594081:1594258 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.12.161<0>
lrdn0951:1448128:1448224 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0951:1448128:1448224 [0] NCCL INFO ncclCommInitRankConfig comm 0xd7ef280 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x453e308f1b794ce5 - Init COMPLETE
lrdn0951:1448128:1448224 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0416:1594081:1594258 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0416:1594081:1594258 [0] NCCL INFO Using network IB
lrdn0416:1594081:1594258 [0] NCCL INFO ncclCommInitRankConfig comm 0xf9118e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x26c2a2d3d282de20 - Init START
lrdn0416:1594081:1594258 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0416:1594081:1594258 [0] NCCL INFO Bootstrap timings total 0.000373 (create 0.000016, send 0.000059, recv 0.000091, ring 0.000001, delay 0.000000)
lrdn1157:1572120:1572216 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1157:1572120:1572216 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1157:1572120:1572216 [0] NCCL INFO comm 0x2f34cfc0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 00/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 01/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 02/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 03/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 04/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 05/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 06/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 07/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 08/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 09/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 10/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 11/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 12/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 13/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 14/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 15/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 16/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 17/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 18/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 19/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 20/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 21/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 22/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 23/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 24/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 25/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 26/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 27/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 28/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 29/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 30/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 31/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 32/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 33/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 34/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 35/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 36/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 37/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 38/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 39/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 40/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 41/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 42/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 43/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 44/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 45/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 46/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 47/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 48/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 49/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 50/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 51/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 52/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 53/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 54/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 55/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 56/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 57/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 58/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 59/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 60/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 61/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 62/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Channel 63/64 : 0
lrdn1157:1572120:1572216 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1157:1572120:1572216 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1157:1572120:1572216 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1157:1572120:1572223 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn1157:1572120:1572222 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0416:1594081:1594258 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0416:1594081:1594258 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0416:1594081:1594258 [0] NCCL INFO comm 0xf9118e0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 00/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 01/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 02/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 03/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 04/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 05/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 06/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 07/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 08/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 09/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 10/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 11/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 12/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 13/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 14/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 15/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 16/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 17/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 18/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 19/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 20/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 21/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 22/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 23/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 24/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 25/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 26/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 27/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 28/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 29/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 30/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 31/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 32/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 33/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 34/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 35/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 36/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 37/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 38/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 39/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 40/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 41/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 42/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 43/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 44/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 45/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 46/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 47/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 48/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 49/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 50/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 51/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 52/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 53/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 54/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 55/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 56/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 57/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 58/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 59/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 60/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 61/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 62/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Channel 63/64 : 0
lrdn0416:1594081:1594258 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0416:1594081:1594258 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0416:1594081:1594258 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0416:1594081:1594265 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 12
lrdn0416:1594081:1594264 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn1061:1436280:1436375 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.22.181<0>
lrdn1157:1572120:1572216 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1157:1572120:1572216 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1061:1436280:1436375 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1061:1436280:1436375 [0] NCCL INFO Using network IB
lrdn1157:1572120:1572216 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1157:1572120:1572216 [0] NCCL INFO ncclCommInitRankConfig comm 0x2f34cfc0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe4e6dbd256fc7c98 - Init COMPLETE
lrdn1157:1572120:1572216 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.02, rest 0.00)
lrdn1061:1436280:1436375 [0] NCCL INFO ncclCommInitRankConfig comm 0xe0a2da0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb7d413b5440d8031 - Init START
lrdn1061:1436280:1436375 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1061:1436280:1436375 [0] NCCL INFO Bootstrap timings total 0.000365 (create 0.000015, send 0.000051, recv 0.000100, ring 0.000001, delay 0.000000)
lrdn0416:1594081:1594258 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0416:1594081:1594258 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0179:2350380:2350477 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.8.237<0>
lrdn0416:1594081:1594258 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0416:1594081:1594258 [0] NCCL INFO ncclCommInitRankConfig comm 0xf9118e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x26c2a2d3d282de20 - Init COMPLETE
lrdn0416:1594081:1594258 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0179:2350380:2350477 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0179:2350380:2350477 [0] NCCL INFO Using network IB
lrdn0179:2350380:2350477 [0] NCCL INFO ncclCommInitRankConfig comm 0xd242960 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x573683d33e17de9f - Init START
lrdn0179:2350380:2350477 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0179:2350380:2350477 [0] NCCL INFO Bootstrap timings total 0.000370 (create 0.000018, send 0.000058, recv 0.000092, ring 0.000001, delay 0.000000)
lrdn1061:1436280:1436375 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1061:1436280:1436375 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1061:1436280:1436375 [0] NCCL INFO comm 0xe0a2da0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 00/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 01/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 02/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 03/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 04/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 05/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 06/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 07/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 08/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 09/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 10/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 11/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 12/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 13/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 14/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 15/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 16/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 17/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 18/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 19/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 20/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 21/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 22/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 23/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 24/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 25/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 26/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 27/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 28/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 29/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 30/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 31/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 32/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 33/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 34/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 35/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 36/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 37/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 38/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 39/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 40/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 41/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 42/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 43/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 44/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 45/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 46/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 47/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 48/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 49/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 50/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 51/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 52/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 53/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 54/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 55/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 56/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 57/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 58/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 59/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 60/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 61/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 62/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Channel 63/64 : 0
lrdn1061:1436280:1436375 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1061:1436280:1436375 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1061:1436280:1436375 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1061:1436280:1436381 [0] NCCL INFO [Proxy Service] Device 0 CPU core 13
lrdn1061:1436280:1436382 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0179:2350380:2350477 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0179:2350380:2350477 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0179:2350380:2350477 [0] NCCL INFO comm 0xd242960 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 00/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 01/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 02/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 03/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 04/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 05/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 06/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 07/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 08/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 09/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 10/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 11/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 12/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 13/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 14/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 15/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 16/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 17/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 18/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 19/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 20/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 21/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 22/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 23/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 24/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 25/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 26/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 27/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 28/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 29/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 30/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 31/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 32/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 33/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 34/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 35/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 36/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 37/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 38/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 39/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 40/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 41/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 42/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 43/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 44/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 45/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 46/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 47/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 48/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 49/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 50/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 51/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 52/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 53/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 54/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 55/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 56/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 57/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 58/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 59/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 60/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 61/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 62/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Channel 63/64 : 0
lrdn0179:2350380:2350477 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0179:2350380:2350477 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0179:2350380:2350477 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0179:2350380:2350484 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1
lrdn0179:2350380:2350483 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn1061:1436280:1436375 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1061:1436280:1436375 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1061:1436280:1436375 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1061:1436280:1436375 [0] NCCL INFO ncclCommInitRankConfig comm 0xe0a2da0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb7d413b5440d8031 - Init COMPLETE
lrdn1061:1436280:1436375 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0179:2350380:2350477 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0179:2350380:2350477 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0179:2350380:2350477 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0179:2350380:2350477 [0] NCCL INFO ncclCommInitRankConfig comm 0xd242960 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x573683d33e17de9f - Init COMPLETE
lrdn0179:2350380:2350477 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0648:1396499:1396499 [0] NCCL INFO Bootstrap: Using ib0:10.128.16.65<0>
lrdn0648:1396499:1396499 [0] NCCL INFO cudaDriverVersion 12020
lrdn0648:1396499:1396499 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0648:1396499:1396499 [0] NCCL INFO Comm config Blocking set to 1
lrdn1172:1854740:1854846 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1172:1854740:1854846 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1172:1854740:1854846 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.24.113<0>
lrdn1172:1854740:1854846 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1172:1854740:1854846 [0] NCCL INFO Using network IB
lrdn1172:1854740:1854846 [0] NCCL INFO ncclCommInitRankConfig comm 0xd48b0f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb1853d3750ce6b26 - Init START
lrdn1172:1854740:1854846 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1172:1854740:1854846 [0] NCCL INFO Bootstrap timings total 0.000396 (create 0.000019, send 0.000072, recv 0.000095, ring 0.000001, delay 0.000000)
lrdn1172:1854740:1854846 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1172:1854740:1854846 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1172:1854740:1854846 [0] NCCL INFO comm 0xd48b0f0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 00/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 01/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 02/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 03/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 04/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 05/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 06/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 07/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 08/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 09/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 10/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 11/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 12/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 13/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 14/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 15/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 16/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 17/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 18/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 19/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 20/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 21/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 22/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 23/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 24/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 25/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 26/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 27/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 28/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 29/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 30/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 31/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 32/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 33/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 34/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 35/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 36/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 37/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 38/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 39/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 40/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 41/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 42/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 43/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 44/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 45/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 46/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 47/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 48/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 49/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 50/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 51/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 52/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 53/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 54/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 55/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 56/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 57/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 58/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 59/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 60/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 61/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 62/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Channel 63/64 : 0
lrdn1172:1854740:1854846 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1172:1854740:1854846 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1172:1854740:1854846 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1172:1854740:1854853 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1172:1854740:1854852 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1172:1854740:1854846 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1172:1854740:1854846 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1172:1854740:1854846 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1172:1854740:1854846 [0] NCCL INFO ncclCommInitRankConfig comm 0xd48b0f0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb1853d3750ce6b26 - Init COMPLETE
lrdn1172:1854740:1854846 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.04, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn0852:1613610:1613610 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.113<0>
lrdn0852:1613610:1613610 [0] NCCL INFO cudaDriverVersion 12020
lrdn0852:1613610:1613610 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0852:1613610:1613610 [0] NCCL INFO Comm config Blocking set to 1
lrdn0360:1607343:1607343 [0] NCCL INFO Bootstrap: Using ib0:10.128.11.193<0>
lrdn0360:1607343:1607343 [0] NCCL INFO cudaDriverVersion 12020
lrdn0360:1607343:1607343 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0360:1607343:1607343 [0] NCCL INFO Comm config Blocking set to 1
lrdn0663:2614220:2614220 [0] NCCL INFO Bootstrap: Using ib0:10.128.16.125<0>
lrdn0663:2614220:2614220 [0] NCCL INFO cudaDriverVersion 12020
lrdn0663:2614220:2614220 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0663:2614220:2614220 [0] NCCL INFO Comm config Blocking set to 1
lrdn1419:2150084:2150084 [0] NCCL INFO Bootstrap: Using ib0:10.128.28.77<0>
lrdn1419:2150084:2150084 [0] NCCL INFO cudaDriverVersion 12020
lrdn1419:2150084:2150084 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1419:2150084:2150084 [0] NCCL INFO Comm config Blocking set to 1
lrdn0648:1396499:1396605 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0648:1396499:1396605 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0648:1396499:1396605 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.16.65<0>
lrdn0648:1396499:1396605 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0648:1396499:1396605 [0] NCCL INFO Using network IB
lrdn0648:1396499:1396605 [0] NCCL INFO ncclCommInitRankConfig comm 0xe2dab70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xccc4eea0267eb1c - Init START
lrdn0648:1396499:1396605 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0648:1396499:1396605 [0] NCCL INFO Bootstrap timings total 0.000471 (create 0.000016, send 0.000061, recv 0.000186, ring 0.000001, delay 0.000000)
lrdn0648:1396499:1396605 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0648:1396499:1396605 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0648:1396499:1396605 [0] NCCL INFO comm 0xe2dab70 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 00/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 01/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 02/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 03/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 04/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 05/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 06/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 07/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 08/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 09/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 10/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 11/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 12/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 13/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 14/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 15/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 16/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 17/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 18/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 19/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 20/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 21/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 22/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 23/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 24/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 25/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 26/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 27/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 28/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 29/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 30/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 31/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 32/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 33/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 34/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 35/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 36/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 37/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 38/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 39/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 40/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 41/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 42/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 43/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 44/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 45/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 46/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 47/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 48/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 49/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 50/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 51/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 52/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 53/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 54/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 55/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 56/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 57/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 58/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 59/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 60/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 61/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 62/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Channel 63/64 : 0
lrdn0648:1396499:1396605 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0648:1396499:1396605 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0648:1396499:1396605 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0648:1396499:1396612 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 15
lrdn0648:1396499:1396611 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn0648:1396499:1396605 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0648:1396499:1396605 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0648:1396499:1396605 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0648:1396499:1396605 [0] NCCL INFO ncclCommInitRankConfig comm 0xe2dab70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xccc4eea0267eb1c - Init COMPLETE
lrdn0648:1396499:1396605 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0852:1613610:1613707 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0852:1613610:1613707 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0360:1607343:1607440 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0360:1607343:1607440 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0852:1613610:1613707 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.113<0>
lrdn0663:2614220:2614395 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0663:2614220:2614395 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0852:1613610:1613707 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0852:1613610:1613707 [0] NCCL INFO Using network IB
lrdn0852:1613610:1613707 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc5c5b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x618eacbf9dc4e4a3 - Init START
lrdn0852:1613610:1613707 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0852:1613610:1613707 [0] NCCL INFO Bootstrap timings total 0.000391 (create 0.000019, send 0.000055, recv 0.000113, ring 0.000001, delay 0.000001)
lrdn1419:2150084:2150196 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1419:2150084:2150196 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0864:1633624:1633624 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.161<0>
lrdn0852:1613610:1613707 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0852:1613610:1613707 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0852:1613610:1613707 [0] NCCL INFO comm 0xdc5c5b0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 00/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 01/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 02/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 03/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 04/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 05/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 06/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 07/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 08/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 09/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 10/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 11/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 12/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 13/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 14/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 15/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 16/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 17/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 18/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 19/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 20/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 21/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 22/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 23/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 24/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 25/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 26/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 27/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 28/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 29/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 30/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 31/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 32/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 33/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 34/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 35/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 36/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 37/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 38/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 39/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 40/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 41/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 42/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 43/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 44/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 45/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 46/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 47/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 48/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 49/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 50/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 51/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 52/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 53/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 54/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 55/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 56/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 57/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 58/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 59/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 60/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 61/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 62/64 : 0
lrdn0852:1613610:1613707 [0] NCCL INFO Channel 63/64 : 0
lrdn0864:1633624:1633624 [0] NCCL INFO cudaDriverVersion 12020
lrdn0852:1613610:1613707 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0852:1613610:1613707 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0852:1613610:1613707 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0864:1633624:1633624 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0852:1613610:1613714 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn0852:1613610:1613713 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn0864:1633624:1633624 [0] NCCL INFO Comm config Blocking set to 1
lrdn0360:1607343:1607440 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.11.193<0>
lrdn0852:1613610:1613707 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0852:1613610:1613707 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0360:1607343:1607440 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0360:1607343:1607440 [0] NCCL INFO Using network IB
lrdn0360:1607343:1607440 [0] NCCL INFO ncclCommInitRankConfig comm 0xce2ef70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x4ced5f27d871fb7b - Init START
lrdn0852:1613610:1613707 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0852:1613610:1613707 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc5c5b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x618eacbf9dc4e4a3 - Init COMPLETE
lrdn0852:1613610:1613707 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0360:1607343:1607440 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0360:1607343:1607440 [0] NCCL INFO Bootstrap timings total 0.000374 (create 0.000017, send 0.000057, recv 0.000100, ring 0.000001, delay 0.000000)
lrdn0663:2614220:2614395 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.16.125<0>
lrdn0663:2614220:2614395 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0663:2614220:2614395 [0] NCCL INFO Using network IB
lrdn0663:2614220:2614395 [0] NCCL INFO ncclCommInitRankConfig comm 0xecd6cd0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x568a71b1d991c213 - Init START
lrdn0663:2614220:2614395 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0663:2614220:2614395 [0] NCCL INFO Bootstrap timings total 0.000501 (create 0.000016, send 0.000059, recv 0.000218, ring 0.000001, delay 0.000000)
lrdn0360:1607343:1607440 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0360:1607343:1607440 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0360:1607343:1607440 [0] NCCL INFO comm 0xce2ef70 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 00/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 01/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 02/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 03/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 04/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 05/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 06/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 07/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 08/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 09/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 10/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 11/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 12/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 13/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 14/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 15/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 16/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 17/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 18/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 19/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 20/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 21/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 22/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 23/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 24/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 25/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 26/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 27/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 28/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 29/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 30/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 31/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 32/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 33/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 34/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 35/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 36/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 37/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 38/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 39/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 40/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 41/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 42/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 43/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 44/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 45/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 46/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 47/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 48/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 49/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 50/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 51/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 52/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 53/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 54/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 55/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 56/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 57/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 58/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 59/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 60/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 61/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 62/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Channel 63/64 : 0
lrdn0360:1607343:1607440 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0360:1607343:1607440 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0360:1607343:1607440 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0360:1607343:1607446 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5
lrdn0360:1607343:1607447 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
lrdn1419:2150084:2150196 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.28.77<0>
lrdn0663:2614220:2614395 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0663:2614220:2614395 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0663:2614220:2614395 [0] NCCL INFO comm 0xecd6cd0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 00/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 01/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 02/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 03/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 04/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 05/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 06/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 07/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 08/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 09/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 10/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 11/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 12/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 13/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 14/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 15/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 16/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 17/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 18/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 19/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 20/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 21/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 22/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 23/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 24/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 25/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 26/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 27/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 28/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 29/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 30/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 31/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 32/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 33/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 34/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 35/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 36/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 37/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 38/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 39/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 40/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 41/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 42/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 43/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 44/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 45/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 46/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 47/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 48/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 49/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 50/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 51/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 52/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 53/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 54/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 55/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 56/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 57/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 58/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 59/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 60/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 61/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 62/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Channel 63/64 : 0
lrdn0663:2614220:2614395 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0663:2614220:2614395 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0663:2614220:2614395 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0663:2614220:2614402 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 1
lrdn0663:2614220:2614401 [0] NCCL INFO [Proxy Service] Device 0 CPU core 0
lrdn1419:2150084:2150196 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1419:2150084:2150196 [0] NCCL INFO Using network IB
lrdn0360:1607343:1607440 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0360:1607343:1607440 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1419:2150084:2150196 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc5b810 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc3002f479d223261 - Init START
lrdn1419:2150084:2150196 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1419:2150084:2150196 [0] NCCL INFO Bootstrap timings total 0.000390 (create 0.000015, send 0.000055, recv 0.000106, ring 0.000001, delay 0.000001)
lrdn1389:1876419:1876419 [0] NCCL INFO Bootstrap: Using ib0:10.128.27.213<0>
lrdn0360:1607343:1607440 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0360:1607343:1607440 [0] NCCL INFO ncclCommInitRankConfig comm 0xce2ef70 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x4ced5f27d871fb7b - Init COMPLETE
lrdn0360:1607343:1607440 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1389:1876419:1876419 [0] NCCL INFO cudaDriverVersion 12020
lrdn1389:1876419:1876419 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1389:1876419:1876419 [0] NCCL INFO Comm config Blocking set to 1
lrdn0663:2614220:2614395 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0663:2614220:2614395 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1419:2150084:2150196 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1419:2150084:2150196 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1419:2150084:2150196 [0] NCCL INFO comm 0xdc5b810 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 00/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 01/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 02/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 03/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 04/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 05/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 06/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 07/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 08/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 09/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 10/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 11/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 12/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 13/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 14/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 15/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 16/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 17/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 18/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 19/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 20/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 21/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 22/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 23/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 24/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 25/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 26/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 27/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 28/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 29/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 30/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 31/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 32/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 33/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 34/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 35/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 36/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 37/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 38/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 39/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 40/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 41/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 42/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 43/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 44/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 45/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 46/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 47/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 48/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 49/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 50/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 51/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 52/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 53/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 54/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 55/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 56/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 57/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 58/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 59/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 60/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 61/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 62/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Channel 63/64 : 0
lrdn1419:2150084:2150196 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1419:2150084:2150196 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1419:2150084:2150196 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1419:2150084:2150203 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 2
lrdn1419:2150084:2150202 [0] NCCL INFO [Proxy Service] Device 0 CPU core 1
lrdn0663:2614220:2614395 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0663:2614220:2614395 [0] NCCL INFO ncclCommInitRankConfig comm 0xecd6cd0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x568a71b1d991c213 - Init COMPLETE
lrdn0663:2614220:2614395 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.02, rest 0.00)
lrdn1419:2150084:2150196 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1419:2150084:2150196 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1419:2150084:2150196 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1419:2150084:2150196 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc5b810 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xc3002f479d223261 - Init COMPLETE
lrdn1419:2150084:2150196 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn0864:1633624:1633720 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0864:1633624:1633720 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0389:1433363:1433363 [0] NCCL INFO Bootstrap: Using ib0:10.128.12.53<0>
lrdn0389:1433363:1433363 [0] NCCL INFO cudaDriverVersion 12020
lrdn0389:1433363:1433363 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0389:1433363:1433363 [0] NCCL INFO Comm config Blocking set to 1
lrdn0618:1830054:1830054 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.201<0>
lrdn0618:1830054:1830054 [0] NCCL INFO cudaDriverVersion 12020
lrdn0618:1830054:1830054 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0618:1830054:1830054 [0] NCCL INFO Comm config Blocking set to 1
lrdn0864:1633624:1633720 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.161<0>
lrdn0864:1633624:1633720 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0864:1633624:1633720 [0] NCCL INFO Using network IB
lrdn0864:1633624:1633720 [0] NCCL INFO ncclCommInitRankConfig comm 0xda90da0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x87733fac18f946e9 - Init START
lrdn0864:1633624:1633720 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0864:1633624:1633720 [0] NCCL INFO Bootstrap timings total 0.000373 (create 0.000015, send 0.000055, recv 0.000096, ring 0.000001, delay 0.000001)
lrdn1389:1876419:1876525 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1389:1876419:1876525 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0864:1633624:1633720 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0864:1633624:1633720 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0864:1633624:1633720 [0] NCCL INFO comm 0xda90da0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 00/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 01/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 02/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 03/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 04/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 05/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 06/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 07/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 08/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 09/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 10/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 11/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 12/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 13/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 14/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 15/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 16/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 17/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 18/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 19/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 20/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 21/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 22/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 23/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 24/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 25/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 26/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 27/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 28/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 29/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 30/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 31/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 32/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 33/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 34/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 35/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 36/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 37/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 38/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 39/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 40/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 41/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 42/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 43/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 44/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 45/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 46/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 47/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 48/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 49/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 50/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 51/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 52/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 53/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 54/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 55/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 56/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 57/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 58/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 59/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 60/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 61/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 62/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Channel 63/64 : 0
lrdn0864:1633624:1633720 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0864:1633624:1633720 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0864:1633624:1633720 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0864:1633624:1633726 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn0864:1633624:1633727 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0864:1633624:1633720 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0864:1633624:1633720 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0864:1633624:1633720 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0864:1633624:1633720 [0] NCCL INFO ncclCommInitRankConfig comm 0xda90da0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x87733fac18f946e9 - Init COMPLETE
lrdn0864:1633624:1633720 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1389:1876419:1876525 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.27.213<0>
lrdn1389:1876419:1876525 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1389:1876419:1876525 [0] NCCL INFO Using network IB
lrdn1389:1876419:1876525 [0] NCCL INFO ncclCommInitRankConfig comm 0xece9df0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x21b8936865b633ab - Init START
lrdn1389:1876419:1876525 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1389:1876419:1876525 [0] NCCL INFO Bootstrap timings total 0.000380 (create 0.000019, send 0.000061, recv 0.000092, ring 0.000001, delay 0.000000)
lrdn1389:1876419:1876525 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1389:1876419:1876525 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1389:1876419:1876525 [0] NCCL INFO comm 0xece9df0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 00/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 01/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 02/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 03/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 04/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 05/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 06/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 07/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 08/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 09/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 10/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 11/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 12/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 13/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 14/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 15/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 16/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 17/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 18/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 19/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 20/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 21/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 22/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 23/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 24/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 25/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 26/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 27/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 28/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 29/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 30/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 31/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 32/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 33/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 34/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 35/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 36/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 37/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 38/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 39/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 40/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 41/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 42/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 43/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 44/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 45/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 46/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 47/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 48/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 49/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 50/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 51/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 52/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 53/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 54/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 55/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 56/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 57/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 58/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 59/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 60/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 61/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 62/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Channel 63/64 : 0
lrdn1389:1876419:1876525 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1389:1876419:1876525 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1389:1876419:1876525 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1389:1876419:1876532 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 10
lrdn1389:1876419:1876531 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn1389:1876419:1876525 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1389:1876419:1876525 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1389:1876419:1876525 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1389:1876419:1876525 [0] NCCL INFO ncclCommInitRankConfig comm 0xece9df0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x21b8936865b633ab - Init COMPLETE
lrdn1389:1876419:1876525 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0389:1433363:1433459 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0389:1433363:1433459 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0618:1830054:1830160 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0618:1830054:1830160 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0389:1433363:1433459 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.12.53<0>
lrdn0389:1433363:1433459 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0389:1433363:1433459 [0] NCCL INFO Using network IB
lrdn0389:1433363:1433459 [0] NCCL INFO ncclCommInitRankConfig comm 0xec89180 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x153a69529e07d4a8 - Init START
lrdn0389:1433363:1433459 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0389:1433363:1433459 [0] NCCL INFO Bootstrap timings total 0.000386 (create 0.000018, send 0.000063, recv 0.000096, ring 0.000001, delay 0.000000)
lrdn0389:1433363:1433459 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0389:1433363:1433459 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0389:1433363:1433459 [0] NCCL INFO comm 0xec89180 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 00/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 01/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 02/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 03/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 04/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 05/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 06/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 07/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 08/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 09/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 10/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 11/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 12/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 13/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 14/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 15/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 16/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 17/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 18/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 19/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 20/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 21/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 22/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 23/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 24/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 25/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 26/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 27/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 28/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 29/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 30/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 31/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 32/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 33/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 34/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 35/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 36/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 37/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 38/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 39/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 40/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 41/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 42/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 43/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 44/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 45/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 46/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 47/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 48/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 49/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 50/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 51/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 52/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 53/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 54/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 55/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 56/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 57/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 58/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 59/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 60/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 61/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 62/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Channel 63/64 : 0
lrdn0389:1433363:1433459 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0389:1433363:1433459 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0389:1433363:1433459 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0389:1433363:1433465 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn0389:1433363:1433466 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 15
lrdn0618:1830054:1830160 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.201<0>
lrdn0618:1830054:1830160 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0618:1830054:1830160 [0] NCCL INFO Using network IB
lrdn0389:1433363:1433459 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0389:1433363:1433459 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0618:1830054:1830160 [0] NCCL INFO ncclCommInitRankConfig comm 0xc6930a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9536991ccf578f68 - Init START
lrdn0618:1830054:1830160 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0618:1830054:1830160 [0] NCCL INFO Bootstrap timings total 0.000373 (create 0.000015, send 0.000062, recv 0.000084, ring 0.000001, delay 0.000000)
lrdn0389:1433363:1433459 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0389:1433363:1433459 [0] NCCL INFO ncclCommInitRankConfig comm 0xec89180 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x153a69529e07d4a8 - Init COMPLETE
lrdn0389:1433363:1433459 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0618:1830054:1830160 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0618:1830054:1830160 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0618:1830054:1830160 [0] NCCL INFO comm 0xc6930a0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 00/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 01/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 02/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 03/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 04/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 05/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 06/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 07/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 08/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 09/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 10/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 11/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 12/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 13/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 14/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 15/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 16/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 17/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 18/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 19/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 20/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 21/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 22/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 23/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 24/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 25/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 26/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 27/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 28/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 29/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 30/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 31/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 32/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 33/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 34/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 35/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 36/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 37/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 38/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 39/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 40/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 41/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 42/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 43/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 44/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 45/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 46/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 47/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 48/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 49/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 50/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 51/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 52/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 53/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 54/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 55/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 56/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 57/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 58/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 59/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 60/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 61/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 62/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Channel 63/64 : 0
lrdn0618:1830054:1830160 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0618:1830054:1830160 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0618:1830054:1830160 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0618:1830054:1830166 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn0618:1830054:1830167 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 15
lrdn0680:1633887:1633887 [0] NCCL INFO Bootstrap: Using ib0:10.128.16.193<0>
lrdn0680:1633887:1633887 [0] NCCL INFO cudaDriverVersion 12020
lrdn0680:1633887:1633887 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0680:1633887:1633887 [0] NCCL INFO Comm config Blocking set to 1
lrdn0618:1830054:1830160 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0618:1830054:1830160 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0618:1830054:1830160 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0618:1830054:1830160 [0] NCCL INFO ncclCommInitRankConfig comm 0xc6930a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x9536991ccf578f68 - Init COMPLETE
lrdn0618:1830054:1830160 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1145:1701595:1701595 [0] NCCL INFO Bootstrap: Using ib0:10.128.24.5<0>
lrdn1145:1701595:1701595 [0] NCCL INFO cudaDriverVersion 12020
lrdn1145:1701595:1701595 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1145:1701595:1701595 [0] NCCL INFO Comm config Blocking set to 1
lrdn0680:1633887:1633996 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0680:1633887:1633996 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0680:1633887:1633996 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.16.193<0>
lrdn0680:1633887:1633996 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0680:1633887:1633996 [0] NCCL INFO Using network IB
lrdn0680:1633887:1633996 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc763a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5c3e3de1a77fc779 - Init START
lrdn0680:1633887:1633996 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0680:1633887:1633996 [0] NCCL INFO Bootstrap timings total 0.000379 (create 0.000016, send 0.000056, recv 0.000095, ring 0.000006, delay 0.000000)
lrdn0680:1633887:1633996 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0680:1633887:1633996 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0680:1633887:1633996 [0] NCCL INFO comm 0xdc763a0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 00/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 01/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 02/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 03/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 04/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 05/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 06/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 07/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 08/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 09/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 10/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 11/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 12/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 13/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 14/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 15/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 16/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 17/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 18/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 19/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 20/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 21/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 22/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 23/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 24/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 25/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 26/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 27/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 28/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 29/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 30/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 31/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 32/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 33/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 34/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 35/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 36/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 37/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 38/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 39/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 40/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 41/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 42/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 43/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 44/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 45/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 46/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 47/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 48/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 49/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 50/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 51/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 52/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 53/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 54/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 55/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 56/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 57/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 58/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 59/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 60/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 61/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 62/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Channel 63/64 : 0
lrdn0680:1633887:1633996 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0680:1633887:1633996 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0680:1633887:1633996 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0680:1633887:1634002 [0] NCCL INFO [Proxy Service] Device 0 CPU core 14
lrdn0680:1633887:1634003 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 12
lrdn1145:1701595:1701702 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1145:1701595:1701702 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0680:1633887:1633996 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0680:1633887:1633996 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0680:1633887:1633996 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0680:1633887:1633996 [0] NCCL INFO ncclCommInitRankConfig comm 0xdc763a0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x5c3e3de1a77fc779 - Init COMPLETE
lrdn0680:1633887:1633996 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.25 (kernels 0.15, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1145:1701595:1701702 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.24.5<0>
lrdn1145:1701595:1701702 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1145:1701595:1701702 [0] NCCL INFO Using network IB
lrdn1145:1701595:1701702 [0] NCCL INFO ncclCommInitRankConfig comm 0xda2a6c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x24c3ff81ce0a9297 - Init START
lrdn1145:1701595:1701702 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1145:1701595:1701702 [0] NCCL INFO Bootstrap timings total 0.000384 (create 0.000017, send 0.000059, recv 0.000102, ring 0.000001, delay 0.000001)
lrdn1145:1701595:1701702 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1145:1701595:1701702 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1145:1701595:1701702 [0] NCCL INFO comm 0xda2a6c0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 00/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 01/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 02/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 03/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 04/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 05/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 06/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 07/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 08/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 09/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 10/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 11/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 12/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 13/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 14/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 15/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 16/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 17/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 18/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 19/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 20/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 21/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 22/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 23/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 24/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 25/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 26/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 27/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 28/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 29/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 30/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 31/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 32/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 33/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 34/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 35/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 36/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 37/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 38/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 39/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 40/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 41/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 42/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 43/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 44/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 45/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 46/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 47/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 48/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 49/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 50/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 51/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 52/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 53/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 54/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 55/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 56/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 57/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 58/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 59/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 60/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 61/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 62/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Channel 63/64 : 0
lrdn1145:1701595:1701702 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1145:1701595:1701702 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1145:1701595:1701702 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1145:1701595:1701708 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn1145:1701595:1701709 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn1145:1701595:1701702 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1145:1701595:1701702 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1145:1701595:1701702 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1145:1701595:1701702 [0] NCCL INFO ncclCommInitRankConfig comm 0xda2a6c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x24c3ff81ce0a9297 - Init COMPLETE
lrdn1145:1701595:1701702 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0627:2297582:2297582 [0] NCCL INFO Bootstrap: Using ib0:10.128.15.237<0>
lrdn0627:2297582:2297582 [0] NCCL INFO cudaDriverVersion 12020
lrdn0627:2297582:2297582 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0627:2297582:2297582 [0] NCCL INFO Comm config Blocking set to 1
lrdn1083:3702380:3702380 [0] NCCL INFO Bootstrap: Using ib0:10.128.23.13<0>
lrdn1083:3702380:3702380 [0] NCCL INFO cudaDriverVersion 12020
lrdn1083:3702380:3702380 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1083:3702380:3702380 [0] NCCL INFO Comm config Blocking set to 1
lrdn1179:3818673:3818673 [0] NCCL INFO Bootstrap: Using ib0:10.128.24.141<0>
lrdn1179:3818673:3818673 [0] NCCL INFO cudaDriverVersion 12020
lrdn1179:3818673:3818673 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1179:3818673:3818673 [0] NCCL INFO Comm config Blocking set to 1
lrdn1413:1703007:1703007 [0] NCCL INFO Bootstrap: Using ib0:10.128.28.53<0>
lrdn1413:1703007:1703007 [0] NCCL INFO cudaDriverVersion 12020
lrdn1413:1703007:1703007 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1413:1703007:1703007 [0] NCCL INFO Comm config Blocking set to 1
lrdn0627:2297582:2297771 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0627:2297582:2297771 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0627:2297582:2297771 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.15.237<0>
lrdn1083:3702380:3702488 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1083:3702380:3702488 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0627:2297582:2297771 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0627:2297582:2297771 [0] NCCL INFO Using network IB
lrdn0627:2297582:2297771 [0] NCCL INFO ncclCommInitRankConfig comm 0xd690460 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xdf1305fe8a140a5a - Init START
lrdn0627:2297582:2297771 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0627:2297582:2297771 [0] NCCL INFO Bootstrap timings total 0.000366 (create 0.000015, send 0.000059, recv 0.000088, ring 0.000001, delay 0.000001)
lrdn0627:2297582:2297771 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0627:2297582:2297771 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0627:2297582:2297771 [0] NCCL INFO comm 0xd690460 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 00/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 01/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 02/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 03/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 04/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 05/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 06/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 07/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 08/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 09/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 10/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 11/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 12/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 13/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 14/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 15/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 16/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 17/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 18/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 19/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 20/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 21/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 22/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 23/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 24/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 25/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 26/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 27/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 28/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 29/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 30/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 31/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 32/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 33/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 34/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 35/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 36/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 37/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 38/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 39/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 40/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 41/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 42/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 43/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 44/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 45/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 46/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 47/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 48/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 49/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 50/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 51/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 52/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 53/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 54/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 55/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 56/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 57/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 58/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 59/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 60/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 61/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 62/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Channel 63/64 : 0
lrdn0627:2297582:2297771 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0627:2297582:2297771 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0627:2297582:2297771 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0627:2297582:2297777 [0] NCCL INFO [Proxy Service] Device 0 CPU core 4
lrdn0627:2297582:2297778 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 5
lrdn0627:2297582:2297771 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0627:2297582:2297771 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1163:1638815:1638815 [0] NCCL INFO Bootstrap: Using ib0:10.128.24.77<0>
lrdn1163:1638815:1638815 [0] NCCL INFO cudaDriverVersion 12020
lrdn1163:1638815:1638815 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0627:2297582:2297771 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0627:2297582:2297771 [0] NCCL INFO ncclCommInitRankConfig comm 0xd690460 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xdf1305fe8a140a5a - Init COMPLETE
lrdn0627:2297582:2297771 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.15, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1163:1638815:1638815 [0] NCCL INFO Comm config Blocking set to 1
lrdn1083:3702380:3702488 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.23.13<0>
lrdn1083:3702380:3702488 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1083:3702380:3702488 [0] NCCL INFO Using network IB
lrdn1083:3702380:3702488 [0] NCCL INFO ncclCommInitRankConfig comm 0xc43f130 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7e89571d6bd1ff10 - Init START
lrdn1083:3702380:3702488 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1083:3702380:3702488 [0] NCCL INFO Bootstrap timings total 0.000372 (create 0.000018, send 0.000060, recv 0.000091, ring 0.000001, delay 0.000000)
lrdn0834:1549970:1549970 [0] NCCL INFO Bootstrap: Using ib0:10.128.19.41<0>
lrdn0834:1549970:1549970 [0] NCCL INFO cudaDriverVersion 12020
lrdn0834:1549970:1549970 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0834:1549970:1549970 [0] NCCL INFO Comm config Blocking set to 1
lrdn1083:3702380:3702488 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1083:3702380:3702488 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1083:3702380:3702488 [0] NCCL INFO comm 0xc43f130 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 00/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 01/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 02/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 03/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 04/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 05/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 06/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 07/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 08/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 09/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 10/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 11/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 12/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 13/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 14/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 15/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 16/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 17/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 18/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 19/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 20/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 21/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 22/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 23/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 24/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 25/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 26/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 27/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 28/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 29/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 30/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 31/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 32/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 33/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 34/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 35/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 36/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 37/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 38/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 39/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 40/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 41/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 42/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 43/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 44/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 45/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 46/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 47/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 48/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 49/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 50/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 51/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 52/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 53/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 54/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 55/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 56/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 57/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 58/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 59/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 60/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 61/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 62/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Channel 63/64 : 0
lrdn1083:3702380:3702488 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1083:3702380:3702488 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1083:3702380:3702488 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1083:3702380:3702494 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn1083:3702380:3702495 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 15
lrdn1179:3818673:3818849 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1179:3818673:3818849 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1083:3702380:3702488 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1083:3702380:3702488 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1083:3702380:3702488 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1083:3702380:3702488 [0] NCCL INFO ncclCommInitRankConfig comm 0xc43f130 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x7e89571d6bd1ff10 - Init COMPLETE
lrdn1083:3702380:3702488 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.14, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1413:1703007:1703103 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1413:1703007:1703103 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1179:3818673:3818849 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.24.141<0>
lrdn1179:3818673:3818849 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1179:3818673:3818849 [0] NCCL INFO Using network IB
lrdn1179:3818673:3818849 [0] NCCL INFO ncclCommInitRankConfig comm 0xe7fc3b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x256d00ad45910147 - Init START
lrdn1179:3818673:3818849 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1179:3818673:3818849 [0] NCCL INFO Bootstrap timings total 0.000378 (create 0.000020, send 0.000058, recv 0.000092, ring 0.000001, delay 0.000001)
lrdn1179:3818673:3818849 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1179:3818673:3818849 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1179:3818673:3818849 [0] NCCL INFO comm 0xe7fc3b0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 00/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 01/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 02/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 03/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 04/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 05/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 06/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 07/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 08/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 09/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 10/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 11/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 12/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 13/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 14/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 15/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 16/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 17/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 18/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 19/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 20/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 21/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 22/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 23/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 24/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 25/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 26/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 27/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 28/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 29/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 30/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 31/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 32/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 33/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 34/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 35/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 36/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 37/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 38/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 39/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 40/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 41/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 42/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 43/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 44/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 45/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 46/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 47/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 48/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 49/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 50/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 51/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 52/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 53/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 54/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 55/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 56/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 57/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 58/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 59/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 60/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 61/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 62/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Channel 63/64 : 0
lrdn1179:3818673:3818849 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1179:3818673:3818849 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1179:3818673:3818849 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1179:3818673:3818855 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1179:3818673:3818856 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn1413:1703007:1703103 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.28.53<0>
lrdn1413:1703007:1703103 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1413:1703007:1703103 [0] NCCL INFO Using network IB
lrdn1413:1703007:1703103 [0] NCCL INFO ncclCommInitRankConfig comm 0xd86f1e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9284114c68d6ea6 - Init START
lrdn1413:1703007:1703103 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1413:1703007:1703103 [0] NCCL INFO Bootstrap timings total 0.000380 (create 0.000018, send 0.000061, recv 0.000093, ring 0.000001, delay 0.000000)
lrdn1179:3818673:3818849 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1179:3818673:3818849 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1179:3818673:3818849 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1179:3818673:3818849 [0] NCCL INFO ncclCommInitRankConfig comm 0xe7fc3b0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x256d00ad45910147 - Init COMPLETE
lrdn1179:3818673:3818849 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.22 (kernels 0.15, alloc 0.04, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1413:1703007:1703103 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1413:1703007:1703103 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1413:1703007:1703103 [0] NCCL INFO comm 0xd86f1e0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 00/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 01/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 02/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 03/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 04/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 05/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 06/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 07/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 08/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 09/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 10/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 11/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 12/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 13/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 14/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 15/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 16/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 17/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 18/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 19/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 20/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 21/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 22/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 23/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 24/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 25/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 26/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 27/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 28/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 29/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 30/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 31/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 32/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 33/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 34/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 35/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 36/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 37/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 38/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 39/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 40/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 41/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 42/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 43/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 44/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 45/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 46/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 47/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 48/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 49/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 50/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 51/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 52/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 53/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 54/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 55/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 56/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 57/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 58/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 59/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 60/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 61/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 62/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Channel 63/64 : 0
lrdn1413:1703007:1703103 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1413:1703007:1703103 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1413:1703007:1703103 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1413:1703007:1703109 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn1413:1703007:1703110 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 8
lrdn1413:1703007:1703103 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1413:1703007:1703103 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1413:1703007:1703103 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1413:1703007:1703103 [0] NCCL INFO ncclCommInitRankConfig comm 0xd86f1e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xe9284114c68d6ea6 - Init COMPLETE
lrdn1413:1703007:1703103 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.01)
lrdn1163:1638815:1638911 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1163:1638815:1638911 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0834:1549970:1550066 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0834:1549970:1550066 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1163:1638815:1638911 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.24.77<0>
lrdn1163:1638815:1638911 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1163:1638815:1638911 [0] NCCL INFO Using network IB
lrdn1163:1638815:1638911 [0] NCCL INFO ncclCommInitRankConfig comm 0xe54f5d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd1cbc623866cf283 - Init START
lrdn1163:1638815:1638911 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1163:1638815:1638911 [0] NCCL INFO Bootstrap timings total 0.000484 (create 0.000016, send 0.000056, recv 0.000208, ring 0.000001, delay 0.000001)
lrdn1163:1638815:1638911 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1163:1638815:1638911 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1163:1638815:1638911 [0] NCCL INFO comm 0xe54f5d0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 00/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 01/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 02/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 03/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 04/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 05/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 06/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 07/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 08/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 09/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 10/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 11/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 12/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 13/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 14/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 15/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 16/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 17/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 18/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 19/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 20/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 21/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 22/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 23/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 24/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 25/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 26/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 27/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 28/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 29/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 30/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 31/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 32/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 33/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 34/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 35/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 36/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 37/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 38/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 39/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 40/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 41/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 42/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 43/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 44/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 45/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 46/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 47/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 48/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 49/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 50/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 51/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 52/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 53/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 54/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 55/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 56/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 57/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 58/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 59/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 60/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 61/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 62/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Channel 63/64 : 0
lrdn1163:1638815:1638911 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1163:1638815:1638911 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1163:1638815:1638911 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1163:1638815:1638918 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 4
lrdn1163:1638815:1638917 [0] NCCL INFO [Proxy Service] Device 0 CPU core 3
lrdn0834:1549970:1550066 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.19.41<0>
lrdn0834:1549970:1550066 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0834:1549970:1550066 [0] NCCL INFO Using network IB
lrdn0834:1549970:1550066 [0] NCCL INFO ncclCommInitRankConfig comm 0xd9fddd0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x436e82923ba4c4a4 - Init START
lrdn0834:1549970:1550066 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0834:1549970:1550066 [0] NCCL INFO Bootstrap timings total 0.000371 (create 0.000017, send 0.000058, recv 0.000093, ring 0.000001, delay 0.000001)
lrdn1163:1638815:1638911 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1163:1638815:1638911 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1163:1638815:1638911 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1163:1638815:1638911 [0] NCCL INFO ncclCommInitRankConfig comm 0xe54f5d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd1cbc623866cf283 - Init COMPLETE
lrdn1163:1638815:1638911 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0834:1549970:1550066 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0834:1549970:1550066 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0834:1549970:1550066 [0] NCCL INFO comm 0xd9fddd0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 00/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 01/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 02/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 03/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 04/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 05/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 06/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 07/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 08/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 09/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 10/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 11/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 12/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 13/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 14/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 15/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 16/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 17/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 18/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 19/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 20/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 21/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 22/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 23/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 24/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 25/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 26/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 27/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 28/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 29/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 30/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 31/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 32/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 33/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 34/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 35/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 36/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 37/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 38/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 39/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 40/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 41/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 42/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 43/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 44/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 45/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 46/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 47/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 48/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 49/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 50/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 51/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 52/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 53/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 54/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 55/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 56/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 57/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 58/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 59/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 60/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 61/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 62/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Channel 63/64 : 0
lrdn0834:1549970:1550066 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0834:1549970:1550066 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0834:1549970:1550066 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0834:1549970:1550073 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
lrdn0834:1549970:1550072 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn0834:1549970:1550066 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0834:1549970:1550066 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0834:1549970:1550066 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0834:1549970:1550066 [0] NCCL INFO ncclCommInitRankConfig comm 0xd9fddd0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x436e82923ba4c4a4 - Init COMPLETE
lrdn0834:1549970:1550066 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0891:1696238:1696238 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.13<0>
lrdn0891:1696238:1696238 [0] NCCL INFO cudaDriverVersion 12020
lrdn0891:1696238:1696238 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0891:1696238:1696238 [0] NCCL INFO Comm config Blocking set to 1
lrdn0927:1653016:1653016 [0] NCCL INFO Bootstrap: Using ib0:10.128.20.157<0>
lrdn0927:1653016:1653016 [0] NCCL INFO cudaDriverVersion 12020
lrdn0927:1653016:1653016 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0927:1653016:1653016 [0] NCCL INFO Comm config Blocking set to 1
lrdn0657:1542713:1542713 [0] NCCL INFO Bootstrap: Using ib0:10.128.16.101<0>
lrdn0657:1542713:1542713 [0] NCCL INFO cudaDriverVersion 12020
lrdn0657:1542713:1542713 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn0657:1542713:1542713 [0] NCCL INFO Comm config Blocking set to 1
lrdn0891:1696238:1696334 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0891:1696238:1696334 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0891:1696238:1696334 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.13<0>
lrdn0891:1696238:1696334 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0891:1696238:1696334 [0] NCCL INFO Using network IB
lrdn0891:1696238:1696334 [0] NCCL INFO ncclCommInitRankConfig comm 0xd59ab20 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb8ecf2cf8df7ef34 - Init START
lrdn0891:1696238:1696334 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0891:1696238:1696334 [0] NCCL INFO Bootstrap timings total 0.000380 (create 0.000017, send 0.000058, recv 0.000098, ring 0.000001, delay 0.000000)
lrdn0891:1696238:1696334 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0891:1696238:1696334 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0891:1696238:1696334 [0] NCCL INFO comm 0xd59ab20 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 00/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 01/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 02/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 03/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 04/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 05/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 06/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 07/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 08/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 09/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 10/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 11/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 12/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 13/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 14/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 15/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 16/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 17/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 18/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 19/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 20/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 21/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 22/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 23/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 24/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 25/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 26/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 27/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 28/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 29/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 30/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 31/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 32/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 33/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 34/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 35/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 36/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 37/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 38/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 39/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 40/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 41/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 42/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 43/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 44/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 45/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 46/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 47/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 48/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 49/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 50/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 51/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 52/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 53/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 54/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 55/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 56/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 57/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 58/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 59/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 60/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 61/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 62/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Channel 63/64 : 0
lrdn0891:1696238:1696334 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0891:1696238:1696334 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0891:1696238:1696334 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0891:1696238:1696341 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 0
lrdn0891:1696238:1696340 [0] NCCL INFO [Proxy Service] Device 0 CPU core 9
lrdn0891:1696238:1696334 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0891:1696238:1696334 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0891:1696238:1696334 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0891:1696238:1696334 [0] NCCL INFO ncclCommInitRankConfig comm 0xd59ab20 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xb8ecf2cf8df7ef34 - Init COMPLETE
lrdn0891:1696238:1696334 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.22 (kernels 0.14, alloc 0.04, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0927:1653016:1653202 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0927:1653016:1653202 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0657:1542713:1542823 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn0657:1542713:1542823 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn0927:1653016:1653202 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.20.157<0>
lrdn0927:1653016:1653202 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0927:1653016:1653202 [0] NCCL INFO Using network IB
lrdn0927:1653016:1653202 [0] NCCL INFO ncclCommInitRankConfig comm 0xdcdecc0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x37e7c7bdd24637bf - Init START
lrdn0927:1653016:1653202 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0927:1653016:1653202 [0] NCCL INFO Bootstrap timings total 0.000365 (create 0.000016, send 0.000051, recv 0.000091, ring 0.000001, delay 0.000001)
lrdn0657:1542713:1542823 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.16.101<0>
lrdn0657:1542713:1542823 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn0657:1542713:1542823 [0] NCCL INFO Using network IB
lrdn0657:1542713:1542823 [0] NCCL INFO ncclCommInitRankConfig comm 0xe693cd0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd43636e99b651f11 - Init START
lrdn0657:1542713:1542823 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn0657:1542713:1542823 [0] NCCL INFO Bootstrap timings total 0.000351 (create 0.000017, send 0.000052, recv 0.000082, ring 0.000001, delay 0.000001)
lrdn0927:1653016:1653202 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0927:1653016:1653202 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0927:1653016:1653202 [0] NCCL INFO comm 0xdcdecc0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 00/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 01/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 02/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 03/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 04/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 05/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 06/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 07/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 08/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 09/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 10/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 11/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 12/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 13/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 14/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 15/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 16/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 17/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 18/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 19/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 20/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 21/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 22/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 23/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 24/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 25/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 26/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 27/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 28/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 29/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 30/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 31/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 32/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 33/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 34/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 35/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 36/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 37/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 38/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 39/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 40/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 41/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 42/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 43/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 44/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 45/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 46/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 47/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 48/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 49/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 50/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 51/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 52/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 53/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 54/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 55/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 56/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 57/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 58/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 59/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 60/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 61/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 62/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Channel 63/64 : 0
lrdn0927:1653016:1653202 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0927:1653016:1653202 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0927:1653016:1653202 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0927:1653016:1653208 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
lrdn0927:1653016:1653209 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 11
lrdn0657:1542713:1542823 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn0657:1542713:1542823 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn0657:1542713:1542823 [0] NCCL INFO comm 0xe693cd0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 00/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 01/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 02/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 03/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 04/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 05/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 06/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 07/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 08/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 09/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 10/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 11/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 12/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 13/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 14/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 15/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 16/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 17/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 18/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 19/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 20/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 21/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 22/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 23/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 24/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 25/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 26/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 27/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 28/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 29/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 30/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 31/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 32/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 33/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 34/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 35/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 36/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 37/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 38/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 39/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 40/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 41/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 42/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 43/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 44/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 45/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 46/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 47/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 48/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 49/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 50/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 51/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 52/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 53/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 54/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 55/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 56/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 57/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 58/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 59/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 60/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 61/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 62/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Channel 63/64 : 0
lrdn0657:1542713:1542823 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn0657:1542713:1542823 [0] NCCL INFO P2P Chunksize set to 524288
lrdn0657:1542713:1542823 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn0657:1542713:1542829 [0] NCCL INFO [Proxy Service] Device 0 CPU core 11
lrdn0657:1542713:1542830 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn0927:1653016:1653202 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0927:1653016:1653202 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0927:1653016:1653202 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0927:1653016:1653202 [0] NCCL INFO ncclCommInitRankConfig comm 0xdcdecc0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x37e7c7bdd24637bf - Init COMPLETE
lrdn0927:1653016:1653202 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0657:1542713:1542823 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn0657:1542713:1542823 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn0657:1542713:1542823 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn0657:1542713:1542823 [0] NCCL INFO ncclCommInitRankConfig comm 0xe693cd0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0xd43636e99b651f11 - Init COMPLETE
lrdn0657:1542713:1542823 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.24 (kernels 0.14, alloc 0.06, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1191:1706749:1706749 [0] NCCL INFO Bootstrap: Using ib0:10.128.24.189<0>
lrdn1191:1706749:1706749 [0] NCCL INFO cudaDriverVersion 12020
lrdn1191:1706749:1706749 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1191:1706749:1706749 [0] NCCL INFO Comm config Blocking set to 1
lrdn1191:1706749:1706846 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1191:1706749:1706846 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1191:1706749:1706846 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.24.189<0>
lrdn1191:1706749:1706846 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1191:1706749:1706846 [0] NCCL INFO Using network IB
lrdn1191:1706749:1706846 [0] NCCL INFO ncclCommInitRankConfig comm 0xd2b6bc0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x54148c1c1226f2b3 - Init START
lrdn1191:1706749:1706846 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1191:1706749:1706846 [0] NCCL INFO Bootstrap timings total 0.000381 (create 0.000017, send 0.000057, recv 0.000095, ring 0.000001, delay 0.000000)
lrdn1191:1706749:1706846 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1191:1706749:1706846 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1191:1706749:1706846 [0] NCCL INFO comm 0xd2b6bc0 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 00/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 01/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 02/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 03/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 04/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 05/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 06/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 07/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 08/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 09/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 10/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 11/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 12/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 13/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 14/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 15/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 16/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 17/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 18/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 19/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 20/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 21/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 22/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 23/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 24/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 25/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 26/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 27/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 28/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 29/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 30/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 31/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 32/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 33/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 34/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 35/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 36/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 37/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 38/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 39/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 40/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 41/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 42/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 43/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 44/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 45/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 46/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 47/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 48/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 49/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 50/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 51/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 52/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 53/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 54/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 55/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 56/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 57/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 58/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 59/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 60/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 61/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 62/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Channel 63/64 : 0
lrdn1191:1706749:1706846 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1191:1706749:1706846 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1191:1706749:1706846 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1191:1706749:1706853 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 13
lrdn1191:1706749:1706852 [0] NCCL INFO [Proxy Service] Device 0 CPU core 12
lrdn1191:1706749:1706846 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1191:1706749:1706846 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1191:1706749:1706846 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1191:1706749:1706846 [0] NCCL INFO ncclCommInitRankConfig comm 0xd2b6bc0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x54148c1c1226f2b3 - Init COMPLETE
lrdn1191:1706749:1706846 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.23 (kernels 0.14, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn1246:1672375:1672375 [0] NCCL INFO Bootstrap: Using ib0:10.128.25.153<0>
lrdn1246:1672375:1672375 [0] NCCL INFO cudaDriverVersion 12020
lrdn1246:1672375:1672375 [0] NCCL INFO NCCL version 2.26.2+cuda12.2
lrdn1246:1672375:1672375 [0] NCCL INFO Comm config Blocking set to 1
lrdn1246:1672375:1672486 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
lrdn1246:1672375:1672486 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
lrdn1246:1672375:1672486 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [RO]; OOB ib0:10.128.25.153<0>
lrdn1246:1672375:1672486 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
lrdn1246:1672375:1672486 [0] NCCL INFO Using network IB
lrdn1246:1672375:1672486 [0] NCCL INFO ncclCommInitRankConfig comm 0xdd09120 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8020f78b3adbf706 - Init START
lrdn1246:1672375:1672486 [0] NCCL INFO RAS client listening socket at ::1<28028>
lrdn1246:1672375:1672486 [0] NCCL INFO Bootstrap timings total 0.000473 (create 0.000020, send 0.000064, recv 0.000185, ring 0.000001, delay 0.000000)
lrdn1246:1672375:1672486 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
lrdn1246:1672375:1672486 [0] NCCL INFO Setting affinity for GPU 0 to ffff
lrdn1246:1672375:1672486 [0] NCCL INFO comm 0xdd09120 rank 0 nRanks 1 nNodes 1 localRanks 1 localRank 0 MNNVL 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 00/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 01/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 02/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 03/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 04/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 05/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 06/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 07/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 08/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 09/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 10/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 11/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 12/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 13/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 14/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 15/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 16/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 17/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 18/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 19/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 20/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 21/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 22/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 23/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 24/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 25/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 26/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 27/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 28/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 29/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 30/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 31/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 32/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 33/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 34/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 35/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 36/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 37/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 38/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 39/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 40/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 41/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 42/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 43/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 44/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 45/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 46/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 47/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 48/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 49/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 50/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 51/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 52/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 53/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 54/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 55/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 56/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 57/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 58/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 59/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 60/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 61/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 62/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Channel 63/64 : 0
lrdn1246:1672375:1672486 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1 [32] -1/-1/-1->0->-1 [33] -1/-1/-1->0->-1 [34] -1/-1/-1->0->-1 [35] -1/-1/-1->0->-1 [36] -1/-1/-1->0->-1 [37] -1/-1/-1->0->-1 [38] -1/-1/-1->0->-1 [39] -1/-1/-1->0->-1 [40] -1/-1/-1->0->-1 [41] -1/-1/-1->0->-1 [42] -1/-1/-1->0->-1 [43] -1/-1/-1->0->-1 [44] -1/-1/-1->0->-1 [45] -1/-1/-1->0->-1 [46] -1/-1/-1->0->-1 [
lrdn1246:1672375:1672486 [0] NCCL INFO P2P Chunksize set to 524288
lrdn1246:1672375:1672486 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 0 directMode 0
lrdn1246:1672375:1672492 [0] NCCL INFO [Proxy Service] Device 0 CPU core 2
lrdn1246:1672375:1672493 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 3
lrdn1246:1672375:1672486 [0] NCCL INFO 64 coll channels, 64 collnet channels, 0 nvls channels, 64 p2p channels, 64 p2p channels per peer
lrdn1246:1672375:1672486 [0] NCCL INFO CC Off, workFifoBytes 1048576
lrdn1246:1672375:1672486 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
lrdn1246:1672375:1672486 [0] NCCL INFO ncclCommInitRankConfig comm 0xdd09120 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x8020f78b3adbf706 - Init COMPLETE
lrdn1246:1672375:1672486 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 1 total 0.26 (kernels 0.18, alloc 0.05, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
lrdn0523:1525899:1525899 [0] NCCL INFO Comm config Blocking set to 1
lrdn0542:1578195:1578195 [0] NCCL INFO Comm config Blocking set to 1
lrdn0634:836731:836731 [0] NCCL INFO Comm config Blocking set to 1
lrdn0539:3885339:3885339 [0] NCCL INFO Comm config Blocking set to 1
lrdn0565:3828397:3828397 [0] NCCL INFO Comm config Blocking set to 1
lrdn0565:3828397:3828503 [0] NCCL INFO Using network IB
lrdn0570:1729371:1729371 [0] NCCL INFO Comm config Blocking set to 1
lrdn0413:1991619:1991619 [0] NCCL INFO Comm config Blocking set to 1
lrdn0493:1513831:1513831 [0] NCCL INFO Comm config Blocking set to 1
lrdn0390:1401765:1401765 [0] NCCL INFO Comm config Blocking set to 1
lrdn0568:2431945:2431945 [0] NCCL INFO Comm config Blocking set to 1
lrdn0507:1445318:1445318 [0] NCCL INFO Comm config Blocking set to 1
lrdn0569:3775528:3775528 [0] NCCL INFO Comm config Blocking set to 1
lrdn0705:1603790:1603790 [0] NCCL INFO Comm config Blocking set to 1
lrdn0604:1730089:1730089 [0] NCCL INFO Comm config Blocking set to 1
lrdn0663:2614220:2614220 [0] NCCL INFO Comm config Blocking set to 1
lrdn0536:1614437:1614437 [0] NCCL INFO Comm config Blocking set to 1
lrdn0389:1433363:1433363 [0] NCCL INFO Comm config Blocking set to 1
lrdn0632:1818398:1818398 [0] NCCL INFO Comm config Blocking set to 1
lrdn0599:161030:161030 [0] NCCL INFO Comm config Blocking set to 1
lrdn0964:1708887:1708887 [0] NCCL INFO Comm config Blocking set to 1
lrdn0823:1648443:1648443 [0] NCCL INFO Comm config Blocking set to 1
lrdn0668:1708195:1708195 [0] NCCL INFO Comm config Blocking set to 1
lrdn0668:1708195:1708311 [0] NCCL INFO Using network IB
lrdn0680:1633887:1633887 [0] NCCL INFO Comm config Blocking set to 1
lrdn0528:2919634:2919634 [0] NCCL INFO Comm config Blocking set to 1
lrdn0528:2919634:2919754 [0] NCCL INFO Using network IB
lrdn0404:1476320:1476320 [0] NCCL INFO Comm config Blocking set to 1
lrdn0963:1608371:1608371 [0] NCCL INFO Comm config Blocking set to 1
lrdn0570:1729371:1729493 [0] NCCL INFO Using network IB
lrdn0413:1991619:1991723 [0] NCCL INFO Using network IB
lrdn0413:1991619:1991723 [0] NCCL INFO ncclCommInitRankScalable comm 0x2b838bd0 rank 58 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0493:1513831:1513936 [0] NCCL INFO Using network IB
lrdn0493:1513831:1513936 [0] NCCL INFO ncclCommInitRankScalable comm 0x1562bd20 rank 69 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0611:2834843:2834843 [0] NCCL INFO Comm config Blocking set to 1
lrdn0611:2834843:2834965 [0] NCCL INFO Using network IB
lrdn0611:2834843:2834965 [0] NCCL INFO ncclCommInitRankScalable comm 0x1662a110 rank 95 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0390:1401765:1401871 [0] NCCL INFO Using network IB
lrdn0390:1401765:1401871 [0] NCCL INFO ncclCommInitRankScalable comm 0x10f2cae0 rank 55 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0773:1742369:1742369 [0] NCCL INFO Comm config Blocking set to 1
lrdn0849:2195906:2195906 [0] NCCL INFO Comm config Blocking set to 1
lrdn0434:838683:838683 [0] NCCL INFO Comm config Blocking set to 1
lrdn0434:838683:838868 [0] NCCL INFO Using network IB
lrdn0434:838683:838868 [0] NCCL INFO ncclCommInitRankScalable comm 0x22591cb0 rank 60 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0815:1752480:1752480 [0] NCCL INFO Comm config Blocking set to 1
lrdn0507:1445318:1445426 [0] NCCL INFO Using network IB
lrdn0507:1445318:1445426 [0] NCCL INFO ncclCommInitRankScalable comm 0x16fe57e0 rank 72 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0569:3775528:3775645 [0] NCCL INFO Using network IB
lrdn0569:3775528:3775645 [0] NCCL INFO ncclCommInitRankScalable comm 0x166e9a70 rank 85 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0705:1603790:1603899 [0] NCCL INFO Using network IB
lrdn0705:1603790:1603899 [0] NCCL INFO ncclCommInitRankScalable comm 0x1782da90 rank 119 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0510:719419:719419 [0] NCCL INFO Comm config Blocking set to 1
lrdn0510:719419:719526 [0] NCCL INFO Using network IB
lrdn0510:719419:719526 [0] NCCL INFO ncclCommInitRankScalable comm 0x116ffc70 rank 73 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0568:2431945:2432050 [0] NCCL INFO Using network IB
lrdn0568:2431945:2432050 [0] NCCL INFO ncclCommInitRankScalable comm 0xe440070 rank 84 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0604:1730089:1730226 [0] NCCL INFO Using network IB
lrdn0604:1730089:1730226 [0] NCCL INFO ncclCommInitRankScalable comm 0xccaf3e0 rank 93 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0709:1450873:1450873 [0] NCCL INFO Comm config Blocking set to 1
lrdn0709:1450873:1450978 [0] NCCL INFO Using network IB
lrdn0709:1450873:1450978 [0] NCCL INFO ncclCommInitRankScalable comm 0x213e1900 rank 120 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0681:1653353:1653353 [0] NCCL INFO Comm config Blocking set to 1
lrdn0681:1653353:1653469 [0] NCCL INFO Using network IB
lrdn0681:1653353:1653469 [0] NCCL INFO ncclCommInitRankScalable comm 0x309a4600 rank 114 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0634:836731:836838 [0] NCCL INFO Using network IB
lrdn0634:836731:836838 [0] NCCL INFO ncclCommInitRankScalable comm 0x2e8e5b30 rank 102 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0663:2614220:2614404 [0] NCCL INFO Using network IB
lrdn0663:2614220:2614404 [0] NCCL INFO ncclCommInitRankScalable comm 0x188267f0 rank 108 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0789:1852874:1852874 [0] NCCL INFO Comm config Blocking set to 1
lrdn0358:1512606:1512606 [0] NCCL INFO Comm config Blocking set to 1
lrdn0358:1512606:1512713 [0] NCCL INFO Using network IB
lrdn0358:1512606:1512713 [0] NCCL INFO ncclCommInitRankScalable comm 0x175a6010 rank 49 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0830:1770477:1770477 [0] NCCL INFO Comm config Blocking set to 1
lrdn0830:1770477:1770595 [0] NCCL INFO Using network IB
lrdn0830:1770477:1770595 [0] NCCL INFO ncclCommInitRankScalable comm 0x177bed40 rank 138 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0953:1677008:1677008 [0] NCCL INFO Comm config Blocking set to 1
lrdn0953:1677008:1677128 [0] NCCL INFO Using network IB
lrdn0953:1677008:1677128 [0] NCCL INFO ncclCommInitRankScalable comm 0x1645e080 rank 170 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0536:1614437:1614558 [0] NCCL INFO Using network IB
lrdn0536:1614437:1614558 [0] NCCL INFO ncclCommInitRankScalable comm 0x2bcee7f0 rank 78 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0478:1546843:1546843 [0] NCCL INFO Comm config Blocking set to 1
lrdn0478:1546843:1546961 [0] NCCL INFO Using network IB
lrdn0478:1546843:1546961 [0] NCCL INFO ncclCommInitRankScalable comm 0x21add1b0 rank 64 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0389:1433363:1433469 [0] NCCL INFO Using network IB
lrdn0389:1433363:1433469 [0] NCCL INFO ncclCommInitRankScalable comm 0x220d08c0 rank 54 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0632:1818398:1818517 [0] NCCL INFO Using network IB
lrdn0632:1818398:1818517 [0] NCCL INFO ncclCommInitRankScalable comm 0x2f440160 rank 101 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0606:1640320:1640320 [0] NCCL INFO Comm config Blocking set to 1
lrdn0606:1640320:1640425 [0] NCCL INFO Using network IB
lrdn0606:1640320:1640425 [0] NCCL INFO ncclCommInitRankScalable comm 0xe671be0 rank 94 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0599:161030:161147 [0] NCCL INFO Using network IB
lrdn0599:161030:161147 [0] NCCL INFO ncclCommInitRankScalable comm 0x10ca5ed0 rank 92 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0964:1708887:1708993 [0] NCCL INFO Using network IB
lrdn0964:1708887:1708993 [0] NCCL INFO ncclCommInitRankScalable comm 0xc920400 rank 172 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0823:1648443:1648560 [0] NCCL INFO Using network IB
lrdn0823:1648443:1648560 [0] NCCL INFO ncclCommInitRankScalable comm 0x2fa16a00 rank 136 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0668:1708195:1708311 [0] NCCL INFO ncclCommInitRankScalable comm 0x1703df80 rank 110 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0680:1633887:1634007 [0] NCCL INFO Using network IB
lrdn0680:1633887:1634007 [0] NCCL INFO ncclCommInitRankScalable comm 0x177b30d0 rank 113 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0528:2919634:2919754 [0] NCCL INFO ncclCommInitRankScalable comm 0x300cdc80 rank 75 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0404:1476320:1476424 [0] NCCL INFO Using network IB
lrdn0404:1476320:1476424 [0] NCCL INFO ncclCommInitRankScalable comm 0x21549ba0 rank 56 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0963:1608371:1608479 [0] NCCL INFO Using network IB
lrdn0963:1608371:1608479 [0] NCCL INFO ncclCommInitRankScalable comm 0x22474b00 rank 171 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0570:1729371:1729493 [0] NCCL INFO ncclCommInitRankScalable comm 0xcc97090 rank 86 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0854:793988:793988 [0] NCCL INFO Comm config Blocking set to 1
lrdn0854:793988:794097 [0] NCCL INFO Using network IB
lrdn0854:793988:794097 [0] NCCL INFO ncclCommInitRankScalable comm 0x14b73470 rank 145 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0544:1567247:1567247 [0] NCCL INFO Comm config Blocking set to 1
lrdn0544:1567247:1567365 [0] NCCL INFO Using network IB
lrdn0544:1567247:1567365 [0] NCCL INFO ncclCommInitRankScalable comm 0x113d9320 rank 81 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0932:1700071:1700071 [0] NCCL INFO Comm config Blocking set to 1
lrdn0932:1700071:1700188 [0] NCCL INFO Using network IB
lrdn0932:1700071:1700188 [0] NCCL INFO ncclCommInitRankScalable comm 0x1110a270 rank 164 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0627:2297582:2297582 [0] NCCL INFO Comm config Blocking set to 1
lrdn0627:2297582:2297782 [0] NCCL INFO Using network IB
lrdn0627:2297582:2297782 [0] NCCL INFO ncclCommInitRankScalable comm 0x16c437f0 rank 99 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0773:1742369:1742492 [0] NCCL INFO Using network IB
lrdn0773:1742369:1742492 [0] NCCL INFO ncclCommInitRankScalable comm 0x208c45c0 rank 130 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0849:2195906:2196012 [0] NCCL INFO Using network IB
lrdn0849:2195906:2196012 [0] NCCL INFO ncclCommInitRankScalable comm 0x2b955420 rank 142 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0697:2705141:2705141 [0] NCCL INFO Comm config Blocking set to 1
lrdn0697:2705141:2705338 [0] NCCL INFO Using network IB
lrdn0697:2705141:2705338 [0] NCCL INFO ncclCommInitRankScalable comm 0xdf8dfd0 rank 117 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0815:1752480:1752600 [0] NCCL INFO Using network IB
lrdn0815:1752480:1752600 [0] NCCL INFO ncclCommInitRankScalable comm 0x2aca5e40 rank 134 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0447:1574775:1574775 [0] NCCL INFO Comm config Blocking set to 1
lrdn0447:1574775:1574880 [0] NCCL INFO Using network IB
lrdn0447:1574775:1574880 [0] NCCL INFO ncclCommInitRankScalable comm 0xd922e50 rank 61 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0539:3885339:3885446 [0] NCCL INFO Using network IB
lrdn0539:3885339:3885446 [0] NCCL INFO ncclCommInitRankScalable comm 0x213aba20 rank 79 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0664:1570998:1570998 [0] NCCL INFO Comm config Blocking set to 1
lrdn0664:1570998:1571119 [0] NCCL INFO Using network IB
lrdn0664:1570998:1571119 [0] NCCL INFO ncclCommInitRankScalable comm 0x21986920 rank 109 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0597:1687014:1687014 [0] NCCL INFO Comm config Blocking set to 1
lrdn0597:1687014:1687132 [0] NCCL INFO Using network IB
lrdn0597:1687014:1687132 [0] NCCL INFO ncclCommInitRankScalable comm 0x21056650 rank 90 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0284:1876255:1876255 [0] NCCL INFO Comm config Blocking set to 1
lrdn0284:1876255:1876359 [0] NCCL INFO Using network IB
lrdn0284:1876255:1876359 [0] NCCL INFO ncclCommInitRankScalable comm 0x215f6650 rank 37 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0828:457320:457320 [0] NCCL INFO Comm config Blocking set to 1
lrdn0828:457320:457441 [0] NCCL INFO Using network IB
lrdn0828:457320:457441 [0] NCCL INFO ncclCommInitRankScalable comm 0x128a6200 rank 137 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0789:1852874:1852994 [0] NCCL INFO Using network IB
lrdn0789:1852874:1852994 [0] NCCL INFO ncclCommInitRankScalable comm 0x211f91f0 rank 131 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0298:1697225:1697225 [0] NCCL INFO Comm config Blocking set to 1
lrdn0298:1697225:1697329 [0] NCCL INFO Using network IB
lrdn0298:1697225:1697329 [0] NCCL INFO ncclCommInitRankScalable comm 0x11bb59d0 rank 42 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0307:1873275:1873275 [0] NCCL INFO Comm config Blocking set to 1
lrdn0307:1873275:1873381 [0] NCCL INFO Using network IB
lrdn0307:1873275:1873381 [0] NCCL INFO ncclCommInitRankScalable comm 0x163fb890 rank 45 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0693:1636112:1636112 [0] NCCL INFO Comm config Blocking set to 1
lrdn0693:1636112:1636217 [0] NCCL INFO Using network IB
lrdn0693:1636112:1636217 [0] NCCL INFO ncclCommInitRankScalable comm 0x220c9520 rank 115 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0696:1664741:1664741 [0] NCCL INFO Comm config Blocking set to 1
lrdn0803:1970378:1970378 [0] NCCL INFO Comm config Blocking set to 1
lrdn0803:1970378:1970563 [0] NCCL INFO Using network IB
lrdn0803:1970378:1970563 [0] NCCL INFO ncclCommInitRankScalable comm 0x212ac560 rank 133 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0387:1604830:1604830 [0] NCCL INFO Comm config Blocking set to 1
lrdn0637:1712537:1712537 [0] NCCL INFO Comm config Blocking set to 1
lrdn0637:1712537:1712644 [0] NCCL INFO Using network IB
lrdn0637:1712537:1712644 [0] NCCL INFO ncclCommInitRankScalable comm 0x18032550 rank 103 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0329:1724546:1724546 [0] NCCL INFO Comm config Blocking set to 1
lrdn0329:1724546:1724654 [0] NCCL INFO Using network IB
lrdn0329:1724546:1724654 [0] NCCL INFO ncclCommInitRankScalable comm 0x118e8b50 rank 48 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0590:1647475:1647475 [0] NCCL INFO Comm config Blocking set to 1
lrdn0590:1647475:1647581 [0] NCCL INFO Using network IB
lrdn0590:1647475:1647581 [0] NCCL INFO ncclCommInitRankScalable comm 0x2b96f5e0 rank 88 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0981:1717822:1717822 [0] NCCL INFO Comm config Blocking set to 1
lrdn0981:1717822:1717943 [0] NCCL INFO Using network IB
lrdn0981:1717822:1717943 [0] NCCL INFO ncclCommInitRankScalable comm 0x213e7b40 rank 177 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0596:1736202:1736202 [0] NCCL INFO Comm config Blocking set to 1
lrdn0596:1736202:1736308 [0] NCCL INFO Using network IB
lrdn0596:1736202:1736308 [0] NCCL INFO ncclCommInitRankScalable comm 0x21fae9f0 rank 89 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0843:1593394:1593394 [0] NCCL INFO Comm config Blocking set to 1
lrdn0843:1593394:1593514 [0] NCCL INFO Using network IB
lrdn0843:1593394:1593514 [0] NCCL INFO ncclCommInitRankScalable comm 0x176253d0 rank 141 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0721:1259508:1259508 [0] NCCL INFO Comm config Blocking set to 1
lrdn0721:1259508:1259616 [0] NCCL INFO Using network IB
lrdn0721:1259508:1259616 [0] NCCL INFO ncclCommInitRankScalable comm 0x21daad20 rank 123 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1030:1505262:1505262 [0] NCCL INFO Comm config Blocking set to 1
lrdn1030:1505262:1505394 [0] NCCL INFO Using network IB
lrdn1030:1505262:1505394 [0] NCCL INFO ncclCommInitRankScalable comm 0x10ab9c40 rank 187 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0409:1639191:1639191 [0] NCCL INFO Comm config Blocking set to 1
lrdn0409:1639191:1639298 [0] NCCL INFO Using network IB
lrdn0409:1639191:1639298 [0] NCCL INFO ncclCommInitRankScalable comm 0xd631ac0 rank 57 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0553:3116443:3116443 [0] NCCL INFO Comm config Blocking set to 1
lrdn0553:3116443:3116576 [0] NCCL INFO Using network IB
lrdn0553:3116443:3116576 [0] NCCL INFO ncclCommInitRankScalable comm 0x2237a160 rank 82 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0919:1801558:1801558 [0] NCCL INFO Comm config Blocking set to 1
lrdn0919:1801558:1801679 [0] NCCL INFO Using network IB
lrdn0919:1801558:1801679 [0] NCCL INFO ncclCommInitRankScalable comm 0x10a53e80 rank 161 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0385:1468992:1468992 [0] NCCL INFO Comm config Blocking set to 1
lrdn0385:1468992:1469114 [0] NCCL INFO Using network IB
lrdn0385:1468992:1469114 [0] NCCL INFO ncclCommInitRankScalable comm 0x1705ca30 rank 52 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0676:1759834:1759834 [0] NCCL INFO Comm config Blocking set to 1
lrdn0676:1759834:1759941 [0] NCCL INFO Using network IB
lrdn0676:1759834:1759941 [0] NCCL INFO ncclCommInitRankScalable comm 0x20b184e0 rank 112 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0535:3448215:3448215 [0] NCCL INFO Comm config Blocking set to 1
lrdn0535:3448215:3448320 [0] NCCL INFO Using network IB
lrdn0535:3448215:3448320 [0] NCCL INFO ncclCommInitRankScalable comm 0x2b736dc0 rank 77 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0624:1835012:1835012 [0] NCCL INFO Comm config Blocking set to 1
lrdn0624:1835012:1835150 [0] NCCL INFO Using network IB
lrdn0624:1835012:1835150 [0] NCCL INFO ncclCommInitRankScalable comm 0xd45c860 rank 98 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0699:3525052:3525052 [0] NCCL INFO Comm config Blocking set to 1
lrdn0699:3525052:3525159 [0] NCCL INFO Using network IB
lrdn0699:3525052:3525159 [0] NCCL INFO ncclCommInitRankScalable comm 0x2247d9d0 rank 118 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0300:1343763:1343763 [0] NCCL INFO Comm config Blocking set to 1
lrdn0300:1343763:1343869 [0] NCCL INFO Using network IB
lrdn0300:1343763:1343869 [0] NCCL INFO ncclCommInitRankScalable comm 0xe2b7a00 rank 43 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0864:1633624:1633624 [0] NCCL INFO Comm config Blocking set to 1
lrdn0864:1633624:1633729 [0] NCCL INFO Using network IB
lrdn0864:1633624:1633729 [0] NCCL INFO ncclCommInitRankScalable comm 0x2ac7bbc0 rank 147 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0991:2352931:2352931 [0] NCCL INFO Comm config Blocking set to 1
lrdn0991:2352931:2353048 [0] NCCL INFO Using network IB
lrdn0991:2352931:2353048 [0] NCCL INFO ncclCommInitRankScalable comm 0x2b5709c0 rank 179 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0965:1473702:1473702 [0] NCCL INFO Comm config Blocking set to 1
lrdn0965:1473702:1473820 [0] NCCL INFO Using network IB
lrdn0965:1473702:1473820 [0] NCCL INFO ncclCommInitRankScalable comm 0x1184f5e0 rank 173 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0646:1603546:1603546 [0] NCCL INFO Comm config Blocking set to 1
lrdn0646:1603546:1603655 [0] NCCL INFO Using network IB
lrdn0646:1603546:1603655 [0] NCCL INFO ncclCommInitRankScalable comm 0x2b7fce10 rank 105 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0642:1597688:1597688 [0] NCCL INFO Comm config Blocking set to 1
lrdn0614:1581089:1581089 [0] NCCL INFO Comm config Blocking set to 1
lrdn0614:1581089:1581210 [0] NCCL INFO Using network IB
lrdn0614:1581089:1581210 [0] NCCL INFO ncclCommInitRankScalable comm 0xe33d720 rank 96 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0657:1542713:1542713 [0] NCCL INFO Comm config Blocking set to 1
lrdn0657:1542713:1542834 [0] NCCL INFO Using network IB
lrdn0657:1542713:1542834 [0] NCCL INFO ncclCommInitRankScalable comm 0x12169ea0 rank 107 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0529:3299727:3299727 [0] NCCL INFO Comm config Blocking set to 1
lrdn0529:3299727:3299832 [0] NCCL INFO Using network IB
lrdn0529:3299727:3299832 [0] NCCL INFO ncclCommInitRankScalable comm 0x11f070a0 rank 76 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1042:1873399:1873399 [0] NCCL INFO Comm config Blocking set to 1
lrdn1042:1873399:1873518 [0] NCCL INFO Using network IB
lrdn1042:1873399:1873518 [0] NCCL INFO ncclCommInitRankScalable comm 0x20907330 rank 188 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0942:2880356:2880356 [0] NCCL INFO Comm config Blocking set to 1
lrdn0942:2880356:2880460 [0] NCCL INFO Using network IB
lrdn0942:2880356:2880460 [0] NCCL INFO ncclCommInitRankScalable comm 0x214d8af0 rank 168 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0480:1479637:1479637 [0] NCCL INFO Comm config Blocking set to 1
lrdn0480:1479637:1479745 [0] NCCL INFO Using network IB
lrdn0480:1479637:1479745 [0] NCCL INFO ncclCommInitRankScalable comm 0xd905af0 rank 66 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0834:1549970:1549970 [0] NCCL INFO Comm config Blocking set to 1
lrdn0834:1549970:1550076 [0] NCCL INFO Using network IB
lrdn0834:1549970:1550076 [0] NCCL INFO ncclCommInitRankScalable comm 0xde658f0 rank 139 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0696:1664741:1664861 [0] NCCL INFO Using network IB
lrdn0696:1664741:1664861 [0] NCCL INFO ncclCommInitRankScalable comm 0x219fa070 rank 116 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1001:490760:490760 [0] NCCL INFO Comm config Blocking set to 1
lrdn1001:490760:490879 [0] NCCL INFO Using network IB
lrdn1001:490760:490879 [0] NCCL INFO ncclCommInitRankScalable comm 0xe415590 rank 181 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0387:1604830:1604938 [0] NCCL INFO Using network IB
lrdn0387:1604830:1604938 [0] NCCL INFO ncclCommInitRankScalable comm 0xd0d75b0 rank 53 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0618:1830054:1830054 [0] NCCL INFO Comm config Blocking set to 1
lrdn0618:1830054:1830171 [0] NCCL INFO Using network IB
lrdn0618:1830054:1830171 [0] NCCL INFO ncclCommInitRankScalable comm 0xcade8c0 rank 97 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0288:1681676:1681676 [0] NCCL INFO Comm config Blocking set to 1
lrdn0288:1681676:1681782 [0] NCCL INFO Using network IB
lrdn0288:1681676:1681782 [0] NCCL INFO ncclCommInitRankScalable comm 0x16c499c0 rank 38 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0475:1717583:1717583 [0] NCCL INFO Comm config Blocking set to 1
lrdn0475:1717583:1717688 [0] NCCL INFO Using network IB
lrdn0475:1717583:1717688 [0] NCCL INFO ncclCommInitRankScalable comm 0xe34ee90 rank 63 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0486:1880055:1880055 [0] NCCL INFO Comm config Blocking set to 1
lrdn0486:1880055:1880242 [0] NCCL INFO Using network IB
lrdn0486:1880055:1880242 [0] NCCL INFO ncclCommInitRankScalable comm 0x20a63070 rank 68 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0499:1654785:1654785 [0] NCCL INFO Comm config Blocking set to 1
lrdn0499:1654785:1654889 [0] NCCL INFO Using network IB
lrdn0499:1654785:1654889 [0] NCCL INFO ncclCommInitRankScalable comm 0x31577f80 rank 70 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0739:2688192:2688192 [0] NCCL INFO Comm config Blocking set to 1
lrdn0739:2688192:2688313 [0] NCCL INFO Using network IB
lrdn0739:2688192:2688313 [0] NCCL INFO ncclCommInitRankScalable comm 0x223646a0 rank 125 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1419:2150084:2150084 [0] NCCL INFO Comm config Blocking set to 1
lrdn1043:3601459:3601459 [0] NCCL INFO Comm config Blocking set to 1
lrdn1131:3988070:3988070 [0] NCCL INFO Comm config Blocking set to 1
lrdn0179:2350380:2350380 [0] NCCL INFO Comm config Blocking set to 1
lrdn1005:1506067:1506067 [0] NCCL INFO Comm config Blocking set to 1
lrdn1005:1506067:1506174 [0] NCCL INFO Using network IB
lrdn1005:1506067:1506174 [0] NCCL INFO ncclCommInitRankScalable comm 0x1824bbe0 rank 183 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0089:3581058:3581058 [0] NCCL INFO Comm config Blocking set to 1
lrdn0089:3581058:3581165 [0] NCCL INFO Using network IB
lrdn1121:3096513:3096513 [0] NCCL INFO Comm config Blocking set to 1
lrdn1157:1572120:1572120 [0] NCCL INFO Comm config Blocking set to 1
lrdn0916:3269502:3269502 [0] NCCL INFO Comm config Blocking set to 1
lrdn0916:3269502:3269640 [0] NCCL INFO Using network IB
lrdn0916:3269502:3269640 [0] NCCL INFO ncclCommInitRankScalable comm 0x1536cc10 rank 160 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0204:1656830:1656830 [0] NCCL INFO Comm config Blocking set to 1
lrdn0204:1656830:1657018 [0] NCCL INFO Using network IB
lrdn0204:1656830:1657018 [0] NCCL INFO ncclCommInitRankScalable comm 0x211171a0 rank 24 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0194:3045078:3045078 [0] NCCL INFO Comm config Blocking set to 1
lrdn0194:3045078:3045185 [0] NCCL INFO Using network IB
lrdn0194:3045078:3045185 [0] NCCL INFO ncclCommInitRankScalable comm 0xfc130a0 rank 22 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1141:1960198:1960198 [0] NCCL INFO Comm config Blocking set to 1
lrdn1413:1703007:1703007 [0] NCCL INFO Comm config Blocking set to 1
lrdn1413:1703007:1703112 [0] NCCL INFO Using network IB
lrdn0542:1578195:1578301 [0] NCCL INFO Using network IB
lrdn0542:1578195:1578301 [0] NCCL INFO ncclCommInitRankScalable comm 0xcd97b10 rank 80 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1104:2727035:2727035 [0] NCCL INFO Comm config Blocking set to 1
lrdn0014:3257777:3257777 [0] NCCL INFO Comm config Blocking set to 1
lrdn0014:3257777:3257891 [0] NCCL INFO Using network IB
lrdn0014:3257777:3257891 [0] NCCL INFO ncclCommInitRankScalable comm 0xd6c6920 rank 0 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1120:1800335:1800335 [0] NCCL INFO Comm config Blocking set to 1
lrdn0318:1611974:1611974 [0] NCCL INFO Comm config Blocking set to 1
lrdn0200:1632217:1632217 [0] NCCL INFO Comm config Blocking set to 1
lrdn0740:4052818:4052818 [0] NCCL INFO Comm config Blocking set to 1
lrdn0283:1744962:1744962 [0] NCCL INFO Comm config Blocking set to 1
lrdn1145:1701595:1701595 [0] NCCL INFO Comm config Blocking set to 1
lrdn0888:298444:298444 [0] NCCL INFO Comm config Blocking set to 1
lrdn0850:3805411:3805411 [0] NCCL INFO Comm config Blocking set to 1
lrdn1053:3252281:3252281 [0] NCCL INFO Comm config Blocking set to 1
lrdn0230:167967:167967 [0] NCCL INFO Comm config Blocking set to 1
lrdn1090:1739574:1739574 [0] NCCL INFO Comm config Blocking set to 1
lrdn1419:2150084:2150207 [0] NCCL INFO Using network IB
lrdn1419:2150084:2150207 [0] NCCL INFO ncclCommInitRankScalable comm 0xe0a77c0 rank 252 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0855:172542:172542 [0] NCCL INFO Comm config Blocking set to 1
lrdn0178:2619425:2619425 [0] NCCL INFO Comm config Blocking set to 1
lrdn1043:3601459:3601575 [0] NCCL INFO Using network IB
lrdn1061:1436280:1436280 [0] NCCL INFO Comm config Blocking set to 1
lrdn1054:163779:163779 [0] NCCL INFO Comm config Blocking set to 1
lrdn1201:1761739:1761739 [0] NCCL INFO Comm config Blocking set to 1
lrdn1201:1761739:1761847 [0] NCCL INFO Using network IB
lrdn1201:1761739:1761847 [0] NCCL INFO ncclCommInitRankScalable comm 0x12178710 rank 220 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0089:3581058:3581165 [0] NCCL INFO ncclCommInitRankScalable comm 0xff02e30 rank 6 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1121:3096513:3096632 [0] NCCL INFO Using network IB
lrdn1121:3096513:3096632 [0] NCCL INFO ncclCommInitRankScalable comm 0x167dabd0 rank 205 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1157:1572120:1572225 [0] NCCL INFO Using network IB
lrdn1112:1633009:1633009 [0] NCCL INFO Comm config Blocking set to 1
lrdn0193:1561269:1561269 [0] NCCL INFO Comm config Blocking set to 1
lrdn1413:1703007:1703112 [0] NCCL INFO ncclCommInitRankScalable comm 0x16db58d0 rank 251 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0238:1761357:1761357 [0] NCCL INFO Comm config Blocking set to 1
lrdn0267:2029357:2029357 [0] NCCL INFO Comm config Blocking set to 1
lrdn0267:2029357:2029466 [0] NCCL INFO Using network IB
lrdn0020:1671373:1671373 [0] NCCL INFO Comm config Blocking set to 1
lrdn0020:1671373:1671480 [0] NCCL INFO Using network IB
lrdn0177:1599647:1599647 [0] NCCL INFO Comm config Blocking set to 1
lrdn0177:1599647:1599763 [0] NCCL INFO Using network IB
lrdn0177:1599647:1599763 [0] NCCL INFO ncclCommInitRankScalable comm 0xd1d7510 rank 18 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1120:1800335:1800443 [0] NCCL INFO Using network IB
lrdn1120:1800335:1800443 [0] NCCL INFO ncclCommInitRankScalable comm 0x218e93f0 rank 204 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0318:1611974:1612079 [0] NCCL INFO Using network IB
lrdn0318:1611974:1612079 [0] NCCL INFO ncclCommInitRankScalable comm 0xcfd1bf0 rank 46 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0200:1632217:1632323 [0] NCCL INFO Using network IB
lrdn0200:1632217:1632323 [0] NCCL INFO ncclCommInitRankScalable comm 0x13c8dcb0 rank 23 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0740:4052818:4052940 [0] NCCL INFO Using network IB
lrdn0740:4052818:4052940 [0] NCCL INFO ncclCommInitRankScalable comm 0xd07ea60 rank 126 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0283:1744962:1745067 [0] NCCL INFO Using network IB
lrdn0283:1744962:1745067 [0] NCCL INFO ncclCommInitRankScalable comm 0xdead070 rank 36 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1145:1701595:1701713 [0] NCCL INFO Using network IB
lrdn1145:1701595:1701713 [0] NCCL INFO ncclCommInitRankScalable comm 0x2ed6f390 rank 209 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0888:298444:298550 [0] NCCL INFO Using network IB
lrdn0888:298444:298550 [0] NCCL INFO ncclCommInitRankScalable comm 0x2a93ba80 rank 152 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0850:3805411:3805520 [0] NCCL INFO Using network IB
lrdn0850:3805411:3805520 [0] NCCL INFO ncclCommInitRankScalable comm 0xcffaeb0 rank 143 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0877:1585785:1585785 [0] NCCL INFO Comm config Blocking set to 1
lrdn0877:1585785:1585893 [0] NCCL INFO Using network IB
lrdn0877:1585785:1585893 [0] NCCL INFO ncclCommInitRankScalable comm 0xdd82f90 rank 151 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1053:3252281:3252401 [0] NCCL INFO Using network IB
lrdn1053:3252281:3252401 [0] NCCL INFO ncclCommInitRankScalable comm 0x16e73970 rank 190 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0230:167967:168086 [0] NCCL INFO Using network IB
lrdn0230:167967:168086 [0] NCCL INFO ncclCommInitRankScalable comm 0x148c29e0 rank 29 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1090:1739574:1739682 [0] NCCL INFO Using network IB
lrdn1090:1739574:1739682 [0] NCCL INFO ncclCommInitRankScalable comm 0x221985f0 rank 199 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0855:172542:172676 [0] NCCL INFO Using network IB
lrdn0855:172542:172676 [0] NCCL INFO ncclCommInitRankScalable comm 0x1228f610 rank 146 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0178:2619425:2619545 [0] NCCL INFO Using network IB
lrdn0178:2619425:2619545 [0] NCCL INFO ncclCommInitRankScalable comm 0x2a969190 rank 19 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1043:3601459:3601575 [0] NCCL INFO ncclCommInitRankScalable comm 0x15cb7fa0 rank 189 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0175:1493980:1493980 [0] NCCL INFO Comm config Blocking set to 1
lrdn0175:1493980:1494088 [0] NCCL INFO Using network IB
lrdn0175:1493980:1494088 [0] NCCL INFO ncclCommInitRankScalable comm 0xce36f30 rank 17 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1061:1436280:1436384 [0] NCCL INFO Using network IB
lrdn1061:1436280:1436384 [0] NCCL INFO ncclCommInitRankScalable comm 0xe4f01c0 rank 192 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1054:163779:163895 [0] NCCL INFO Using network IB
lrdn1054:163779:163895 [0] NCCL INFO ncclCommInitRankScalable comm 0x214b4940 rank 191 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0295:1874970:1874970 [0] NCCL INFO Comm config Blocking set to 1
lrdn0295:1874970:1875158 [0] NCCL INFO Using network IB
lrdn0295:1874970:1875158 [0] NCCL INFO ncclCommInitRankScalable comm 0x10fdbe80 rank 40 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1157:1572120:1572225 [0] NCCL INFO ncclCommInitRankScalable comm 0xcf98550 rank 211 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1111:1712948:1712948 [0] NCCL INFO Comm config Blocking set to 1
lrdn1111:1712948:1713067 [0] NCCL INFO Using network IB
lrdn1111:1712948:1713067 [0] NCCL INFO ncclCommInitRankScalable comm 0x21b7a500 rank 202 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0598:2941148:2941148 [0] NCCL INFO Comm config Blocking set to 1
lrdn0598:2941148:2941267 [0] NCCL INFO Using network IB
lrdn0598:2941148:2941267 [0] NCCL INFO ncclCommInitRankScalable comm 0xe7c4d60 rank 91 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0170:1950768:1950768 [0] NCCL INFO Comm config Blocking set to 1
lrdn0170:1950768:1950953 [0] NCCL INFO Using network IB
lrdn0416:1594081:1594081 [0] NCCL INFO Comm config Blocking set to 1
lrdn0416:1594081:1594267 [0] NCCL INFO Using network IB
lrdn0416:1594081:1594267 [0] NCCL INFO ncclCommInitRankScalable comm 0x1765b0d0 rank 59 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1112:1633009:1633115 [0] NCCL INFO Using network IB
lrdn1112:1633009:1633115 [0] NCCL INFO ncclCommInitRankScalable comm 0xdef6750 rank 203 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0193:1561269:1561374 [0] NCCL INFO Using network IB
lrdn0193:1561269:1561374 [0] NCCL INFO ncclCommInitRankScalable comm 0x212a6e50 rank 21 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0238:1761357:1761464 [0] NCCL INFO Using network IB
lrdn0238:1761357:1761464 [0] NCCL INFO ncclCommInitRankScalable comm 0xcea7620 rank 30 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1318:2081362:2081362 [0] NCCL INFO Comm config Blocking set to 1
lrdn0485:1537110:1537110 [0] NCCL INFO Comm config Blocking set to 1
lrdn0485:1537110:1537233 [0] NCCL INFO Using network IB
lrdn0485:1537110:1537233 [0] NCCL INFO ncclCommInitRankScalable comm 0x213e8100 rank 67 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0267:2029357:2029466 [0] NCCL INFO ncclCommInitRankScalable comm 0xca7af10 rank 34 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0583:1639762:1639762 [0] NCCL INFO Comm config Blocking set to 1
lrdn0583:1639762:1639882 [0] NCCL INFO Using network IB
lrdn0583:1639762:1639882 [0] NCCL INFO ncclCommInitRankScalable comm 0xddbc600 rank 87 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1261:2025781:2025781 [0] NCCL INFO Comm config Blocking set to 1
lrdn1261:2025781:2025887 [0] NCCL INFO Using network IB
lrdn1261:2025781:2025887 [0] NCCL INFO ncclCommInitRankScalable comm 0x20d17fc0 rank 232 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0020:1671373:1671480 [0] NCCL INFO ncclCommInitRankScalable comm 0x17b98300 rank 1 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1104:2727035:2727141 [0] NCCL INFO Using network IB
lrdn1104:2727035:2727141 [0] NCCL INFO ncclCommInitRankScalable comm 0x21378580 rank 201 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0205:2793889:2793889 [0] NCCL INFO Comm config Blocking set to 1
lrdn0205:2793889:2794010 [0] NCCL INFO Using network IB
lrdn0205:2793889:2794010 [0] NCCL INFO ncclCommInitRankScalable comm 0x2a15e0c0 rank 25 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0479:600014:600014 [0] NCCL INFO Comm config Blocking set to 1
lrdn0479:600014:600118 [0] NCCL INFO Using network IB
lrdn0479:600014:600118 [0] NCCL INFO ncclCommInitRankScalable comm 0xca25540 rank 65 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1232:2814890:2814890 [0] NCCL INFO Comm config Blocking set to 1
lrdn1232:2814890:2815076 [0] NCCL INFO Using network IB
lrdn1232:2814890:2815076 [0] NCCL INFO ncclCommInitRankScalable comm 0x22511b90 rank 226 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0648:1396499:1396499 [0] NCCL INFO Comm config Blocking set to 1
lrdn0648:1396499:1396616 [0] NCCL INFO Using network IB
lrdn0648:1396499:1396616 [0] NCCL INFO ncclCommInitRankScalable comm 0xe728890 rank 106 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1234:1664788:1664788 [0] NCCL INFO Comm config Blocking set to 1
lrdn1234:1664788:1664906 [0] NCCL INFO Using network IB
lrdn1234:1664788:1664906 [0] NCCL INFO ncclCommInitRankScalable comm 0x1686aa40 rank 227 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0975:1356366:1356366 [0] NCCL INFO Comm config Blocking set to 1
lrdn0975:1356366:1356472 [0] NCCL INFO Using network IB
lrdn0975:1356366:1356472 [0] NCCL INFO ncclCommInitRankScalable comm 0xd86ff00 rank 176 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0223:1701386:1701386 [0] NCCL INFO Comm config Blocking set to 1
lrdn0223:1701386:1701508 [0] NCCL INFO Using network IB
lrdn0223:1701386:1701508 [0] NCCL INFO ncclCommInitRankScalable comm 0xe130f50 rank 27 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0672:2798667:2798667 [0] NCCL INFO Comm config Blocking set to 1
lrdn0672:2798667:2798785 [0] NCCL INFO Using network IB
lrdn0672:2798667:2798785 [0] NCCL INFO ncclCommInitRankScalable comm 0x2be24210 rank 111 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1141:1960198:1960317 [0] NCCL INFO Using network IB
lrdn1141:1960198:1960317 [0] NCCL INFO ncclCommInitRankScalable comm 0x173c2010 rank 208 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0717:2059502:2059502 [0] NCCL INFO Comm config Blocking set to 1
lrdn0717:2059502:2059622 [0] NCCL INFO Using network IB
lrdn0717:2059502:2059622 [0] NCCL INFO ncclCommInitRankScalable comm 0x21161480 rank 122 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1245:1634563:1634563 [0] NCCL INFO Comm config Blocking set to 1
lrdn1245:1634563:1634682 [0] NCCL INFO Using network IB
lrdn1245:1634563:1634682 [0] NCCL INFO ncclCommInitRankScalable comm 0x1781a980 rank 230 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0791:3812973:3812973 [0] NCCL INFO Comm config Blocking set to 1
lrdn0791:3812973:3813106 [0] NCCL INFO Using network IB
lrdn0791:3812973:3813106 [0] NCCL INFO ncclCommInitRankScalable comm 0x22077ac0 rank 132 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0179:2350380:2350486 [0] NCCL INFO Using network IB
lrdn0179:2350380:2350486 [0] NCCL INFO ncclCommInitRankScalable comm 0xd691d30 rank 20 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0970:779132:779132 [0] NCCL INFO Comm config Blocking set to 1
lrdn0970:779132:779239 [0] NCCL INFO Using network IB
lrdn0970:779132:779239 [0] NCCL INFO ncclCommInitRankScalable comm 0x11e1eba0 rank 174 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0254:1202236:1202236 [0] NCCL INFO Comm config Blocking set to 1
lrdn0254:1202236:1202354 [0] NCCL INFO Using network IB
lrdn0254:1202236:1202354 [0] NCCL INFO ncclCommInitRankScalable comm 0xd3734a0 rank 32 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0523:1525899:1526034 [0] NCCL INFO Using network IB
lrdn0523:1525899:1526034 [0] NCCL INFO ncclCommInitRankScalable comm 0x20c6cbd0 rank 74 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1131:3988070:3988190 [0] NCCL INFO Using network IB
lrdn1131:3988070:3988190 [0] NCCL INFO ncclCommInitRankScalable comm 0x10e38f50 rank 206 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0951:1448128:1448128 [0] NCCL INFO Comm config Blocking set to 1
lrdn0951:1448128:1448249 [0] NCCL INFO Using network IB
lrdn0951:1448128:1448249 [0] NCCL INFO ncclCommInitRankScalable comm 0x214449f0 rank 169 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1376:1889406:1889406 [0] NCCL INFO Comm config Blocking set to 1
lrdn1376:1889406:1889526 [0] NCCL INFO Using network IB
lrdn1376:1889406:1889526 [0] NCCL INFO ncclCommInitRankScalable comm 0x16af5350 rank 248 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0992:1403591:1403591 [0] NCCL INFO Comm config Blocking set to 1
lrdn1290:1434779:1434779 [0] NCCL INFO Comm config Blocking set to 1
lrdn1290:1434779:1434887 [0] NCCL INFO Using network IB
lrdn1290:1434779:1434887 [0] NCCL INFO ncclCommInitRankScalable comm 0xe1d1ba0 rank 237 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0727:2905734:2905734 [0] NCCL INFO Comm config Blocking set to 1
lrdn1240:1764188:1764188 [0] NCCL INFO Comm config Blocking set to 1
lrdn1240:1764188:1764305 [0] NCCL INFO Using network IB
lrdn1240:1764188:1764305 [0] NCCL INFO ncclCommInitRankScalable comm 0x11c94620 rank 228 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0642:1597688:1597793 [0] NCCL INFO Using network IB
lrdn0642:1597688:1597793 [0] NCCL INFO ncclCommInitRankScalable comm 0x15a33b10 rank 104 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0170:1950768:1950953 [0] NCCL INFO ncclCommInitRankScalable comm 0x16ccd1a0 rank 16 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0506:2407304:2407304 [0] NCCL INFO Comm config Blocking set to 1
lrdn0506:2407304:2407425 [0] NCCL INFO Using network IB
lrdn0506:2407304:2407425 [0] NCCL INFO ncclCommInitRankScalable comm 0xd947b90 rank 71 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0147:1522198:1522198 [0] NCCL INFO Comm config Blocking set to 1
lrdn0147:1522198:1522303 [0] NCCL INFO Using network IB
lrdn0147:1522198:1522303 [0] NCCL INFO ncclCommInitRankScalable comm 0x2fce1e10 rank 12 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0326:1692035:1692035 [0] NCCL INFO Comm config Blocking set to 1
lrdn1271:1865323:1865323 [0] NCCL INFO Comm config Blocking set to 1
lrdn1271:1865323:1865443 [0] NCCL INFO Using network IB
lrdn1271:1865323:1865443 [0] NCCL INFO ncclCommInitRankScalable comm 0x1629ac50 rank 234 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0297:281832:281832 [0] NCCL INFO Comm config Blocking set to 1
lrdn1152:3008174:3008174 [0] NCCL INFO Comm config Blocking set to 1
lrdn1152:3008174:3008280 [0] NCCL INFO Using network IB
lrdn1152:3008174:3008280 [0] NCCL INFO ncclCommInitRankScalable comm 0x116a4ea0 rank 210 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0746:1587636:1587636 [0] NCCL INFO Comm config Blocking set to 1
lrdn0746:1587636:1587741 [0] NCCL INFO Using network IB
lrdn0746:1587636:1587741 [0] NCCL INFO ncclCommInitRankScalable comm 0x217c24f0 rank 127 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1318:2081362:2081479 [0] NCCL INFO Using network IB
lrdn1318:2081362:2081479 [0] NCCL INFO ncclCommInitRankScalable comm 0xd47b100 rank 242 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0763:1858216:1858216 [0] NCCL INFO Comm config Blocking set to 1
lrdn0763:1858216:1858321 [0] NCCL INFO Using network IB
lrdn0763:1858216:1858321 [0] NCCL INFO ncclCommInitRankScalable comm 0x224e0f80 rank 128 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1221:1683744:1683744 [0] NCCL INFO Comm config Blocking set to 1
lrdn1221:1683744:1683866 [0] NCCL INFO Using network IB
lrdn1221:1683744:1683866 [0] NCCL INFO ncclCommInitRankScalable comm 0x15a407f0 rank 224 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0842:3750746:3750746 [0] NCCL INFO Comm config Blocking set to 1
lrdn0842:3750746:3750852 [0] NCCL INFO Using network IB
lrdn0842:3750746:3750852 [0] NCCL INFO ncclCommInitRankScalable comm 0x17641250 rank 140 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1223:1586205:1586205 [0] NCCL INFO Comm config Blocking set to 1
lrdn1223:1586205:1586313 [0] NCCL INFO Using network IB
lrdn1223:1586205:1586313 [0] NCCL INFO ncclCommInitRankScalable comm 0x224b50c0 rank 225 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0367:1637221:1637221 [0] NCCL INFO Comm config Blocking set to 1
lrdn0367:1637221:1637329 [0] NCCL INFO Using network IB
lrdn0367:1637221:1637329 [0] NCCL INFO ncclCommInitRankScalable comm 0xe51e170 rank 51 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1262:1584708:1584708 [0] NCCL INFO Comm config Blocking set to 1
lrdn1262:1584708:1584826 [0] NCCL INFO Using network IB
lrdn1262:1584708:1584826 [0] NCCL INFO ncclCommInitRankScalable comm 0x21c6d0d0 rank 233 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0927:1653016:1653016 [0] NCCL INFO Comm config Blocking set to 1
lrdn1162:1559282:1559282 [0] NCCL INFO Comm config Blocking set to 1
lrdn1162:1559282:1559390 [0] NCCL INFO Using network IB
lrdn1162:1559282:1559390 [0] NCCL INFO ncclCommInitRankScalable comm 0x31d6e2f0 rank 212 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1006:1697051:1697051 [0] NCCL INFO Comm config Blocking set to 1
lrdn1191:1706749:1706749 [0] NCCL INFO Comm config Blocking set to 1
lrdn1191:1706749:1706855 [0] NCCL INFO Using network IB
lrdn1191:1706749:1706855 [0] NCCL INFO ncclCommInitRankScalable comm 0x10d96eb0 rank 219 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0923:1505817:1505817 [0] NCCL INFO Comm config Blocking set to 1
lrdn1211:2766564:2766564 [0] NCCL INFO Comm config Blocking set to 1
lrdn1211:2766564:2766680 [0] NCCL INFO Using network IB
lrdn1211:2766564:2766680 [0] NCCL INFO ncclCommInitRankScalable comm 0x2a2f2880 rank 223 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0905:2134816:2134816 [0] NCCL INFO Comm config Blocking set to 1
lrdn0905:2134816:2134922 [0] NCCL INFO Using network IB
lrdn0905:2134816:2134922 [0] NCCL INFO ncclCommInitRankScalable comm 0x21878de0 rank 157 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1324:788381:788381 [0] NCCL INFO Comm config Blocking set to 1
lrdn1324:788381:788487 [0] NCCL INFO Using network IB
lrdn1324:788381:788487 [0] NCCL INFO ncclCommInitRankScalable comm 0x21b4d0b0 rank 243 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0110:4136466:4136466 [0] NCCL INFO Comm config Blocking set to 1
lrdn0038:667426:667426 [0] NCCL INFO Comm config Blocking set to 1
lrdn0038:667426:667533 [0] NCCL INFO Using network IB
lrdn0038:667426:667533 [0] NCCL INFO ncclCommInitRankScalable comm 0x2a9450b0 rank 3 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0941:1513798:1513798 [0] NCCL INFO Comm config Blocking set to 1
lrdn0941:1513798:1513917 [0] NCCL INFO Using network IB
lrdn0941:1513798:1513917 [0] NCCL INFO ncclCommInitRankScalable comm 0x14841a20 rank 167 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0992:1403591:1403697 [0] NCCL INFO Using network IB
lrdn0992:1403591:1403697 [0] NCCL INFO ncclCommInitRankScalable comm 0x112cbe10 rank 180 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0891:1696238:1696238 [0] NCCL INFO Comm config Blocking set to 1
lrdn0891:1696238:1696343 [0] NCCL INFO Using network IB
lrdn0891:1696238:1696343 [0] NCCL INFO ncclCommInitRankScalable comm 0xd9e74c0 rank 154 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0727:2905734:2905841 [0] NCCL INFO Using network IB
lrdn0727:2905734:2905841 [0] NCCL INFO ncclCommInitRankScalable comm 0xd797bb0 rank 124 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0029:1561975:1561975 [0] NCCL INFO Comm config Blocking set to 1
lrdn0326:1692035:1692152 [0] NCCL INFO Using network IB
lrdn0326:1692035:1692152 [0] NCCL INFO ncclCommInitRankScalable comm 0x21daa1a0 rank 47 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0565:3828397:3828503 [0] NCCL INFO ncclCommInitRankScalable comm 0xe682b00 rank 83 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0297:281832:281969 [0] NCCL INFO Using network IB
lrdn0297:281832:281969 [0] NCCL INFO ncclCommInitRankScalable comm 0xd57e130 rank 41 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0114:1700246:1700246 [0] NCCL INFO Comm config Blocking set to 1
lrdn0927:1653016:1653213 [0] NCCL INFO Using network IB
lrdn0927:1653016:1653213 [0] NCCL INFO ncclCommInitRankScalable comm 0x20932df0 rank 163 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1181:21073:21073 [0] NCCL INFO Comm config Blocking set to 1
lrdn1181:21073:21179 [0] NCCL INFO Using network IB
lrdn1181:21073:21179 [0] NCCL INFO ncclCommInitRankScalable comm 0x2182c330 rank 217 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1006:1697051:1697156 [0] NCCL INFO Using network IB
lrdn1006:1697051:1697156 [0] NCCL INFO ncclCommInitRankScalable comm 0x15febd40 rank 184 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1375:1499504:1499504 [0] NCCL INFO Comm config Blocking set to 1
lrdn0923:1505817:1505924 [0] NCCL INFO Using network IB
lrdn0923:1505817:1505924 [0] NCCL INFO ncclCommInitRankScalable comm 0xd002f50 rank 162 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0458:1951839:1951839 [0] NCCL INFO Comm config Blocking set to 1
lrdn0458:1951839:1951958 [0] NCCL INFO Using network IB
lrdn0458:1951839:1951958 [0] NCCL INFO ncclCommInitRankScalable comm 0x180e8e00 rank 62 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0110:4136466:4136583 [0] NCCL INFO Using network IB
lrdn0110:4136466:4136583 [0] NCCL INFO ncclCommInitRankScalable comm 0x17ae82d0 rank 8 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1029:3237462:3237462 [0] NCCL INFO Comm config Blocking set to 1
lrdn0029:1561975:1562082 [0] NCCL INFO Using network IB
lrdn0029:1561975:1562082 [0] NCCL INFO ncclCommInitRankScalable comm 0xd0368f0 rank 2 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0248:1992477:1992477 [0] NCCL INFO Comm config Blocking set to 1
lrdn0114:1700246:1700350 [0] NCCL INFO Using network IB
lrdn0114:1700246:1700350 [0] NCCL INFO ncclCommInitRankScalable comm 0x164c45b0 rank 9 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1179:3818673:3818673 [0] NCCL INFO Comm config Blocking set to 1
lrdn1375:1499504:1499625 [0] NCCL INFO Using network IB
lrdn1375:1499504:1499625 [0] NCCL INFO ncclCommInitRankScalable comm 0xcff0800 rank 247 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0936:1846638:1846638 [0] NCCL INFO Comm config Blocking set to 1
lrdn0936:1846638:1846757 [0] NCCL INFO Using network IB
lrdn0936:1846638:1846757 [0] NCCL INFO ncclCommInitRankScalable comm 0x2a447900 rank 166 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1029:3237462:3237582 [0] NCCL INFO Using network IB
lrdn1029:3237462:3237582 [0] NCCL INFO ncclCommInitRankScalable comm 0xdb1f760 rank 186 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0875:123346:123346 [0] NCCL INFO Comm config Blocking set to 1
lrdn0875:123346:123463 [0] NCCL INFO Using network IB
lrdn0875:123346:123463 [0] NCCL INFO ncclCommInitRankScalable comm 0x1659e400 rank 150 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0248:1992477:1992583 [0] NCCL INFO Using network IB
lrdn0248:1992477:1992583 [0] NCCL INFO ncclCommInitRankScalable comm 0x164132c0 rank 31 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0852:1613610:1613610 [0] NCCL INFO Comm config Blocking set to 1
lrdn0852:1613610:1613716 [0] NCCL INFO Using network IB
lrdn0852:1613610:1613716 [0] NCCL INFO ncclCommInitRankScalable comm 0x171eb9e0 rank 144 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1179:3818673:3818859 [0] NCCL INFO Using network IB
lrdn1179:3818673:3818859 [0] NCCL INFO ncclCommInitRankScalable comm 0x2ba0a0c0 rank 216 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0305:1129623:1129623 [0] NCCL INFO Comm config Blocking set to 1
lrdn0305:1129623:1129729 [0] NCCL INFO Using network IB
lrdn0305:1129623:1129729 [0] NCCL INFO ncclCommInitRankScalable comm 0xe7a5730 rank 44 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0819:1714151:1714151 [0] NCCL INFO Comm config Blocking set to 1
lrdn0819:1714151:1714275 [0] NCCL INFO Using network IB
lrdn0819:1714151:1714275 [0] NCCL INFO ncclCommInitRankScalable comm 0x21a28a90 rank 135 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0933:2108154:2108154 [0] NCCL INFO Comm config Blocking set to 1
lrdn0907:3310455:3310455 [0] NCCL INFO Comm config Blocking set to 1
lrdn0933:2108154:2108270 [0] NCCL INFO Using network IB
lrdn0933:2108154:2108270 [0] NCCL INFO ncclCommInitRankScalable comm 0xc9cf050 rank 165 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1246:1672375:1672375 [0] NCCL INFO Comm config Blocking set to 1
lrdn0907:3310455:3310574 [0] NCCL INFO Using network IB
lrdn0907:3310455:3310574 [0] NCCL INFO ncclCommInitRankScalable comm 0xe845b20 rank 158 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0972:1487882:1487882 [0] NCCL INFO Comm config Blocking set to 1
lrdn0972:1487882:1487989 [0] NCCL INFO Using network IB
lrdn0872:536285:536285 [0] NCCL INFO Comm config Blocking set to 1
lrdn0872:536285:536420 [0] NCCL INFO Using network IB
lrdn0872:536285:536420 [0] NCCL INFO ncclCommInitRankScalable comm 0x20cde580 rank 149 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1246:1672375:1672497 [0] NCCL INFO Using network IB
lrdn1246:1672375:1672497 [0] NCCL INFO ncclCommInitRankScalable comm 0xe1578c0 rank 231 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1171:1612854:1612854 [0] NCCL INFO Comm config Blocking set to 1
lrdn1171:1612854:1612977 [0] NCCL INFO Using network IB
lrdn1171:1612854:1612977 [0] NCCL INFO ncclCommInitRankScalable comm 0xd4b3530 rank 214 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0972:1487882:1487989 [0] NCCL INFO ncclCommInitRankScalable comm 0x22332230 rank 175 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0865:1721410:1721410 [0] NCCL INFO Comm config Blocking set to 1
lrdn0117:3684502:3684502 [0] NCCL INFO Comm config Blocking set to 1
lrdn0865:1721410:1721532 [0] NCCL INFO Using network IB
lrdn0865:1721410:1721532 [0] NCCL INFO ncclCommInitRankScalable comm 0xcb93830 rank 148 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0090:1669266:1669266 [0] NCCL INFO Comm config Blocking set to 1
lrdn0117:3684502:3684624 [0] NCCL INFO Using network IB
lrdn0117:3684502:3684624 [0] NCCL INFO ncclCommInitRankScalable comm 0x2aeb32d0 rank 10 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1389:1876419:1876419 [0] NCCL INFO Comm config Blocking set to 1
lrdn0090:1669266:1669371 [0] NCCL INFO Using network IB
lrdn0090:1669266:1669371 [0] NCCL INFO ncclCommInitRankScalable comm 0x1764ed60 rank 7 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0912:2074724:2074724 [0] NCCL INFO Comm config Blocking set to 1
lrdn1389:1876419:1876536 [0] NCCL INFO Using network IB
lrdn1389:1876419:1876536 [0] NCCL INFO ncclCommInitRankScalable comm 0x127d7fc0 rank 249 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1331:1964040:1964040 [0] NCCL INFO Comm config Blocking set to 1
lrdn0912:2074724:2074842 [0] NCCL INFO Using network IB
lrdn0912:2074724:2074842 [0] NCCL INFO ncclCommInitRankScalable comm 0x12cae050 rank 159 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1335:2311607:2311607 [0] NCCL INFO Comm config Blocking set to 1
lrdn1331:1964040:1964157 [0] NCCL INFO Using network IB
lrdn1331:1964040:1964157 [0] NCCL INFO ncclCommInitRankScalable comm 0x173f40e0 rank 244 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0080:2202741:2202741 [0] NCCL INFO Comm config Blocking set to 1
lrdn1335:2311607:2311742 [0] NCCL INFO Using network IB
lrdn1335:2311607:2311742 [0] NCCL INFO ncclCommInitRankScalable comm 0x12371bf0 rank 245 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0360:1607343:1607343 [0] NCCL INFO Comm config Blocking set to 1
lrdn0360:1607343:1607449 [0] NCCL INFO Using network IB
lrdn0360:1607343:1607449 [0] NCCL INFO ncclCommInitRankScalable comm 0x15bae9a0 rank 50 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0080:2202741:2202862 [0] NCCL INFO Using network IB
lrdn0080:2202741:2202862 [0] NCCL INFO ncclCommInitRankScalable comm 0x20f832d0 rank 5 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0716:1747185:1747185 [0] NCCL INFO Comm config Blocking set to 1
lrdn0716:1747185:1747303 [0] NCCL INFO Using network IB
lrdn0228:333030:333030 [0] NCCL INFO Comm config Blocking set to 1
lrdn0716:1747185:1747303 [0] NCCL INFO ncclCommInitRankScalable comm 0x169eb780 rank 121 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0266:1530030:1530030 [0] NCCL INFO Comm config Blocking set to 1
lrdn0228:333030:333153 [0] NCCL INFO Using network IB
lrdn0228:333030:333153 [0] NCCL INFO ncclCommInitRankScalable comm 0x16370100 rank 28 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1241:1468891:1468891 [0] NCCL INFO Comm config Blocking set to 1
lrdn0266:1530030:1530135 [0] NCCL INFO Using network IB
lrdn0266:1530030:1530135 [0] NCCL INFO ncclCommInitRankScalable comm 0x20aabe60 rank 33 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1075:2205415:2205415 [0] NCCL INFO Comm config Blocking set to 1
lrdn1241:1468891:1469012 [0] NCCL INFO Using network IB
lrdn1241:1468891:1469012 [0] NCCL INFO ncclCommInitRankScalable comm 0x1238b270 rank 229 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1203:1551578:1551578 [0] NCCL INFO Comm config Blocking set to 1
lrdn1075:2205415:2205532 [0] NCCL INFO Using network IB
lrdn1075:2205415:2205532 [0] NCCL INFO ncclCommInitRankScalable comm 0xcdd2370 rank 195 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0896:1508101:1508101 [0] NCCL INFO Comm config Blocking set to 1
lrdn1203:1551578:1551694 [0] NCCL INFO Using network IB
lrdn1203:1551578:1551694 [0] NCCL INFO ncclCommInitRankScalable comm 0xcb7f550 rank 221 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1172:1854740:1854740 [0] NCCL INFO Comm config Blocking set to 1
lrdn0896:1508101:1508206 [0] NCCL INFO Using network IB
lrdn0896:1508101:1508206 [0] NCCL INFO ncclCommInitRankScalable comm 0xe2ca880 rank 156 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1172:1854740:1854858 [0] NCCL INFO Using network IB
lrdn1172:1854740:1854858 [0] NCCL INFO ncclCommInitRankScalable comm 0x29e76ec0 rank 215 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0890:2121848:2121848 [0] NCCL INFO Comm config Blocking set to 1
lrdn0060:925357:925357 [0] NCCL INFO Comm config Blocking set to 1
lrdn1002:1707642:1707642 [0] NCCL INFO Comm config Blocking set to 1
lrdn1002:1707642:1707751 [0] NCCL INFO Using network IB
lrdn1002:1707642:1707751 [0] NCCL INFO ncclCommInitRankScalable comm 0x16603d70 rank 182 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0890:2121848:2122034 [0] NCCL INFO Using network IB
lrdn0890:2121848:2122034 [0] NCCL INFO ncclCommInitRankScalable comm 0x172f1c60 rank 153 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1186:2692910:2692910 [0] NCCL INFO Comm config Blocking set to 1
lrdn0060:925357:925477 [0] NCCL INFO Using network IB
lrdn0060:925357:925477 [0] NCCL INFO ncclCommInitRankScalable comm 0x2241bae0 rank 4 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0893:1602515:1602515 [0] NCCL INFO Comm config Blocking set to 1
lrdn1186:2692910:2693019 [0] NCCL INFO Using network IB
lrdn1186:2692910:2693019 [0] NCCL INFO ncclCommInitRankScalable comm 0xce17290 rank 218 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1294:2066796:2066796 [0] NCCL INFO Comm config Blocking set to 1
lrdn0893:1602515:1602620 [0] NCCL INFO Using network IB
lrdn0893:1602515:1602620 [0] NCCL INFO ncclCommInitRankScalable comm 0xcc62fa0 rank 155 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0628:1681364:1681364 [0] NCCL INFO Comm config Blocking set to 1
lrdn0628:1681364:1681471 [0] NCCL INFO Using network IB
lrdn0628:1681364:1681471 [0] NCCL INFO ncclCommInitRankScalable comm 0xe74ac30 rank 100 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1294:2066796:2066912 [0] NCCL INFO Using network IB
lrdn1294:2066796:2066912 [0] NCCL INFO ncclCommInitRankScalable comm 0x24ead150 rank 238 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0210:1733148:1733148 [0] NCCL INFO Comm config Blocking set to 1
lrdn1438:3364708:3364708 [0] NCCL INFO Comm config Blocking set to 1
lrdn1438:3364708:3364832 [0] NCCL INFO Using network IB
lrdn0210:1733148:1733256 [0] NCCL INFO Using network IB
lrdn0210:1733148:1733256 [0] NCCL INFO ncclCommInitRankScalable comm 0x2a71a6d0 rank 26 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0270:1598262:1598262 [0] NCCL INFO Comm config Blocking set to 1
lrdn1438:3364708:3364832 [0] NCCL INFO ncclCommInitRankScalable comm 0xd7f3ed0 rank 255 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1103:1654006:1654006 [0] NCCL INFO Comm config Blocking set to 1
lrdn0270:1598262:1598369 [0] NCCL INFO Using network IB
lrdn0270:1598262:1598369 [0] NCCL INFO ncclCommInitRankScalable comm 0x222fd160 rank 35 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1295:3087500:3087500 [0] NCCL INFO Comm config Blocking set to 1
lrdn1103:1654006:1654111 [0] NCCL INFO Using network IB
lrdn1103:1654006:1654111 [0] NCCL INFO ncclCommInitRankScalable comm 0x26ae4cf0 rank 200 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1301:1723839:1723839 [0] NCCL INFO Comm config Blocking set to 1
lrdn1295:3087500:3087617 [0] NCCL INFO Using network IB
lrdn1295:3087500:3087617 [0] NCCL INFO ncclCommInitRankScalable comm 0x123cd7d0 rank 239 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1397:2438823:2438823 [0] NCCL INFO Comm config Blocking set to 1
lrdn1301:1723839:1723971 [0] NCCL INFO Using network IB
lrdn1301:1723839:1723971 [0] NCCL INFO ncclCommInitRankScalable comm 0x116cd4c0 rank 240 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0164:1702171:1702171 [0] NCCL INFO Comm config Blocking set to 1
lrdn0164:1702171:1702275 [0] NCCL INFO Using network IB
lrdn0164:1702171:1702275 [0] NCCL INFO ncclCommInitRankScalable comm 0x2bddf2f0 rank 15 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1397:2438823:2438928 [0] NCCL INFO Using network IB
lrdn1397:2438823:2438928 [0] NCCL INFO ncclCommInitRankScalable comm 0x2ad0b100 rank 250 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0150:2596900:2596900 [0] NCCL INFO Comm config Blocking set to 1
lrdn0289:1476853:1476853 [0] NCCL INFO Comm config Blocking set to 1
lrdn0150:2596900:2597005 [0] NCCL INFO Using network IB
lrdn0150:2596900:2597005 [0] NCCL INFO ncclCommInitRankScalable comm 0x139ecf30 rank 13 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1313:368130:368130 [0] NCCL INFO Comm config Blocking set to 1
lrdn1313:368130:368250 [0] NCCL INFO Using network IB
lrdn1313:368130:368250 [0] NCCL INFO ncclCommInitRankScalable comm 0x21108560 rank 241 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0289:1476853:1476960 [0] NCCL INFO Using network IB
lrdn0289:1476853:1476960 [0] NCCL INFO ncclCommInitRankScalable comm 0x2c873dc0 rank 39 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0766:1565834:1565834 [0] NCCL INFO Comm config Blocking set to 1
lrdn0766:1565834:1565956 [0] NCCL INFO Using network IB
lrdn0766:1565834:1565956 [0] NCCL INFO ncclCommInitRankScalable comm 0x292464e0 rank 129 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0134:2712778:2712778 [0] NCCL INFO Comm config Blocking set to 1
lrdn0134:2712778:2712963 [0] NCCL INFO Using network IB
lrdn0134:2712778:2712963 [0] NCCL INFO ncclCommInitRankScalable comm 0xca60900 rank 11 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1027:1745942:1745942 [0] NCCL INFO Comm config Blocking set to 1
lrdn1065:1831945:1831945 [0] NCCL INFO Comm config Blocking set to 1
lrdn1065:1831945:1832051 [0] NCCL INFO Using network IB
lrdn1065:1831945:1832051 [0] NCCL INFO ncclCommInitRankScalable comm 0xcb084d0 rank 193 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1027:1745942:1746061 [0] NCCL INFO Using network IB
lrdn1027:1745942:1746061 [0] NCCL INFO ncclCommInitRankScalable comm 0x20cbe4c0 rank 185 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1276:1691254:1691254 [0] NCCL INFO Comm config Blocking set to 1
lrdn1276:1691254:1691370 [0] NCCL INFO Using network IB
lrdn1276:1691254:1691370 [0] NCCL INFO ncclCommInitRankScalable comm 0x106a12e0 rank 235 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1083:3702380:3702380 [0] NCCL INFO Comm config Blocking set to 1
lrdn1083:3702380:3702499 [0] NCCL INFO Using network IB
lrdn1083:3702380:3702499 [0] NCCL INFO ncclCommInitRankScalable comm 0xc88b2a0 rank 197 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0163:3297581:3297581 [0] NCCL INFO Comm config Blocking set to 1
lrdn0163:3297581:3297689 [0] NCCL INFO Using network IB
lrdn0163:3297581:3297689 [0] NCCL INFO ncclCommInitRankScalable comm 0x22f42f90 rank 14 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1082:1968917:1968917 [0] NCCL INFO Comm config Blocking set to 1
lrdn1082:1968917:1969039 [0] NCCL INFO Using network IB
lrdn1344:1924538:1924538 [0] NCCL INFO Comm config Blocking set to 1
lrdn1344:1924538:1924644 [0] NCCL INFO Using network IB
lrdn1344:1924538:1924644 [0] NCCL INFO ncclCommInitRankScalable comm 0xcb81110 rank 246 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1082:1968917:1969039 [0] NCCL INFO ncclCommInitRankScalable comm 0xd0b2f00 rank 196 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1068:1524729:1524729 [0] NCCL INFO Comm config Blocking set to 1
lrdn1068:1524729:1524836 [0] NCCL INFO Using network IB
lrdn1068:1524729:1524836 [0] NCCL INFO ncclCommInitRankScalable comm 0xf612760 rank 194 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0982:1374856:1374856 [0] NCCL INFO Comm config Blocking set to 1
lrdn0982:1374856:1374965 [0] NCCL INFO Using network IB
lrdn0982:1374856:1374965 [0] NCCL INFO ncclCommInitRankScalable comm 0x2a8acf80 rank 178 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1424:3084189:3084189 [0] NCCL INFO Comm config Blocking set to 1
lrdn1089:1671209:1671209 [0] NCCL INFO Comm config Blocking set to 1
lrdn1089:1671209:1671314 [0] NCCL INFO Using network IB
lrdn1089:1671209:1671314 [0] NCCL INFO ncclCommInitRankScalable comm 0x164b8ea0 rank 198 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1424:3084189:3084293 [0] NCCL INFO Using network IB
lrdn1424:3084189:3084293 [0] NCCL INFO ncclCommInitRankScalable comm 0x2299f880 rank 253 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1283:2320097:2320097 [0] NCCL INFO Comm config Blocking set to 1
lrdn1283:2320097:2320214 [0] NCCL INFO Using network IB
lrdn1283:2320097:2320214 [0] NCCL INFO ncclCommInitRankScalable comm 0x22239480 rank 236 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1428:1483363:1483363 [0] NCCL INFO Comm config Blocking set to 1
lrdn1428:1483363:1483468 [0] NCCL INFO Using network IB
lrdn1428:1483363:1483468 [0] NCCL INFO ncclCommInitRankScalable comm 0x219bd3e0 rank 254 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1132:1427407:1427407 [0] NCCL INFO Comm config Blocking set to 1
lrdn1132:1427407:1427529 [0] NCCL INFO Using network IB
lrdn1132:1427407:1427529 [0] NCCL INFO ncclCommInitRankScalable comm 0x226dcca0 rank 207 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1210:161042:161042 [0] NCCL INFO Comm config Blocking set to 1
lrdn1210:161042:161159 [0] NCCL INFO Using network IB
lrdn1210:161042:161159 [0] NCCL INFO ncclCommInitRankScalable comm 0x2aedb430 rank 222 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn1163:1638815:1638815 [0] NCCL INFO Comm config Blocking set to 1
lrdn1163:1638815:1638920 [0] NCCL INFO Using network IB
lrdn1163:1638815:1638920 [0] NCCL INFO ncclCommInitRankScalable comm 0x2b75d9b0 rank 213 nranks 256 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x3355f52129d647dd - Init START
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (1/34) after sleep for 100 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (1/34) after sleep for 100 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (2/34) after sleep for 200 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (2/34) after sleep for 200 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (3/34) after sleep for 300 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (3/34) after sleep for 300 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (4/34) after sleep for 400 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (4/34) after sleep for 400 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (5/34) after sleep for 500 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (5/34) after sleep for 500 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (6/34) after sleep for 600 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (6/34) after sleep for 600 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (7/34) after sleep for 700 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (7/34) after sleep for 700 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (8/34) after sleep for 800 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (8/34) after sleep for 800 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (9/34) after sleep for 900 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (9/34) after sleep for 900 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (10/34) after sleep for 1000 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (10/34) after sleep for 1000 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (11/34) after sleep for 1100 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (11/34) after sleep for 1100 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (12/34) after sleep for 1200 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (12/34) after sleep for 1200 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (13/34) after sleep for 1300 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (13/34) after sleep for 1300 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (14/34) after sleep for 1400 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (14/34) after sleep for 1400 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (15/34) after sleep for 1500 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (15/34) after sleep for 1500 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (16/34) after sleep for 1600 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (16/34) after sleep for 1600 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (17/34) after sleep for 1700 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (17/34) after sleep for 1700 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (18/34) after sleep for 1800 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (18/34) after sleep for 1800 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (19/34) after sleep for 1900 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (19/34) after sleep for 1900 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (20/34) after sleep for 2000 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (20/34) after sleep for 2000 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (21/34) after sleep for 2100 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (21/34) after sleep for 2100 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (22/34) after sleep for 2200 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (22/34) after sleep for 2200 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (23/34) after sleep for 2300 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (23/34) after sleep for 2300 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (24/34) after sleep for 2400 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (24/34) after sleep for 2400 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (25/34) after sleep for 2500 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (25/34) after sleep for 2500 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (26/34) after sleep for 2600 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (26/34) after sleep for 2600 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (27/34) after sleep for 2700 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (27/34) after sleep for 2700 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (28/34) after sleep for 2800 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (28/34) after sleep for 2800 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (29/34) after sleep for 2900 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (29/34) after sleep for 2900 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (30/34) after sleep for 3000 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (30/34) after sleep for 3000 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (31/34) after sleep for 3100 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (31/34) after sleep for 3100 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (32/34) after sleep for 3200 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (32/34) after sleep for 3200 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (33/34) after sleep for 3300 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (33/34) after sleep for 3300 msec
lrdn0597:1687014:1687132 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (34/34) after sleep for 3400 msec
lrdn1438:3364708:3364832 [0] NCCL INFO socketPollConnect: connect returned No route to host, retrying (34/34) after sleep for 3400 msec

[2025-08-02 08:52:16] lrdn0597:1687014:1687132 [0] misc/socket.cc:544 NCCL WARN socketPollConnect: connect returned No route to host, exceeded error retry count (35)
lrdn0597:1687014:1687132 [0] NCCL INFO misc/socket.cc:635 -> 6
lrdn0597:1687014:1687132 [0] NCCL INFO misc/socket.cc:684 -> 6
lrdn0597:1687014:1687132 [0] NCCL INFO bootstrap.cc:605 -> 6
lrdn0597:1687014:1687132 [0] NCCL INFO bootstrap.cc:687 -> 6
lrdn0597:1687014:1687132 [0] NCCL INFO init.cc:1404 -> 6
lrdn0597:1687014:1687132 [0] NCCL INFO group.cc:75 -> 6 [Async thread]
lrdn0597:1687014:1687014 [0] NCCL INFO group.cc:422 -> 6
lrdn0597:1687014:1687014 [0] NCCL INFO group.cc:581 -> 6
lrdn0597:1687014:1687014 [0] NCCL INFO init.cc:1871 -> 6

[2025-08-02 08:52:16] lrdn1438:3364708:3364832 [0] misc/socket.cc:544 NCCL WARN socketPollConnect: connect returned No route to host, exceeded error retry count (35)
lrdn1438:3364708:3364832 [0] NCCL INFO misc/socket.cc:635 -> 6
lrdn1438:3364708:3364832 [0] NCCL INFO misc/socket.cc:684 -> 6
lrdn1438:3364708:3364832 [0] NCCL INFO bootstrap.cc:551 -> 6
lrdn1438:3364708:3364832 [0] NCCL INFO bootstrap.cc:713 -> 6
lrdn1438:3364708:3364832 [0] NCCL INFO init.cc:1404 -> 6
lrdn1438:3364708:3364832 [0] NCCL INFO group.cc:75 -> 6 [Async thread]
lrdn1438:3364708:3364708 [0] NCCL INFO group.cc:422 -> 6
lrdn1438:3364708:3364708 [0] NCCL INFO group.cc:581 -> 6
lrdn1438:3364708:3364708 [0] NCCL INFO init.cc:1871 -> 6
[38;5;196m2025-08-02 08:52:16,574 |         __main__ |    ERROR | NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:101, remote process exited or there was a network error, NCCL version 2.26.2
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketPollConnect: connect returned No route to host, exceeded error retry count (35)
Exception raised from create_scalable at /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:101 (most recent call first):
C++ CapturedTraceback:
#4 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libc10.so:624207
#5 c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libc10.so:632295
#6 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18565821
#7 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:14679937
#8 c10d::ProcessGroupNCCL::initNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::Device&, c10d::OpType, int, bool) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18776911
#9 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18796418
#10 c10d::ProcessGroupNCCL::allreduce_impl(at::Tensor&, char const*, c10d::AllreduceOptions const&) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18798461
#11 c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18800357
#12 c10d::ProcessGroupWrapper::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:96050585
#13 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:95701202
#14 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:95768412
#15 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:86058415
#16 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:95813870
#17 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:95875363
#18 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_python.so:13306653
#19 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_python.so:3711403
#20 ?? from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1740594
#21 _PyObject_MakeTpCall from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1407196
#22 _PyEval_EvalFrameDefault from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1037563
#23 ?? from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2444479
#24 _PyObject_Call from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1406721
#25 _PyEval_EvalFrameDefault from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1030395
#26 PyEval_EvalCode from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2444132
#27 ?? from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2736355
#28 _PyRun_SimpleFileObject from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2742292
#29 _PyRun_AnyFileObject from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2743755
#30 Py_RunMain from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2870431
#31 Py_BytesMain from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2871714
#32 __libc_start_main from /lib64/libc.so.6:241028
#33 _start from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/bin/python3.11:4198269
[0m
Traceback (most recent call last):
  File "/leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py", line 275, in main
    pretraining(
  File "/leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py", line 233, in pretraining
    benchmark_aggregation(
  File "/leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/distributed/aggregation.py", line 749, in benchmark_aggregation
    _all_reduce(strategy=strategy, state=state)
  File "/leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/distributed/utils.py", line 114, in _all_reduce
    dist.all_reduce(
  File "/leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2810, in all_reduce
    work = group.allreduce([tensor], opts)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:101, remote process exited or there was a network error, NCCL version 2.26.2
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketPollConnect: connect returned No route to host, exceeded error retry count (35)
Exception raised from create_scalable at /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:101 (most recent call first):
C++ CapturedTraceback:
#4 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libc10.so:624207
#5 c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libc10.so:632295
#6 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18565821
#7 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:14679937
#8 c10d::ProcessGroupNCCL::initNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::Device&, c10d::OpType, int, bool) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18776911
#9 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18796418
#10 c10d::ProcessGroupNCCL::allreduce_impl(at::Tensor&, char const*, c10d::AllreduceOptions const&) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18798461
#11 c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18800357
#12 c10d::ProcessGroupWrapper::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:96050585
#13 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:95701202
#14 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:95768412
#15 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:86058415
#16 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:95813870
#17 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:95875363
#18 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_python.so:13306653
#19 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_python.so:3711403
#20 ?? from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1740594
#21 _PyObject_MakeTpCall from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1407196
#22 _PyEval_EvalFrameDefault from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1037563
#23 ?? from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2444479
#24 _PyObject_Call from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1406721
#25 _PyEval_EvalFrameDefault from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1030395
#26 PyEval_EvalCode from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2444132
#27 ?? from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2736355
#28 _PyRun_SimpleFileObject from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2742292
#29 _PyRun_AnyFileObject from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2743755
#30 Py_RunMain from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2870431
#31 Py_BytesMain from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2871714
#32 __libc_start_main from /lib64/libc.so.6:241028
#33 _start from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/bin/python3.11:4198269

[38;5;196m2025-08-02 08:52:16,576 |         __main__ |    ERROR | NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:101, remote process exited or there was a network error, NCCL version 2.26.2
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketPollConnect: connect returned No route to host, exceeded error retry count (35)
Exception raised from create_scalable at /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:101 (most recent call first):
C++ CapturedTraceback:
#4 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libc10.so:624207
#5 c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libc10.so:632295
#6 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18565821
#7 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:14679937
#8 c10d::ProcessGroupNCCL::initNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::Device&, c10d::OpType, int, bool) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18776911
#9 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18796418
#10 c10d::ProcessGroupNCCL::allreduce_impl(at::Tensor&, char const*, c10d::AllreduceOptions const&) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18798461
#11 c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18800357
#12 c10d::ProcessGroupWrapper::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:96050585
#13 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:95701202
#14 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:95768412
#15 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:86058415
#16 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:95813870
#17 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:95875363
#18 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_python.so:13306653
#19 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_python.so:3711403
#20 ?? from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1740594
#21 _PyObject_MakeTpCall from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1407196
#22 _PyEval_EvalFrameDefault from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1037563
#23 ?? from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2444479
#24 _PyObject_Call from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1406721
#25 _PyEval_EvalFrameDefault from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1030395
#26 PyEval_EvalCode from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2444132
#27 ?? from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2736355
#28 _PyRun_SimpleFileObject from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2742292
#29 _PyRun_AnyFileObject from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2743755
#30 Py_RunMain from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2870431
#31 Py_BytesMain from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2871714
#32 __libc_start_main from /lib64/libc.so.6:241028
#33 _start from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/bin/python3.11:4198269
[0m
Traceback (most recent call last):
  File "/leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py", line 275, in main
    pretraining(
  File "/leonardo_scratch/fast/uToID_bench/xffl/examples/simulation/03_LLM/training.py", line 233, in pretraining
    benchmark_aggregation(
  File "/leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/distributed/aggregation.py", line 749, in benchmark_aggregation
    _all_reduce(strategy=strategy, state=state)
  File "/leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/distributed/utils.py", line 114, in _all_reduce
    dist.all_reduce(
  File "/leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py", line 2810, in all_reduce
    work = group.allreduce([tensor], opts)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:101, remote process exited or there was a network error, NCCL version 2.26.2
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketPollConnect: connect returned No route to host, exceeded error retry count (35)
Exception raised from create_scalable at /pytorch/torch/csrc/distributed/c10d/NCCLUtils.cpp:101 (most recent call first):
C++ CapturedTraceback:
#4 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libc10.so:624207
#5 c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libc10.so:632295
#6 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18565821
#7 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:14679937
#8 c10d::ProcessGroupNCCL::initNCCLComm(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, c10::Device&, c10d::OpType, int, bool) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18776911
#9 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18796418
#10 c10d::ProcessGroupNCCL::allreduce_impl(at::Tensor&, char const*, c10d::AllreduceOptions const&) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18798461
#11 c10d::ProcessGroupNCCL::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so:18800357
#12 c10d::ProcessGroupWrapper::allreduce(std::vector<at::Tensor, std::allocator<at::Tensor> >&, c10d::AllreduceOptions const&) from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:96050585
#13 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:95701202
#14 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:95768412
#15 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:86058415
#16 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:95813870
#17 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so:95875363
#18 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_python.so:13306653
#19 ?? from /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/torch/lib/libtorch_python.so:3711403
#20 ?? from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1740594
#21 _PyObject_MakeTpCall from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1407196
#22 _PyEval_EvalFrameDefault from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1037563
#23 ?? from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2444479
#24 _PyObject_Call from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1406721
#25 _PyEval_EvalFrameDefault from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:1030395
#26 PyEval_EvalCode from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2444132
#27 ?? from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2736355
#28 _PyRun_SimpleFileObject from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2742292
#29 _PyRun_AnyFileObject from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2743755
#30 Py_RunMain from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2870431
#31 Py_BytesMain from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/lib/libpython3.11.so.1.0:2871714
#32 __libc_start_main from /lib64/libc.so.6:241028
#33 _start from /leonardo/prod/spack/5.2/install/0.21/linux-rhel8-icelake/gcc-8.5.0/python-3.11.6-i5k3c6ggftqkzgqyymfbkynpgm2lgjtd/bin/python3.11:4198269

lrdn0597:1687014:1687387 [0] NCCL INFO comm 0xd40f2a0 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
lrdn1438:3364708:3365080 [0] NCCL INFO comm 0x323aa020 rank 0 nranks 1 cudaDev 0 busId 1d000 - Abort COMPLETE
