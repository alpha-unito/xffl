################
### LEONARDO ###
################
facility_a: "leonardo"
repository_a:
  class: Directory
  path: 23_llama_sc24

# Quantity of data for 1h of training of Leonardo
train_samples_a: 16384
test_samples_a: 1024
gpus_per_node_a: 4

############
### LUMI ###
############
facility_b: "lumi"
repository_b:
  class: Directory
  path: 23_llama_sc24

# Quantity of data for 1h of training of Lumi
# Lumi is 10 times slower then Leonardo.
train_samples_b: 4096
test_samples_b: 1024
gpus_per_node_b: 8

################
### MELUXINA ###
################
facility_c: "meluxina"
repository_c:
  class: Directory
  path: 23_llama_sc24

# Quantity of data for 1h of training of MeluXina
# MeluXina is 5 times slower then Leonardo.
train_samples_c: 8192
test_samples_c: 1024
gpus_per_node_c: 8

####################
### MARENOSTRUM5 ###
####################
facility_c: "meluxina"
repository_c:
  class: Directory
  path: 23_llama_sc24

# Quantity of data for 1h of training of MeluXina
# MeluXina is 5 times slower then Leonardo.
train_samples_c: 8192
test_samples_c: 1024
gpus_per_node_c: 8


#############
### CLOUD ###
#############
script_train: 
  class: File
  path: scripts/exec_llama.sh
  
script_aggregation: 
  class: File
  path: scripts/aggregation.py
model:
  class: Directory
  path: llama-3-8b
tokenizer:
  class: Directory
  path: llama-3-tokenizer

epochs: 1
model_basename: "llama-3-8b"
max_rounds: 1

