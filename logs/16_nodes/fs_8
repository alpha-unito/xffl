2, 1, 2
[38;20m2025-03-26 00:40:41,873 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
[38;5;226m2025-03-26 00:40:41,873 |   xffl.cli.utils |  WARNING | CLI argument "help" has got default value "False"[0m
[38;5;226m2025-03-26 00:40:41,873 |   xffl.cli.utils |  WARNING | CLI argument "workdir" has got default value "/leonardo_scratch/fast/uToID_bench/xffl"[0m
[38;5;226m2025-03-26 00:40:41,874 |   xffl.cli.utils |  WARNING | CLI argument "image" has got default value "None"[0m
[38;5;39m2025-03-26 00:40:41,876 | xffl.cli.simulate |    DEBUG | Using virtual environment: /leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate[0m
[38;5;39m2025-03-26 00:40:41,876 | xffl.cli.simulate |    DEBUG | New local simulation xFFL environment variables: {'XFFL_WORLD_SIZE': '64', 'XFFL_NUM_NODES': '16', 'MASTER_ADDR': 'lrdn0001', 'VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;5;39m2025-03-26 00:40:41,877 | xffl.cli.simulate |    DEBUG | Updated xFFL environment: {'XFFL_WORLD_SIZE': '64', 'XFFL_NUM_NODES': '16', 'MASTER_ADDR': 'lrdn0001', 'VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;20m2025-03-26 00:40:41,877 | xffl.cli.simulate |     INFO | Running local simulation...[0m
[38;5;39m2025-03-26 00:40:41,877 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0001: ssh -oStrictHostKeyChecking=no lrdn0001 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=0 XFFL_FEDERATED_RANK=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:40:41,877 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0010: ssh -oStrictHostKeyChecking=no lrdn0010 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=1 XFFL_FEDERATED_RANK=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:40:41,877 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0182: ssh -oStrictHostKeyChecking=no lrdn0182 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=2 XFFL_FEDERATED_RANK=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:40:41,878 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0189: ssh -oStrictHostKeyChecking=no lrdn0189 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=3 XFFL_FEDERATED_RANK=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:40:41,878 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0361: ssh -oStrictHostKeyChecking=no lrdn0361 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=4 XFFL_FEDERATED_RANK=2 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:40:41,878 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0370: ssh -oStrictHostKeyChecking=no lrdn0370 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=5 XFFL_FEDERATED_RANK=2 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:40:41,878 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0542: ssh -oStrictHostKeyChecking=no lrdn0542 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=6 XFFL_FEDERATED_RANK=3 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:40:41,878 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0549: ssh -oStrictHostKeyChecking=no lrdn0549 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=7 XFFL_FEDERATED_RANK=3 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:40:41,879 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0722: ssh -oStrictHostKeyChecking=no lrdn0722 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=8 XFFL_FEDERATED_RANK=4 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:40:41,879 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0730: ssh -oStrictHostKeyChecking=no lrdn0730 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=9 XFFL_FEDERATED_RANK=4 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:40:41,879 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0901: ssh -oStrictHostKeyChecking=no lrdn0901 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=10 XFFL_FEDERATED_RANK=5 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:40:41,879 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0911: ssh -oStrictHostKeyChecking=no lrdn0911 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=11 XFFL_FEDERATED_RANK=5 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:40:41,880 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1084: ssh -oStrictHostKeyChecking=no lrdn1084 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=12 XFFL_FEDERATED_RANK=6 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:40:41,880 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1090: ssh -oStrictHostKeyChecking=no lrdn1090 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=13 XFFL_FEDERATED_RANK=6 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:40:41,880 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1261: ssh -oStrictHostKeyChecking=no lrdn1261 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=14 XFFL_FEDERATED_RANK=7 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:40:41,881 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1281: ssh -oStrictHostKeyChecking=no lrdn1281 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=15 XFFL_FEDERATED_RANK=7 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:40:51,245 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,245 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,245 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,245 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,246 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,246 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,246 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,246 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,246 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,246 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,246 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,246 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,246 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,246 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,246 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,246 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,183 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,183 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,183 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,183 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,246 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,246 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,246 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,246 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,397 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,397 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,397 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,397 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,466 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,466 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,466 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,466 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,411 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,411 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,411 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,411 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,573 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,573 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,573 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:51,573 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,137 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,137 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,137 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,137 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,515 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,515 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,515 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,515 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,798 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,798 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,798 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,798 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,752 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,752 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,752 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,752 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,816 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,816 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,816 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,816 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,844 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,844 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,844 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:52,844 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:40:53,927 | xffl.learning.distributed |    DEBUG | [Rank 49]: distributed setup: DistributedState(rank=49, world_size=64, group_local_rank=1, group_local_size=4, group_rank=12, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7e548bf770>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7e548bf7b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7e548bfaf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,927 | xffl.learning.distributed |    DEBUG | [Rank 48]: distributed setup: DistributedState(rank=48, world_size=64, group_local_rank=0, group_local_size=4, group_rank=12, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7faea982e7b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7faea982e7f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7faea982eaf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,927 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:40:53,927 | xffl.learning.distributed |    DEBUG | [Rank 50]: distributed setup: DistributedState(rank=50, world_size=64, group_local_rank=2, group_local_size=4, group_rank=12, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa2b5cff0b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa2b5cff0f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa2b5cff430>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,927 | xffl.learning.distributed |    DEBUG | [Rank 52]: distributed setup: DistributedState(rank=52, world_size=64, group_local_rank=0, group_local_size=4, group_rank=13, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8add20eff0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8add20f030>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8add20f330>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,927 | xffl.learning.distributed |    DEBUG | [Rank 55]: distributed setup: DistributedState(rank=55, world_size=64, group_local_rank=3, group_local_size=4, group_rank=13, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa8116b2af0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa8116b2bf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa8116b2a70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,928 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:40:53,928 | xffl.learning.distributed |    DEBUG | [Rank 54]: distributed setup: DistributedState(rank=54, world_size=64, group_local_rank=2, group_local_size=4, group_rank=13, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3983dbbcb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3983dbbcf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3983de0070>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,928 | xffl.learning.distributed |    DEBUG | [Rank 0]: distributed setup: DistributedState(rank=0, world_size=64, group_local_rank=0, group_local_size=4, group_rank=0, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f549ee7ff70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f549eeb01b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f549eeb00f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,928 |         __main__ |    DEBUG | Randez-vous time: 1.41 seconds[0m
[38;5;39m2025-03-26 00:40:53,928 | xffl.learning.distributed |    DEBUG | [Rank 53]: distributed setup: DistributedState(rank=53, world_size=64, group_local_rank=1, group_local_size=4, group_rank=13, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb39460f830>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb39460f870>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb39460f970>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,928 | xffl.learning.distributed |    DEBUG | [Rank 51]: distributed setup: DistributedState(rank=51, world_size=64, group_local_rank=3, group_local_size=4, group_rank=12, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f85fd77e830>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f85fd77e870>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f85fd77ebb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,928 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:40:53,928 | xffl.learning.distributed |    DEBUG | [Rank 1]: distributed setup: DistributedState(rank=1, world_size=64, group_local_rank=1, group_local_size=4, group_rank=0, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1c76a0efb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1c76a0eff0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1c76a0f0f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,928 | xffl.learning.distributed |    DEBUG | [Rank 2]: distributed setup: DistributedState(rank=2, world_size=64, group_local_rank=2, group_local_size=4, group_rank=0, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f901e4bfb30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f901e4bfb70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f901e4bfeb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,929 | xffl.learning.distributed |    DEBUG | [Rank 3]: distributed setup: DistributedState(rank=3, world_size=64, group_local_rank=3, group_local_size=4, group_rank=0, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f25e5c3bcb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f25e5c3bcf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f25e5c64030>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,929 | xffl.learning.distributed |    DEBUG | [Rank 4]: distributed setup: DistributedState(rank=4, world_size=64, group_local_rank=0, group_local_size=4, group_rank=1, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f84b2f2adb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f84b2f2adf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f84b2f2aef0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,929 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:40:53,929 | xffl.learning.distributed |    DEBUG | [Rank 8]: distributed setup: DistributedState(rank=8, world_size=64, group_local_rank=0, group_local_size=4, group_rank=2, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe6d3e38230>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe6d3e38270>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe6d3e385b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,930 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:40:53,930 | xffl.learning.distributed |    DEBUG | [Rank 9]: distributed setup: DistributedState(rank=9, world_size=64, group_local_rank=1, group_local_size=4, group_rank=2, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5e5ee976f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5e5ee97730>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5e5ee97a70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,930 | xffl.learning.distributed |    DEBUG | [Rank 5]: distributed setup: DistributedState(rank=5, world_size=64, group_local_rank=1, group_local_size=4, group_rank=1, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff6452db230>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff6452db270>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff6452db370>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,930 | xffl.learning.distributed |    DEBUG | [Rank 6]: distributed setup: DistributedState(rank=6, world_size=64, group_local_rank=2, group_local_size=4, group_rank=1, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5b0bf631b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5b0bf631f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5b0bf632f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,930 | xffl.learning.distributed |    DEBUG | [Rank 7]: distributed setup: DistributedState(rank=7, world_size=64, group_local_rank=3, group_local_size=4, group_rank=1, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcced5f6bf0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcced5f6d30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcced5f6cf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,930 | xffl.learning.distributed |    DEBUG | [Rank 10]: distributed setup: DistributedState(rank=10, world_size=64, group_local_rank=2, group_local_size=4, group_rank=2, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe05c8ab430>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe05c8ab470>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe05c8ab730>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,930 | xffl.learning.distributed |    DEBUG | [Rank 11]: distributed setup: DistributedState(rank=11, world_size=64, group_local_rank=3, group_local_size=4, group_rank=2, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6f980b2ff0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6f980b3030>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6f980b3370>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,930 | xffl.learning.distributed |    DEBUG | [Rank 24]: distributed setup: DistributedState(rank=24, world_size=64, group_local_rank=0, group_local_size=4, group_rank=6, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb72279ecb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb72279ecf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb72279eff0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,930 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:40:53,930 | xffl.learning.distributed |    DEBUG | [Rank 12]: distributed setup: DistributedState(rank=12, world_size=64, group_local_rank=0, group_local_size=4, group_rank=3, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f03cf5a3630>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f03cf5a3670>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f03cf5a39b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,930 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:40:53,931 | xffl.learning.distributed |    DEBUG | [Rank 28]: distributed setup: DistributedState(rank=28, world_size=64, group_local_rank=0, group_local_size=4, group_rank=7, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0f052d76f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0f052d7830>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0f052d77f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,931 | xffl.learning.distributed |    DEBUG | [Rank 29]: distributed setup: DistributedState(rank=29, world_size=64, group_local_rank=1, group_local_size=4, group_rank=7, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd57dcdaab0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd57dcdaaf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd57dcdabf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,930 | xffl.learning.distributed |    DEBUG | [Rank 26]: distributed setup: DistributedState(rank=26, world_size=64, group_local_rank=2, group_local_size=4, group_rank=6, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8771cdb2b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8771cdb2f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8771cdb630>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,931 | xffl.learning.distributed |    DEBUG | [Rank 13]: distributed setup: DistributedState(rank=13, world_size=64, group_local_rank=1, group_local_size=4, group_rank=3, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbb8200b330>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbb8200b370>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbb8200b6b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,931 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:40:53,931 | xffl.learning.distributed |    DEBUG | [Rank 25]: distributed setup: DistributedState(rank=25, world_size=64, group_local_rank=1, group_local_size=4, group_rank=6, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f68f6ce34b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f68f6ce34f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f68f6ce3830>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,931 | xffl.learning.distributed |    DEBUG | [Rank 27]: distributed setup: DistributedState(rank=27, world_size=64, group_local_rank=3, group_local_size=4, group_rank=6, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f14d889a9f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f14d889aa30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f14d889ad30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,931 | xffl.learning.distributed |    DEBUG | [Rank 31]: distributed setup: DistributedState(rank=31, world_size=64, group_local_rank=3, group_local_size=4, group_rank=7, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f691f4fb7f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f691f4fb830>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f691f4fbb70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,868 | xffl.learning.distributed |    DEBUG | [Rank 32]: distributed setup: DistributedState(rank=32, world_size=64, group_local_rank=0, group_local_size=4, group_rank=8, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f37b66cbbf0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f37b66cbc30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f37b66cbf70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,868 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:40:53,868 | xffl.learning.distributed |    DEBUG | [Rank 34]: distributed setup: DistributedState(rank=34, world_size=64, group_local_rank=2, group_local_size=4, group_rank=8, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd593f9f230>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd593f9f270>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd593f9f5b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,868 | xffl.learning.distributed |    DEBUG | [Rank 33]: distributed setup: DistributedState(rank=33, world_size=64, group_local_rank=1, group_local_size=4, group_rank=8, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f49258a7770>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f49258a77b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f49258a7af0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,931 | xffl.learning.distributed |    DEBUG | [Rank 30]: distributed setup: DistributedState(rank=30, world_size=64, group_local_rank=2, group_local_size=4, group_rank=7, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6329f2aeb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6329f2aef0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6329f2b230>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,931 | xffl.learning.distributed |    DEBUG | [Rank 15]: distributed setup: DistributedState(rank=15, world_size=64, group_local_rank=3, group_local_size=4, group_rank=3, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0e759932b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0e759932f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0e75993630>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,931 | xffl.learning.distributed |    DEBUG | [Rank 14]: distributed setup: DistributedState(rank=14, world_size=64, group_local_rank=2, group_local_size=4, group_rank=3, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f89f3496930>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f89f3496970>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f89f3496cb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,869 | xffl.learning.distributed |    DEBUG | [Rank 37]: distributed setup: DistributedState(rank=37, world_size=64, group_local_rank=1, group_local_size=4, group_rank=9, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f59ddbfb530>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f59ddbfb570>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f59ddbfb8b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,869 | xffl.learning.distributed |    DEBUG | [Rank 35]: distributed setup: DistributedState(rank=35, world_size=64, group_local_rank=3, group_local_size=4, group_rank=8, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6ed366f2f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6ed366f330>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6ed366f670>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,869 | xffl.learning.distributed |    DEBUG | [Rank 36]: distributed setup: DistributedState(rank=36, world_size=64, group_local_rank=0, group_local_size=4, group_rank=9, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7feaec54ae70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7feaec54aeb0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7feaec54afb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,869 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:40:53,869 | xffl.learning.distributed |    DEBUG | [Rank 40]: distributed setup: DistributedState(rank=40, world_size=64, group_local_rank=0, group_local_size=4, group_rank=10, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4e2ecc34b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4e2ecc35f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4e2ecc37f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,869 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:40:53,869 | xffl.learning.distributed |    DEBUG | [Rank 41]: distributed setup: DistributedState(rank=41, world_size=64, group_local_rank=1, group_local_size=4, group_rank=10, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f85e91fa4b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f85e91fa4f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f85e91fa830>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,869 | xffl.learning.distributed |    DEBUG | [Rank 42]: distributed setup: DistributedState(rank=42, world_size=64, group_local_rank=2, group_local_size=4, group_rank=10, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f018e513a30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f018e513a70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f018e513db0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,869 | xffl.learning.distributed |    DEBUG | [Rank 38]: distributed setup: DistributedState(rank=38, world_size=64, group_local_rank=2, group_local_size=4, group_rank=9, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff4fafaf9b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff4fafaf9f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff4fafafd30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,869 | xffl.learning.distributed |    DEBUG | [Rank 39]: distributed setup: DistributedState(rank=39, world_size=64, group_local_rank=3, group_local_size=4, group_rank=9, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7faa031cf5b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7faa031cf5f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7faa031cf930>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,870 | xffl.learning.distributed |    DEBUG | [Rank 44]: distributed setup: DistributedState(rank=44, world_size=64, group_local_rank=0, group_local_size=4, group_rank=11, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f39823a76b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f39823a76f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f39823a7a30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,870 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:40:53,870 | xffl.learning.distributed |    DEBUG | [Rank 43]: distributed setup: DistributedState(rank=43, world_size=64, group_local_rank=3, group_local_size=4, group_rank=10, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f10b4ffb3f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f10b4ffb430>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f10b4ffb770>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,870 | xffl.learning.distributed |    DEBUG | [Rank 47]: distributed setup: DistributedState(rank=47, world_size=64, group_local_rank=3, group_local_size=4, group_rank=11, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe003062870>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe0030628b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe0030629b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,870 | xffl.learning.distributed |    DEBUG | [Rank 46]: distributed setup: DistributedState(rank=46, world_size=64, group_local_rank=2, group_local_size=4, group_rank=11, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f35f73d7bb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f35f73d7bf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f35f73d7f30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,870 | xffl.learning.distributed |    DEBUG | [Rank 45]: distributed setup: DistributedState(rank=45, world_size=64, group_local_rank=1, group_local_size=4, group_rank=11, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fdd57afb030>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fdd57afb070>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fdd57afb3b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.distributed |    DEBUG | [Rank 16]: distributed setup: DistributedState(rank=16, world_size=64, group_local_rank=0, group_local_size=4, group_rank=4, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd68f073530>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd68f073570>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd68f0738b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.distributed |    DEBUG | [Rank 17]: distributed setup: DistributedState(rank=17, world_size=64, group_local_rank=1, group_local_size=4, group_rank=4, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc696942970>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc6969429b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc696942cb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.distributed |    DEBUG | [Rank 19]: distributed setup: DistributedState(rank=19, world_size=64, group_local_rank=3, group_local_size=4, group_rank=4, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe3a2cebb30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe3a2cebb70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe3a2cebeb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.distributed |    DEBUG | [Rank 20]: distributed setup: DistributedState(rank=20, world_size=64, group_local_rank=0, group_local_size=4, group_rank=5, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff268973b70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff268973bb0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff268973ef0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.distributed |    DEBUG | [Rank 18]: distributed setup: DistributedState(rank=18, world_size=64, group_local_rank=2, group_local_size=4, group_rank=4, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8efb0036f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8efb003730>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8efb003a70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.distributed |    DEBUG | [Rank 58]: distributed setup: DistributedState(rank=58, world_size=64, group_local_rank=2, group_local_size=4, group_rank=14, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9fa16e6eb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9fa16e6ef0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9fa16e6ff0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.distributed |    DEBUG | [Rank 56]: distributed setup: DistributedState(rank=56, world_size=64, group_local_rank=0, group_local_size=4, group_rank=14, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd1bc5d2a70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd1bc5d2ab0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd1bc5d2df0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.distributed |    DEBUG | [Rank 21]: distributed setup: DistributedState(rank=21, world_size=64, group_local_rank=1, group_local_size=4, group_rank=5, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc9fff96db0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc9fff96df0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc9fff97130>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.distributed |    DEBUG | [Rank 60]: distributed setup: DistributedState(rank=60, world_size=64, group_local_rank=0, group_local_size=4, group_rank=15, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ffa1c5eafb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ffa1c5eaff0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ffa1c5eb0f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.distributed |    DEBUG | [Rank 57]: distributed setup: DistributedState(rank=57, world_size=64, group_local_rank=1, group_local_size=4, group_rank=14, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7608580030>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7608580070>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7608580370>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.distributed |    DEBUG | [Rank 23]: distributed setup: DistributedState(rank=23, world_size=64, group_local_rank=3, group_local_size=4, group_rank=5, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6c5f9271b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6c5f9271f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6c5f927530>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.distributed |    DEBUG | [Rank 22]: distributed setup: DistributedState(rank=22, world_size=64, group_local_rank=2, group_local_size=4, group_rank=5, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3b30d6bbf0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3b30d6bc30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3b30d6bf70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.distributed |    DEBUG | [Rank 59]: distributed setup: DistributedState(rank=59, world_size=64, group_local_rank=3, group_local_size=4, group_rank=14, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f062021b970>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f062021b9b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f062021bcf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.distributed |    DEBUG | [Rank 61]: distributed setup: DistributedState(rank=61, world_size=64, group_local_rank=1, group_local_size=4, group_rank=15, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6e243032f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6e24303330>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6e24303670>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.distributed |    DEBUG | [Rank 63]: distributed setup: DistributedState(rank=63, world_size=64, group_local_rank=3, group_local_size=4, group_rank=15, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4bb52db7b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4bb52db7f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4bb52db8f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:53,933 | xffl.learning.distributed |    DEBUG | [Rank 62]: distributed setup: DistributedState(rank=62, world_size=64, group_local_rank=2, group_local_size=4, group_rank=15, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7efe6988eb30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7efe6988eb70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7efe6988eeb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:40:54,031 | xffl.learning.utils |    DEBUG | [Rank 48]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:54,033 | xffl.learning.utils |    DEBUG | [Rank 4]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:54,034 | xffl.learning.utils |    DEBUG | [Rank 0]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:54,035 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:54,035 | xffl.learning.utils |    DEBUG | [Rank 24]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:53,974 | xffl.learning.utils |    DEBUG | [Rank 40]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:54,037 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:54,037 | xffl.learning.utils |    DEBUG | [Rank 16]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:54,038 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:54,038 | xffl.learning.utils |    DEBUG | [Rank 20]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:54,039 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:53,978 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:54,041 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:54,042 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:54,052 | xffl.learning.utils |    DEBUG | [Rank 52]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:54,056 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:54,002 | xffl.learning.utils |    DEBUG | [Rank 32]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:54,006 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:54,080 | xffl.learning.utils |    DEBUG | [Rank 28]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:54,021 | xffl.learning.utils |    DEBUG | [Rank 44]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:54,084 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:54,026 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:54,091 | xffl.learning.utils |    DEBUG | [Rank 60]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:54,091 | xffl.learning.utils |    DEBUG | [Rank 12]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:54,095 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:54,095 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:54,115 | xffl.learning.utils |    DEBUG | [Rank 56]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:54,119 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:54,123 | xffl.learning.utils |    DEBUG | [Rank 8]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:54,062 | xffl.learning.utils |    DEBUG | [Rank 36]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:54,127 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:54,066 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,653 | xffl.learning.utils |    DEBUG | [Rank 55]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,656 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,598 | xffl.learning.utils |    DEBUG | [Rank 47]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,602 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,675 | xffl.learning.utils |    DEBUG | [Rank 63]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,676 | xffl.learning.utils |    DEBUG | [Rank 58]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,676 | xffl.learning.utils |    DEBUG | [Rank 18]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,677 | xffl.learning.utils |    DEBUG | [Rank 31]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,677 | xffl.learning.utils |    DEBUG | [Rank 3]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,677 | xffl.learning.utils |    DEBUG | [Rank 54]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,678 | xffl.learning.utils |    DEBUG | [Rank 59]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,679 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,680 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,680 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,681 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,681 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,681 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,619 | xffl.learning.utils |    DEBUG | [Rank 35]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,682 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,620 | xffl.learning.utils |    DEBUG | [Rank 34]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,682 | xffl.learning.utils |    DEBUG | [Rank 27]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,685 | xffl.learning.utils |    DEBUG | [Rank 2]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,623 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,686 | xffl.learning.utils |    DEBUG | [Rank 62]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,623 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,686 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,624 | xffl.learning.utils |    DEBUG | [Rank 46]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,687 | xffl.learning.utils |    DEBUG | [Rank 30]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,625 | xffl.learning.utils |    DEBUG | [Rank 38]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,688 | xffl.learning.utils |    DEBUG | [Rank 51]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,689 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,690 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,627 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,691 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,629 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,692 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,630 | xffl.learning.utils |    DEBUG | [Rank 42]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,696 | xffl.learning.utils |    DEBUG | [Rank 50]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,634 | xffl.learning.utils |    DEBUG | [Rank 39]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,634 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,698 | xffl.learning.utils |    DEBUG | [Rank 26]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,699 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,638 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,702 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,705 | xffl.learning.utils |    DEBUG | [Rank 7]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,710 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,710 | xffl.learning.utils |    DEBUG | [Rank 22]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,713 | xffl.learning.utils |    DEBUG | [Rank 19]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,714 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,715 | xffl.learning.utils |    DEBUG | [Rank 6]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,717 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,655 | xffl.learning.utils |    DEBUG | [Rank 43]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,718 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,718 | xffl.learning.utils |    DEBUG | [Rank 23]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,719 | xffl.learning.utils |    DEBUG | [Rank 10]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,659 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,722 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,723 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,724 | xffl.learning.utils |    DEBUG | [Rank 15]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,728 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,728 | xffl.learning.utils |    DEBUG | [Rank 53]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,730 | xffl.learning.utils |    DEBUG | [Rank 1]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,732 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,734 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,736 | xffl.learning.utils |    DEBUG | [Rank 61]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,736 | xffl.learning.utils |    DEBUG | [Rank 11]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,740 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,740 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,742 | xffl.learning.utils |    DEBUG | [Rank 25]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,746 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,747 | xffl.learning.utils |    DEBUG | [Rank 14]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,750 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,751 | xffl.learning.utils |    DEBUG | [Rank 17]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,751 | xffl.learning.utils |    DEBUG | [Rank 13]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,753 | xffl.learning.utils |    DEBUG | [Rank 49]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,755 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,756 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,693 | xffl.learning.utils |    DEBUG | [Rank 41]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,757 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,697 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,760 | xffl.learning.utils |    DEBUG | [Rank 29]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,761 | xffl.learning.utils |    DEBUG | [Rank 9]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,764 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,765 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,704 | xffl.learning.utils |    DEBUG | [Rank 45]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,770 | xffl.learning.utils |    DEBUG | [Rank 21]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,708 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,774 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,715 | xffl.learning.utils |    DEBUG | [Rank 37]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,716 | xffl.learning.utils |    DEBUG | [Rank 33]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,719 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,720 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,784 | xffl.learning.utils |    DEBUG | [Rank 5]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,786 | xffl.learning.utils |    DEBUG | [Rank 57]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:40:55,787 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:55,790 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:40:59,571 | xffl.learning.modelling |    DEBUG | [Rank 4]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:40:59,571 | xffl.learning.distributed |    DEBUG | [Rank 4]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:40:59,588 | xffl.learning.modelling |    DEBUG | [Rank 60]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:40:59,588 | xffl.learning.distributed |    DEBUG | [Rank 60]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:40:59,611 | xffl.learning.modelling |    DEBUG | [Rank 52]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:40:59,612 | xffl.learning.distributed |    DEBUG | [Rank 52]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:40:59,698 | xffl.learning.modelling |    DEBUG | [Rank 48]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:40:59,698 | xffl.learning.distributed |    DEBUG | [Rank 48]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:40:59,701 | xffl.learning.modelling |    DEBUG | [Rank 12]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:40:59,701 | xffl.learning.distributed |    DEBUG | [Rank 12]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:40:59,730 | xffl.learning.modelling |    DEBUG | [Rank 56]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:40:59,730 | xffl.learning.distributed |    DEBUG | [Rank 56]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:40:59,734 | xffl.learning.modelling |    DEBUG | [Rank 28]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:40:59,734 | xffl.learning.distributed |    DEBUG | [Rank 28]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:40:59,735 |         __main__ |    DEBUG | Model loading time: 5.17 seconds[0m
[38;5;39m2025-03-26 00:40:59,736 |         __main__ |    DEBUG | Training llama3.1-8b: 8030.26 million trainable parameters[0m
[38;5;39m2025-03-26 00:40:59,736 | xffl.learning.modelling |    DEBUG | [Rank 0]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:40:59,736 | xffl.learning.distributed |    DEBUG | [Rank 0]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:40:59,743 | xffl.learning.modelling |    DEBUG | [Rank 20]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:40:59,743 | xffl.learning.distributed |    DEBUG | [Rank 20]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:40:59,745 | xffl.learning.modelling |    DEBUG | [Rank 24]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:40:59,745 | xffl.learning.distributed |    DEBUG | [Rank 24]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:40:59,683 | xffl.learning.modelling |    DEBUG | [Rank 40]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:40:59,683 | xffl.learning.distributed |    DEBUG | [Rank 40]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:40:59,758 | xffl.learning.modelling |    DEBUG | [Rank 8]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:40:59,758 | xffl.learning.distributed |    DEBUG | [Rank 8]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:40:59,696 | xffl.learning.modelling |    DEBUG | [Rank 32]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:40:59,696 | xffl.learning.distributed |    DEBUG | [Rank 32]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:40:59,701 | xffl.learning.modelling |    DEBUG | [Rank 44]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:40:59,702 | xffl.learning.distributed |    DEBUG | [Rank 44]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:40:59,782 | xffl.learning.modelling |    DEBUG | [Rank 16]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:40:59,782 | xffl.learning.distributed |    DEBUG | [Rank 16]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:40:59,752 | xffl.learning.modelling |    DEBUG | [Rank 36]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:40:59,752 | xffl.learning.distributed |    DEBUG | [Rank 36]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,388 | xffl.learning.modelling |    DEBUG | [Rank 49]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,388 | xffl.learning.distributed |    DEBUG | [Rank 49]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,430 | xffl.learning.modelling |    DEBUG | [Rank 55]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,430 | xffl.learning.distributed |    DEBUG | [Rank 55]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,430 | xffl.learning.modelling |    DEBUG | [Rank 9]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,430 | xffl.learning.distributed |    DEBUG | [Rank 9]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,435 | xffl.learning.modelling |    DEBUG | [Rank 53]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,435 | xffl.learning.distributed |    DEBUG | [Rank 53]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,435 | xffl.learning.modelling |    DEBUG | [Rank 29]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,436 | xffl.learning.distributed |    DEBUG | [Rank 29]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,375 | xffl.learning.modelling |    DEBUG | [Rank 35]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,375 | xffl.learning.distributed |    DEBUG | [Rank 35]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,382 | xffl.learning.modelling |    DEBUG | [Rank 34]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,383 | xffl.learning.distributed |    DEBUG | [Rank 34]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,455 | xffl.learning.modelling |    DEBUG | [Rank 25]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,455 | xffl.learning.distributed |    DEBUG | [Rank 25]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,473 | xffl.learning.modelling |    DEBUG | [Rank 54]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,473 | xffl.learning.distributed |    DEBUG | [Rank 54]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,473 | xffl.learning.modelling |    DEBUG | [Rank 5]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,474 | xffl.learning.distributed |    DEBUG | [Rank 5]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,476 | xffl.learning.modelling |    DEBUG | [Rank 21]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,476 | xffl.learning.distributed |    DEBUG | [Rank 21]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,485 | xffl.learning.modelling |    DEBUG | [Rank 31]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,486 | xffl.learning.distributed |    DEBUG | [Rank 31]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,500 | xffl.learning.modelling |    DEBUG | [Rank 30]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,500 | xffl.learning.distributed |    DEBUG | [Rank 30]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,437 | xffl.learning.modelling |    DEBUG | [Rank 33]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,437 | xffl.learning.distributed |    DEBUG | [Rank 33]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,514 | xffl.learning.modelling |    DEBUG | [Rank 13]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,514 | xffl.learning.distributed |    DEBUG | [Rank 13]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,453 | xffl.learning.modelling |    DEBUG | [Rank 39]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,453 | xffl.learning.distributed |    DEBUG | [Rank 39]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,455 | xffl.learning.modelling |    DEBUG | [Rank 38]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,455 | xffl.learning.distributed |    DEBUG | [Rank 38]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,519 | xffl.learning.modelling |    DEBUG | [Rank 51]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,519 | xffl.learning.distributed |    DEBUG | [Rank 51]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,525 | xffl.learning.modelling |    DEBUG | [Rank 27]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,526 | xffl.learning.distributed |    DEBUG | [Rank 27]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,467 | xffl.learning.modelling |    DEBUG | [Rank 41]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,467 | xffl.learning.distributed |    DEBUG | [Rank 41]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,531 | xffl.learning.modelling |    DEBUG | [Rank 11]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,531 | xffl.learning.distributed |    DEBUG | [Rank 11]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,534 | xffl.learning.modelling |    DEBUG | [Rank 10]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,534 | xffl.learning.distributed |    DEBUG | [Rank 10]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,537 | xffl.learning.modelling |    DEBUG | [Rank 26]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,537 | xffl.learning.distributed |    DEBUG | [Rank 26]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,539 | xffl.learning.modelling |    DEBUG | [Rank 7]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,539 | xffl.learning.distributed |    DEBUG | [Rank 7]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,539 | xffl.learning.modelling |    DEBUG | [Rank 6]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,540 | xffl.learning.distributed |    DEBUG | [Rank 6]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,484 | xffl.learning.modelling |    DEBUG | [Rank 37]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,484 | xffl.learning.distributed |    DEBUG | [Rank 37]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,550 | xffl.learning.modelling |    DEBUG | [Rank 50]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,551 | xffl.learning.distributed |    DEBUG | [Rank 50]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,594 | xffl.learning.modelling |    DEBUG | [Rank 1]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,595 | xffl.learning.distributed |    DEBUG | [Rank 1]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,597 | xffl.learning.modelling |    DEBUG | [Rank 61]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,597 | xffl.learning.distributed |    DEBUG | [Rank 61]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,628 | xffl.learning.modelling |    DEBUG | [Rank 15]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,628 | xffl.learning.distributed |    DEBUG | [Rank 15]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,636 | xffl.learning.modelling |    DEBUG | [Rank 14]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,636 | xffl.learning.distributed |    DEBUG | [Rank 14]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,574 | xffl.learning.modelling |    DEBUG | [Rank 42]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,575 | xffl.learning.distributed |    DEBUG | [Rank 42]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,580 | xffl.learning.modelling |    DEBUG | [Rank 43]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,580 | xffl.learning.distributed |    DEBUG | [Rank 43]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,599 | xffl.learning.modelling |    DEBUG | [Rank 45]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,600 | xffl.learning.distributed |    DEBUG | [Rank 45]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,686 | xffl.learning.modelling |    DEBUG | [Rank 59]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,686 | xffl.learning.distributed |    DEBUG | [Rank 59]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,686 | xffl.learning.modelling |    DEBUG | [Rank 58]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,686 | xffl.learning.distributed |    DEBUG | [Rank 58]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,638 | xffl.learning.modelling |    DEBUG | [Rank 47]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,638 | xffl.learning.distributed |    DEBUG | [Rank 47]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,702 | xffl.learning.modelling |    DEBUG | [Rank 57]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,702 | xffl.learning.distributed |    DEBUG | [Rank 57]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,703 | xffl.learning.modelling |    DEBUG | [Rank 22]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,703 | xffl.learning.distributed |    DEBUG | [Rank 22]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,719 | xffl.learning.modelling |    DEBUG | [Rank 23]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,719 | xffl.learning.distributed |    DEBUG | [Rank 23]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,662 | xffl.learning.modelling |    DEBUG | [Rank 46]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,662 | xffl.learning.distributed |    DEBUG | [Rank 46]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,727 | xffl.learning.modelling |    DEBUG | [Rank 3]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,727 | xffl.learning.modelling |    DEBUG | [Rank 2]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,727 | xffl.learning.distributed |    DEBUG | [Rank 3]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,727 | xffl.learning.distributed |    DEBUG | [Rank 2]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,730 | xffl.learning.modelling |    DEBUG | [Rank 18]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,730 | xffl.learning.distributed |    DEBUG | [Rank 18]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,733 | xffl.learning.modelling |    DEBUG | [Rank 19]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,733 | xffl.learning.distributed |    DEBUG | [Rank 19]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,734 | xffl.learning.modelling |    DEBUG | [Rank 17]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,734 | xffl.learning.distributed |    DEBUG | [Rank 17]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,755 | xffl.learning.modelling |    DEBUG | [Rank 62]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,756 | xffl.learning.distributed |    DEBUG | [Rank 62]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:00,759 | xffl.learning.modelling |    DEBUG | [Rank 63]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:41:00,759 | xffl.learning.distributed |    DEBUG | [Rank 63]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:41:08,526 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,526 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,526 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,526 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,527 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,527 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,527 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,528 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,616 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,616 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,617 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,617 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,617 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,620 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,620 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,620 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,640 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,640 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,640 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,640 |         __main__ |    DEBUG | FSDP wrapping setup time: 8.90 seconds[0m
[38;5;39m2025-03-26 00:41:08,642 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,643 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,645 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,645 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,646 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,585 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,585 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,585 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,585 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,586 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,586 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,586 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,649 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,649 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,650 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,650 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,587 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,650 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,650 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,650 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,650 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,612 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,612 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,613 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,612 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,613 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,613 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,613 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,613 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,685 |         __main__ |    DEBUG | [Rank 52]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,685 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,685 |         __main__ |    DEBUG | [Rank 54]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:41:08,685 |         __main__ |    DEBUG | [Rank 55]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,685 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:41:08,685 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,685 |         __main__ |    DEBUG | [Rank 48]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,686 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,687 |         __main__ |    DEBUG | [Rank 50]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,687 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,688 |         __main__ |    DEBUG | [Rank 51]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,688 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,702 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,703 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,703 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,703 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,704 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,704 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,705 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,705 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,706 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,706 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,707 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,707 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,707 |         __main__ |    DEBUG | [Rank 49]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,707 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,708 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,708 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,708 |         __main__ |    DEBUG | [Rank 53]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,708 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,709 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,709 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:41:08,798 |         __main__ |    DEBUG | [Rank 6]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,799 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,800 |         __main__ |    DEBUG | [Rank 5]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,800 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,800 |         __main__ |    DEBUG | [Rank 4]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,800 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,804 |         __main__ |    DEBUG | [Rank 13]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,804 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,804 |         __main__ |    DEBUG | [Rank 15]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,804 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,804 |         __main__ |    DEBUG | [Rank 14]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,805 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,818 |         __main__ |    DEBUG | [Rank 11]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,818 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,818 |         __main__ |    DEBUG | [Rank 9]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,818 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,818 |         __main__ |    DEBUG | [Rank 7]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,818 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,819 |         __main__ |    DEBUG | [Rank 10]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,819 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,825 |         __main__ |    DEBUG | [Rank 12]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,826 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,843 |         __main__ |    DEBUG | [Rank 8]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,843 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,781 |         __main__ |    DEBUG | [Rank 39]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,781 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,781 |         __main__ |    DEBUG | [Rank 33]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,781 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,781 |         __main__ |    DEBUG | [Rank 36]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,781 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,781 |         __main__ |    DEBUG | [Rank 34]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,781 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,781 |         __main__ |    DEBUG | [Rank 32]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,782 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,783 |         __main__ |    DEBUG | [Rank 37]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,783 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,858 |         __main__ |    DEBUG | [Rank 30]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,859 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,858 |         __main__ |    DEBUG | [Rank 31]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:41:08,858 |         __main__ |    DEBUG | [Rank 29]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,859 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:41:08,859 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,800 |         __main__ |    DEBUG | [Rank 38]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,801 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,801 |         __main__ |    DEBUG | [Rank 35]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,801 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,868 |         __main__ |    DEBUG | [Rank 24]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,868 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,868 |         __main__ |    DEBUG | [Rank 26]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:41:08,868 |         __main__ |    DEBUG | [Rank 27]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,868 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:41:08,868 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,871 |         __main__ |    DEBUG | [Rank 61]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,871 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,872 |         __main__ |    DEBUG | [Rank 23]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,872 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,874 |         __main__ |    DEBUG | Dataset loading time: 0.23 seconds[0m
[38;5;39m2025-03-26 00:41:08,874 |         __main__ |    DEBUG | [Rank 20]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,874 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,874 |         __main__ |    DEBUG | [Rank 21]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,874 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,812 |         __main__ |    DEBUG | [Rank 41]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,813 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,878 |         __main__ |    DEBUG | [Rank 28]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,878 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,880 |         __main__ |    DEBUG | [Rank 59]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,881 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,819 |         __main__ |    DEBUG | [Rank 42]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,819 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,819 |         __main__ |    DEBUG | [Rank 43]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,819 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,887 |         __main__ |    DEBUG | [Rank 25]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,888 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,888 |         __main__ |    DEBUG | [Rank 62]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,888 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,891 |         __main__ |    DEBUG | [Rank 63]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,891 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,893 |         __main__ |    DEBUG | [Rank 22]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,893 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,893 |         __main__ |    DEBUG | [Rank 58]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,894 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,896 |         __main__ |    DEBUG | [Rank 57]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,896 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,838 |         __main__ |    DEBUG | [Rank 40]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,839 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,907 |         __main__ |    DEBUG | train set size: 16384 samples[0m
[38;5;39m2025-03-26 00:41:08,908 |         __main__ |    DEBUG | train dataloader size: 32 minibatches[0m
[38;5;39m2025-03-26 00:41:08,908 |         __main__ |    DEBUG | val set size: 5311 samples[0m
[38;5;39m2025-03-26 00:41:08,908 |         __main__ |    DEBUG | val dataloader size: 82 minibatches[0m
[38;5;39m2025-03-26 00:41:08,908 |         __main__ |    DEBUG | Dataloaders creation time: 0.03 seconds[0m
[38;5;39m2025-03-26 00:41:08,908 |         __main__ |    DEBUG | Learning rate adjusted to: 0.0002[0m
[38;5;39m2025-03-26 00:41:08,909 |         __main__ |    DEBUG | Total setup time: 16.39 seconds[0m
[38;5;39m2025-03-26 00:41:08,909 |         __main__ |    DEBUG | [Rank 0]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,909 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,911 |         __main__ |    DEBUG | [Rank 60]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,911 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,911 |         __main__ |    DEBUG | [Rank 3]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,911 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,911 |         __main__ |    DEBUG | [Rank 2]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,912 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,850 |         __main__ |    DEBUG | [Rank 47]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,850 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,914 |         __main__ |    DEBUG | [Rank 56]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,914 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,853 |         __main__ |    DEBUG | [Rank 46]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,853 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,858 |         __main__ |    DEBUG | [Rank 44]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,859 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,928 |         __main__ |    DEBUG | [Rank 19]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,928 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,928 |         __main__ |    DEBUG | [Rank 17]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,929 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,930 |         __main__ |    DEBUG | [Rank 16]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,930 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,932 |         __main__ |    DEBUG | [Rank 1]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,932 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,877 |         __main__ |    DEBUG | [Rank 45]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,877 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:41:08,948 |         __main__ |    DEBUG | [Rank 18]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:41:08,949 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:43:50,352 | xffl.learning.processing |     INFO | Epoch 1: train_perplexity=35710.2266, train_epoch_loss=10.4832, epoch time 160.42672165599652s[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 1]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 3]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 2]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 9]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 11]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 8]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 13]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 15]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 12]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 61]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 60]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 62]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 6]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 7]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 5]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 4]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 63]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 25]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 27]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 26]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 24]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 56]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 57]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 58]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 59]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 23]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 21]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 20]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 53]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 54]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 55]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 52]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 31]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 28]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 29]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 30]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,291 | xffl.learning.distributed |    DEBUG | [Rank 41]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,291 | xffl.learning.distributed |    DEBUG | [Rank 40]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,291 | xffl.learning.distributed |    DEBUG | [Rank 43]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 |         __main__ |    DEBUG | Key: avg_epoch_time, Value: 160.42672165599652[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 50]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 49]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 51]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 48]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,291 | xffl.learning.distributed |    DEBUG | [Rank 42]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 14]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 10]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 22]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,291 | xffl.learning.distributed |    DEBUG | [Rank 33]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,291 | xffl.learning.distributed |    DEBUG | [Rank 35]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,291 | xffl.learning.distributed |    DEBUG | [Rank 32]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 |         __main__ |    DEBUG | Key: avg_train_perp, Value: 35710.2265625[0m
[38;5;39m2025-03-26 00:43:50,353 | xffl.learning.distributed |    DEBUG | [Rank 16]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 19]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 17]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 | xffl.learning.distributed |    DEBUG | [Rank 18]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,354 |         __main__ |    DEBUG | Key: avg_train_loss, Value: 10.483192443847656[0m
[38;5;39m2025-03-26 00:43:50,291 | xffl.learning.distributed |    DEBUG | [Rank 45]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,291 | xffl.learning.distributed |    DEBUG | [Rank 46]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,291 | xffl.learning.distributed |    DEBUG | [Rank 44]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,291 | xffl.learning.distributed |    DEBUG | [Rank 47]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,291 | xffl.learning.distributed |    DEBUG | [Rank 34]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,291 | xffl.learning.distributed |    DEBUG | [Rank 37]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,291 | xffl.learning.distributed |    DEBUG | [Rank 36]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,291 | xffl.learning.distributed |    DEBUG | [Rank 38]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,291 | xffl.learning.distributed |    DEBUG | [Rank 39]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:43:50,355 | xffl.learning.distributed |    DEBUG | [Rank 0]: calling destroy_process_group[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-i27wkwau[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-i27wkwau/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-24qsihwp[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-24qsihwp/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-b8ff92q3[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-b8ff92q3/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004054-yo7xflvk[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004054-yo7xflvk/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: 
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-kr0jtcof[0m
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-lkswyrz7[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-kr0jtcof/logs[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-lkswyrz7/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-56ajbqa5[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-56ajbqa5/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004054-6bkf2ctb[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004054-6bkf2ctb/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-41sx9k51[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-41sx9k51/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-1305t3u3[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-1305t3u3/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004054-20sji0ub[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004054-20sji0ub/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-7jqkslg5[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-7jqkslg5/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-duwoshmn[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-duwoshmn/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-pho9fgm4[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-pho9fgm4/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004054-9850bse6[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004054-9850bse6/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-nbg9167g[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-nbg9167g/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-u8hjimi8[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-u8hjimi8/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-vesabuor[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-vesabuor/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-gz01o1ha[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-gz01o1ha/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004054-y59xd3tu[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004054-y59xd3tu/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-3l9amc6g[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-3l9amc6g/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-r7gcbcqo[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-r7gcbcqo/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-yiyo4nrq[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-yiyo4nrq/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004054-fqpzw629[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004054-fqpzw629/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-iv9awmxk[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-iv9awmxk/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-dueckndw[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-dueckndw/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-xem5537q[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-xem5537q/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004054-8swa18wx[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004054-8swa18wx/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-88ljjfrl[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-88ljjfrl/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-mu9tqjtt[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-mu9tqjtt/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-5l4gq1yl[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-5l4gq1yl/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004054-5j9t7m35[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004054-5j9t7m35/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-ke7hgu65[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-ke7hgu65/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-94wn3eif[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-94wn3eif/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-1zbx1tuo[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-1zbx1tuo/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-9m2bosp7[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-9m2bosp7/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004054-xkb7co51[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004054-xkb7co51/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-90or01h2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-90or01h2/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004054-ktc3smxl[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004054-ktc3smxl/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-t7ei24br[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-t7ei24br/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-grlkuua0[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-grlkuua0/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004054-h0hib9j8[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004054-h0hib9j8/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-zk9djb7w[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-zk9djb7w/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-16sy6axn[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-16sy6axn/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004054-lokaoa7h[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004054-lokaoa7h/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-2gpjfhq3[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-2gpjfhq3/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-kixue4ae[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-kixue4ae/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-7f5zv6dy[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-7f5zv6dy/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-p6c887fa[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-p6c887fa/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004054-7hxgcnp7[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004054-7hxgcnp7/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-0wbw19ui[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-0wbw19ui/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-pvfzoc0k[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-pvfzoc0k/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-9n85b0jz[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-9n85b0jz/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-cv9fj8gv[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-cv9fj8gv/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-96d9mqs9[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-96d9mqs9/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-k6fr7fef[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-k6fr7fef/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004054-ssg2zrta[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004054-ssg2zrta/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-zq9vfkfr[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-zq9vfkfr/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004053-uz0swrbz[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004053-uz0swrbz/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-h8r0rymp[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-h8r0rymp/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004054-nxgbh6aw[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004054-nxgbh6aw/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-ctk7jle0[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-ctk7jle0/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-sy7ihdl1[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-sy7ihdl1/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_004055-5mkb5zug[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_004055-5mkb5zug/logs[0m
[38;20m2025-03-26 00:43:55,041 | xffl.cli.simulate |     INFO | Total simulation execution time: 193.16 seconds[0m
[38;20m2025-03-26 00:43:55,041 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
