2, 1, 2
[38;20m2025-03-26 00:32:55,759 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
[38;5;226m2025-03-26 00:32:55,759 |   xffl.cli.utils |  WARNING | CLI argument "help" has got default value "False"[0m
[38;5;226m2025-03-26 00:32:55,760 |   xffl.cli.utils |  WARNING | CLI argument "workdir" has got default value "/leonardo_scratch/fast/uToID_bench/xffl"[0m
[38;5;226m2025-03-26 00:32:55,760 |   xffl.cli.utils |  WARNING | CLI argument "image" has got default value "None"[0m
[38;5;39m2025-03-26 00:32:55,762 | xffl.cli.simulate |    DEBUG | Using virtual environment: /leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate[0m
[38;5;39m2025-03-26 00:32:55,762 | xffl.cli.simulate |    DEBUG | New local simulation xFFL environment variables: {'XFFL_WORLD_SIZE': '64', 'XFFL_NUM_NODES': '16', 'MASTER_ADDR': 'lrdn0001', 'VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;5;39m2025-03-26 00:32:55,762 | xffl.cli.simulate |    DEBUG | Updated xFFL environment: {'XFFL_WORLD_SIZE': '64', 'XFFL_NUM_NODES': '16', 'MASTER_ADDR': 'lrdn0001', 'VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;20m2025-03-26 00:32:55,762 | xffl.cli.simulate |     INFO | Running local simulation...[0m
[38;5;39m2025-03-26 00:32:55,762 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0001: ssh -oStrictHostKeyChecking=no lrdn0001 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=0 XFFL_FEDERATED_RANK=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:32:55,762 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0010: ssh -oStrictHostKeyChecking=no lrdn0010 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=1 XFFL_FEDERATED_RANK=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:32:55,763 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0182: ssh -oStrictHostKeyChecking=no lrdn0182 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=2 XFFL_FEDERATED_RANK=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:32:55,763 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0189: ssh -oStrictHostKeyChecking=no lrdn0189 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=3 XFFL_FEDERATED_RANK=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:32:55,763 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0361: ssh -oStrictHostKeyChecking=no lrdn0361 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=4 XFFL_FEDERATED_RANK=2 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:32:55,763 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0370: ssh -oStrictHostKeyChecking=no lrdn0370 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=5 XFFL_FEDERATED_RANK=2 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:32:55,764 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0542: ssh -oStrictHostKeyChecking=no lrdn0542 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=6 XFFL_FEDERATED_RANK=3 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:32:55,764 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0549: ssh -oStrictHostKeyChecking=no lrdn0549 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=7 XFFL_FEDERATED_RANK=3 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:32:55,764 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0722: ssh -oStrictHostKeyChecking=no lrdn0722 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=8 XFFL_FEDERATED_RANK=4 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:32:55,764 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0730: ssh -oStrictHostKeyChecking=no lrdn0730 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=9 XFFL_FEDERATED_RANK=4 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:32:55,765 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0901: ssh -oStrictHostKeyChecking=no lrdn0901 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=10 XFFL_FEDERATED_RANK=5 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:32:55,765 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0911: ssh -oStrictHostKeyChecking=no lrdn0911 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=11 XFFL_FEDERATED_RANK=5 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:32:55,765 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1084: ssh -oStrictHostKeyChecking=no lrdn1084 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=12 XFFL_FEDERATED_RANK=6 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:32:55,765 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1090: ssh -oStrictHostKeyChecking=no lrdn1090 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=13 XFFL_FEDERATED_RANK=6 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:32:55,766 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1261: ssh -oStrictHostKeyChecking=no lrdn1261 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=14 XFFL_FEDERATED_RANK=7 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:32:55,766 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1281: ssh -oStrictHostKeyChecking=no lrdn1281 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=15 XFFL_FEDERATED_RANK=7 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:33:05,270 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,270 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,270 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,270 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,270 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,270 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,270 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,270 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,283 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,283 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,283 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,283 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,307 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,307 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,307 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,307 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,316 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,316 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,316 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,316 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,386 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,386 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,386 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,386 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,529 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,529 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,529 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,529 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,701 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,701 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,701 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:05,701 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:10,957 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:10,957 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:10,957 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:10,957 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:10,997 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:10,997 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:10,997 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:10,997 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,105 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,105 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,105 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,105 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,109 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,109 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,109 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,109 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,541 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,541 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,541 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,541 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,706 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,706 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,706 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,706 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,867 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,867 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,867 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,867 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,818 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,818 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,818 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:11,818 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:33:12,003 | xffl.learning.distributed |    DEBUG | [Rank 0]: distributed setup: DistributedState(rank=0, world_size=64, group_local_rank=0, group_local_size=4, group_rank=0, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7a6357b270>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7a6357b2b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7a6357b3b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,003 |         __main__ |    DEBUG | Randez-vous time: 6.47 seconds[0m
[38;5;39m2025-03-26 00:33:12,003 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:33:12,003 | xffl.learning.distributed |    DEBUG | [Rank 1]: distributed setup: DistributedState(rank=1, world_size=64, group_local_rank=1, group_local_size=4, group_rank=0, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6bac1a3df0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6bac1a3ef0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6bac1d40b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,003 | xffl.learning.distributed |    DEBUG | [Rank 2]: distributed setup: DistributedState(rank=2, world_size=64, group_local_rank=2, group_local_size=4, group_rank=0, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fdbac5feb70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fdbac5fecb0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fdbac5fed70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,003 | xffl.learning.distributed |    DEBUG | [Rank 4]: distributed setup: DistributedState(rank=4, world_size=64, group_local_rank=0, group_local_size=4, group_rank=1, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbe9e3fab30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbe9e3fac70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbe9e3fac30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,003 | xffl.learning.distributed |    DEBUG | [Rank 5]: distributed setup: DistributedState(rank=5, world_size=64, group_local_rank=1, group_local_size=4, group_rank=1, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7d8b0b3430>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7d8b0b3470>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7d8b0b3570>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,003 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:33:12,004 | xffl.learning.distributed |    DEBUG | [Rank 3]: distributed setup: DistributedState(rank=3, world_size=64, group_local_rank=3, group_local_size=4, group_rank=0, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f32fdb37470>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f32fdb374b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f32fdb375b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,003 | xffl.learning.distributed |    DEBUG | [Rank 7]: distributed setup: DistributedState(rank=7, world_size=64, group_local_rank=3, group_local_size=4, group_rank=1, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f32245b6df0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f32245b6f30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f32245b6ef0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,004 | xffl.learning.distributed |    DEBUG | [Rank 6]: distributed setup: DistributedState(rank=6, world_size=64, group_local_rank=2, group_local_size=4, group_rank=1, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2e2e443370>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2e2e4433b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2e2e443630>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,008 | xffl.learning.distributed |    DEBUG | [Rank 56]: distributed setup: DistributedState(rank=56, world_size=64, group_local_rank=0, group_local_size=4, group_rank=14, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f54e6592ff0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f54e6593030>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f54e6593370>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,008 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:33:12,008 | xffl.learning.distributed |    DEBUG | [Rank 58]: distributed setup: DistributedState(rank=58, world_size=64, group_local_rank=2, group_local_size=4, group_rank=14, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f41fadffa30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f41fadffa70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f41fadffdb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 57]: distributed setup: DistributedState(rank=57, world_size=64, group_local_rank=1, group_local_size=4, group_rank=14, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8cc53532b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8cc53532f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8cc5353630>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,008 | xffl.learning.distributed |    DEBUG | [Rank 61]: distributed setup: DistributedState(rank=61, world_size=64, group_local_rank=1, group_local_size=4, group_rank=15, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb0a22d2a30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb0a22d2b70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb0a22d2d30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 62]: distributed setup: DistributedState(rank=62, world_size=64, group_local_rank=2, group_local_size=4, group_rank=15, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb69dd1b570>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb69dd1b5b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb69dd1b8b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:11,946 | xffl.learning.distributed |    DEBUG | [Rank 40]: distributed setup: DistributedState(rank=40, world_size=64, group_local_rank=0, group_local_size=4, group_rank=10, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f80a5c42fb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f80a5c42ff0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f80a5c43330>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:11,946 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:33:11,946 | xffl.learning.distributed |    DEBUG | [Rank 41]: distributed setup: DistributedState(rank=41, world_size=64, group_local_rank=1, group_local_size=4, group_rank=10, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff03b9134f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff03b913530>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff03b913870>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 18]: distributed setup: DistributedState(rank=18, world_size=64, group_local_rank=2, group_local_size=4, group_rank=4, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1df9ca6e70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1df9ca6eb0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1df9ca71f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 16]: distributed setup: DistributedState(rank=16, world_size=64, group_local_rank=0, group_local_size=4, group_rank=4, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f74df98b770>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f74df98b7b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f74df98baf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 17]: distributed setup: DistributedState(rank=17, world_size=64, group_local_rank=1, group_local_size=4, group_rank=4, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd114d93430>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd114d93470>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd114d937b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 60]: distributed setup: DistributedState(rank=60, world_size=64, group_local_rank=0, group_local_size=4, group_rank=15, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3803597230>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3803597270>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f38035975b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 59]: distributed setup: DistributedState(rank=59, world_size=64, group_local_rank=3, group_local_size=4, group_rank=14, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f17da240270>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f17da2402b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f17da2405f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 19]: distributed setup: DistributedState(rank=19, world_size=64, group_local_rank=3, group_local_size=4, group_rank=4, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc73dcdacb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc73dcdacf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc73dcdb030>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 21]: distributed setup: DistributedState(rank=21, world_size=64, group_local_rank=1, group_local_size=4, group_rank=5, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0d442935f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0d44293630>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0d44293970>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 20]: distributed setup: DistributedState(rank=20, world_size=64, group_local_rank=0, group_local_size=4, group_rank=5, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc31b85a630>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc31b85a670>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc31b85a9b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 24]: distributed setup: DistributedState(rank=24, world_size=64, group_local_rank=0, group_local_size=4, group_rank=6, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd9f9454230>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd9f9454270>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd9f94545b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 28]: distributed setup: DistributedState(rank=28, world_size=64, group_local_rank=0, group_local_size=4, group_rank=7, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa87b5eaab0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa87b5eaaf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa87b5eadf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 30]: distributed setup: DistributedState(rank=30, world_size=64, group_local_rank=2, group_local_size=4, group_rank=7, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff1d0c2f5f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff1d0c2f630>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff1d0c2f970>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 29]: distributed setup: DistributedState(rank=29, world_size=64, group_local_rank=1, group_local_size=4, group_rank=7, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcaa1d7ea30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcaa1d7ea70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcaa1d7edb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 8]: distributed setup: DistributedState(rank=8, world_size=64, group_local_rank=0, group_local_size=4, group_rank=2, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa2b6beebb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa2b6beebf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa2b6beef30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:33:11,947 | xffl.learning.distributed |    DEBUG | [Rank 42]: distributed setup: DistributedState(rank=42, world_size=64, group_local_rank=2, group_local_size=4, group_rank=10, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc8fad02ef0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc8fad02f30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc8fad03270>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 25]: distributed setup: DistributedState(rank=25, world_size=64, group_local_rank=1, group_local_size=4, group_rank=6, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3c30d62830>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3c30d62870>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3c30d62970>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:11,947 | xffl.learning.distributed |    DEBUG | [Rank 43]: distributed setup: DistributedState(rank=43, world_size=64, group_local_rank=3, group_local_size=4, group_rank=10, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f71178bf170>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f71178bf1b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f71178bf4f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 26]: distributed setup: DistributedState(rank=26, world_size=64, group_local_rank=2, group_local_size=4, group_rank=6, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f108386e470>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f108386e4b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f108386e7f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 22]: distributed setup: DistributedState(rank=22, world_size=64, group_local_rank=2, group_local_size=4, group_rank=5, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f46a86dfcf0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f46a86dfd30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f46a87040b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:11,947 | xffl.learning.distributed |    DEBUG | [Rank 44]: distributed setup: DistributedState(rank=44, world_size=64, group_local_rank=0, group_local_size=4, group_rank=11, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa10acb70f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa10acb7130>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa10acb7470>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:11,947 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:33:11,947 | xffl.learning.distributed |    DEBUG | [Rank 46]: distributed setup: DistributedState(rank=46, world_size=64, group_local_rank=2, group_local_size=4, group_rank=11, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f46f79efbf0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f46f79efc30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f46f79eff70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 9]: distributed setup: DistributedState(rank=9, world_size=64, group_local_rank=1, group_local_size=4, group_rank=2, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f591ab53430>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f591ab53470>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f591ab53770>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:11,947 | xffl.learning.distributed |    DEBUG | [Rank 45]: distributed setup: DistributedState(rank=45, world_size=64, group_local_rank=1, group_local_size=4, group_rank=11, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5a75e36ab0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5a75e36af0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5a75e36e30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 31]: distributed setup: DistributedState(rank=31, world_size=64, group_local_rank=3, group_local_size=4, group_rank=7, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc34a5f7ab0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc34a5f7af0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc34a5f7e30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,009 | xffl.learning.distributed |    DEBUG | [Rank 27]: distributed setup: DistributedState(rank=27, world_size=64, group_local_rank=3, group_local_size=4, group_rank=6, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa6195b2ab0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa6195b2af0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa6195b2e30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:11,947 | xffl.learning.distributed |    DEBUG | [Rank 47]: distributed setup: DistributedState(rank=47, world_size=64, group_local_rank=3, group_local_size=4, group_rank=11, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb740b16ef0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb740b16f30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb740b17270>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,010 | xffl.learning.distributed |    DEBUG | [Rank 10]: distributed setup: DistributedState(rank=10, world_size=64, group_local_rank=2, group_local_size=4, group_rank=2, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f30b976fbb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f30b976fbf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f30b976fcf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,010 | xffl.learning.distributed |    DEBUG | [Rank 23]: distributed setup: DistributedState(rank=23, world_size=64, group_local_rank=3, group_local_size=4, group_rank=5, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f765a0bf230>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f765a0bf270>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f765a0bf5b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,010 | xffl.learning.distributed |    DEBUG | [Rank 63]: distributed setup: DistributedState(rank=63, world_size=64, group_local_rank=3, group_local_size=4, group_rank=15, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f49bf4e28b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f49bf4e28f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f49bf4e2bf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,010 | xffl.learning.distributed |    DEBUG | [Rank 12]: distributed setup: DistributedState(rank=12, world_size=64, group_local_rank=0, group_local_size=4, group_rank=3, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7efd1bb6f5f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7efd1bb6f630>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7efd1bb6f970>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,010 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:33:12,010 | xffl.learning.distributed |    DEBUG | [Rank 15]: distributed setup: DistributedState(rank=15, world_size=64, group_local_rank=3, group_local_size=4, group_rank=3, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7faf362473f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7faf36247430>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7faf36247770>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,010 | xffl.learning.distributed |    DEBUG | [Rank 11]: distributed setup: DistributedState(rank=11, world_size=64, group_local_rank=3, group_local_size=4, group_rank=2, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f44615773b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f44615774f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f44615776f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,010 | xffl.learning.distributed |    DEBUG | [Rank 14]: distributed setup: DistributedState(rank=14, world_size=64, group_local_rank=2, group_local_size=4, group_rank=3, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb1df8778b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb1df8778f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb1df877c30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,010 | xffl.learning.distributed |    DEBUG | [Rank 13]: distributed setup: DistributedState(rank=13, world_size=64, group_local_rank=1, group_local_size=4, group_rank=3, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9e70583470>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9e705834b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9e705837f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,010 | xffl.learning.distributed |    DEBUG | [Rank 49]: distributed setup: DistributedState(rank=49, world_size=64, group_local_rank=1, group_local_size=4, group_rank=12, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f79e2aaef70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f79e2aaf0b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f79e2aaf2b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,010 | xffl.learning.distributed |    DEBUG | [Rank 48]: distributed setup: DistributedState(rank=48, world_size=64, group_local_rank=0, group_local_size=4, group_rank=12, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6c3807b030>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6c3807b070>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6c3807b170>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,010 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:33:12,010 | xffl.learning.distributed |    DEBUG | [Rank 53]: distributed setup: DistributedState(rank=53, world_size=64, group_local_rank=1, group_local_size=4, group_rank=13, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa12ed2b1b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa12ed2b1f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa12ed2b530>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,011 | xffl.learning.distributed |    DEBUG | [Rank 52]: distributed setup: DistributedState(rank=52, world_size=64, group_local_rank=0, group_local_size=4, group_rank=13, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc5215f68b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc5215f68f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc5215f6bf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,011 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:33:12,011 | xffl.learning.distributed |    DEBUG | [Rank 54]: distributed setup: DistributedState(rank=54, world_size=64, group_local_rank=2, group_local_size=4, group_rank=13, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7feee654af30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7feee654af70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7feee654b2b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:11,948 | xffl.learning.distributed |    DEBUG | [Rank 33]: distributed setup: DistributedState(rank=33, world_size=64, group_local_rank=1, group_local_size=4, group_rank=8, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5cf42230f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5cf4223130>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5cf4223470>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:11,948 | xffl.learning.distributed |    DEBUG | [Rank 32]: distributed setup: DistributedState(rank=32, world_size=64, group_local_rank=0, group_local_size=4, group_rank=8, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f672212f7f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f672212f830>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f672212fb70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:11,948 | xffl.learning.distributed |    DEBUG | [Rank 34]: distributed setup: DistributedState(rank=34, world_size=64, group_local_rank=2, group_local_size=4, group_rank=8, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2d4d44eaf0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2d4d44eb30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2d4d44ee70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:11,948 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:33:11,948 | xffl.learning.distributed |    DEBUG | [Rank 37]: distributed setup: DistributedState(rank=37, world_size=64, group_local_rank=1, group_local_size=4, group_rank=9, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa74dccf130>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa74dccf170>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa74dccf4b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,011 | xffl.learning.distributed |    DEBUG | [Rank 55]: distributed setup: DistributedState(rank=55, world_size=64, group_local_rank=3, group_local_size=4, group_rank=13, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4aa726eaf0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4aa726ebf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4aa726ee30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,011 | xffl.learning.distributed |    DEBUG | [Rank 51]: distributed setup: DistributedState(rank=51, world_size=64, group_local_rank=3, group_local_size=4, group_rank=12, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f490b9129b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f490b9129f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f490b912d30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,011 | xffl.learning.distributed |    DEBUG | [Rank 50]: distributed setup: DistributedState(rank=50, world_size=64, group_local_rank=2, group_local_size=4, group_rank=12, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcb7dfff730>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcb7dfff870>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcb7dfff830>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:11,949 | xffl.learning.distributed |    DEBUG | [Rank 39]: distributed setup: DistributedState(rank=39, world_size=64, group_local_rank=3, group_local_size=4, group_rank=9, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6e977e7670>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6e977e77b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6e977e79b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:11,949 | xffl.learning.distributed |    DEBUG | [Rank 36]: distributed setup: DistributedState(rank=36, world_size=64, group_local_rank=0, group_local_size=4, group_rank=9, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3839b52d30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3839b52e70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3839b53070>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:11,949 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:33:11,949 | xffl.learning.distributed |    DEBUG | [Rank 38]: distributed setup: DistributedState(rank=38, world_size=64, group_local_rank=2, group_local_size=4, group_rank=9, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f707c6b7670>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f707c6b76b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f707c6b79f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:11,949 | xffl.learning.distributed |    DEBUG | [Rank 35]: distributed setup: DistributedState(rank=35, world_size=64, group_local_rank=3, group_local_size=4, group_rank=8, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f748632f530>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f748632f570>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f748632f8b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:33:12,109 | xffl.learning.utils |    DEBUG | [Rank 0]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:12,113 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:12,114 | xffl.learning.utils |    DEBUG | [Rank 12]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:12,114 | xffl.learning.utils |    DEBUG | [Rank 16]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:12,115 | xffl.learning.utils |    DEBUG | [Rank 20]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:12,115 | xffl.learning.utils |    DEBUG | [Rank 8]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:12,116 | xffl.learning.utils |    DEBUG | [Rank 24]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:12,118 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:12,118 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:12,119 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:12,119 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:12,120 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:12,164 | xffl.learning.utils |    DEBUG | [Rank 60]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:12,171 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:12,114 | xffl.learning.utils |    DEBUG | [Rank 44]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:12,178 | xffl.learning.utils |    DEBUG | [Rank 56]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:12,116 | xffl.learning.utils |    DEBUG | [Rank 32]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:12,120 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:12,121 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:12,186 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:12,187 | xffl.learning.utils |    DEBUG | [Rank 4]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:12,125 | xffl.learning.utils |    DEBUG | [Rank 36]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:12,191 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:12,192 | xffl.learning.utils |    DEBUG | [Rank 28]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:12,130 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:12,196 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:12,135 | xffl.learning.utils |    DEBUG | [Rank 40]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:12,141 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:12,226 | xffl.learning.utils |    DEBUG | [Rank 52]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:12,234 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:12,234 | xffl.learning.utils |    DEBUG | [Rank 48]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:12,240 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,735 | xffl.learning.utils |    DEBUG | [Rank 18]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,739 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,743 | xffl.learning.utils |    DEBUG | [Rank 10]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,747 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,750 | xffl.learning.utils |    DEBUG | [Rank 7]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,754 | xffl.learning.utils |    DEBUG | [Rank 19]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,754 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,756 | xffl.learning.utils |    DEBUG | [Rank 26]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,756 | xffl.learning.utils |    DEBUG | [Rank 3]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,757 | xffl.learning.utils |    DEBUG | [Rank 11]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,757 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,760 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,760 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,761 | xffl.learning.utils |    DEBUG | [Rank 2]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,761 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,761 | xffl.learning.utils |    DEBUG | [Rank 15]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,765 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,766 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,767 | xffl.learning.utils |    DEBUG | [Rank 6]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,769 | xffl.learning.utils |    DEBUG | [Rank 30]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,771 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,773 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,773 | xffl.learning.utils |    DEBUG | [Rank 27]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,774 | xffl.learning.utils |    DEBUG | [Rank 31]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,776 | xffl.learning.utils |    DEBUG | [Rank 23]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,777 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,778 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,780 | xffl.learning.utils |    DEBUG | [Rank 22]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,780 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,780 | xffl.learning.utils |    DEBUG | [Rank 5]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,781 | xffl.learning.utils |    DEBUG | [Rank 14]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,784 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,784 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,785 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,800 | xffl.learning.utils |    DEBUG | [Rank 13]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,803 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,811 | xffl.learning.utils |    DEBUG | [Rank 17]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,812 | xffl.learning.utils |    DEBUG | [Rank 9]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,812 | xffl.learning.utils |    DEBUG | [Rank 25]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,815 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,815 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,816 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,827 | xffl.learning.utils |    DEBUG | [Rank 1]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,831 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,842 | xffl.learning.utils |    DEBUG | [Rank 21]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,844 | xffl.learning.utils |    DEBUG | [Rank 29]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:13,846 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:13,848 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,484 | xffl.learning.utils |    DEBUG | [Rank 34]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,488 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,526 | xffl.learning.utils |    DEBUG | [Rank 35]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,530 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,544 | xffl.learning.utils |    DEBUG | [Rank 47]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,548 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,559 | xffl.learning.utils |    DEBUG | [Rank 46]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,563 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,588 | xffl.learning.utils |    DEBUG | [Rank 43]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,651 | xffl.learning.utils |    DEBUG | [Rank 62]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,652 | xffl.learning.utils |    DEBUG | [Rank 63]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,592 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,655 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,656 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,600 | xffl.learning.utils |    DEBUG | [Rank 42]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,604 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,679 | xffl.learning.utils |    DEBUG | [Rank 45]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,683 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,693 | xffl.learning.utils |    DEBUG | [Rank 33]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,697 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,764 | xffl.learning.utils |    DEBUG | [Rank 61]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,768 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,775 | xffl.learning.utils |    DEBUG | [Rank 55]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,776 | xffl.learning.utils |    DEBUG | [Rank 54]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,779 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,779 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,798 | xffl.learning.utils |    DEBUG | [Rank 59]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,800 | xffl.learning.utils |    DEBUG | [Rank 58]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,802 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,804 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,743 | xffl.learning.utils |    DEBUG | [Rank 41]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,747 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,760 | xffl.learning.utils |    DEBUG | [Rank 39]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,761 | xffl.learning.utils |    DEBUG | [Rank 38]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,824 | xffl.learning.utils |    DEBUG | [Rank 50]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,764 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,764 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,828 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,851 | xffl.learning.utils |    DEBUG | [Rank 51]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,855 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,935 | xffl.learning.utils |    DEBUG | [Rank 53]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,938 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,884 | xffl.learning.utils |    DEBUG | [Rank 37]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,888 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,958 | xffl.learning.utils |    DEBUG | [Rank 57]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,962 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:14,993 | xffl.learning.utils |    DEBUG | [Rank 49]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:33:14,997 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:33:17,910 | xffl.learning.modelling |    DEBUG | [Rank 4]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:17,911 | xffl.learning.distributed |    DEBUG | [Rank 4]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:17,925 | xffl.learning.modelling |    DEBUG | [Rank 28]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:17,925 | xffl.learning.distributed |    DEBUG | [Rank 28]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,007 |         __main__ |    DEBUG | Model loading time: 4.99 seconds[0m
[38;5;39m2025-03-26 00:33:18,008 |         __main__ |    DEBUG | Training llama3.1-8b: 8030.26 million trainable parameters[0m
[38;5;39m2025-03-26 00:33:18,008 | xffl.learning.modelling |    DEBUG | [Rank 0]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,008 | xffl.learning.distributed |    DEBUG | [Rank 0]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,027 | xffl.learning.modelling |    DEBUG | [Rank 12]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,027 | xffl.learning.distributed |    DEBUG | [Rank 12]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,082 | xffl.learning.modelling |    DEBUG | [Rank 8]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,083 | xffl.learning.distributed |    DEBUG | [Rank 8]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,086 | xffl.learning.modelling |    DEBUG | [Rank 20]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,086 | xffl.learning.distributed |    DEBUG | [Rank 20]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,096 | xffl.learning.modelling |    DEBUG | [Rank 16]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,096 | xffl.learning.distributed |    DEBUG | [Rank 16]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,105 | xffl.learning.modelling |    DEBUG | [Rank 24]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,106 | xffl.learning.distributed |    DEBUG | [Rank 24]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,463 | xffl.learning.modelling |    DEBUG | [Rank 5]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,463 | xffl.learning.distributed |    DEBUG | [Rank 5]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,528 | xffl.learning.modelling |    DEBUG | [Rank 18]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,528 | xffl.learning.distributed |    DEBUG | [Rank 18]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,534 | xffl.learning.modelling |    DEBUG | [Rank 3]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,534 | xffl.learning.distributed |    DEBUG | [Rank 3]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,536 | xffl.learning.modelling |    DEBUG | [Rank 2]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,536 | xffl.learning.distributed |    DEBUG | [Rank 2]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,536 | xffl.learning.modelling |    DEBUG | [Rank 19]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,536 | xffl.learning.distributed |    DEBUG | [Rank 19]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,541 | xffl.learning.modelling |    DEBUG | [Rank 17]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,541 | xffl.learning.distributed |    DEBUG | [Rank 17]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,544 | xffl.learning.modelling |    DEBUG | [Rank 29]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,544 | xffl.learning.distributed |    DEBUG | [Rank 29]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,571 | xffl.learning.modelling |    DEBUG | [Rank 6]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,571 | xffl.learning.distributed |    DEBUG | [Rank 6]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,571 | xffl.learning.modelling |    DEBUG | [Rank 30]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,571 | xffl.learning.distributed |    DEBUG | [Rank 30]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,572 | xffl.learning.modelling |    DEBUG | [Rank 31]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,572 | xffl.learning.distributed |    DEBUG | [Rank 31]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,576 | xffl.learning.modelling |    DEBUG | [Rank 27]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,577 | xffl.learning.distributed |    DEBUG | [Rank 27]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,579 | xffl.learning.modelling |    DEBUG | [Rank 26]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,579 | xffl.learning.distributed |    DEBUG | [Rank 26]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,584 | xffl.learning.modelling |    DEBUG | [Rank 13]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,584 | xffl.learning.distributed |    DEBUG | [Rank 13]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,588 | xffl.learning.modelling |    DEBUG | [Rank 10]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,588 | xffl.learning.distributed |    DEBUG | [Rank 10]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,594 | xffl.learning.modelling |    DEBUG | [Rank 25]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,594 | xffl.learning.distributed |    DEBUG | [Rank 25]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,596 | xffl.learning.modelling |    DEBUG | [Rank 7]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,596 | xffl.learning.distributed |    DEBUG | [Rank 7]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,600 | xffl.learning.modelling |    DEBUG | [Rank 9]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,600 | xffl.learning.distributed |    DEBUG | [Rank 9]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,600 | xffl.learning.modelling |    DEBUG | [Rank 21]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,601 | xffl.learning.distributed |    DEBUG | [Rank 21]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,608 | xffl.learning.modelling |    DEBUG | [Rank 14]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,608 | xffl.learning.distributed |    DEBUG | [Rank 14]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,611 | xffl.learning.modelling |    DEBUG | [Rank 1]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,612 | xffl.learning.distributed |    DEBUG | [Rank 1]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,615 | xffl.learning.modelling |    DEBUG | [Rank 11]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,615 | xffl.learning.distributed |    DEBUG | [Rank 11]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,623 | xffl.learning.modelling |    DEBUG | [Rank 15]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,623 | xffl.learning.distributed |    DEBUG | [Rank 15]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,669 | xffl.learning.modelling |    DEBUG | [Rank 23]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,669 | xffl.learning.distributed |    DEBUG | [Rank 23]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,669 | xffl.learning.modelling |    DEBUG | [Rank 22]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,669 | xffl.learning.distributed |    DEBUG | [Rank 22]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:18,985 | xffl.learning.modelling |    DEBUG | [Rank 56]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:18,986 | xffl.learning.distributed |    DEBUG | [Rank 56]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,012 | xffl.learning.modelling |    DEBUG | [Rank 48]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,012 | xffl.learning.distributed |    DEBUG | [Rank 48]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,032 | xffl.learning.modelling |    DEBUG | [Rank 60]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,032 | xffl.learning.distributed |    DEBUG | [Rank 60]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,041 | xffl.learning.modelling |    DEBUG | [Rank 32]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,041 | xffl.learning.distributed |    DEBUG | [Rank 32]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,047 | xffl.learning.modelling |    DEBUG | [Rank 44]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,047 | xffl.learning.distributed |    DEBUG | [Rank 44]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,113 | xffl.learning.modelling |    DEBUG | [Rank 52]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,113 | xffl.learning.distributed |    DEBUG | [Rank 52]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,060 | xffl.learning.modelling |    DEBUG | [Rank 36]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,060 | xffl.learning.distributed |    DEBUG | [Rank 36]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,063 | xffl.learning.modelling |    DEBUG | [Rank 40]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,063 | xffl.learning.distributed |    DEBUG | [Rank 40]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,427 | xffl.learning.modelling |    DEBUG | [Rank 43]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,427 | xffl.learning.distributed |    DEBUG | [Rank 43]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,443 | xffl.learning.modelling |    DEBUG | [Rank 42]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,444 | xffl.learning.distributed |    DEBUG | [Rank 42]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,453 | xffl.learning.modelling |    DEBUG | [Rank 47]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,453 | xffl.learning.distributed |    DEBUG | [Rank 47]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,476 | xffl.learning.modelling |    DEBUG | [Rank 35]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,476 | xffl.learning.modelling |    DEBUG | [Rank 46]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,476 | xffl.learning.distributed |    DEBUG | [Rank 35]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,476 | xffl.learning.distributed |    DEBUG | [Rank 46]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,552 | xffl.learning.modelling |    DEBUG | [Rank 63]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,552 | xffl.learning.distributed |    DEBUG | [Rank 63]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,491 | xffl.learning.modelling |    DEBUG | [Rank 34]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,492 | xffl.learning.distributed |    DEBUG | [Rank 34]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,557 | xffl.learning.modelling |    DEBUG | [Rank 62]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,557 | xffl.learning.distributed |    DEBUG | [Rank 62]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,607 | xffl.learning.modelling |    DEBUG | [Rank 53]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,608 | xffl.learning.distributed |    DEBUG | [Rank 53]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,638 | xffl.learning.modelling |    DEBUG | [Rank 54]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,639 | xffl.learning.distributed |    DEBUG | [Rank 54]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,649 | xffl.learning.modelling |    DEBUG | [Rank 61]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,650 | xffl.learning.distributed |    DEBUG | [Rank 61]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,655 | xffl.learning.modelling |    DEBUG | [Rank 55]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,655 | xffl.learning.distributed |    DEBUG | [Rank 55]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,595 | xffl.learning.modelling |    DEBUG | [Rank 39]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,595 | xffl.learning.distributed |    DEBUG | [Rank 39]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,596 | xffl.learning.modelling |    DEBUG | [Rank 38]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,596 | xffl.learning.distributed |    DEBUG | [Rank 38]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,597 | xffl.learning.modelling |    DEBUG | [Rank 33]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,597 | xffl.learning.distributed |    DEBUG | [Rank 33]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,706 | xffl.learning.modelling |    DEBUG | [Rank 51]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,706 | xffl.learning.distributed |    DEBUG | [Rank 51]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,714 | xffl.learning.modelling |    DEBUG | [Rank 50]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,714 | xffl.learning.distributed |    DEBUG | [Rank 50]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,718 | xffl.learning.modelling |    DEBUG | [Rank 59]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,718 | xffl.learning.distributed |    DEBUG | [Rank 59]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,663 | xffl.learning.modelling |    DEBUG | [Rank 41]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,663 | xffl.learning.distributed |    DEBUG | [Rank 41]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,734 | xffl.learning.modelling |    DEBUG | [Rank 58]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,734 | xffl.learning.distributed |    DEBUG | [Rank 58]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,686 | xffl.learning.modelling |    DEBUG | [Rank 37]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,686 | xffl.learning.distributed |    DEBUG | [Rank 37]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,898 | xffl.learning.modelling |    DEBUG | [Rank 49]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,898 | xffl.learning.distributed |    DEBUG | [Rank 49]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,835 | xffl.learning.modelling |    DEBUG | [Rank 45]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,836 | xffl.learning.distributed |    DEBUG | [Rank 45]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:19,898 | xffl.learning.modelling |    DEBUG | [Rank 57]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:33:19,898 | xffl.learning.distributed |    DEBUG | [Rank 57]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:33:26,861 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,861 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,861 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,861 |         __main__ |    DEBUG | FSDP wrapping setup time: 8.85 seconds[0m
[38;5;39m2025-03-26 00:33:26,862 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,862 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,863 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,864 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,864 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,905 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,906 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,906 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,906 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,906 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,906 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,906 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,907 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,958 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,958 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,958 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,959 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,959 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,959 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,959 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,959 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:26,988 |         __main__ |    DEBUG | Dataset loading time: 0.13 seconds[0m
[38;5;39m2025-03-26 00:33:27,018 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:27,018 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:27,018 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:27,018 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:27,018 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:27,018 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:27,018 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:27,018 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:27,019 |         __main__ |    DEBUG | [Rank 5]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,019 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,019 |         __main__ |    DEBUG | [Rank 4]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:33:27,019 |         __main__ |    DEBUG | [Rank 6]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,019 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:33:27,019 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,021 |         __main__ |    DEBUG | train set size: 16384 samples[0m
[38;5;39m2025-03-26 00:33:27,021 |         __main__ |    DEBUG | [Rank 2]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,021 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,021 |         __main__ |    DEBUG | train dataloader size: 32 minibatches[0m
[38;5;39m2025-03-26 00:33:27,021 |         __main__ |    DEBUG | val set size: 5311 samples[0m
[38;5;39m2025-03-26 00:33:27,021 |         __main__ |    DEBUG | val dataloader size: 82 minibatches[0m
[38;5;39m2025-03-26 00:33:27,021 |         __main__ |    DEBUG | Dataloaders creation time: 0.03 seconds[0m
[38;5;39m2025-03-26 00:33:27,022 |         __main__ |    DEBUG | Learning rate adjusted to: 0.0002[0m
[38;5;39m2025-03-26 00:33:27,022 |         __main__ |    DEBUG | [Rank 1]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,022 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,023 |         __main__ |    DEBUG | Total setup time: 21.49 seconds[0m
[38;5;39m2025-03-26 00:33:27,023 |         __main__ |    DEBUG | [Rank 0]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,023 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,042 |         __main__ |    DEBUG | [Rank 7]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,042 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,043 |         __main__ |    DEBUG | [Rank 3]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,043 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,060 |         __main__ |    DEBUG | [Rank 30]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,060 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,060 |         __main__ |    DEBUG | [Rank 31]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:33:27,061 |         __main__ |    DEBUG | [Rank 29]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,061 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:33:27,061 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,065 |         __main__ |    DEBUG | [Rank 27]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:33:27,065 |         __main__ |    DEBUG | [Rank 25]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,066 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:33:27,066 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,065 |         __main__ |    DEBUG | [Rank 26]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,066 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,081 |         __main__ |    DEBUG | [Rank 28]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,082 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,085 |         __main__ |    DEBUG | [Rank 24]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,085 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,113 |         __main__ |    DEBUG | [Rank 22]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,113 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,114 |         __main__ |    DEBUG | [Rank 20]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:33:27,114 |         __main__ |    DEBUG | [Rank 21]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,114 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:33:27,114 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,117 |         __main__ |    DEBUG | [Rank 18]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,117 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,117 |         __main__ |    DEBUG | [Rank 16]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:33:27,117 |         __main__ |    DEBUG | [Rank 17]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,117 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:33:27,118 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,132 |         __main__ |    DEBUG | [Rank 23]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,132 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,137 |         __main__ |    DEBUG | [Rank 19]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,137 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,175 |         __main__ |    DEBUG | [Rank 13]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,175 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,176 |         __main__ |    DEBUG | [Rank 14]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,176 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,177 |         __main__ |    DEBUG | [Rank 15]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:33:27,177 |         __main__ |    DEBUG | [Rank 8]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,177 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:33:27,177 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,177 |         __main__ |    DEBUG | [Rank 10]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,178 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,177 |         __main__ |    DEBUG | [Rank 11]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,178 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,196 |         __main__ |    DEBUG | [Rank 9]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,197 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:27,197 |         __main__ |    DEBUG | [Rank 12]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:27,197 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,247 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,247 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,247 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,247 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,247 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,250 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,251 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,251 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,345 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,345 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,345 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,346 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,347 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,347 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,348 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,348 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,307 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,307 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,307 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,307 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,307 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,307 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,308 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,308 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,324 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,325 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,325 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,325 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,325 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,325 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,325 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,325 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:33:28,430 |         __main__ |    DEBUG | [Rank 59]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,430 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,431 |         __main__ |    DEBUG | [Rank 57]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:33:28,431 |         __main__ |    DEBUG | [Rank 56]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,431 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:33:28,431 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,434 |         __main__ |    DEBUG | [Rank 63]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:33:28,434 |         __main__ |    DEBUG | [Rank 60]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,434 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:33:28,435 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,434 |         __main__ |    DEBUG | [Rank 62]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,435 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,464 |         __main__ |    DEBUG | [Rank 58]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,464 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,468 |         __main__ |    DEBUG | [Rank 61]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,468 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,522 |         __main__ |    DEBUG | [Rank 54]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,522 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,522 |         __main__ |    DEBUG | [Rank 52]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,522 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,522 |         __main__ |    DEBUG | [Rank 55]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,522 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,524 |         __main__ |    DEBUG | [Rank 51]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,524 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,524 |         __main__ |    DEBUG | [Rank 48]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,525 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,525 |         __main__ |    DEBUG | [Rank 49]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,525 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,538 |         __main__ |    DEBUG | [Rank 53]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,539 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,541 |         __main__ |    DEBUG | [Rank 50]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,542 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,493 |         __main__ |    DEBUG | [Rank 42]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,494 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,494 |         __main__ |    DEBUG | [Rank 40]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,494 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,494 |         __main__ |    DEBUG | [Rank 43]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,494 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,501 |         __main__ |    DEBUG | [Rank 33]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,501 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,501 |         __main__ |    DEBUG | [Rank 34]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:33:28,501 |         __main__ |    DEBUG | [Rank 32]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,501 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:33:28,501 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,503 |         __main__ |    DEBUG | [Rank 36]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,503 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,504 |         __main__ |    DEBUG | [Rank 38]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:33:28,504 |         __main__ |    DEBUG | [Rank 37]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,504 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:33:28,504 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,512 |         __main__ |    DEBUG | [Rank 41]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,513 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,516 |         __main__ |    DEBUG | [Rank 35]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,516 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,517 |         __main__ |    DEBUG | [Rank 44]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,517 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,517 |         __main__ |    DEBUG | [Rank 46]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:33:28,517 |         __main__ |    DEBUG | [Rank 45]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,518 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:33:28,518 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,519 |         __main__ |    DEBUG | [Rank 39]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,519 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:33:28,536 |         __main__ |    DEBUG | [Rank 47]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:33:28,536 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:36:35,253 | xffl.learning.processing |     INFO | Epoch 1: train_perplexity=41255.5781, train_epoch_loss=10.6275, epoch time 187.20222861296497s[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 25]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 26]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 27]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 24]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 29]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 28]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 30]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 31]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 21]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 20]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 23]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 22]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 18]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 19]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 17]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 16]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 14]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 13]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 12]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 15]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 11]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 10]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 9]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,253 | xffl.learning.distributed |    DEBUG | [Rank 8]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,254 | xffl.learning.distributed |    DEBUG | [Rank 1]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,254 | xffl.learning.distributed |    DEBUG | [Rank 4]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,254 | xffl.learning.distributed |    DEBUG | [Rank 7]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,254 | xffl.learning.distributed |    DEBUG | [Rank 6]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,254 | xffl.learning.distributed |    DEBUG | [Rank 5]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,255 | xffl.learning.distributed |    DEBUG | [Rank 3]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,255 | xffl.learning.distributed |    DEBUG | [Rank 2]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,255 |         __main__ |    DEBUG | Key: avg_epoch_time, Value: 187.20222861296497[0m
[38;5;39m2025-03-26 00:36:35,255 |         __main__ |    DEBUG | Key: avg_train_perp, Value: 41255.578125[0m
[38;5;39m2025-03-26 00:36:35,255 |         __main__ |    DEBUG | Key: avg_train_loss, Value: 10.627541542053223[0m
[38;5;39m2025-03-26 00:36:35,192 | xffl.learning.distributed |    DEBUG | [Rank 35]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,192 | xffl.learning.distributed |    DEBUG | [Rank 33]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,192 | xffl.learning.distributed |    DEBUG | [Rank 34]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,192 | xffl.learning.distributed |    DEBUG | [Rank 32]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,192 | xffl.learning.distributed |    DEBUG | [Rank 37]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,192 | xffl.learning.distributed |    DEBUG | [Rank 36]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,192 | xffl.learning.distributed |    DEBUG | [Rank 38]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,192 | xffl.learning.distributed |    DEBUG | [Rank 39]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,256 | xffl.learning.distributed |    DEBUG | [Rank 0]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,256 | xffl.learning.distributed |    DEBUG | [Rank 51]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,256 | xffl.learning.distributed |    DEBUG | [Rank 49]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,256 | xffl.learning.distributed |    DEBUG | [Rank 50]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,256 | xffl.learning.distributed |    DEBUG | [Rank 48]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,256 | xffl.learning.distributed |    DEBUG | [Rank 52]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,256 | xffl.learning.distributed |    DEBUG | [Rank 54]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,256 | xffl.learning.distributed |    DEBUG | [Rank 53]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,256 | xffl.learning.distributed |    DEBUG | [Rank 55]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,219 | xffl.learning.distributed |    DEBUG | [Rank 41]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,219 | xffl.learning.distributed |    DEBUG | [Rank 42]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,219 | xffl.learning.distributed |    DEBUG | [Rank 40]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,219 | xffl.learning.distributed |    DEBUG | [Rank 45]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,219 | xffl.learning.distributed |    DEBUG | [Rank 47]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,219 | xffl.learning.distributed |    DEBUG | [Rank 44]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,220 | xffl.learning.distributed |    DEBUG | [Rank 43]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,219 | xffl.learning.distributed |    DEBUG | [Rank 46]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,351 | xffl.learning.distributed |    DEBUG | [Rank 57]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,351 | xffl.learning.distributed |    DEBUG | [Rank 56]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,351 | xffl.learning.distributed |    DEBUG | [Rank 58]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,351 | xffl.learning.distributed |    DEBUG | [Rank 59]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,351 | xffl.learning.distributed |    DEBUG | [Rank 62]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,351 | xffl.learning.distributed |    DEBUG | [Rank 63]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,351 | xffl.learning.distributed |    DEBUG | [Rank 60]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:36:35,351 | xffl.learning.distributed |    DEBUG | [Rank 61]: calling destroy_process_group[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-gw39fswo[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-gw39fswo/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-87ryas41[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-87ryas41/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-fnnh1myl[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-fnnh1myl/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003312-999gn8vx[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003312-999gn8vx/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-9q03wwam[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-9q03wwam/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-cgqcevb7[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-cgqcevb7/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003312-je8a84kq[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003312-je8a84kq/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-jv8dbyyz[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-jv8dbyyz/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003312-3xg2p72k[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003312-3xg2p72k/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003312-z0vm3tpq[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003312-z0vm3tpq/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-0cmnbd7g[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-0cmnbd7g/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-fn9gscc6[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-fn9gscc6/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-w4r8gi63[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-w4r8gi63/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-yhra3po5[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-yhra3po5/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-01glbpl0[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-01glbpl0/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003312-scwf5n7l[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003312-scwf5n7l/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003312-nkfkro9j[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003312-nkfkro9j/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-igdqxbdf[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-igdqxbdf/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-gg8dd1aa[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-gg8dd1aa/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-5uc1dq5k[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-5uc1dq5k/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003312-xqsgfzt2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003312-xqsgfzt2/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-o8rnjwqf[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-o8rnjwqf/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-p6ilfk2y[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-p6ilfk2y/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-4o6jtd8u[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-4o6jtd8u/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003312-hao6cji2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003312-hao6cji2/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-2yjhkedo[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-2yjhkedo/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-3vd0rel4[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-3vd0rel4/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-hr3iz5ix[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-hr3iz5ix/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003312-xhxpr80n[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003312-xhxpr80n/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-fu0eeya2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-fu0eeya2/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-uf7sxr8z[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-uf7sxr8z/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-3ewqpmz6[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-3ewqpmz6/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003312-4ia08785[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003312-4ia08785/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-1sotuy3o[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-1sotuy3o/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-1w23j29e[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-1w23j29e/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-drz54rir[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-drz54rir/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003312-nek5wgwx[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003312-nek5wgwx/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-kx1123vw[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-kx1123vw/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-pyf2wd5l[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-pyf2wd5l/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003312-itca829x[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003312-itca829x/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-q9ln17sx[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-q9ln17sx/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-o2svppsl[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-o2svppsl/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-g60b3z83[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-g60b3z83/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-hsp7ctlu[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-hsp7ctlu/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-jav91t95[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-jav91t95/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-v821i6qg[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-v821i6qg/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003312-ty3xjq2q[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003312-ty3xjq2q/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-6g3nivvh[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-6g3nivvh/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-z2coctnw[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-z2coctnw/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-35ulq8c2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-35ulq8c2/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003312-7ymoqotu[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003312-7ymoqotu/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003312-4kezvg1d[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003312-4kezvg1d/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-nwfimw2b[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-nwfimw2b/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003313-6ocvrcwq[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003313-6ocvrcwq/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-ug4k6vdh[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-ug4k6vdh/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-0ppbspfj[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-0ppbspfj/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-vveo1sks[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-vveo1sks/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-ambq9ydr[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-ambq9ydr/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-mm7cj2k8[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-mm7cj2k8/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-ddee8myb[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-ddee8myb/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003312-kvgqm0y3[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003312-kvgqm0y3/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-g3nwixd5[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-g3nwixd5/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-f66c4lf3[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-f66c4lf3/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003314-2wi42q0z[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003314-2wi42q0z/logs[0m
[38;20m2025-03-26 00:36:40,038 | xffl.cli.simulate |     INFO | Total simulation execution time: 224.28 seconds[0m
[38;20m2025-03-26 00:36:40,039 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
