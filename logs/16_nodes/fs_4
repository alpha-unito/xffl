2, 1, 2
[38;20m2025-03-26 00:36:50,261 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
[38;5;226m2025-03-26 00:36:50,262 |   xffl.cli.utils |  WARNING | CLI argument "help" has got default value "False"[0m
[38;5;226m2025-03-26 00:36:50,262 |   xffl.cli.utils |  WARNING | CLI argument "workdir" has got default value "/leonardo_scratch/fast/uToID_bench/xffl"[0m
[38;5;226m2025-03-26 00:36:50,262 |   xffl.cli.utils |  WARNING | CLI argument "image" has got default value "None"[0m
[38;5;39m2025-03-26 00:36:50,264 | xffl.cli.simulate |    DEBUG | Using virtual environment: /leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate[0m
[38;5;39m2025-03-26 00:36:50,264 | xffl.cli.simulate |    DEBUG | New local simulation xFFL environment variables: {'XFFL_WORLD_SIZE': '64', 'XFFL_NUM_NODES': '16', 'MASTER_ADDR': 'lrdn0001', 'VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;5;39m2025-03-26 00:36:50,264 | xffl.cli.simulate |    DEBUG | Updated xFFL environment: {'XFFL_WORLD_SIZE': '64', 'XFFL_NUM_NODES': '16', 'MASTER_ADDR': 'lrdn0001', 'VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;20m2025-03-26 00:36:50,264 | xffl.cli.simulate |     INFO | Running local simulation...[0m
[38;5;39m2025-03-26 00:36:50,264 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0001: ssh -oStrictHostKeyChecking=no lrdn0001 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=0 XFFL_FEDERATED_RANK=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:36:50,265 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0010: ssh -oStrictHostKeyChecking=no lrdn0010 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=1 XFFL_FEDERATED_RANK=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:36:50,265 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0182: ssh -oStrictHostKeyChecking=no lrdn0182 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=2 XFFL_FEDERATED_RANK=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:36:50,265 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0189: ssh -oStrictHostKeyChecking=no lrdn0189 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=3 XFFL_FEDERATED_RANK=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:36:50,265 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0361: ssh -oStrictHostKeyChecking=no lrdn0361 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=4 XFFL_FEDERATED_RANK=2 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:36:50,266 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0370: ssh -oStrictHostKeyChecking=no lrdn0370 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=5 XFFL_FEDERATED_RANK=2 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:36:50,266 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0542: ssh -oStrictHostKeyChecking=no lrdn0542 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=6 XFFL_FEDERATED_RANK=3 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:36:50,266 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0549: ssh -oStrictHostKeyChecking=no lrdn0549 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=7 XFFL_FEDERATED_RANK=3 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:36:50,266 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0722: ssh -oStrictHostKeyChecking=no lrdn0722 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=8 XFFL_FEDERATED_RANK=4 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:36:50,267 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0730: ssh -oStrictHostKeyChecking=no lrdn0730 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=9 XFFL_FEDERATED_RANK=4 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:36:50,267 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0901: ssh -oStrictHostKeyChecking=no lrdn0901 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=10 XFFL_FEDERATED_RANK=5 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:36:50,267 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0911: ssh -oStrictHostKeyChecking=no lrdn0911 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=11 XFFL_FEDERATED_RANK=5 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:36:50,267 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1084: ssh -oStrictHostKeyChecking=no lrdn1084 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=12 XFFL_FEDERATED_RANK=6 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:36:50,268 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1090: ssh -oStrictHostKeyChecking=no lrdn1090 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=13 XFFL_FEDERATED_RANK=6 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:36:50,268 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1261: ssh -oStrictHostKeyChecking=no lrdn1261 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=14 XFFL_FEDERATED_RANK=7 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:36:50,268 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn1281: ssh -oStrictHostKeyChecking=no lrdn1281 " XFFL_WORLD_SIZE=64 XFFL_NUM_NODES=16 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2,2,2,2,2  XFFL_NODEID=15 XFFL_FEDERATED_RANK=7 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 16384 -dbg -t 8 -wb -name llama3.1-16_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:36:59,648 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:36:59,648 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:36:59,648 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:36:59,648 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:36:59,710 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:36:59,710 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:36:59,710 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:36:59,710 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:36:59,711 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:36:59,711 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:36:59,711 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:36:59,711 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:36:59,711 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:36:59,711 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:36:59,711 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:36:59,711 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,058 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,058 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,058 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,058 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,059 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,059 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,059 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,059 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,115 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,115 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,115 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,115 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,203 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,203 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,203 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,203 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,239 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,239 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,239 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,239 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,182 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,182 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,182 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,182 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,943 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,943 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,943 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:00,943 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,267 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,267 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,267 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,267 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,567 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,567 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,567 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,567 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,511 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,511 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,511 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,511 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,586 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,586 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,586 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,586 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,589 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,589 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,589 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:01,589 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:37:02,892 | xffl.learning.distributed |    DEBUG | [Rank 0]: distributed setup: DistributedState(rank=0, world_size=64, group_local_rank=0, group_local_size=4, group_rank=0, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f29895073f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2989507530>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f29895074f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,892 |         __main__ |    DEBUG | Randez-vous time: 1.62 seconds[0m
[38;5;39m2025-03-26 00:37:02,892 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:37:02,892 | xffl.learning.distributed |    DEBUG | [Rank 1]: distributed setup: DistributedState(rank=1, world_size=64, group_local_rank=1, group_local_size=4, group_rank=0, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0b0fc72ab0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0b0fc72af0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0b0fc72bb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,893 | xffl.learning.distributed |    DEBUG | [Rank 2]: distributed setup: DistributedState(rank=2, world_size=64, group_local_rank=2, group_local_size=4, group_rank=0, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f692c4df5b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f692c4df5f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f692c4df6b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,893 | xffl.learning.distributed |    DEBUG | [Rank 3]: distributed setup: DistributedState(rank=3, world_size=64, group_local_rank=3, group_local_size=4, group_rank=0, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f90e4d9af30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f90e4d9af70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f90e4d9b070>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,893 | xffl.learning.distributed |    DEBUG | [Rank 9]: distributed setup: DistributedState(rank=9, world_size=64, group_local_rank=1, group_local_size=4, group_rank=2, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc31e08abf0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc31e08ac30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc31e08af70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.distributed |    DEBUG | [Rank 8]: distributed setup: DistributedState(rank=8, world_size=64, group_local_rank=0, group_local_size=4, group_rank=2, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f750ee637f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f750ee63830>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f750ee63af0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,831 | xffl.learning.distributed |    DEBUG | [Rank 32]: distributed setup: DistributedState(rank=32, world_size=64, group_local_rank=0, group_local_size=4, group_rank=8, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcfae04aff0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcfae04b030>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcfae04b370>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,831 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:37:02,831 | xffl.learning.distributed |    DEBUG | [Rank 33]: distributed setup: DistributedState(rank=33, world_size=64, group_local_rank=1, group_local_size=4, group_rank=8, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0bc9e36a70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0bc9e36ab0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0bc9e36db0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.distributed |    DEBUG | [Rank 13]: distributed setup: DistributedState(rank=13, world_size=64, group_local_rank=1, group_local_size=4, group_rank=3, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd48d7ef370>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd48d7ef3b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd48d7ef6f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.distributed |    DEBUG | [Rank 14]: distributed setup: DistributedState(rank=14, world_size=64, group_local_rank=2, group_local_size=4, group_rank=3, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fde85edae30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fde85edae70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fde85edb1b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.distributed |    DEBUG | [Rank 15]: distributed setup: DistributedState(rank=15, world_size=64, group_local_rank=3, group_local_size=4, group_rank=3, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2151e170b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2151e170f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2151e17430>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.distributed |    DEBUG | [Rank 10]: distributed setup: DistributedState(rank=10, world_size=64, group_local_rank=2, group_local_size=4, group_rank=2, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f30329127b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f30329127f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3032912ab0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.distributed |    DEBUG | [Rank 4]: distributed setup: DistributedState(rank=4, world_size=64, group_local_rank=0, group_local_size=4, group_rank=1, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f23b2be7370>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f23b2be73b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f23b2be74b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.distributed |    DEBUG | [Rank 12]: distributed setup: DistributedState(rank=12, world_size=64, group_local_rank=0, group_local_size=4, group_rank=3, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc5d68ef3b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc5d68ef3f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc5d68ef730>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.distributed |    DEBUG | [Rank 17]: distributed setup: DistributedState(rank=17, world_size=64, group_local_rank=1, group_local_size=4, group_rank=4, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3c4f7f7630>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3c4f7f7670>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3c4f7f79b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.distributed |    DEBUG | [Rank 11]: distributed setup: DistributedState(rank=11, world_size=64, group_local_rank=3, group_local_size=4, group_rank=2, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=1, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f73b36bacb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f73b36bacf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f73b36bb030>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.distributed |    DEBUG | [Rank 5]: distributed setup: DistributedState(rank=5, world_size=64, group_local_rank=1, group_local_size=4, group_rank=1, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe07fe92db0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe07fe92df0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe07fe92ef0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.distributed |    DEBUG | [Rank 16]: distributed setup: DistributedState(rank=16, world_size=64, group_local_rank=0, group_local_size=4, group_rank=4, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6c79b83270>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6c79b832b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6c79b835f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:37:02,832 | xffl.learning.distributed |    DEBUG | [Rank 34]: distributed setup: DistributedState(rank=34, world_size=64, group_local_rank=2, group_local_size=4, group_rank=8, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1686fef230>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1686fef370>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1686fef530>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.distributed |    DEBUG | [Rank 18]: distributed setup: DistributedState(rank=18, world_size=64, group_local_rank=2, group_local_size=4, group_rank=4, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4ca7a0f9b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4ca7a0f9f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4ca7a0fd30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.distributed |    DEBUG | [Rank 7]: distributed setup: DistributedState(rank=7, world_size=64, group_local_rank=3, group_local_size=4, group_rank=1, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc8c18932b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc8c18932f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc8c18933f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.distributed |    DEBUG | [Rank 6]: distributed setup: DistributedState(rank=6, world_size=64, group_local_rank=2, group_local_size=4, group_rank=1, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=0, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f230a1b4130>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f230a1b4170>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f230a1b4270>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.distributed |    DEBUG | [Rank 20]: distributed setup: DistributedState(rank=20, world_size=64, group_local_rank=0, group_local_size=4, group_rank=5, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff8aea9f430>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff8aea9f470>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff8aea9f7b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,894 | xffl.learning.distributed |    DEBUG | [Rank 21]: distributed setup: DistributedState(rank=21, world_size=64, group_local_rank=1, group_local_size=4, group_rank=5, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3fd292aab0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3fd292aaf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3fd292ae30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.distributed |    DEBUG | [Rank 48]: distributed setup: DistributedState(rank=48, world_size=64, group_local_rank=0, group_local_size=4, group_rank=12, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9e5f44ee70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9e5f44eeb0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9e5f44f1f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,832 | xffl.learning.distributed |    DEBUG | [Rank 35]: distributed setup: DistributedState(rank=35, world_size=64, group_local_rank=3, group_local_size=4, group_rank=8, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff11bf32a70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff11bf32ab0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff11bf32bb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:37:02,832 | xffl.learning.distributed |    DEBUG | [Rank 36]: distributed setup: DistributedState(rank=36, world_size=64, group_local_rank=0, group_local_size=4, group_rank=9, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6061e3bb30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6061e3bb70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6061e3beb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.distributed |    DEBUG | [Rank 19]: distributed setup: DistributedState(rank=19, world_size=64, group_local_rank=3, group_local_size=4, group_rank=4, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fea68924030>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fea68924170>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fea68924370>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,832 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:37:02,832 | xffl.learning.distributed |    DEBUG | [Rank 37]: distributed setup: DistributedState(rank=37, world_size=64, group_local_rank=1, group_local_size=4, group_rank=9, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f60675cbb30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f60675cbb70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f60675cbeb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,832 | xffl.learning.distributed |    DEBUG | [Rank 39]: distributed setup: DistributedState(rank=39, world_size=64, group_local_rank=3, group_local_size=4, group_rank=9, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7cdc874830>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7cdc874870>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7cdc874bb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.distributed |    DEBUG | [Rank 22]: distributed setup: DistributedState(rank=22, world_size=64, group_local_rank=2, group_local_size=4, group_rank=5, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc73f066d30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc73f066d70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc73f0670b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.distributed |    DEBUG | [Rank 49]: distributed setup: DistributedState(rank=49, world_size=64, group_local_rank=1, group_local_size=4, group_rank=12, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f590ad1c270>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f590ad1c2b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f590ad1c5f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,832 | xffl.learning.distributed |    DEBUG | [Rank 38]: distributed setup: DistributedState(rank=38, world_size=64, group_local_rank=2, group_local_size=4, group_rank=9, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=4, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0e125feeb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0e125feef0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0e125ff230>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.distributed |    DEBUG | [Rank 23]: distributed setup: DistributedState(rank=23, world_size=64, group_local_rank=3, group_local_size=4, group_rank=5, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=2, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcb1e563530>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcb1e563570>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcb1e563870>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.distributed |    DEBUG | [Rank 24]: distributed setup: DistributedState(rank=24, world_size=64, group_local_rank=0, group_local_size=4, group_rank=6, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f53f7346fb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f53f7346ff0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f53f73470f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:37:02,832 | xffl.learning.distributed |    DEBUG | [Rank 44]: distributed setup: DistributedState(rank=44, world_size=64, group_local_rank=0, group_local_size=4, group_rank=11, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f389750b730>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f389750b770>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f389750bab0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,833 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.distributed |    DEBUG | [Rank 25]: distributed setup: DistributedState(rank=25, world_size=64, group_local_rank=1, group_local_size=4, group_rank=6, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6ee2ecbe70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6ee2ecbeb0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6ee2efc230>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,833 | xffl.learning.distributed |    DEBUG | [Rank 40]: distributed setup: DistributedState(rank=40, world_size=64, group_local_rank=0, group_local_size=4, group_rank=10, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9bb71235b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9bb71235f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9bb71238f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,833 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:37:02,833 | xffl.learning.distributed |    DEBUG | [Rank 41]: distributed setup: DistributedState(rank=41, world_size=64, group_local_rank=1, group_local_size=4, group_rank=10, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fad97ebad30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fad97ebad70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fad97ebb0b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.distributed |    DEBUG | [Rank 50]: distributed setup: DistributedState(rank=50, world_size=64, group_local_rank=2, group_local_size=4, group_rank=12, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2e43e53b30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2e43e53b70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2e43e53eb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.distributed |    DEBUG | [Rank 56]: distributed setup: DistributedState(rank=56, world_size=64, group_local_rank=0, group_local_size=4, group_rank=14, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f23c8b42a70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f23c8b42ab0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f23c8b42df0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,833 | xffl.learning.distributed |    DEBUG | [Rank 42]: distributed setup: DistributedState(rank=42, world_size=64, group_local_rank=2, group_local_size=4, group_rank=10, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f66a1326ff0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f66a1327130>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f66a13272f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,833 | xffl.learning.distributed |    DEBUG | [Rank 43]: distributed setup: DistributedState(rank=43, world_size=64, group_local_rank=3, group_local_size=4, group_rank=10, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7faf6e72b670>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7faf6e72b6b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7faf6e72b9b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:37:02,833 | xffl.learning.distributed |    DEBUG | [Rank 46]: distributed setup: DistributedState(rank=46, world_size=64, group_local_rank=2, group_local_size=4, group_rank=11, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5479e5eb30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5479e5eb70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5479e5eeb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,833 | xffl.learning.distributed |    DEBUG | [Rank 45]: distributed setup: DistributedState(rank=45, world_size=64, group_local_rank=1, group_local_size=4, group_rank=11, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb0e683eaf0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb0e683eb30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb0e683ee70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.distributed |    DEBUG | [Rank 29]: distributed setup: DistributedState(rank=29, world_size=64, group_local_rank=1, group_local_size=4, group_rank=7, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f52e93d79f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f52e93d7a30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f52e93d7d70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.distributed |    DEBUG | [Rank 28]: distributed setup: DistributedState(rank=28, world_size=64, group_local_rank=0, group_local_size=4, group_rank=7, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f075d3baf30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f075d3baf70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f075d3bb2b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.distributed |    DEBUG | [Rank 60]: distributed setup: DistributedState(rank=60, world_size=64, group_local_rank=0, group_local_size=4, group_rank=15, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f83b588b2f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f83b588b330>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f83b588b670>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.distributed |    DEBUG | [Rank 61]: distributed setup: DistributedState(rank=61, world_size=64, group_local_rank=1, group_local_size=4, group_rank=15, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f756dc6f6f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f756dc6f830>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f756dc6f7f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.distributed |    DEBUG | [Rank 26]: distributed setup: DistributedState(rank=26, world_size=64, group_local_rank=2, group_local_size=4, group_rank=6, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f50cecb35f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f50cecb3630>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f50cecb3970>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,895 | xffl.learning.distributed |    DEBUG | [Rank 63]: distributed setup: DistributedState(rank=63, world_size=64, group_local_rank=3, group_local_size=4, group_rank=15, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3d3ab32a70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3d3ab32ab0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3d3ab32df0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.distributed |    DEBUG | [Rank 57]: distributed setup: DistributedState(rank=57, world_size=64, group_local_rank=1, group_local_size=4, group_rank=14, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc7732d33b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc7732d33f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc7732d3730>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.distributed |    DEBUG | [Rank 52]: distributed setup: DistributedState(rank=52, world_size=64, group_local_rank=0, group_local_size=4, group_rank=13, group_world_size=16, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe16621aa30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe16621aa70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe16621ab70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:37:02,833 | xffl.learning.distributed |    DEBUG | [Rank 47]: distributed setup: DistributedState(rank=47, world_size=64, group_local_rank=3, group_local_size=4, group_rank=11, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=5, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f22d2dcf2f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f22d2dcf330>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f22d2dcf5f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.distributed |    DEBUG | [Rank 27]: distributed setup: DistributedState(rank=27, world_size=64, group_local_rank=3, group_local_size=4, group_rank=6, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe7cae44030>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe7cae44070>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe7cae443b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.distributed |    DEBUG | [Rank 59]: distributed setup: DistributedState(rank=59, world_size=64, group_local_rank=3, group_local_size=4, group_rank=14, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f732d5ab1f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f732d5ab230>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f732d5ab570>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.distributed |    DEBUG | [Rank 62]: distributed setup: DistributedState(rank=62, world_size=64, group_local_rank=2, group_local_size=4, group_rank=15, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f86fb4a27f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f86fb4a2930>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f86fb4a2b30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.distributed |    DEBUG | [Rank 58]: distributed setup: DistributedState(rank=58, world_size=64, group_local_rank=2, group_local_size=4, group_rank=14, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=7, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9aff03f870>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9aff03f8b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9aff03fbf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.distributed |    DEBUG | [Rank 53]: distributed setup: DistributedState(rank=53, world_size=64, group_local_rank=1, group_local_size=4, group_rank=13, group_world_size=16, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd27757f4f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd27757f530>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd27757f870>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.distributed |    DEBUG | [Rank 51]: distributed setup: DistributedState(rank=51, world_size=64, group_local_rank=3, group_local_size=4, group_rank=12, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff2f294f170>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff2f294f1b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff2f294f4f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.distributed |    DEBUG | [Rank 30]: distributed setup: DistributedState(rank=30, world_size=64, group_local_rank=2, group_local_size=4, group_rank=7, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4f0837af30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4f0837af70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4f0837b2b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.distributed |    DEBUG | [Rank 31]: distributed setup: DistributedState(rank=31, world_size=64, group_local_rank=3, group_local_size=4, group_rank=7, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=3, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa208523130>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa208523170>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa2085234b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.distributed |    DEBUG | [Rank 54]: distributed setup: DistributedState(rank=54, world_size=64, group_local_rank=2, group_local_size=4, group_rank=13, group_world_size=16, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc892e8b3b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc892e8b3f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc892e8b730>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,896 | xffl.learning.distributed |    DEBUG | [Rank 55]: distributed setup: DistributedState(rank=55, world_size=64, group_local_rank=3, group_local_size=4, group_rank=13, group_world_size=16, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2, 2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8, 8, 8, 8, 8), federated_rank=6, federated_world_size=8, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f787309b9f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f787309ba30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f787309bcf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:37:02,935 | xffl.learning.utils |    DEBUG | [Rank 32]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:02,998 | xffl.learning.utils |    DEBUG | [Rank 8]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:02,999 | xffl.learning.utils |    DEBUG | [Rank 20]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:03,000 | xffl.learning.utils |    DEBUG | [Rank 28]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:03,000 | xffl.learning.utils |    DEBUG | [Rank 4]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:03,001 | xffl.learning.utils |    DEBUG | [Rank 24]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:03,001 | xffl.learning.utils |    DEBUG | [Rank 52]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:02,939 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:02,939 | xffl.learning.utils |    DEBUG | [Rank 40]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:03,002 | xffl.learning.utils |    DEBUG | [Rank 56]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:03,002 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:03,003 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:03,004 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:03,004 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:03,005 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:03,005 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:02,943 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:03,006 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:03,024 | xffl.learning.utils |    DEBUG | [Rank 48]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:03,024 | xffl.learning.utils |    DEBUG | [Rank 12]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:03,025 | xffl.learning.utils |    DEBUG | [Rank 60]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:02,962 | xffl.learning.utils |    DEBUG | [Rank 36]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:03,025 | xffl.learning.utils |    DEBUG | [Rank 16]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:02,964 | xffl.learning.utils |    DEBUG | [Rank 44]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:03,028 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:03,028 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:03,028 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:02,966 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:03,029 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:02,969 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:03,042 | xffl.learning.utils |    DEBUG | [Rank 0]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:03,046 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,613 | xffl.learning.utils |    DEBUG | [Rank 3]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,617 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,630 | xffl.learning.utils |    DEBUG | [Rank 63]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,631 | xffl.learning.utils |    DEBUG | [Rank 59]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,632 | xffl.learning.utils |    DEBUG | [Rank 2]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,569 | xffl.learning.utils |    DEBUG | [Rank 35]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,634 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,635 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,635 | xffl.learning.utils |    DEBUG | [Rank 50]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,636 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,636 | xffl.learning.utils |    DEBUG | [Rank 19]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,574 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,639 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,640 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,641 | xffl.learning.utils |    DEBUG | [Rank 55]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,642 | xffl.learning.utils |    DEBUG | [Rank 27]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,644 | xffl.learning.utils |    DEBUG | [Rank 18]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,645 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,645 | xffl.learning.utils |    DEBUG | [Rank 51]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,646 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,647 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,648 | xffl.learning.utils |    DEBUG | [Rank 26]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,649 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,649 | xffl.learning.utils |    DEBUG | [Rank 58]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,650 | xffl.learning.utils |    DEBUG | [Rank 31]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,652 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,652 | xffl.learning.utils |    DEBUG | [Rank 15]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,652 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,591 | xffl.learning.utils |    DEBUG | [Rank 39]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,654 | xffl.learning.utils |    DEBUG | [Rank 14]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,654 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,592 | xffl.learning.utils |    DEBUG | [Rank 38]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,655 | xffl.learning.utils |    DEBUG | [Rank 54]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,593 | xffl.learning.utils |    DEBUG | [Rank 34]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,656 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,656 | xffl.learning.utils |    DEBUG | [Rank 30]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,657 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,657 | xffl.learning.utils |    DEBUG | [Rank 11]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,595 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,596 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,596 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,659 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,659 | xffl.learning.utils |    DEBUG | [Rank 10]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,660 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,598 | xffl.learning.utils |    DEBUG | [Rank 43]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,661 | xffl.learning.utils |    DEBUG | [Rank 62]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,661 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,663 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,665 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,603 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,604 | xffl.learning.utils |    DEBUG | [Rank 42]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,667 | xffl.learning.utils |    DEBUG | [Rank 7]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,605 | xffl.learning.utils |    DEBUG | [Rank 46]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,608 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,671 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,609 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,612 | xffl.learning.utils |    DEBUG | [Rank 47]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,615 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,683 | xffl.learning.utils |    DEBUG | [Rank 23]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,687 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,700 | xffl.learning.utils |    DEBUG | [Rank 22]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,704 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,704 | xffl.learning.utils |    DEBUG | [Rank 57]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,705 | xffl.learning.utils |    DEBUG | [Rank 25]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,707 | xffl.learning.utils |    DEBUG | [Rank 49]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,707 | xffl.learning.utils |    DEBUG | [Rank 61]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,708 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,709 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,710 | xffl.learning.utils |    DEBUG | [Rank 6]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,710 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,711 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,714 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,717 | xffl.learning.utils |    DEBUG | [Rank 53]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,721 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,724 | xffl.learning.utils |    DEBUG | [Rank 9]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,727 | xffl.learning.utils |    DEBUG | [Rank 1]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,665 | xffl.learning.utils |    DEBUG | [Rank 37]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,728 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,666 | xffl.learning.utils |    DEBUG | [Rank 41]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,731 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,669 | xffl.learning.utils |    DEBUG | [Rank 33]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,669 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,670 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,733 | xffl.learning.utils |    DEBUG | [Rank 17]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,673 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,737 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,740 | xffl.learning.utils |    DEBUG | [Rank 13]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,740 | xffl.learning.utils |    DEBUG | [Rank 5]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,743 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,743 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,745 | xffl.learning.utils |    DEBUG | [Rank 29]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,683 | xffl.learning.utils |    DEBUG | [Rank 45]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,748 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,687 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:04,757 | xffl.learning.utils |    DEBUG | [Rank 21]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:37:04,761 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:37:08,537 | xffl.learning.modelling |    DEBUG | [Rank 52]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:08,537 | xffl.learning.distributed |    DEBUG | [Rank 52]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:08,601 | xffl.learning.modelling |    DEBUG | [Rank 4]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:08,601 | xffl.learning.distributed |    DEBUG | [Rank 4]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:08,619 | xffl.learning.modelling |    DEBUG | [Rank 60]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:08,620 | xffl.learning.distributed |    DEBUG | [Rank 60]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:08,626 |         __main__ |    DEBUG | Model loading time: 5.06 seconds[0m
[38;5;39m2025-03-26 00:37:08,627 |         __main__ |    DEBUG | Training llama3.1-8b: 8030.26 million trainable parameters[0m
[38;5;39m2025-03-26 00:37:08,627 | xffl.learning.modelling |    DEBUG | [Rank 0]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:08,627 | xffl.learning.distributed |    DEBUG | [Rank 0]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:08,633 | xffl.learning.modelling |    DEBUG | [Rank 8]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:08,633 | xffl.learning.distributed |    DEBUG | [Rank 8]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:08,643 | xffl.learning.modelling |    DEBUG | [Rank 48]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:08,643 | xffl.learning.distributed |    DEBUG | [Rank 48]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:08,585 | xffl.learning.modelling |    DEBUG | [Rank 40]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:08,585 | xffl.learning.distributed |    DEBUG | [Rank 40]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:08,601 | xffl.learning.modelling |    DEBUG | [Rank 32]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:08,602 | xffl.learning.distributed |    DEBUG | [Rank 32]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:08,677 | xffl.learning.modelling |    DEBUG | [Rank 12]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:08,678 | xffl.learning.distributed |    DEBUG | [Rank 12]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:08,702 | xffl.learning.modelling |    DEBUG | [Rank 56]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:08,703 | xffl.learning.distributed |    DEBUG | [Rank 56]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:08,706 | xffl.learning.modelling |    DEBUG | [Rank 28]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:08,706 | xffl.learning.distributed |    DEBUG | [Rank 28]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:08,709 | xffl.learning.modelling |    DEBUG | [Rank 24]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:08,710 | xffl.learning.distributed |    DEBUG | [Rank 24]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:08,722 | xffl.learning.modelling |    DEBUG | [Rank 20]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:08,723 | xffl.learning.distributed |    DEBUG | [Rank 20]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:08,663 | xffl.learning.modelling |    DEBUG | [Rank 44]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:08,663 | xffl.learning.distributed |    DEBUG | [Rank 44]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:08,728 | xffl.learning.modelling |    DEBUG | [Rank 16]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:08,728 | xffl.learning.distributed |    DEBUG | [Rank 16]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:08,710 | xffl.learning.modelling |    DEBUG | [Rank 36]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:08,710 | xffl.learning.distributed |    DEBUG | [Rank 36]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,352 | xffl.learning.modelling |    DEBUG | [Rank 49]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,352 | xffl.learning.distributed |    DEBUG | [Rank 49]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,376 | xffl.learning.modelling |    DEBUG | [Rank 9]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,377 | xffl.learning.distributed |    DEBUG | [Rank 9]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,407 | xffl.learning.modelling |    DEBUG | [Rank 53]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,408 | xffl.learning.distributed |    DEBUG | [Rank 53]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,409 | xffl.learning.modelling |    DEBUG | [Rank 25]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,409 | xffl.learning.distributed |    DEBUG | [Rank 25]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,373 | xffl.learning.modelling |    DEBUG | [Rank 33]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,374 | xffl.learning.distributed |    DEBUG | [Rank 33]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,455 | xffl.learning.modelling |    DEBUG | [Rank 21]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,456 | xffl.learning.distributed |    DEBUG | [Rank 21]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,461 | xffl.learning.modelling |    DEBUG | [Rank 5]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,461 | xffl.learning.distributed |    DEBUG | [Rank 5]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,478 | xffl.learning.modelling |    DEBUG | [Rank 11]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,478 | xffl.learning.distributed |    DEBUG | [Rank 11]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,480 | xffl.learning.modelling |    DEBUG | [Rank 10]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,480 | xffl.learning.distributed |    DEBUG | [Rank 10]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,487 | xffl.learning.modelling |    DEBUG | [Rank 29]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,487 | xffl.learning.distributed |    DEBUG | [Rank 29]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,519 | xffl.learning.modelling |    DEBUG | [Rank 54]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,519 | xffl.learning.distributed |    DEBUG | [Rank 54]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,520 | xffl.learning.modelling |    DEBUG | [Rank 55]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,520 | xffl.learning.distributed |    DEBUG | [Rank 55]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,523 | xffl.learning.modelling |    DEBUG | [Rank 13]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,523 | xffl.learning.distributed |    DEBUG | [Rank 13]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,470 | xffl.learning.modelling |    DEBUG | [Rank 41]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,470 | xffl.learning.distributed |    DEBUG | [Rank 41]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,472 | xffl.learning.modelling |    DEBUG | [Rank 37]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,473 | xffl.learning.distributed |    DEBUG | [Rank 37]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,546 | xffl.learning.modelling |    DEBUG | [Rank 26]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,546 | xffl.learning.distributed |    DEBUG | [Rank 26]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,558 | xffl.learning.modelling |    DEBUG | [Rank 15]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,558 | xffl.learning.distributed |    DEBUG | [Rank 15]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,496 | xffl.learning.modelling |    DEBUG | [Rank 35]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,496 | xffl.learning.distributed |    DEBUG | [Rank 35]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,558 | xffl.learning.modelling |    DEBUG | [Rank 61]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,559 | xffl.learning.distributed |    DEBUG | [Rank 61]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,561 | xffl.learning.modelling |    DEBUG | [Rank 14]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,561 | xffl.learning.distributed |    DEBUG | [Rank 14]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,498 | xffl.learning.modelling |    DEBUG | [Rank 34]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,499 | xffl.learning.distributed |    DEBUG | [Rank 34]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,563 | xffl.learning.modelling |    DEBUG | [Rank 27]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,563 | xffl.learning.distributed |    DEBUG | [Rank 27]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,572 | xffl.learning.modelling |    DEBUG | [Rank 50]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,572 | xffl.learning.distributed |    DEBUG | [Rank 50]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,576 | xffl.learning.modelling |    DEBUG | [Rank 51]: is calling FSDP on device mesh DeviceMesh('cuda', [[48, 49, 50, 51], [52, 53, 54, 55]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,576 | xffl.learning.distributed |    DEBUG | [Rank 51]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,582 | xffl.learning.modelling |    DEBUG | [Rank 31]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,583 | xffl.learning.distributed |    DEBUG | [Rank 31]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,585 | xffl.learning.modelling |    DEBUG | [Rank 30]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,585 | xffl.learning.distributed |    DEBUG | [Rank 30]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,586 | xffl.learning.modelling |    DEBUG | [Rank 6]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,587 | xffl.learning.distributed |    DEBUG | [Rank 6]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,592 | xffl.learning.modelling |    DEBUG | [Rank 7]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,592 | xffl.learning.distributed |    DEBUG | [Rank 7]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,596 | xffl.learning.modelling |    DEBUG | [Rank 57]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,596 | xffl.learning.distributed |    DEBUG | [Rank 57]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,605 | xffl.learning.modelling |    DEBUG | [Rank 1]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,605 | xffl.learning.distributed |    DEBUG | [Rank 1]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,548 | xffl.learning.modelling |    DEBUG | [Rank 39]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,548 | xffl.learning.distributed |    DEBUG | [Rank 39]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,550 | xffl.learning.modelling |    DEBUG | [Rank 38]: is calling FSDP on device mesh DeviceMesh('cuda', [[32, 33, 34, 35], [36, 37, 38, 39]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,550 | xffl.learning.distributed |    DEBUG | [Rank 38]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,584 | xffl.learning.modelling |    DEBUG | [Rank 45]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,584 | xffl.learning.distributed |    DEBUG | [Rank 45]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,613 | xffl.learning.modelling |    DEBUG | [Rank 43]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,613 | xffl.learning.distributed |    DEBUG | [Rank 43]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,619 | xffl.learning.modelling |    DEBUG | [Rank 42]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,619 | xffl.learning.distributed |    DEBUG | [Rank 42]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,696 | xffl.learning.modelling |    DEBUG | [Rank 17]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,696 | xffl.learning.distributed |    DEBUG | [Rank 17]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,697 | xffl.learning.modelling |    DEBUG | [Rank 22]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,697 | xffl.learning.modelling |    DEBUG | [Rank 23]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,697 | xffl.learning.distributed |    DEBUG | [Rank 22]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,697 | xffl.learning.distributed |    DEBUG | [Rank 23]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,706 | xffl.learning.modelling |    DEBUG | [Rank 58]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,706 | xffl.learning.distributed |    DEBUG | [Rank 58]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,725 | xffl.learning.modelling |    DEBUG | [Rank 3]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,725 | xffl.learning.distributed |    DEBUG | [Rank 3]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,728 | xffl.learning.modelling |    DEBUG | [Rank 2]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,728 | xffl.learning.distributed |    DEBUG | [Rank 2]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,729 | xffl.learning.modelling |    DEBUG | [Rank 59]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,729 | xffl.learning.distributed |    DEBUG | [Rank 59]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,751 | xffl.learning.modelling |    DEBUG | [Rank 18]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,751 | xffl.learning.distributed |    DEBUG | [Rank 18]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,766 | xffl.learning.modelling |    DEBUG | [Rank 19]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,766 | xffl.learning.distributed |    DEBUG | [Rank 19]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,878 | xffl.learning.modelling |    DEBUG | [Rank 46]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,878 | xffl.learning.distributed |    DEBUG | [Rank 46]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,940 | xffl.learning.modelling |    DEBUG | [Rank 62]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,878 | xffl.learning.modelling |    DEBUG | [Rank 47]: is calling FSDP on device mesh DeviceMesh('cuda', [[40, 41, 42, 43], [44, 45, 46, 47]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,878 | xffl.learning.distributed |    DEBUG | [Rank 47]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,941 | xffl.learning.distributed |    DEBUG | [Rank 62]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:09,941 | xffl.learning.modelling |    DEBUG | [Rank 63]: is calling FSDP on device mesh DeviceMesh('cuda', [[56, 57, 58, 59], [60, 61, 62, 63]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:37:09,941 | xffl.learning.distributed |    DEBUG | [Rank 63]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:37:17,421 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,422 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,422 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,422 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,423 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,425 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,426 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,426 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,557 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,557 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,557 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,558 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,558 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,558 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,558 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,558 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,502 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,503 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,503 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,503 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,503 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,504 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,505 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,505 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,571 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,571 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,571 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,572 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,572 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,572 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,573 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,575 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,578 |         __main__ |    DEBUG | [Rank 54]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,578 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,578 |         __main__ |    DEBUG | [Rank 53]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,578 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,578 |         __main__ |    DEBUG | [Rank 52]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,579 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,578 |         __main__ |    DEBUG | [Rank 49]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,579 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,579 |         __main__ |    DEBUG | [Rank 51]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:37:17,579 |         __main__ |    DEBUG | [Rank 50]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,579 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:37:17,579 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,597 |         __main__ |    DEBUG | [Rank 55]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,597 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,599 |         __main__ |    DEBUG | [Rank 48]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,599 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,604 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,604 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,604 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,604 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,604 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,604 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,604 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,604 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,564 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,564 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,564 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,564 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,564 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,564 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,564 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,565 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,669 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,669 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,670 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,669 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,669 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,670 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,670 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,670 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,670 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,671 |         __main__ |    DEBUG | FSDP wrapping setup time: 9.04 seconds[0m
[38;5;39m2025-03-26 00:37:17,671 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,671 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,671 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,671 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,671 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,672 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,672 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:37:17,712 |         __main__ |    DEBUG | [Rank 9]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,713 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,713 |         __main__ |    DEBUG | [Rank 11]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,713 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,714 |         __main__ |    DEBUG | [Rank 13]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:37:17,714 |         __main__ |    DEBUG | [Rank 15]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:37:17,714 |         __main__ |    DEBUG | [Rank 10]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:37:17,714 |         __main__ |    DEBUG | [Rank 14]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,714 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:37:17,714 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:37:17,714 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:37:17,714 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,727 |         __main__ |    DEBUG | [Rank 29]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,728 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,728 |         __main__ |    DEBUG | [Rank 28]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,728 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,728 |         __main__ |    DEBUG | [Rank 31]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,728 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,667 |         __main__ |    DEBUG | [Rank 41]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,667 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,667 |         __main__ |    DEBUG | [Rank 43]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,667 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,668 |         __main__ |    DEBUG | [Rank 42]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,668 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,732 |         __main__ |    DEBUG | [Rank 8]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,732 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,734 |         __main__ |    DEBUG | [Rank 12]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,734 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,736 |         __main__ |    DEBUG | [Rank 25]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,736 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,736 |         __main__ |    DEBUG | [Rank 27]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,736 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,737 |         __main__ |    DEBUG | [Rank 26]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,737 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,680 |         __main__ |    DEBUG | [Rank 45]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,680 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,686 |         __main__ |    DEBUG | [Rank 44]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,686 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,686 |         __main__ |    DEBUG | [Rank 47]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,686 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,752 |         __main__ |    DEBUG | [Rank 30]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,752 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,755 |         __main__ |    DEBUG | [Rank 24]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,755 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,702 |         __main__ |    DEBUG | [Rank 40]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,702 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,715 |         __main__ |    DEBUG | [Rank 46]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,716 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,723 |         __main__ |    DEBUG | [Rank 39]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,723 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,723 |         __main__ |    DEBUG | [Rank 37]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,723 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,724 |         __main__ |    DEBUG | [Rank 36]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,725 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,726 |         __main__ |    DEBUG | [Rank 32]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,726 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,726 |         __main__ |    DEBUG | [Rank 35]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,726 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,727 |         __main__ |    DEBUG | [Rank 33]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,727 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,799 |         __main__ |    DEBUG | [Rank 61]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,799 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,800 |         __main__ |    DEBUG | [Rank 63]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,800 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,804 |         __main__ |    DEBUG | [Rank 60]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,804 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,742 |         __main__ |    DEBUG | [Rank 38]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,742 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,806 |         __main__ |    DEBUG | [Rank 58]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,806 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,744 |         __main__ |    DEBUG | [Rank 34]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,744 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,807 |         __main__ |    DEBUG | [Rank 56]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:37:17,807 |         __main__ |    DEBUG | [Rank 59]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,808 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:37:17,808 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,827 |         __main__ |    DEBUG | [Rank 62]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,827 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,828 |         __main__ |    DEBUG | [Rank 23]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:37:17,828 |         __main__ |    DEBUG | [Rank 22]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,828 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:37:17,828 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,829 |         __main__ |    DEBUG | [Rank 21]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,829 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,830 |         __main__ |    DEBUG | [Rank 57]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,830 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,830 |         __main__ |    DEBUG | [Rank 4]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,831 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,831 |         __main__ |    DEBUG | [Rank 5]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,831 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,832 |         __main__ |    DEBUG | [Rank 7]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,832 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,835 |         __main__ |    DEBUG | Dataset loading time: 0.16 seconds[0m
[38;5;39m2025-03-26 00:37:17,847 |         __main__ |    DEBUG | [Rank 20]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,847 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,851 |         __main__ |    DEBUG | [Rank 6]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,851 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,856 |         __main__ |    DEBUG | [Rank 19]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,856 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,858 |         __main__ |    DEBUG | [Rank 16]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,858 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,859 |         __main__ |    DEBUG | [Rank 18]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,859 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,861 |         __main__ |    DEBUG | [Rank 2]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,862 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,862 |         __main__ |    DEBUG | [Rank 1]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,863 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,864 |         __main__ |    DEBUG | [Rank 3]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,864 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,881 |         __main__ |    DEBUG | [Rank 17]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,881 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:37:17,884 |         __main__ |    DEBUG | train set size: 16384 samples[0m
[38;5;39m2025-03-26 00:37:17,885 |         __main__ |    DEBUG | train dataloader size: 32 minibatches[0m
[38;5;39m2025-03-26 00:37:17,885 |         __main__ |    DEBUG | val set size: 5311 samples[0m
[38;5;39m2025-03-26 00:37:17,885 |         __main__ |    DEBUG | val dataloader size: 82 minibatches[0m
[38;5;39m2025-03-26 00:37:17,885 |         __main__ |    DEBUG | Dataloaders creation time: 0.05 seconds[0m
[38;5;39m2025-03-26 00:37:17,885 |         __main__ |    DEBUG | Learning rate adjusted to: 0.0002[0m
[38;5;39m2025-03-26 00:37:17,886 |         __main__ |    DEBUG | Total setup time: 16.62 seconds[0m
[38;5;39m2025-03-26 00:37:17,886 |         __main__ |    DEBUG | [Rank 0]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:37:17,886 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:40:02,672 | xffl.learning.processing |     INFO | Epoch 1: train_perplexity=20786.0742, train_epoch_loss=9.9420, epoch time 163.76896669110283s[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 51]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 50]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 49]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 61]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 63]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 62]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 60]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 55]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 54]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 53]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 52]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 48]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 9]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 8]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 11]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 10]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 14]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 15]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 12]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 13]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 59]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 56]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 57]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 58]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 2]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 3]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 1]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 17]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 16]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 19]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 21]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 22]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 23]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 20]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 18]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 28]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 29]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 31]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 30]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,674 |         __main__ |    DEBUG | Key: avg_epoch_time, Value: 163.76896669110283[0m
[38;5;39m2025-03-26 00:40:02,610 | xffl.learning.distributed |    DEBUG | [Rank 35]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,610 | xffl.learning.distributed |    DEBUG | [Rank 34]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,611 | xffl.learning.distributed |    DEBUG | [Rank 32]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,611 | xffl.learning.distributed |    DEBUG | [Rank 33]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 5]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 7]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 6]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 4]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,611 | xffl.learning.distributed |    DEBUG | [Rank 40]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,611 | xffl.learning.distributed |    DEBUG | [Rank 41]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,674 |         __main__ |    DEBUG | Key: avg_train_perp, Value: 20786.07421875[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 26]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 25]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 24]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,673 | xffl.learning.distributed |    DEBUG | [Rank 27]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,611 | xffl.learning.distributed |    DEBUG | [Rank 37]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,611 | xffl.learning.distributed |    DEBUG | [Rank 39]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,611 | xffl.learning.distributed |    DEBUG | [Rank 38]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,611 | xffl.learning.distributed |    DEBUG | [Rank 36]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,674 |         __main__ |    DEBUG | Key: avg_train_loss, Value: 9.942038536071777[0m
[38;5;39m2025-03-26 00:40:02,611 | xffl.learning.distributed |    DEBUG | [Rank 43]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,611 | xffl.learning.distributed |    DEBUG | [Rank 42]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,611 | xffl.learning.distributed |    DEBUG | [Rank 45]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,611 | xffl.learning.distributed |    DEBUG | [Rank 44]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,611 | xffl.learning.distributed |    DEBUG | [Rank 46]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,611 | xffl.learning.distributed |    DEBUG | [Rank 47]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:40:02,675 | xffl.learning.distributed |    DEBUG | [Rank 0]: calling destroy_process_group[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-e9ailkxp[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-e9ailkxp/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-qasd2uww[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-qasd2uww/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-l9coc13a[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-l9coc13a/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003703-weqosx21[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003703-weqosx21/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-y6bwgzat[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-y6bwgzat/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-ypzr7139[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-ypzr7139/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-x6fto4xl[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-x6fto4xl/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003703-qo8o5g81[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003703-qo8o5g81/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-58edvqzq[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-58edvqzq/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-mcmx6y2x[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-mcmx6y2x/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-xsuxbejf[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-xsuxbejf/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-zcsn4fan[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-zcsn4fan/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-orh57z17[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-orh57z17/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003703-kvwy03dx[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003703-kvwy03dx/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-ryuw9hhk[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-ryuw9hhk/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-ozjms9su[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-ozjms9su/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-a2lf0mfk[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-a2lf0mfk/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-sofjjq44[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-sofjjq44/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-nr02v4fe[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-nr02v4fe/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-vhh0lwuy[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-vhh0lwuy/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-36i2jo6c[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-36i2jo6c/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003703-msb9sjbc[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003703-msb9sjbc/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-78ym1w93[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-78ym1w93/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-3e8s7uet[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-3e8s7uet/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-cwoowl4j[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-cwoowl4j/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-9qumn3ul[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-9qumn3ul/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003702-tqytl8oi[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003702-tqytl8oi/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-xirf0bfw[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-xirf0bfw/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-veqsni6d[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-veqsni6d/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-f8zha2es[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-f8zha2es/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003702-22i32pxm[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003702-22i32pxm/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-se21px6w[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-se21px6w/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-g1pbx2un[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-g1pbx2un/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-kiqs0vlf[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-kiqs0vlf/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003703-zob1iexd[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003703-zob1iexd/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-1xrn7bjt[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-1xrn7bjt/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-2gkl92vu[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-2gkl92vu/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-lmryvjjm[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-lmryvjjm/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003703-85vxvdzx[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003703-85vxvdzx/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-n5o39q0l[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-n5o39q0l/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003703-ww0hwt3e[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003703-ww0hwt3e/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-aife4rrd[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-aife4rrd/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003703-4as0ibur[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003703-4as0ibur/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-wxosd2fe[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-wxosd2fe/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003703-trp7qn0e[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003703-trp7qn0e/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-hi17htts[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-hi17htts/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-fkp9bd0z[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-fkp9bd0z/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-4t9q5hrw[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-4t9q5hrw/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-k3i6td54[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-k3i6td54/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-y0sg67s1[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-y0sg67s1/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-4orr1w5l[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-4orr1w5l/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003703-31zjuim9[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003703-31zjuim9/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-ouge2wbb[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-ouge2wbb/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-xfszxdsz[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-xfszxdsz/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003703-5b8m26tn[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003703-5b8m26tn/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-r4g5ym2j[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-r4g5ym2j/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003702-g7rkr86i[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003702-g7rkr86i/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-xfjct6kb[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-xfjct6kb/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-4id101hi[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-4id101hi/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003702-d9e7sjtg[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003702-d9e7sjtg/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-sy0emaky[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-sy0emaky/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003703-bcixav8c[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003703-bcixav8c/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-kcztvgs0[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-kcztvgs0/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_003704-tue8rl3g[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_003704-tue8rl3g/logs[0m
[38;20m2025-03-26 00:40:07,266 | xffl.cli.simulate |     INFO | Total simulation execution time: 197.00 seconds[0m
[38;20m2025-03-26 00:40:07,266 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
