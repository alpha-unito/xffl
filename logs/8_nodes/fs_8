2, 1, 2
[38;20m2025-03-26 00:29:29,750 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
[38;5;226m2025-03-26 00:29:29,751 |   xffl.cli.utils |  WARNING | CLI argument "help" has got default value "False"[0m
[38;5;226m2025-03-26 00:29:29,751 |   xffl.cli.utils |  WARNING | CLI argument "workdir" has got default value "/leonardo_scratch/fast/uToID_bench/xffl"[0m
[38;5;226m2025-03-26 00:29:29,751 |   xffl.cli.utils |  WARNING | CLI argument "image" has got default value "None"[0m
[38;5;39m2025-03-26 00:29:29,752 | xffl.cli.simulate |    DEBUG | Using virtual environment: /leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate[0m
[38;5;39m2025-03-26 00:29:29,752 | xffl.cli.simulate |    DEBUG | New local simulation xFFL environment variables: {'XFFL_WORLD_SIZE': '32', 'XFFL_NUM_NODES': '8', 'MASTER_ADDR': 'lrdn0001', 'VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;5;39m2025-03-26 00:29:29,752 | xffl.cli.simulate |    DEBUG | Updated xFFL environment: {'XFFL_WORLD_SIZE': '32', 'XFFL_NUM_NODES': '8', 'MASTER_ADDR': 'lrdn0001', 'VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;20m2025-03-26 00:29:29,752 | xffl.cli.simulate |     INFO | Running local simulation...[0m
[38;5;39m2025-03-26 00:29:29,753 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0001: ssh -oStrictHostKeyChecking=no lrdn0001 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=0 XFFL_FEDERATED_RANK=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:29:29,753 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0010: ssh -oStrictHostKeyChecking=no lrdn0010 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=1 XFFL_FEDERATED_RANK=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:29:29,753 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0182: ssh -oStrictHostKeyChecking=no lrdn0182 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=2 XFFL_FEDERATED_RANK=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:29:29,753 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0189: ssh -oStrictHostKeyChecking=no lrdn0189 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=3 XFFL_FEDERATED_RANK=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:29:29,754 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0361: ssh -oStrictHostKeyChecking=no lrdn0361 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=4 XFFL_FEDERATED_RANK=2 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:29:29,754 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0370: ssh -oStrictHostKeyChecking=no lrdn0370 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=5 XFFL_FEDERATED_RANK=2 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:29:29,754 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0542: ssh -oStrictHostKeyChecking=no lrdn0542 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=6 XFFL_FEDERATED_RANK=3 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:29:29,754 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0549: ssh -oStrictHostKeyChecking=no lrdn0549 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=7 XFFL_FEDERATED_RANK=3 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 8 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS8 -mode offline "[0m
[38;5;39m2025-03-26 00:29:39,008 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:39,008 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:39,008 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:39,008 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:39,851 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:39,851 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:39,851 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:39,851 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,609 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,609 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,609 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,609 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,754 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,754 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,754 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,754 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,793 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,793 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,793 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,793 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,819 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,819 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,819 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,819 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,821 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,821 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,821 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,821 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,826 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,826 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,826 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,826 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:29:44,921 | xffl.learning.distributed |    DEBUG | [Rank 0]: distributed setup: DistributedState(rank=0, world_size=32, group_local_rank=0, group_local_size=4, group_rank=0, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0887b7b5b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0887b7b630>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0887b7b6b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,921 |         __main__ |    DEBUG | Randez-vous time: 5.06 seconds[0m
[38;5;39m2025-03-26 00:29:44,921 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:29:44,921 | xffl.learning.distributed |    DEBUG | [Rank 1]: distributed setup: DistributedState(rank=1, world_size=32, group_local_rank=1, group_local_size=4, group_rank=0, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc8e877f330>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc8e877f3b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc8e877f430>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,921 | xffl.learning.distributed |    DEBUG | [Rank 2]: distributed setup: DistributedState(rank=2, world_size=32, group_local_rank=2, group_local_size=4, group_rank=0, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe3e71efd30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe3e71efd70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe3e71eff70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,922 | xffl.learning.distributed |    DEBUG | [Rank 3]: distributed setup: DistributedState(rank=3, world_size=32, group_local_rank=3, group_local_size=4, group_rank=0, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f98d33b6c70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f98d33b6cf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f98d33b6d70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,922 | xffl.learning.distributed |    DEBUG | [Rank 4]: distributed setup: DistributedState(rank=4, world_size=32, group_local_rank=0, group_local_size=4, group_rank=1, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7efbaa00bbf0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7efbaa00bcb0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7efbaa00be30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,922 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:29:44,922 | xffl.learning.distributed |    DEBUG | [Rank 5]: distributed setup: DistributedState(rank=5, world_size=32, group_local_rank=1, group_local_size=4, group_rank=1, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9913596c70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9913596cf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f9913596d70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,922 | xffl.learning.distributed |    DEBUG | [Rank 8]: distributed setup: DistributedState(rank=8, world_size=32, group_local_rank=0, group_local_size=4, group_rank=2, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff3588178f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff3588179f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff358817b70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.distributed |    DEBUG | [Rank 7]: distributed setup: DistributedState(rank=7, world_size=32, group_local_rank=3, group_local_size=4, group_rank=1, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3337437070>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f33374370f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3337437170>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,922 | xffl.learning.distributed |    DEBUG | [Rank 16]: distributed setup: DistributedState(rank=16, world_size=32, group_local_rank=0, group_local_size=4, group_rank=4, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb5e94bb370>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb5e94bb3b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb5e94bb5f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,922 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:29:44,922 | xffl.learning.distributed |    DEBUG | [Rank 17]: distributed setup: DistributedState(rank=17, world_size=32, group_local_rank=1, group_local_size=4, group_rank=4, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff9a1957630>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff9a19576b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff9a19578f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.distributed |    DEBUG | [Rank 6]: distributed setup: DistributedState(rank=6, world_size=32, group_local_rank=2, group_local_size=4, group_rank=1, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f79aa3579b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f79aa357ab0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f79aa357870>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.distributed |    DEBUG | [Rank 18]: distributed setup: DistributedState(rank=18, world_size=32, group_local_rank=2, group_local_size=4, group_rank=4, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f88ce43e930>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f88ce43e470>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f88ce43eb70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.distributed |    DEBUG | [Rank 9]: distributed setup: DistributedState(rank=9, world_size=32, group_local_rank=1, group_local_size=4, group_rank=2, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb404453330>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb4044533b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb4044535f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.distributed |    DEBUG | [Rank 20]: distributed setup: DistributedState(rank=20, world_size=32, group_local_rank=0, group_local_size=4, group_rank=5, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbf28342ab0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbf28342b30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbf28342d70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.distributed |    DEBUG | [Rank 10]: distributed setup: DistributedState(rank=10, world_size=32, group_local_rank=2, group_local_size=4, group_rank=2, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fee6a7da770>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fee6a7da7f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fee6a7da870>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.distributed |    DEBUG | [Rank 19]: distributed setup: DistributedState(rank=19, world_size=32, group_local_rank=3, group_local_size=4, group_rank=4, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2a53c72b70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2a53c72a30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2a53c72df0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.distributed |    DEBUG | [Rank 13]: distributed setup: DistributedState(rank=13, world_size=32, group_local_rank=1, group_local_size=4, group_rank=3, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f54f018f330>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f54f018f3b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f54f018f430>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.distributed |    DEBUG | [Rank 15]: distributed setup: DistributedState(rank=15, world_size=32, group_local_rank=3, group_local_size=4, group_rank=3, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f19be1faf70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f19be1fb130>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f19be1fb2f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.distributed |    DEBUG | [Rank 22]: distributed setup: DistributedState(rank=22, world_size=32, group_local_rank=2, group_local_size=4, group_rank=5, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1443bfb870>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1443bfb8f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1443bfbaf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.distributed |    DEBUG | [Rank 11]: distributed setup: DistributedState(rank=11, world_size=32, group_local_rank=3, group_local_size=4, group_rank=2, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f206e323370>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f206e3233f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f206e323630>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.distributed |    DEBUG | [Rank 21]: distributed setup: DistributedState(rank=21, world_size=32, group_local_rank=1, group_local_size=4, group_rank=5, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa004f1b6f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa004f1b830>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fa004f1ba30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.distributed |    DEBUG | [Rank 12]: distributed setup: DistributedState(rank=12, world_size=32, group_local_rank=0, group_local_size=4, group_rank=3, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fec265bb9b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fec265bba30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fec265bbc70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.distributed |    DEBUG | [Rank 14]: distributed setup: DistributedState(rank=14, world_size=32, group_local_rank=2, group_local_size=4, group_rank=3, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f405249b070>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f405249b170>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f405249b2f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,923 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:29:44,924 | xffl.learning.distributed |    DEBUG | [Rank 23]: distributed setup: DistributedState(rank=23, world_size=32, group_local_rank=3, group_local_size=4, group_rank=5, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f622aecb7f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f622aecb870>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f622aecba70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,924 | xffl.learning.distributed |    DEBUG | [Rank 24]: distributed setup: DistributedState(rank=24, world_size=32, group_local_rank=0, group_local_size=4, group_rank=6, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f937cc7f030>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f937cc7f0b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f937cc7f2f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,924 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:29:44,924 | xffl.learning.distributed |    DEBUG | [Rank 25]: distributed setup: DistributedState(rank=25, world_size=32, group_local_rank=1, group_local_size=4, group_rank=6, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f89c673b4f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f89c673b570>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f89c673b7b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,924 | xffl.learning.distributed |    DEBUG | [Rank 27]: distributed setup: DistributedState(rank=27, world_size=32, group_local_rank=3, group_local_size=4, group_rank=6, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1fbd66b2b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1fbd66b330>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1fbd66b570>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,924 | xffl.learning.distributed |    DEBUG | [Rank 29]: distributed setup: DistributedState(rank=29, world_size=32, group_local_rank=1, group_local_size=4, group_rank=7, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f530f5a3b30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f530f5a3c30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f530f5a3db0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,924 | xffl.learning.distributed |    DEBUG | [Rank 28]: distributed setup: DistributedState(rank=28, world_size=32, group_local_rank=0, group_local_size=4, group_rank=7, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7efc939c7e70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7efc939c7ef0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7efc939f0170>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,924 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:29:44,924 | xffl.learning.distributed |    DEBUG | [Rank 26]: distributed setup: DistributedState(rank=26, world_size=32, group_local_rank=2, group_local_size=4, group_rank=6, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8b3d4d3330>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8b3d4d33b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8b3d4d3430>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,924 | xffl.learning.distributed |    DEBUG | [Rank 30]: distributed setup: DistributedState(rank=30, world_size=32, group_local_rank=2, group_local_size=4, group_rank=7, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f93472ffaf0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f93472ffcf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f93472ffe70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:44,925 | xffl.learning.distributed |    DEBUG | [Rank 31]: distributed setup: DistributedState(rank=31, world_size=32, group_local_rank=3, group_local_size=4, group_rank=7, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6d02c32ef0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6d02c32f70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6d02c331b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:29:45,077 | xffl.learning.utils |    DEBUG | [Rank 4]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:45,082 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:45,082 | xffl.learning.utils |    DEBUG | [Rank 8]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:45,086 | xffl.learning.utils |    DEBUG | [Rank 20]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:45,086 | xffl.learning.utils |    DEBUG | [Rank 28]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:45,089 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:45,090 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:45,091 | xffl.learning.utils |    DEBUG | [Rank 16]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:45,092 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:45,096 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:45,115 | xffl.learning.utils |    DEBUG | [Rank 24]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:45,120 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:45,135 | xffl.learning.utils |    DEBUG | [Rank 0]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:45,141 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:45,143 | xffl.learning.utils |    DEBUG | [Rank 12]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:45,151 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:46,694 | xffl.learning.utils |    DEBUG | [Rank 23]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:46,698 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:46,730 | xffl.learning.utils |    DEBUG | [Rank 22]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:46,733 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:46,752 | xffl.learning.utils |    DEBUG | [Rank 21]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:46,755 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,512 | xffl.learning.utils |    DEBUG | [Rank 15]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,514 | xffl.learning.utils |    DEBUG | [Rank 30]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,514 | xffl.learning.utils |    DEBUG | [Rank 31]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,516 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,518 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,518 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,530 | xffl.learning.utils |    DEBUG | [Rank 14]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,534 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,541 | xffl.learning.utils |    DEBUG | [Rank 27]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,543 | xffl.learning.utils |    DEBUG | [Rank 26]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,545 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,546 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,556 | xffl.learning.utils |    DEBUG | [Rank 11]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,557 | xffl.learning.utils |    DEBUG | [Rank 19]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,557 | xffl.learning.utils |    DEBUG | [Rank 18]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,560 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,560 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,561 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,571 | xffl.learning.utils |    DEBUG | [Rank 10]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,575 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,603 | xffl.learning.utils |    DEBUG | [Rank 7]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,604 | xffl.learning.utils |    DEBUG | [Rank 6]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,606 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,607 | xffl.learning.utils |    DEBUG | [Rank 3]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,608 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,611 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,620 | xffl.learning.utils |    DEBUG | [Rank 29]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,624 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,628 | xffl.learning.utils |    DEBUG | [Rank 2]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,632 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,675 | xffl.learning.utils |    DEBUG | [Rank 17]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,678 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,695 | xffl.learning.utils |    DEBUG | [Rank 13]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,698 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,707 | xffl.learning.utils |    DEBUG | [Rank 25]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,711 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,726 | xffl.learning.utils |    DEBUG | [Rank 9]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,730 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,739 | xffl.learning.utils |    DEBUG | [Rank 1]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,743 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:47,751 | xffl.learning.utils |    DEBUG | [Rank 5]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:29:47,755 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:29:50,695 | xffl.learning.modelling |    DEBUG | [Rank 20]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:50,695 | xffl.learning.distributed |    DEBUG | [Rank 20]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:51,381 | xffl.learning.modelling |    DEBUG | [Rank 12]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:51,381 | xffl.learning.distributed |    DEBUG | [Rank 12]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:51,390 | xffl.learning.modelling |    DEBUG | [Rank 4]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:51,391 | xffl.learning.distributed |    DEBUG | [Rank 4]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:51,391 | xffl.learning.modelling |    DEBUG | [Rank 21]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:51,391 | xffl.learning.distributed |    DEBUG | [Rank 21]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:51,434 | xffl.learning.modelling |    DEBUG | [Rank 16]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:51,434 | xffl.learning.distributed |    DEBUG | [Rank 16]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:51,461 |         __main__ |    DEBUG | Model loading time: 5.76 seconds[0m
[38;5;39m2025-03-26 00:29:51,462 |         __main__ |    DEBUG | Training llama3.1-8b: 8030.26 million trainable parameters[0m
[38;5;39m2025-03-26 00:29:51,462 | xffl.learning.modelling |    DEBUG | [Rank 0]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:51,462 | xffl.learning.distributed |    DEBUG | [Rank 0]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:51,479 | xffl.learning.modelling |    DEBUG | [Rank 8]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:51,480 | xffl.learning.distributed |    DEBUG | [Rank 8]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:51,507 | xffl.learning.modelling |    DEBUG | [Rank 28]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:51,507 | xffl.learning.distributed |    DEBUG | [Rank 28]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:51,579 | xffl.learning.modelling |    DEBUG | [Rank 24]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:51,580 | xffl.learning.distributed |    DEBUG | [Rank 24]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:51,607 | xffl.learning.modelling |    DEBUG | [Rank 23]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:51,607 | xffl.learning.distributed |    DEBUG | [Rank 23]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:51,614 | xffl.learning.modelling |    DEBUG | [Rank 22]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:51,614 | xffl.learning.distributed |    DEBUG | [Rank 22]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,366 | xffl.learning.modelling |    DEBUG | [Rank 30]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,367 | xffl.learning.distributed |    DEBUG | [Rank 30]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,370 | xffl.learning.modelling |    DEBUG | [Rank 31]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,370 | xffl.learning.distributed |    DEBUG | [Rank 31]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,394 | xffl.learning.modelling |    DEBUG | [Rank 11]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,394 | xffl.learning.distributed |    DEBUG | [Rank 11]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,394 | xffl.learning.modelling |    DEBUG | [Rank 10]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,394 | xffl.learning.distributed |    DEBUG | [Rank 10]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,412 | xffl.learning.modelling |    DEBUG | [Rank 18]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,412 | xffl.learning.distributed |    DEBUG | [Rank 18]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,415 | xffl.learning.modelling |    DEBUG | [Rank 19]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,416 | xffl.learning.distributed |    DEBUG | [Rank 19]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,416 | xffl.learning.modelling |    DEBUG | [Rank 26]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,417 | xffl.learning.distributed |    DEBUG | [Rank 26]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,425 | xffl.learning.modelling |    DEBUG | [Rank 27]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,425 | xffl.learning.distributed |    DEBUG | [Rank 27]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,565 | xffl.learning.modelling |    DEBUG | [Rank 25]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,565 | xffl.learning.distributed |    DEBUG | [Rank 25]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.modelling |    DEBUG | [Rank 15]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.distributed |    DEBUG | [Rank 15]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,567 | xffl.learning.modelling |    DEBUG | [Rank 2]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.distributed |    DEBUG | [Rank 2]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.modelling |    DEBUG | [Rank 3]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.modelling |    DEBUG | [Rank 14]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.distributed |    DEBUG | [Rank 3]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.modelling |    DEBUG | [Rank 7]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.modelling |    DEBUG | [Rank 6]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.distributed |    DEBUG | [Rank 7]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.modelling |    DEBUG | [Rank 5]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.distributed |    DEBUG | [Rank 14]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.modelling |    DEBUG | [Rank 9]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.modelling |    DEBUG | [Rank 29]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.distributed |    DEBUG | [Rank 6]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.distributed |    DEBUG | [Rank 9]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.distributed |    DEBUG | [Rank 29]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.distributed |    DEBUG | [Rank 5]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.modelling |    DEBUG | [Rank 17]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,568 | xffl.learning.distributed |    DEBUG | [Rank 17]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,624 | xffl.learning.modelling |    DEBUG | [Rank 13]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,624 | xffl.learning.distributed |    DEBUG | [Rank 13]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:29:52,637 | xffl.learning.modelling |    DEBUG | [Rank 1]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:29:52,637 | xffl.learning.distributed |    DEBUG | [Rank 1]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:30:00,755 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,755 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,755 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,755 |         __main__ |    DEBUG | FSDP wrapping setup time: 9.29 seconds[0m
[38;5;39m2025-03-26 00:30:00,755 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,755 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,755 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,755 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,756 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,756 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,756 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,756 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,759 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,759 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,760 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,760 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,760 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,894 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,895 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,895 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,895 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,895 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,895 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,895 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,895 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,903 |         __main__ |    DEBUG | Dataset loading time: 0.15 seconds[0m
[38;5;39m2025-03-26 00:30:00,905 |         __main__ |    DEBUG | [Rank 23]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:00,905 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:00,906 |         __main__ |    DEBUG | [Rank 20]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:00,906 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:00,906 |         __main__ |    DEBUG | [Rank 21]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:00,906 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:00,924 |         __main__ |    DEBUG | [Rank 22]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:00,924 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:00,927 |         __main__ |    DEBUG | train set size: 8192 samples[0m
[38;5;39m2025-03-26 00:30:00,927 |         __main__ |    DEBUG | [Rank 3]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:30:00,928 |         __main__ |    DEBUG | train dataloader size: 32 minibatches[0m
[38;20m2025-03-26 00:30:00,928 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:00,928 |         __main__ |    DEBUG | [Rank 4]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:30:00,928 |         __main__ |    DEBUG | [Rank 7]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:00,928 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:00,928 |         __main__ |    DEBUG | val set size: 5311 samples[0m
[38;20m2025-03-26 00:30:00,928 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:00,928 |         __main__ |    DEBUG | [Rank 5]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:30:00,928 |         __main__ |    DEBUG | val dataloader size: 165 minibatches[0m
[38;5;39m2025-03-26 00:30:00,928 |         __main__ |    DEBUG | Dataloaders creation time: 0.03 seconds[0m
[38;20m2025-03-26 00:30:00,928 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:00,928 |         __main__ |    DEBUG | Learning rate adjusted to: 0.0002[0m
[38;5;39m2025-03-26 00:30:00,928 |         __main__ |    DEBUG | [Rank 1]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:00,929 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:00,929 |         __main__ |    DEBUG | [Rank 17]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:00,929 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:00,929 |         __main__ |    DEBUG | Total setup time: 21.08 seconds[0m
[38;5;39m2025-03-26 00:30:00,929 |         __main__ |    DEBUG | [Rank 0]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:00,929 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:00,929 |         __main__ |    DEBUG | [Rank 18]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:30:00,929 |         __main__ |    DEBUG | [Rank 16]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:00,930 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:30:00,930 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:00,944 |         __main__ |    DEBUG | [Rank 6]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:00,945 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:00,945 |         __main__ |    DEBUG | [Rank 2]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:00,945 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:00,946 |         __main__ |    DEBUG | [Rank 19]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:00,946 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:00,971 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,971 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,971 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,972 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,971 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,972 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,974 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:00,974 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:30:01,065 |         __main__ |    DEBUG | [Rank 9]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:30:01,065 |         __main__ |    DEBUG | [Rank 10]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:01,065 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:01,065 |         __main__ |    DEBUG | [Rank 11]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:01,065 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:30:01,065 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:01,079 |         __main__ |    DEBUG | [Rank 14]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:01,079 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:01,079 |         __main__ |    DEBUG | [Rank 13]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:30:01,079 |         __main__ |    DEBUG | [Rank 15]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:01,080 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:30:01,080 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:01,082 |         __main__ |    DEBUG | [Rank 8]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:01,083 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:01,100 |         __main__ |    DEBUG | [Rank 12]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:01,100 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:01,192 |         __main__ |    DEBUG | [Rank 28]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:01,192 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:01,192 |         __main__ |    DEBUG | [Rank 29]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:30:01,192 |         __main__ |    DEBUG | [Rank 30]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:01,193 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:30:01,193 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:01,194 |         __main__ |    DEBUG | [Rank 25]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:01,194 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:01,195 |         __main__ |    DEBUG | [Rank 26]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:01,195 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:01,195 |         __main__ |    DEBUG | [Rank 27]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:01,195 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:01,209 |         __main__ |    DEBUG | [Rank 31]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:01,209 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:30:01,210 |         __main__ |    DEBUG | [Rank 24]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:30:01,211 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:32:41,726 | xffl.learning.processing |     INFO | Epoch 1: train_perplexity=41654.2773, train_epoch_loss=10.6372, epoch time 159.83478442812338s[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 11]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 9]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 10]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 8]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 13]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 14]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 15]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 12]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 2]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 3]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 1]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 4]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 5]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 7]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 6]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 |         __main__ |    DEBUG | Key: avg_epoch_time, Value: 159.83478442812338[0m
[38;5;39m2025-03-26 00:32:41,728 |         __main__ |    DEBUG | Key: avg_train_perp, Value: 41654.27734375[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 30]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 29]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 28]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 31]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 24]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 25]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 26]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 27]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,728 |         __main__ |    DEBUG | Key: avg_train_loss, Value: 10.63715934753418[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 22]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 21]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 20]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 16]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 18]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 17]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 19]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,727 | xffl.learning.distributed |    DEBUG | [Rank 23]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:32:41,728 | xffl.learning.distributed |    DEBUG | [Rank 0]: calling destroy_process_group[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-b8ws90xz[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-b8ws90xz/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-m9lxua23[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-m9lxua23/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-uhsiuisa[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-uhsiuisa/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002945-zwgphpvo[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002945-zwgphpvo/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-6xg30e4q[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-6xg30e4q/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-fpui5gh9[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-fpui5gh9/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-fsglhby3[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-fsglhby3/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002945-7ukweeux[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002945-7ukweeux/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-0n5qkray[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-0n5qkray/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-7dwclrxb[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-7dwclrxb/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002946-b4g6cm82[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002946-b4g6cm82/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002945-e7ohw71m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002945-e7ohw71m/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-zrk5m4qs[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-zrk5m4qs/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-mbj5n04u[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-mbj5n04u/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-6oz8ldbs[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-6oz8ldbs/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002945-tluoloe2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002945-tluoloe2/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-4fvih3fb[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-4fvih3fb/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-gy5ipcn8[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-gy5ipcn8/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-094xlrd4[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-094xlrd4/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002946-zjhispcp[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002946-zjhispcp/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002946-mcmruf7e[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002946-mcmruf7e/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-czujp35t[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-czujp35t/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-xggn9ve4[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-xggn9ve4/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002945-ldk1qdw9[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002945-ldk1qdw9/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-wbkb50el[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-wbkb50el/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-yad4p7hu[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-yad4p7hu/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002945-28c6fscl[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002945-28c6fscl/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-2smn9kvl[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-2smn9kvl/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-bmdjqbww[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-bmdjqbww/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002945-ielbgu6g[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002945-ielbgu6g/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002945-kjcmeor7[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002945-kjcmeor7/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002947-q94dim8m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002947-q94dim8m/logs[0m
[38;20m2025-03-26 00:32:46,065 | xffl.cli.simulate |     INFO | Total simulation execution time: 196.31 seconds[0m
[38;20m2025-03-26 00:32:46,065 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
