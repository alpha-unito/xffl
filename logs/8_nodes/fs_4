2, 1, 2
[38;20m2025-03-26 00:22:56,431 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
[38;5;226m2025-03-26 00:22:56,432 |   xffl.cli.utils |  WARNING | CLI argument "help" has got default value "False"[0m
[38;5;226m2025-03-26 00:22:56,432 |   xffl.cli.utils |  WARNING | CLI argument "workdir" has got default value "/leonardo_scratch/fast/uToID_bench/xffl"[0m
[38;5;226m2025-03-26 00:22:56,432 |   xffl.cli.utils |  WARNING | CLI argument "image" has got default value "None"[0m
[38;5;39m2025-03-26 00:22:56,434 | xffl.cli.simulate |    DEBUG | Using virtual environment: /leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate[0m
[38;5;39m2025-03-26 00:22:56,434 | xffl.cli.simulate |    DEBUG | New local simulation xFFL environment variables: {'XFFL_WORLD_SIZE': '32', 'XFFL_NUM_NODES': '8', 'MASTER_ADDR': 'lrdn0001', 'VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;5;39m2025-03-26 00:22:56,434 | xffl.cli.simulate |    DEBUG | Updated xFFL environment: {'XFFL_WORLD_SIZE': '32', 'XFFL_NUM_NODES': '8', 'MASTER_ADDR': 'lrdn0001', 'VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;20m2025-03-26 00:22:56,434 | xffl.cli.simulate |     INFO | Running local simulation...[0m
[38;5;39m2025-03-26 00:22:56,434 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0001: ssh -oStrictHostKeyChecking=no lrdn0001 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=0 XFFL_FEDERATED_RANK=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:22:56,435 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0010: ssh -oStrictHostKeyChecking=no lrdn0010 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=1 XFFL_FEDERATED_RANK=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:22:56,435 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0182: ssh -oStrictHostKeyChecking=no lrdn0182 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=2 XFFL_FEDERATED_RANK=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:22:56,435 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0189: ssh -oStrictHostKeyChecking=no lrdn0189 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=3 XFFL_FEDERATED_RANK=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:22:56,435 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0361: ssh -oStrictHostKeyChecking=no lrdn0361 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=4 XFFL_FEDERATED_RANK=2 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:22:56,435 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0370: ssh -oStrictHostKeyChecking=no lrdn0370 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=5 XFFL_FEDERATED_RANK=2 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:22:56,436 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0542: ssh -oStrictHostKeyChecking=no lrdn0542 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=6 XFFL_FEDERATED_RANK=3 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:22:56,436 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0549: ssh -oStrictHostKeyChecking=no lrdn0549 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=7 XFFL_FEDERATED_RANK=3 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 4 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS4 -mode offline "[0m
[38;5;39m2025-03-26 00:23:05,829 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,829 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,829 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,829 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,829 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,829 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,829 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,829 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,829 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,829 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,830 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,830 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,830 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,830 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,830 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,830 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,830 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,830 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,830 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,830 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,831 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,831 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,831 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,831 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,831 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,831 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,831 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,831 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,832 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,832 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,832 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:05,832 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:23:10,898 | xffl.learning.distributed |    DEBUG | [Rank 0]: distributed setup: DistributedState(rank=0, world_size=32, group_local_rank=0, group_local_size=4, group_rank=0, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0d9bc130b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0d9bc13130>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0d9bc131b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,898 |         __main__ |    DEBUG | Randez-vous time: 5.06 seconds[0m
[38;5;39m2025-03-26 00:23:10,898 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:23:10,898 | xffl.learning.distributed |    DEBUG | [Rank 1]: distributed setup: DistributedState(rank=1, world_size=32, group_local_rank=1, group_local_size=4, group_rank=0, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbd4c2e73b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbd4c2e7430>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbd4c2e74b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,898 | xffl.learning.distributed |    DEBUG | [Rank 2]: distributed setup: DistributedState(rank=2, world_size=32, group_local_rank=2, group_local_size=4, group_rank=0, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fdac3126ff0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fdac3127030>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fdac31270b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,899 | xffl.learning.distributed |    DEBUG | [Rank 3]: distributed setup: DistributedState(rank=3, world_size=32, group_local_rank=3, group_local_size=4, group_rank=0, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2eb0dcaf30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2eb0dcafb0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2eb0dcb030>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,899 | xffl.learning.distributed |    DEBUG | [Rank 4]: distributed setup: DistributedState(rank=4, world_size=32, group_local_rank=0, group_local_size=4, group_rank=1, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f02954e73f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f02954e7470>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f02954e74f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,899 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:23:10,899 | xffl.learning.distributed |    DEBUG | [Rank 5]: distributed setup: DistributedState(rank=5, world_size=32, group_local_rank=1, group_local_size=4, group_rank=1, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f41ace03f70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f41ace2c130>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f41ace2c1b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,899 | xffl.learning.distributed |    DEBUG | [Rank 7]: distributed setup: DistributedState(rank=7, world_size=32, group_local_rank=3, group_local_size=4, group_rank=1, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f901da36f30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f901da36fb0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f901da37030>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,899 | xffl.learning.distributed |    DEBUG | [Rank 6]: distributed setup: DistributedState(rank=6, world_size=32, group_local_rank=2, group_local_size=4, group_rank=1, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2603f8f4b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2603f8f530>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2603f8f5b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,902 | xffl.learning.distributed |    DEBUG | [Rank 8]: distributed setup: DistributedState(rank=8, world_size=32, group_local_rank=0, group_local_size=4, group_rank=2, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f60570ab1f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f60570ab2b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f60570ab3f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,902 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:23:10,902 | xffl.learning.distributed |    DEBUG | [Rank 9]: distributed setup: DistributedState(rank=9, world_size=32, group_local_rank=1, group_local_size=4, group_rank=2, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f10bf656e70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f10bf656f30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f10bf6570f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,903 | xffl.learning.distributed |    DEBUG | [Rank 10]: distributed setup: DistributedState(rank=10, world_size=32, group_local_rank=2, group_local_size=4, group_rank=2, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb789b476f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb789b47770>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb789b479b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,903 | xffl.learning.distributed |    DEBUG | [Rank 11]: distributed setup: DistributedState(rank=11, world_size=32, group_local_rank=3, group_local_size=4, group_rank=2, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb92f9e32b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb92f9e3330>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb92f9e33b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,903 | xffl.learning.distributed |    DEBUG | [Rank 13]: distributed setup: DistributedState(rank=13, world_size=32, group_local_rank=1, group_local_size=4, group_rank=3, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f08589c7430>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f08589c77b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f08589c79b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,903 | xffl.learning.distributed |    DEBUG | [Rank 12]: distributed setup: DistributedState(rank=12, world_size=32, group_local_rank=0, group_local_size=4, group_rank=3, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe724b6aa70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe724b6aaf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe724b6ad30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,903 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:23:10,903 | xffl.learning.distributed |    DEBUG | [Rank 15]: distributed setup: DistributedState(rank=15, world_size=32, group_local_rank=3, group_local_size=4, group_rank=3, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7efb038c8330>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7efb038c8430>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7efb038c85b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,903 | xffl.learning.distributed |    DEBUG | [Rank 14]: distributed setup: DistributedState(rank=14, world_size=32, group_local_rank=2, group_local_size=4, group_rank=3, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6ddfe831f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6ddfe83270>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6ddfe834b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,903 | xffl.learning.distributed |    DEBUG | [Rank 24]: distributed setup: DistributedState(rank=24, world_size=32, group_local_rank=0, group_local_size=4, group_rank=6, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f731a427df0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f731a427ef0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f731a4500b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,903 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.distributed |    DEBUG | [Rank 28]: distributed setup: DistributedState(rank=28, world_size=32, group_local_rank=0, group_local_size=4, group_rank=7, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7faddcd8aeb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7faddcd8aef0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7faddcd8b130>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.distributed |    DEBUG | [Rank 25]: distributed setup: DistributedState(rank=25, world_size=32, group_local_rank=1, group_local_size=4, group_rank=6, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbd213db2b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbd213db330>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbd213db570>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.distributed |    DEBUG | [Rank 18]: distributed setup: DistributedState(rank=18, world_size=32, group_local_rank=2, group_local_size=4, group_rank=4, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5f219ab4b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5f219ab530>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5f219ab770>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.distributed |    DEBUG | [Rank 16]: distributed setup: DistributedState(rank=16, world_size=32, group_local_rank=0, group_local_size=4, group_rank=4, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3babd10170>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3babd101b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3babd103b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.distributed |    DEBUG | [Rank 26]: distributed setup: DistributedState(rank=26, world_size=32, group_local_rank=2, group_local_size=4, group_rank=6, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f74dd6cf770>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f74dd6cf7f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f74dd6cfa30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.distributed |    DEBUG | [Rank 27]: distributed setup: DistributedState(rank=27, world_size=32, group_local_rank=3, group_local_size=4, group_rank=6, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb62aa53030>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb62aa530b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb62aa532f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.distributed |    DEBUG | [Rank 17]: distributed setup: DistributedState(rank=17, world_size=32, group_local_rank=1, group_local_size=4, group_rank=4, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe1712673b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe171267430>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fe171267670>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.distributed |    DEBUG | [Rank 29]: distributed setup: DistributedState(rank=29, world_size=32, group_local_rank=1, group_local_size=4, group_rank=7, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f278c34ae30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f278c34aeb0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f278c34b0f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.distributed |    DEBUG | [Rank 20]: distributed setup: DistributedState(rank=20, world_size=32, group_local_rank=0, group_local_size=4, group_rank=5, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f82959d3fb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8295a04030>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f8295a042b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.distributed |    DEBUG | [Rank 19]: distributed setup: DistributedState(rank=19, world_size=32, group_local_rank=3, group_local_size=4, group_rank=4, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f81b2c774b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f81b2c77530>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f81b2c77770>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.distributed |    DEBUG | [Rank 21]: distributed setup: DistributedState(rank=21, world_size=32, group_local_rank=1, group_local_size=4, group_rank=5, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f96275ffa30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f96275ffab0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f96275ffcf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.distributed |    DEBUG | [Rank 31]: distributed setup: DistributedState(rank=31, world_size=32, group_local_rank=3, group_local_size=4, group_rank=7, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7def8db370>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7def8db3f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7def8db630>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.distributed |    DEBUG | [Rank 30]: distributed setup: DistributedState(rank=30, world_size=32, group_local_rank=2, group_local_size=4, group_rank=7, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd31bb07930>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd31bb079b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd31bb07bf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.distributed |    DEBUG | [Rank 22]: distributed setup: DistributedState(rank=22, world_size=32, group_local_rank=2, group_local_size=4, group_rank=5, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd8466eadb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd8466eae30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fd8466eb030>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:10,904 | xffl.learning.distributed |    DEBUG | [Rank 23]: distributed setup: DistributedState(rank=23, world_size=32, group_local_rank=3, group_local_size=4, group_rank=5, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fef6b8d6c70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fef6b8d6df0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fef6b8d7030>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:23:11,008 | xffl.learning.utils |    DEBUG | [Rank 20]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:11,008 | xffl.learning.utils |    DEBUG | [Rank 16]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:11,012 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:11,012 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:11,032 | xffl.learning.utils |    DEBUG | [Rank 4]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:11,036 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:11,036 | xffl.learning.utils |    DEBUG | [Rank 28]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:11,040 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:11,049 | xffl.learning.utils |    DEBUG | [Rank 0]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:11,053 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:11,057 | xffl.learning.utils |    DEBUG | [Rank 24]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:11,061 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:11,063 | xffl.learning.utils |    DEBUG | [Rank 8]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:11,064 | xffl.learning.utils |    DEBUG | [Rank 12]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:11,067 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:11,068 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,626 | xffl.learning.utils |    DEBUG | [Rank 3]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,630 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,636 | xffl.learning.utils |    DEBUG | [Rank 19]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,636 | xffl.learning.utils |    DEBUG | [Rank 27]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,640 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,640 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,644 | xffl.learning.utils |    DEBUG | [Rank 7]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,646 | xffl.learning.utils |    DEBUG | [Rank 26]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,648 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,649 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,653 | xffl.learning.utils |    DEBUG | [Rank 18]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,655 | xffl.learning.utils |    DEBUG | [Rank 11]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,656 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,659 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,662 | xffl.learning.utils |    DEBUG | [Rank 30]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,664 | xffl.learning.utils |    DEBUG | [Rank 10]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,665 | xffl.learning.utils |    DEBUG | [Rank 15]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,666 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,667 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,668 | xffl.learning.utils |    DEBUG | [Rank 2]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,669 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,671 | xffl.learning.utils |    DEBUG | [Rank 14]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,672 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,672 | xffl.learning.utils |    DEBUG | [Rank 31]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,675 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,676 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,677 | xffl.learning.utils |    DEBUG | [Rank 23]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,681 | xffl.learning.utils |    DEBUG | [Rank 6]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,681 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,685 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,691 | xffl.learning.utils |    DEBUG | [Rank 22]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,695 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,713 | xffl.learning.utils |    DEBUG | [Rank 13]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,717 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,724 | xffl.learning.utils |    DEBUG | [Rank 1]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,728 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,728 | xffl.learning.utils |    DEBUG | [Rank 25]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,732 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,741 | xffl.learning.utils |    DEBUG | [Rank 5]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,745 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,749 | xffl.learning.utils |    DEBUG | [Rank 21]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,753 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,755 | xffl.learning.utils |    DEBUG | [Rank 17]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,757 | xffl.learning.utils |    DEBUG | [Rank 29]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,758 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,761 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:12,765 | xffl.learning.utils |    DEBUG | [Rank 9]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:23:12,769 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:23:16,721 | xffl.learning.modelling |    DEBUG | [Rank 28]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:16,721 | xffl.learning.distributed |    DEBUG | [Rank 28]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:16,725 | xffl.learning.modelling |    DEBUG | [Rank 4]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:16,725 | xffl.learning.distributed |    DEBUG | [Rank 4]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:16,726 | xffl.learning.modelling |    DEBUG | [Rank 12]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:16,726 | xffl.learning.distributed |    DEBUG | [Rank 12]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:16,743 | xffl.learning.modelling |    DEBUG | [Rank 16]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:16,743 | xffl.learning.distributed |    DEBUG | [Rank 16]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:16,760 | xffl.learning.modelling |    DEBUG | [Rank 24]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:16,760 | xffl.learning.distributed |    DEBUG | [Rank 24]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:16,762 |         __main__ |    DEBUG | Model loading time: 5.19 seconds[0m
[38;5;39m2025-03-26 00:23:16,763 |         __main__ |    DEBUG | Training llama3.1-8b: 8030.26 million trainable parameters[0m
[38;5;39m2025-03-26 00:23:16,763 | xffl.learning.modelling |    DEBUG | [Rank 0]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:16,763 | xffl.learning.distributed |    DEBUG | [Rank 0]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:16,779 | xffl.learning.modelling |    DEBUG | [Rank 20]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:16,779 | xffl.learning.distributed |    DEBUG | [Rank 20]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:16,791 | xffl.learning.modelling |    DEBUG | [Rank 8]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:16,791 | xffl.learning.distributed |    DEBUG | [Rank 8]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,440 | xffl.learning.modelling |    DEBUG | [Rank 19]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,440 | xffl.learning.distributed |    DEBUG | [Rank 19]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,440 | xffl.learning.modelling |    DEBUG | [Rank 21]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,441 | xffl.learning.distributed |    DEBUG | [Rank 21]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,443 | xffl.learning.modelling |    DEBUG | [Rank 15]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,443 | xffl.learning.distributed |    DEBUG | [Rank 15]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,445 | xffl.learning.modelling |    DEBUG | [Rank 3]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,445 | xffl.learning.distributed |    DEBUG | [Rank 3]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,446 | xffl.learning.modelling |    DEBUG | [Rank 14]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,446 | xffl.learning.distributed |    DEBUG | [Rank 14]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,450 | xffl.learning.modelling |    DEBUG | [Rank 18]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,450 | xffl.learning.distributed |    DEBUG | [Rank 18]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,452 | xffl.learning.modelling |    DEBUG | [Rank 13]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,453 | xffl.learning.distributed |    DEBUG | [Rank 13]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,453 | xffl.learning.modelling |    DEBUG | [Rank 5]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,453 | xffl.learning.distributed |    DEBUG | [Rank 5]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,454 | xffl.learning.modelling |    DEBUG | [Rank 1]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,454 | xffl.learning.distributed |    DEBUG | [Rank 1]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,467 | xffl.learning.modelling |    DEBUG | [Rank 26]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,467 | xffl.learning.distributed |    DEBUG | [Rank 26]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,467 | xffl.learning.modelling |    DEBUG | [Rank 27]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,467 | xffl.learning.distributed |    DEBUG | [Rank 27]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,469 | xffl.learning.modelling |    DEBUG | [Rank 29]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,469 | xffl.learning.distributed |    DEBUG | [Rank 29]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,472 | xffl.learning.modelling |    DEBUG | [Rank 2]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,472 | xffl.learning.distributed |    DEBUG | [Rank 2]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,480 | xffl.learning.modelling |    DEBUG | [Rank 9]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,480 | xffl.learning.distributed |    DEBUG | [Rank 9]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,482 | xffl.learning.modelling |    DEBUG | [Rank 25]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,483 | xffl.learning.distributed |    DEBUG | [Rank 25]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,492 | xffl.learning.modelling |    DEBUG | [Rank 30]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,492 | xffl.learning.distributed |    DEBUG | [Rank 30]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,493 | xffl.learning.modelling |    DEBUG | [Rank 17]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,493 | xffl.learning.distributed |    DEBUG | [Rank 17]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,495 | xffl.learning.modelling |    DEBUG | [Rank 31]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,495 | xffl.learning.distributed |    DEBUG | [Rank 31]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,511 | xffl.learning.modelling |    DEBUG | [Rank 11]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,511 | xffl.learning.distributed |    DEBUG | [Rank 11]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,513 | xffl.learning.modelling |    DEBUG | [Rank 10]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,513 | xffl.learning.distributed |    DEBUG | [Rank 10]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,518 | xffl.learning.modelling |    DEBUG | [Rank 7]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,519 | xffl.learning.distributed |    DEBUG | [Rank 7]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,529 | xffl.learning.modelling |    DEBUG | [Rank 6]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,529 | xffl.learning.distributed |    DEBUG | [Rank 6]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,573 | xffl.learning.modelling |    DEBUG | [Rank 22]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,573 | xffl.learning.modelling |    DEBUG | [Rank 23]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:23:17,573 | xffl.learning.distributed |    DEBUG | [Rank 23]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:17,573 | xffl.learning.distributed |    DEBUG | [Rank 22]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:23:25,651 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,651 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,651 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,651 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,651 |         __main__ |    DEBUG | FSDP wrapping setup time: 8.89 seconds[0m
[38;5;39m2025-03-26 00:23:25,652 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,652 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,652 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,652 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,684 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,684 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,684 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,685 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,684 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,685 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,685 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,686 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,778 |         __main__ |    DEBUG | Dataset loading time: 0.13 seconds[0m
[38;5;39m2025-03-26 00:23:25,785 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,786 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,786 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,786 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,786 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,787 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,787 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,787 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,798 |         __main__ |    DEBUG | [Rank 6]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,798 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,798 |         __main__ |    DEBUG | [Rank 5]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,798 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,798 |         __main__ |    DEBUG | [Rank 4]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,798 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,800 |         __main__ |    DEBUG | train set size: 8192 samples[0m
[38;5;39m2025-03-26 00:23:25,801 |         __main__ |    DEBUG | train dataloader size: 32 minibatches[0m
[38;5;39m2025-03-26 00:23:25,801 |         __main__ |    DEBUG | val set size: 5311 samples[0m
[38;5;39m2025-03-26 00:23:25,801 |         __main__ |    DEBUG | val dataloader size: 165 minibatches[0m
[38;5;39m2025-03-26 00:23:25,801 |         __main__ |    DEBUG | Dataloaders creation time: 0.02 seconds[0m
[38;5;39m2025-03-26 00:23:25,801 |         __main__ |    DEBUG | Learning rate adjusted to: 0.0002[0m
[38;5;39m2025-03-26 00:23:25,802 |         __main__ |    DEBUG | [Rank 3]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:23:25,802 |         __main__ |    DEBUG | [Rank 2]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,802 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:23:25,802 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,802 |         __main__ |    DEBUG | Total setup time: 19.97 seconds[0m
[38;5;39m2025-03-26 00:23:25,802 |         __main__ |    DEBUG | [Rank 0]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,802 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,819 |         __main__ |    DEBUG | [Rank 7]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,820 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,821 |         __main__ |    DEBUG | [Rank 1]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,821 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,830 |         __main__ |    DEBUG | [Rank 12]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:23:25,830 |         __main__ |    DEBUG | [Rank 14]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:23:25,830 |         __main__ |    DEBUG | [Rank 15]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,831 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:23:25,831 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:23:25,831 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,831 |         __main__ |    DEBUG | [Rank 11]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,832 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,832 |         __main__ |    DEBUG | [Rank 9]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,832 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,832 |         __main__ |    DEBUG | [Rank 10]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,832 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,850 |         __main__ |    DEBUG | [Rank 13]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,851 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,851 |         __main__ |    DEBUG | [Rank 8]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,851 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,856 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,857 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,857 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,857 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,858 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,858 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,860 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,860 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:23:25,930 |         __main__ |    DEBUG | [Rank 31]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:23:25,930 |         __main__ |    DEBUG | [Rank 29]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,931 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:23:25,931 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,931 |         __main__ |    DEBUG | [Rank 30]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,931 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,934 |         __main__ |    DEBUG | [Rank 25]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,935 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,935 |         __main__ |    DEBUG | [Rank 27]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,935 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,935 |         __main__ |    DEBUG | [Rank 24]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,935 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,949 |         __main__ |    DEBUG | [Rank 28]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,949 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:25,953 |         __main__ |    DEBUG | [Rank 26]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:25,953 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:26,002 |         __main__ |    DEBUG | [Rank 23]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:26,003 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:26,002 |         __main__ |    DEBUG | [Rank 22]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:23:26,002 |         __main__ |    DEBUG | [Rank 20]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:26,003 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:23:26,003 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:26,007 |         __main__ |    DEBUG | [Rank 18]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:23:26,007 |         __main__ |    DEBUG | [Rank 16]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:23:26,007 |         __main__ |    DEBUG | [Rank 19]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:26,007 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:23:26,007 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:23:26,007 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:26,021 |         __main__ |    DEBUG | [Rank 21]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:26,022 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:23:26,026 |         __main__ |    DEBUG | [Rank 17]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:23:26,026 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:26:09,820 | xffl.learning.processing |     INFO | Epoch 1: train_perplexity=13211.3369, train_epoch_loss=9.4888, epoch time 163.0732572991401s[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 10]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 8]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 9]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 21]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 20]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 23]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 22]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 12]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 13]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 14]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 2]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 3]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 1]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 |         __main__ |    DEBUG | Key: avg_epoch_time, Value: 163.0732572991401[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 17]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 19]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 16]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 18]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 11]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,822 |         __main__ |    DEBUG | Key: avg_train_perp, Value: 13211.3369140625[0m
[38;5;39m2025-03-26 00:26:09,822 |         __main__ |    DEBUG | Key: avg_train_loss, Value: 9.48883056640625[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 15]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 6]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 7]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 5]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,821 | xffl.learning.distributed |    DEBUG | [Rank 4]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,822 | xffl.learning.distributed |    DEBUG | [Rank 0]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,825 | xffl.learning.distributed |    DEBUG | [Rank 31]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,825 | xffl.learning.distributed |    DEBUG | [Rank 28]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,825 | xffl.learning.distributed |    DEBUG | [Rank 30]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,825 | xffl.learning.distributed |    DEBUG | [Rank 29]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,825 | xffl.learning.distributed |    DEBUG | [Rank 26]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,825 | xffl.learning.distributed |    DEBUG | [Rank 27]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,825 | xffl.learning.distributed |    DEBUG | [Rank 25]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:26:09,825 | xffl.learning.distributed |    DEBUG | [Rank 24]: calling destroy_process_group[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-xcqnir9m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-xcqnir9m/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-nindx2fg[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-nindx2fg/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-uukwriq8[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-uukwriq8/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002311-8zxwvnzu[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002311-8zxwvnzu/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-abjicetg[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-abjicetg/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-q45ipkzm[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-q45ipkzm/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-fsihnf28[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-fsihnf28/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002311-o5ofq9tc[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002311-o5ofq9tc/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-2xzfs9p6[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-2xzfs9p6/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-7tpi4kti[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-7tpi4kti/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-35sbsz6u[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-35sbsz6u/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002311-ljpohz4h[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002311-ljpohz4h/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-n1lp0vqa[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-n1lp0vqa/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-44aljrbt[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-44aljrbt/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-bzotsvut[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-bzotsvut/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002311-3o5q5lbs[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002311-3o5q5lbs/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002311-ba879wbs[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002311-ba879wbs/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-dv884ofa[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-dv884ofa/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-ytd3d0f3[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-ytd3d0f3/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-4trra576[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-4trra576/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-bix151ws[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-bix151ws/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-m9lybds8[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-m9lybds8/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-9vyw3i6y[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-9vyw3i6y/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002311-c8hfhhbw[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002311-c8hfhhbw/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-gl7yuofj[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-gl7yuofj/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-3b2e0ljv[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-3b2e0ljv/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-d3vdp3gq[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-d3vdp3gq/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-li6pua4w[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-li6pua4w/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002311-ug803zpa[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002311-ug803zpa/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002311-dqf3xrvg[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002311-dqf3xrvg/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-8cqarwjd[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-8cqarwjd/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_002312-whp2pifn[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_002312-whp2pifn/logs[0m
[38;20m2025-03-26 00:26:14,078 | xffl.cli.simulate |     INFO | Total simulation execution time: 197.64 seconds[0m
[38;20m2025-03-26 00:26:14,078 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
