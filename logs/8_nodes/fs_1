2, 1, 2
[38;20m2025-03-26 00:17:30,702 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
[38;5;226m2025-03-26 00:17:30,702 |   xffl.cli.utils |  WARNING | CLI argument "help" has got default value "False"[0m
[38;5;226m2025-03-26 00:17:30,702 |   xffl.cli.utils |  WARNING | CLI argument "workdir" has got default value "/leonardo_scratch/fast/uToID_bench/xffl"[0m
[38;5;226m2025-03-26 00:17:30,702 |   xffl.cli.utils |  WARNING | CLI argument "image" has got default value "None"[0m
[38;5;39m2025-03-26 00:17:30,705 | xffl.cli.simulate |    DEBUG | Using virtual environment: /leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate[0m
[38;5;39m2025-03-26 00:17:30,705 | xffl.cli.simulate |    DEBUG | New local simulation xFFL environment variables: {'XFFL_WORLD_SIZE': '32', 'XFFL_NUM_NODES': '8', 'MASTER_ADDR': 'lrdn0001', 'VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;5;39m2025-03-26 00:17:30,705 | xffl.cli.simulate |    DEBUG | Updated xFFL environment: {'XFFL_WORLD_SIZE': '32', 'XFFL_NUM_NODES': '8', 'MASTER_ADDR': 'lrdn0001', 'VENV': '/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate', 'XFFL_FACILITY': 'leonardo', 'XFFL_SIMULATION': 'true'}[0m
[38;20m2025-03-26 00:17:30,705 | xffl.cli.simulate |     INFO | Running local simulation...[0m
[38;5;39m2025-03-26 00:17:30,705 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0001: ssh -oStrictHostKeyChecking=no lrdn0001 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=0 XFFL_FEDERATED_RANK=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:17:30,706 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0010: ssh -oStrictHostKeyChecking=no lrdn0010 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=1 XFFL_FEDERATED_RANK=0 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:17:30,706 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0182: ssh -oStrictHostKeyChecking=no lrdn0182 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=2 XFFL_FEDERATED_RANK=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:17:30,706 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0189: ssh -oStrictHostKeyChecking=no lrdn0189 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=3 XFFL_FEDERATED_RANK=1 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:17:30,707 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0361: ssh -oStrictHostKeyChecking=no lrdn0361 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=4 XFFL_FEDERATED_RANK=2 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:17:30,707 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0370: ssh -oStrictHostKeyChecking=no lrdn0370 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=5 XFFL_FEDERATED_RANK=2 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:17:30,707 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0542: ssh -oStrictHostKeyChecking=no lrdn0542 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=6 XFFL_FEDERATED_RANK=3 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:17:30,708 | xffl.cli.simulate |    DEBUG | Simulation command on lrdn0549: ssh -oStrictHostKeyChecking=no lrdn0549 " XFFL_WORLD_SIZE=32 XFFL_NUM_NODES=8 MASTER_ADDR=lrdn0001 VENV=/leonardo_scratch/fast/uToID_bench/xffl/.venv/bin/activate XFFL_FACILITY=leonardo XFFL_SIMULATION=true XFFL_FEDERATED_LOCAL_WORLD_SIZE=2,2,2,2  XFFL_NODEID=7 XFFL_FEDERATED_RANK=3 /leonardo_scratch/fast/uToID_bench/xffl/.venv/lib/python3.11/site-packages/xffl/workflow/scripts/facilitator.sh /leonardo_scratch/fast/uToID_bench/xffl/examples/llm/client/src/training.py -mn llama3.1-8b -mp /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b -dn clean_mc4_it -dp /leonardo_scratch/fast/uToID_bench/xffl/datasets/clean_mc4_it --seed 42 -fspan 1 -hsdp 4 --subsampling 8192 -dbg -t 8 -wb -name llama3.1-8_nodes-HSDP_4-FS -mode offline "[0m
[38;5;39m2025-03-26 00:17:39,722 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,722 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,722 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,722 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,722 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,722 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,722 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,722 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,723 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,723 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,723 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,723 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,741 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,741 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,741 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,741 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,746 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,746 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,746 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,746 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,792 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,792 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,792 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,792 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,794 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,794 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,794 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,794 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,798 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,798 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,798 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,798 | xffl.learning.utils |    DEBUG | Setting RNGs seed to 42[0m
[38;5;39m2025-03-26 00:17:39,848 | xffl.learning.distributed |    DEBUG | [Rank 0]: distributed setup: DistributedState(rank=0, world_size=32, group_local_rank=0, group_local_size=4, group_rank=0, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f97b64c6e30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f97b64c6eb0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f97b64c6f30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,848 |         __main__ |    DEBUG | Randez-vous time: 0.12 seconds[0m
[38;5;39m2025-03-26 00:17:39,848 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:17:39,848 | xffl.learning.distributed |    DEBUG | [Rank 1]: distributed setup: DistributedState(rank=1, world_size=32, group_local_rank=1, group_local_size=4, group_rank=0, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1dc97f2d70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1dc97f2df0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1dc97f2e70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,848 | xffl.learning.distributed |    DEBUG | [Rank 2]: distributed setup: DistributedState(rank=2, world_size=32, group_local_rank=2, group_local_size=4, group_rank=0, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f46c84c2930>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f46c84c2970>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f46c84c29f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,849 | xffl.learning.distributed |    DEBUG | [Rank 3]: distributed setup: DistributedState(rank=3, world_size=32, group_local_rank=3, group_local_size=4, group_rank=0, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5cdd282e70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5cdd282ef0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5cdd282f70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,849 | xffl.learning.distributed |    DEBUG | [Rank 4]: distributed setup: DistributedState(rank=4, world_size=32, group_local_rank=0, group_local_size=4, group_rank=1, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4852bdf570>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4852bdf5f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4852bdf670>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,849 | xffl.learning.distributed |    DEBUG | [Rank 5]: distributed setup: DistributedState(rank=5, world_size=32, group_local_rank=1, group_local_size=4, group_rank=1, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4945ecacb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4945ecad30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4945ecadb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,849 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:17:39,850 | xffl.learning.distributed |    DEBUG | [Rank 6]: distributed setup: DistributedState(rank=6, world_size=32, group_local_rank=2, group_local_size=4, group_rank=1, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4bbaefb3f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4bbaefb470>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f4bbaefb4f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,850 | xffl.learning.distributed |    DEBUG | [Rank 7]: distributed setup: DistributedState(rank=7, world_size=32, group_local_rank=3, group_local_size=4, group_rank=1, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8), federated_rank=0, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7c03937670>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7c039377f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7c03937870>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,856 | xffl.learning.distributed |    DEBUG | [Rank 24]: distributed setup: DistributedState(rank=24, world_size=32, group_local_rank=0, group_local_size=4, group_rank=6, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f79f8dffcb0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f79f8dffd30>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f79f8dfff30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,856 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:17:39,856 | xffl.learning.distributed |    DEBUG | [Rank 25]: distributed setup: DistributedState(rank=25, world_size=32, group_local_rank=1, group_local_size=4, group_rank=6, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fdc503f3930>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fdc503f39b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fdc503f3bf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,856 | xffl.learning.distributed |    DEBUG | [Rank 26]: distributed setup: DistributedState(rank=26, world_size=32, group_local_rank=2, group_local_size=4, group_rank=6, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff43d9c7070>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff43d9c70f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7ff43d9c7170>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,856 | xffl.learning.distributed |    DEBUG | [Rank 8]: distributed setup: DistributedState(rank=8, world_size=32, group_local_rank=0, group_local_size=4, group_rank=2, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5cebc87930>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5cebc87970>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f5cebc87bb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,856 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:17:39,856 | xffl.learning.distributed |    DEBUG | [Rank 10]: distributed setup: DistributedState(rank=10, world_size=32, group_local_rank=2, group_local_size=4, group_rank=2, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7e7bef71b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7e7bef72f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f7e7bef7530>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,856 | xffl.learning.distributed |    DEBUG | [Rank 9]: distributed setup: DistributedState(rank=9, world_size=32, group_local_rank=1, group_local_size=4, group_rank=2, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f806ce22ef0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f806ce22f70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f806ce231b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,856 | xffl.learning.distributed |    DEBUG | [Rank 12]: distributed setup: DistributedState(rank=12, world_size=32, group_local_rank=0, group_local_size=4, group_rank=3, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1160e63a30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1160e63ab0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1160e63cf0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 17]: distributed setup: DistributedState(rank=17, world_size=32, group_local_rank=1, group_local_size=4, group_rank=4, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=1, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3540af71f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3540af7270>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f3540af74b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 27]: distributed setup: DistributedState(rank=27, world_size=32, group_local_rank=3, group_local_size=4, group_rank=6, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcce6767330>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcce67673b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fcce67675f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 23]: distributed setup: DistributedState(rank=23, world_size=32, group_local_rank=3, group_local_size=4, group_rank=5, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbb1bbe0270>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbb1bbe02f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fbb1bbe0530>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 28]: distributed setup: DistributedState(rank=28, world_size=32, group_local_rank=0, group_local_size=4, group_rank=7, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6ddfadb1b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6ddfadb230>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f6ddfadb430>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 13]: distributed setup: DistributedState(rank=13, world_size=32, group_local_rank=1, group_local_size=4, group_rank=3, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f47693c74b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f47693c7530>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f47693c7770>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 19]: distributed setup: DistributedState(rank=19, world_size=32, group_local_rank=3, group_local_size=4, group_rank=4, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f52b65e7b30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f52b65e7bb0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f52b65e7df0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 16]: distributed setup: DistributedState(rank=16, world_size=32, group_local_rank=0, group_local_size=4, group_rank=4, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=0, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0d1ea0f8b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0d1ea0f930>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f0d1ea0fb70>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 20]: distributed setup: DistributedState(rank=20, world_size=32, group_local_rank=0, group_local_size=4, group_rank=5, group_world_size=8, replica_local_rank=0, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=4, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2db6c8b4b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2db6c8b530>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f2db6c8b770>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 11]: distributed setup: DistributedState(rank=11, world_size=32, group_local_rank=3, group_local_size=4, group_rank=2, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=3, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f415d5b7430>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f415d5b74b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f415d5b76f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 21]: distributed setup: DistributedState(rank=21, world_size=32, group_local_rank=1, group_local_size=4, group_rank=5, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb2667633b0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb266763430>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fb266763670>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.utils |    DEBUG | Preloading: /leonardo_scratch/fast/uToID_bench/xffl/models/llama3.1-8b[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 29]: distributed setup: DistributedState(rank=29, world_size=32, group_local_rank=1, group_local_size=4, group_rank=7, group_world_size=8, replica_local_rank=1, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=5, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fec1be07c30>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fec1be07cb0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fec1be07ef0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 18]: distributed setup: DistributedState(rank=18, world_size=32, group_local_rank=2, group_local_size=4, group_rank=4, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=0, replica_world_size=(2, 2, 2, 2), federated_local_rank=2, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f813bfaac70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f813bfaacf0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f813bfaaef0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 14]: distributed setup: DistributedState(rank=14, world_size=32, group_local_rank=2, group_local_size=4, group_rank=3, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7eff9d4bb1f0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7eff9d4bb270>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7eff9d4bb4b0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 15]: distributed setup: DistributedState(rank=15, world_size=32, group_local_rank=3, group_local_size=4, group_rank=3, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8), federated_rank=1, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1b48766d70>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1b48766df0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f1b48767030>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 22]: distributed setup: DistributedState(rank=22, world_size=32, group_local_rank=2, group_local_size=4, group_rank=5, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8), federated_rank=2, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f27396d2cf0>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f27396d2d70>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f27396d2fb0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 31]: distributed setup: DistributedState(rank=31, world_size=32, group_local_rank=3, group_local_size=4, group_rank=7, group_world_size=8, replica_local_rank=3, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=7, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc64cff7970>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc64cff79f0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7fc64cff7c30>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,857 | xffl.learning.distributed |    DEBUG | [Rank 30]: distributed setup: DistributedState(rank=30, world_size=32, group_local_rank=2, group_local_size=4, group_rank=7, group_world_size=8, replica_local_rank=2, replica_local_size=4, replica_rank=1, replica_world_size=(2, 2, 2, 2), federated_local_rank=6, federated_local_size=(8, 8, 8, 8), federated_rank=3, federated_world_size=4, fsdp_mesh=None, hsdp_mesh=DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')), federated_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f54c96ef130>, replica_group=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f54c96ef1b0>, federation=<torch.distributed.distributed_c10d.ProcessGroup object at 0x7f54c96ef3f0>, backend='nccl', master_addr='lrdn0001', master_port=29500, device='cuda')[0m
[38;5;39m2025-03-26 00:17:39,963 | xffl.learning.utils |    DEBUG | [Rank 20]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:39,964 | xffl.learning.utils |    DEBUG | [Rank 12]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:39,967 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:39,969 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:39,986 | xffl.learning.utils |    DEBUG | [Rank 8]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:39,990 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:39,996 | xffl.learning.utils |    DEBUG | [Rank 0]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:40,000 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:40,009 | xffl.learning.utils |    DEBUG | [Rank 24]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:40,012 | xffl.learning.utils |    DEBUG | [Rank 4]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:40,013 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:40,014 | xffl.learning.utils |    DEBUG | [Rank 16]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:40,015 | xffl.learning.utils |    DEBUG | [Rank 28]: assigned local execution device 0, initialisation device set to cpu; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:40,015 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:40,018 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:40,019 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,583 | xffl.learning.utils |    DEBUG | [Rank 2]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,584 | xffl.learning.utils |    DEBUG | [Rank 3]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,585 | xffl.learning.utils |    DEBUG | [Rank 19]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,587 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,588 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,589 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,589 | xffl.learning.utils |    DEBUG | [Rank 11]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,589 | xffl.learning.utils |    DEBUG | [Rank 18]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,593 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,593 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,594 | xffl.learning.utils |    DEBUG | [Rank 10]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,594 | xffl.learning.utils |    DEBUG | [Rank 27]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,598 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,598 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,604 | xffl.learning.utils |    DEBUG | [Rank 30]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,608 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,608 | xffl.learning.utils |    DEBUG | [Rank 26]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,612 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,615 | xffl.learning.utils |    DEBUG | [Rank 7]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,616 | xffl.learning.utils |    DEBUG | [Rank 6]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,619 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,620 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,622 | xffl.learning.utils |    DEBUG | [Rank 23]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,625 | xffl.learning.utils |    DEBUG | [Rank 31]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,625 | xffl.learning.utils |    DEBUG | [Rank 14]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,626 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,629 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,629 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,629 | xffl.learning.utils |    DEBUG | [Rank 15]: assigned local execution device 3, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,632 | xffl.learning.utils |    DEBUG | [Rank 25]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,633 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,636 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,638 | xffl.learning.utils |    DEBUG | [Rank 22]: assigned local execution device 2, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,641 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,642 | xffl.learning.utils |    DEBUG | [Rank 9]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,643 | xffl.learning.utils |    DEBUG | [Rank 1]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,645 | xffl.learning.utils |    DEBUG | [Rank 17]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,646 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,647 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,648 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,661 | xffl.learning.utils |    DEBUG | [Rank 29]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,662 | xffl.learning.utils |    DEBUG | [Rank 13]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,663 | xffl.learning.utils |    DEBUG | [Rank 21]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,664 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,665 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,667 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:41,682 | xffl.learning.utils |    DEBUG | [Rank 5]: assigned local execution device 1, initialisation device set to meta; meta initialization set to True[0m
[38;5;39m2025-03-26 00:17:41,686 | wandb.sdk.lib.gitlib |    DEBUG | git repository is invalid[0m
[38;5;39m2025-03-26 00:17:45,976 | xffl.learning.modelling |    DEBUG | [Rank 28]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:45,977 | xffl.learning.distributed |    DEBUG | [Rank 28]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:45,995 | xffl.learning.modelling |    DEBUG | [Rank 12]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:45,995 | xffl.learning.distributed |    DEBUG | [Rank 12]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,013 |         __main__ |    DEBUG | Model loading time: 4.95 seconds[0m
[38;5;39m2025-03-26 00:17:46,014 |         __main__ |    DEBUG | Training llama3.1-8b: 8030.26 million trainable parameters[0m
[38;5;39m2025-03-26 00:17:46,014 | xffl.learning.modelling |    DEBUG | [Rank 0]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,014 | xffl.learning.distributed |    DEBUG | [Rank 0]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,018 | xffl.learning.modelling |    DEBUG | [Rank 20]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,019 | xffl.learning.distributed |    DEBUG | [Rank 20]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,020 | xffl.learning.modelling |    DEBUG | [Rank 4]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,020 | xffl.learning.distributed |    DEBUG | [Rank 4]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,032 | xffl.learning.modelling |    DEBUG | [Rank 24]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,033 | xffl.learning.distributed |    DEBUG | [Rank 24]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,033 | xffl.learning.modelling |    DEBUG | [Rank 16]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,033 | xffl.learning.distributed |    DEBUG | [Rank 16]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,083 | xffl.learning.modelling |    DEBUG | [Rank 8]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,083 | xffl.learning.distributed |    DEBUG | [Rank 8]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,357 | xffl.learning.modelling |    DEBUG | [Rank 3]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,358 | xffl.learning.distributed |    DEBUG | [Rank 3]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,359 | xffl.learning.modelling |    DEBUG | [Rank 2]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,359 | xffl.learning.distributed |    DEBUG | [Rank 2]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,393 | xffl.learning.modelling |    DEBUG | [Rank 18]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,393 | xffl.learning.distributed |    DEBUG | [Rank 18]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,401 | xffl.learning.modelling |    DEBUG | [Rank 10]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,402 | xffl.learning.distributed |    DEBUG | [Rank 10]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,402 | xffl.learning.modelling |    DEBUG | [Rank 11]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,402 | xffl.learning.distributed |    DEBUG | [Rank 11]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,412 | xffl.learning.modelling |    DEBUG | [Rank 19]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,412 | xffl.learning.distributed |    DEBUG | [Rank 19]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,415 | xffl.learning.modelling |    DEBUG | [Rank 29]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,415 | xffl.learning.distributed |    DEBUG | [Rank 29]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,416 | xffl.learning.modelling |    DEBUG | [Rank 30]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,417 | xffl.learning.distributed |    DEBUG | [Rank 30]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,417 | xffl.learning.modelling |    DEBUG | [Rank 31]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,417 | xffl.learning.distributed |    DEBUG | [Rank 31]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,422 | xffl.learning.modelling |    DEBUG | [Rank 26]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,422 | xffl.learning.distributed |    DEBUG | [Rank 26]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,423 | xffl.learning.modelling |    DEBUG | [Rank 25]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,423 | xffl.learning.distributed |    DEBUG | [Rank 25]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,423 | xffl.learning.modelling |    DEBUG | [Rank 27]: is calling FSDP on device mesh DeviceMesh('cuda', [[24, 25, 26, 27], [28, 29, 30, 31]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,423 | xffl.learning.distributed |    DEBUG | [Rank 27]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,430 | xffl.learning.modelling |    DEBUG | [Rank 6]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,430 | xffl.learning.modelling |    DEBUG | [Rank 7]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,430 | xffl.learning.distributed |    DEBUG | [Rank 6]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,430 | xffl.learning.distributed |    DEBUG | [Rank 7]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,449 | xffl.learning.modelling |    DEBUG | [Rank 1]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,450 | xffl.learning.distributed |    DEBUG | [Rank 1]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,458 | xffl.learning.modelling |    DEBUG | [Rank 15]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,459 | xffl.learning.distributed |    DEBUG | [Rank 15]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,459 | xffl.learning.modelling |    DEBUG | [Rank 21]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,459 | xffl.learning.distributed |    DEBUG | [Rank 21]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,459 | xffl.learning.modelling |    DEBUG | [Rank 14]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,460 | xffl.learning.distributed |    DEBUG | [Rank 14]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,493 | xffl.learning.modelling |    DEBUG | [Rank 17]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,493 | xffl.learning.distributed |    DEBUG | [Rank 17]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,494 | xffl.learning.modelling |    DEBUG | [Rank 5]: is calling FSDP on device mesh DeviceMesh('cuda', [[0, 1, 2, 3], [4, 5, 6, 7]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,494 | xffl.learning.distributed |    DEBUG | [Rank 5]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,511 | xffl.learning.modelling |    DEBUG | [Rank 9]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,511 | xffl.learning.distributed |    DEBUG | [Rank 9]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,519 | xffl.learning.modelling |    DEBUG | [Rank 13]: is calling FSDP on device mesh DeviceMesh('cuda', [[8, 9, 10, 11], [12, 13, 14, 15]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,519 | xffl.learning.distributed |    DEBUG | [Rank 13]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,529 | xffl.learning.modelling |    DEBUG | [Rank 23]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,529 | xffl.learning.distributed |    DEBUG | [Rank 23]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:46,539 | xffl.learning.modelling |    DEBUG | [Rank 22]: is calling FSDP on device mesh DeviceMesh('cuda', [[16, 17, 18, 19], [20, 21, 22, 23]], mesh_dim_names=('replica', 'shard')) with meta initialization True[0m
[38;5;39m2025-03-26 00:17:46,539 | xffl.learning.distributed |    DEBUG | [Rank 22]: Activating "ShardingStrategy.HYBRID_SHARD" sharding strategy[0m
[38;5;39m2025-03-26 00:17:54,953 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,953 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,953 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,953 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,953 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,953 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,954 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,955 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,966 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,966 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,967 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,967 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,967 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,967 |         __main__ |    DEBUG | FSDP wrapping setup time: 8.95 seconds[0m
[38;5;39m2025-03-26 00:17:54,967 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,967 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,967 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,979 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,980 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,980 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,980 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,980 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,980 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,980 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,980 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,991 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,992 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,992 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,992 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,992 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,992 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,992 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:54,993 | xffl.learning.utils |    DEBUG | Activated non-reentrant model (gradient) checkpointing[0m
[38;5;39m2025-03-26 00:17:55,092 |         __main__ |    DEBUG | Dataset loading time: 0.12 seconds[0m
[38;5;39m2025-03-26 00:17:55,102 |         __main__ |    DEBUG | [Rank 13]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:17:55,102 |         __main__ |    DEBUG | [Rank 14]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,102 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:17:55,102 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,102 |         __main__ |    DEBUG | [Rank 15]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,102 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,102 |         __main__ |    DEBUG | [Rank 10]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,103 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,103 |         __main__ |    DEBUG | [Rank 11]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:17:55,103 |         __main__ |    DEBUG | [Rank 9]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,103 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:17:55,103 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,113 |         __main__ |    DEBUG | [Rank 7]: --- STARTING TRAINING ---[0m
[38;5;39m2025-03-26 00:17:55,113 |         __main__ |    DEBUG | [Rank 6]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,113 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:17:55,113 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,114 |         __main__ |    DEBUG | [Rank 5]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,114 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,115 |         __main__ |    DEBUG | train set size: 8192 samples[0m
[38;5;39m2025-03-26 00:17:55,115 |         __main__ |    DEBUG | train dataloader size: 32 minibatches[0m
[38;5;39m2025-03-26 00:17:55,115 |         __main__ |    DEBUG | val set size: 5311 samples[0m
[38;5;39m2025-03-26 00:17:55,115 |         __main__ |    DEBUG | val dataloader size: 165 minibatches[0m
[38;5;39m2025-03-26 00:17:55,115 |         __main__ |    DEBUG | Dataloaders creation time: 0.02 seconds[0m
[38;5;39m2025-03-26 00:17:55,115 |         __main__ |    DEBUG | Learning rate adjusted to: 0.0002[0m
[38;5;39m2025-03-26 00:17:55,116 |         __main__ |    DEBUG | [Rank 2]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,116 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,117 |         __main__ |    DEBUG | Total setup time: 15.39 seconds[0m
[38;5;39m2025-03-26 00:17:55,117 |         __main__ |    DEBUG | [Rank 0]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,117 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,117 |         __main__ |    DEBUG | [Rank 3]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,117 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,121 |         __main__ |    DEBUG | [Rank 8]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,121 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,125 |         __main__ |    DEBUG | [Rank 12]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,125 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,129 |         __main__ |    DEBUG | [Rank 30]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,130 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,130 |         __main__ |    DEBUG | [Rank 31]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,130 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,131 |         __main__ |    DEBUG | [Rank 29]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,131 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,132 |         __main__ |    DEBUG | [Rank 27]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,132 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,133 |         __main__ |    DEBUG | [Rank 4]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,133 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,134 |         __main__ |    DEBUG | [Rank 25]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,134 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,136 |         __main__ |    DEBUG | [Rank 1]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,136 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,138 |         __main__ |    DEBUG | [Rank 26]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,138 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,140 |         __main__ |    DEBUG | [Rank 23]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,141 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,144 |         __main__ |    DEBUG | [Rank 21]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,144 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,145 |         __main__ |    DEBUG | [Rank 20]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,146 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,146 |         __main__ |    DEBUG | [Rank 16]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,147 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,147 |         __main__ |    DEBUG | [Rank 18]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,147 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,150 |         __main__ |    DEBUG | [Rank 19]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,150 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,151 |         __main__ |    DEBUG | [Rank 28]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,151 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,156 |         __main__ |    DEBUG | [Rank 24]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,156 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,166 |         __main__ |    DEBUG | [Rank 22]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,167 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;5;39m2025-03-26 00:17:55,170 |         __main__ |    DEBUG | [Rank 17]: --- STARTING TRAINING ---[0m
[38;20m2025-03-26 00:17:55,170 | xffl.learning.processing |     INFO | Starting epoch 0/1[0m
[38;20m2025-03-26 00:20:58,420 | xffl.learning.processing |     INFO | Epoch 1: train_perplexity=47941.4922, train_epoch_loss=10.7777, epoch time 182.35659777396359s[0m
[38;5;39m2025-03-26 00:20:58,421 | xffl.learning.distributed |    DEBUG | [Rank 1]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,421 | xffl.learning.distributed |    DEBUG | [Rank 2]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,421 | xffl.learning.distributed |    DEBUG | [Rank 3]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,421 |         __main__ |    DEBUG | Key: avg_epoch_time, Value: 182.35659777396359[0m
[38;5;39m2025-03-26 00:20:58,422 |         __main__ |    DEBUG | Key: avg_train_perp, Value: 47941.4921875[0m
[38;5;39m2025-03-26 00:20:58,421 | xffl.learning.distributed |    DEBUG | [Rank 30]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,421 | xffl.learning.distributed |    DEBUG | [Rank 31]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,421 | xffl.learning.distributed |    DEBUG | [Rank 29]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,421 | xffl.learning.distributed |    DEBUG | [Rank 28]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,422 |         __main__ |    DEBUG | Key: avg_train_loss, Value: 10.77773666381836[0m
[38;5;39m2025-03-26 00:20:58,421 | xffl.learning.distributed |    DEBUG | [Rank 25]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,421 | xffl.learning.distributed |    DEBUG | [Rank 26]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,421 | xffl.learning.distributed |    DEBUG | [Rank 24]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,421 | xffl.learning.distributed |    DEBUG | [Rank 27]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,421 | xffl.learning.distributed |    DEBUG | [Rank 7]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,421 | xffl.learning.distributed |    DEBUG | [Rank 5]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,421 | xffl.learning.distributed |    DEBUG | [Rank 6]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,421 | xffl.learning.distributed |    DEBUG | [Rank 4]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,422 | xffl.learning.distributed |    DEBUG | [Rank 0]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,426 | xffl.learning.distributed |    DEBUG | [Rank 9]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,426 | xffl.learning.distributed |    DEBUG | [Rank 11]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,426 | xffl.learning.distributed |    DEBUG | [Rank 10]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,426 | xffl.learning.distributed |    DEBUG | [Rank 8]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,426 | xffl.learning.distributed |    DEBUG | [Rank 12]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,426 | xffl.learning.distributed |    DEBUG | [Rank 14]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,426 | xffl.learning.distributed |    DEBUG | [Rank 15]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,427 | xffl.learning.distributed |    DEBUG | [Rank 13]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,428 | xffl.learning.distributed |    DEBUG | [Rank 17]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,428 | xffl.learning.distributed |    DEBUG | [Rank 19]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,428 | xffl.learning.distributed |    DEBUG | [Rank 18]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,428 | xffl.learning.distributed |    DEBUG | [Rank 16]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,428 | xffl.learning.distributed |    DEBUG | [Rank 21]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,428 | xffl.learning.distributed |    DEBUG | [Rank 23]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,428 | xffl.learning.distributed |    DEBUG | [Rank 22]: calling destroy_process_group[0m
[38;5;39m2025-03-26 00:20:58,428 | xffl.learning.distributed |    DEBUG | [Rank 20]: calling destroy_process_group[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-ztj2bwpi[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-ztj2bwpi/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-o8jmximf[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-o8jmximf/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-dofh5ry1[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-dofh5ry1/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-005lf022[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-005lf022/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001740-8z9qtjyf[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001740-8z9qtjyf/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-g19wgz3f[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-g19wgz3f/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-g6huuaii[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-g6huuaii/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-9m5jde42[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-9m5jde42/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001739-z32fviu1[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001739-z32fviu1/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-f0cao0i0[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-f0cao0i0/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-i6tca001[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-i6tca001/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-k1t2iwnv[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-k1t2iwnv/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001739-mtkkacix[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001739-mtkkacix/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-ev6hzpa2[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-ev6hzpa2/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-yfmdp4oo[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-yfmdp4oo/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-zf1kpuo4[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-zf1kpuo4/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001740-ezmibmmx[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001740-ezmibmmx/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001739-rheaz0yt[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001739-rheaz0yt/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-h8e2takb[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-h8e2takb/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-7u3pp8f0[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-7u3pp8f0/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-ncmsgydg[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-ncmsgydg/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001740-ltd4gzst[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001740-ltd4gzst/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-9dt0ywxy[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-9dt0ywxy/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-ccb9bz4a[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-ccb9bz4a/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001740-4awrnqas[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001740-4awrnqas/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-vlin33vk[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-vlin33vk/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-v40pigid[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-v40pigid/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-iec73sqs[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-iec73sqs/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001740-mixaci7n[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001740-mixaci7n/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-05z3g0au[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-05z3g0au/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-vrni8civ[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-vrni8civ/logs[0m
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /leonardo/home/userexternal/gmittone/wandb/offline-run-20250326_001741-vcmz0uxy[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20250326_001741-vcmz0uxy/logs[0m
[38;20m2025-03-26 00:21:02,833 | xffl.cli.simulate |     INFO | Total simulation execution time: 212.13 seconds[0m
[38;20m2025-03-26 00:21:02,834 | xffl.cli.simulate |     INFO | *** Cross-Facility Federated Learning (xFFL) - Simulation ***[0m
