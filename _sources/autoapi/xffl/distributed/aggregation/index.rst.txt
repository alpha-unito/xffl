xffl.distributed.aggregation
============================

.. py:module:: xffl.distributed.aggregation

.. autoapi-nested-parse::

   Aggregation strategies for local xFFL



Attributes
----------

.. autoapisummary::

   xffl.distributed.aggregation.logger


Functions
---------

.. autoapisummary::

   xffl.distributed.aggregation.layer_by_layer
   xffl.distributed.aggregation.layer_by_layer_
   xffl.distributed.aggregation.layer_by_layer_optimized
   xffl.distributed.aggregation.layer_by_layer_optimized_
   xffl.distributed.aggregation.bucket_flatten
   xffl.distributed.aggregation.bucket_coalesced
   xffl.distributed.aggregation.bucket_flatten_
   xffl.distributed.aggregation.bucket_coalesced_
   xffl.distributed.aggregation.bucket_optimized_flatten
   xffl.distributed.aggregation.bucket_optimized_coalesced
   xffl.distributed.aggregation.bucket_optimized_flatten_
   xffl.distributed.aggregation.bucket_optimized_coalesced_
   xffl.distributed.aggregation.benchmark_aggregation


Module Contents
---------------

.. py:data:: logger
   :type:  logging.Logger

   Default xFFL logger


.. py:function:: layer_by_layer(model: torch.nn.Module, state: xffl.distributed.distributed_state.DistributedState, use_multiple_cuda_streams: bool = False, use_contiguous_memory: bool = False) -> xffl.distributed.utils.Strategy

   Layer-by-layer aggregation

   In case of multiple CUDA streams the layers are assigned to each of them in a round-robin fashion

   :param model: PyTorch model
   :type model: nn.Module
   :param state: xFFL distributed state
   :type state: DistributedState
   :param use_multiple_cuda_streams: use multiple CUDA streams if available, defaults to False
   :type use_multiple_cuda_streams: bool
   :param use_contiguous_memory: convert tensors to a contiguous memory representation, defaults to False
   :type use_contiguous_memory: bool
   :returns: The Aggregation strategy configuration
   :rtype: Strategy


.. py:function:: layer_by_layer_(model: torch.nn.Module, state: xffl.distributed.distributed_state.DistributedState, use_multiple_cuda_streams: bool = False, use_contiguous_memory: bool = False) -> None

   Layer-by-layer aggregation

   In case of multiple CUDA streams the layers are assigned to each of them in a round-robin fashion

   :param model: PyTorch model
   :type model: nn.Module
   :param state: xFFL distributed state
   :type state: DistributedState
   :param use_multiple_cuda_streams: use multiple CUDA streams if available, defaults to False
   :type use_multiple_cuda_streams: bool
   :param use_contiguous_memory: convert tensors to a contiguous memory representation, defaults to False
   :type use_contiguous_memory: bool


.. py:function:: layer_by_layer_optimized(model: torch.nn.Module, state: xffl.distributed.distributed_state.DistributedState, use_multiple_cuda_streams: bool = False, use_contiguous_memory: bool = False) -> xffl.distributed.utils.Strategy

   Layer-by-layer aggregation through a bucket-based approach

   In case of multiple CUDA streams the layers are assigned to each of them trying to divide the parameters equally

   :param model: PyTorch model
   :type model: nn.Module
   :param state: xFFL distributed state
   :type state: DistributedState
   :param use_multiple_cuda_streams: use multiple CUDA streams if available, defaults to False
   :type use_multiple_cuda_streams: bool
   :param use_contiguous_memory: convert tensors to a contiguous memory representation, defaults to False
   :type use_contiguous_memory: bool
   :returns: The Aggregation strategy configuration
   :rtype: Strategy


.. py:function:: layer_by_layer_optimized_(model: torch.nn.Module, state: xffl.distributed.distributed_state.DistributedState, use_multiple_cuda_streams: bool = False, use_contiguous_memory: bool = False) -> None

   Layer-by-layer aggregation through a bucket-based approach

   In case of multiple CUDA streams the layers are assigned to each of them trying to divide the parameters equally

   :param model: PyTorch model
   :type model: nn.Module
   :param state: xFFL distributed state
   :type state: DistributedState
   :param use_multiple_cuda_streams: use multiple CUDA streams if available, defaults to False
   :type use_multiple_cuda_streams: bool
   :param use_contiguous_memory: convert tensors to a contiguous memory representation, defaults to False
   :type use_contiguous_memory: bool


.. py:function:: bucket_flatten(model: torch.nn.Module, state: xffl.distributed.distributed_state.DistributedState, use_multiple_cuda_streams: bool = False, use_contiguous_memory: bool = False) -> xffl.distributed.utils.Strategy

   Weights averaging through a bucket-based approach

   All layers are stacked into "buckets" and sent on different CUDA streams in a round-robin fashion

   :param model: PyTorch model
   :type model: nn.Module
   :param state: xFFL distributed state
   :type state: DistributedState
   :param use_multiple_cuda_streams: use multiple CUDA streams if available, defaults to False
   :type use_multiple_cuda_streams: bool
   :param use_contiguous_memory: convert tensors to a contiguous memory representation, defaults to False
   :type use_contiguous_memory: bool
   :returns: The Aggregation strategy configuration
   :rtype: Strategy


.. py:function:: bucket_coalesced(model: torch.nn.Module, state: xffl.distributed.distributed_state.DistributedState, use_multiple_cuda_streams: bool = False, use_contiguous_memory: bool = False) -> xffl.distributed.utils.Strategy

   Weights averaging through a bucket-based approach

   All layers are stacked into "buckets" and sent on different CUDA streams in a round-robin fashion

   :param model: PyTorch model
   :type model: nn.Module
   :param state: xFFL distributed state
   :type state: DistributedState
   :param use_multiple_cuda_streams: use multiple CUDA streams if available, defaults to False
   :type use_multiple_cuda_streams: bool
   :param use_contiguous_memory: convert tensors to a contiguous memory representation, defaults to False
   :type use_contiguous_memory: bool
   :returns: The Aggregation strategy configuration
   :rtype: Strategy


.. py:function:: bucket_flatten_(model: torch.nn.Module, state: xffl.distributed.distributed_state.DistributedState, use_multiple_cuda_streams: bool = False, use_contiguous_memory: bool = False) -> None

   Weights averaging through a bucket-based approach

   All layers are stacked into "buckets" and sent on different CUDA streams in a round-robin fashion

   :param model: PyTorch model
   :type model: nn.Module
   :param state: xFFL distributed state
   :type state: DistributedState
   :param use_multiple_cuda_streams: use multiple CUDA streams if available, defaults to False
   :type use_multiple_cuda_streams: bool
   :param use_contiguous_memory: convert tensors to a contiguous memory representation, defaults to False
   :type use_contiguous_memory: bool
   :returns: The Aggregation strategy configuration
   :rtype: Strategy


.. py:function:: bucket_coalesced_(model: torch.nn.Module, state: xffl.distributed.distributed_state.DistributedState, use_multiple_cuda_streams: bool = False, use_contiguous_memory: bool = False) -> None

   Weights averaging through a bucket-based approach

   All layers are stacked into "buckets" and sent on different CUDA streams in a round-robin fashion

   :param model: PyTorch model
   :type model: nn.Module
   :param state: xFFL distributed state
   :type state: DistributedState
   :param use_multiple_cuda_streams: use multiple CUDA streams if available, defaults to False
   :type use_multiple_cuda_streams: bool
   :param use_contiguous_memory: convert tensors to a contiguous memory representation, defaults to False
   :type use_contiguous_memory: bool
   :returns: The Aggregation strategy configuration
   :rtype: Strategy


.. py:function:: bucket_optimized_flatten(model: torch.nn.Module, state: xffl.distributed.distributed_state.DistributedState, use_multiple_cuda_streams: bool = False, use_contiguous_memory: bool = False) -> xffl.distributed.utils.Strategy

   Weights averaging through a stacking-based approach

   All layers are stacked into "buckets" according to their size and sent on different CUDA streams trying to divide the information sent equally between them

   :param model: PyTorch model
   :type model: nn.Module
   :param state: xFFL distributed state
   :type state: DistributedState
   :param use_multiple_cuda_streams: use multiple CUDA streams if available, defaults to False
   :type use_multiple_cuda_streams: bool
   :param use_contiguous_memory: convert tensors to a contiguous memory representation, defaults to False
   :type use_contiguous_memory: bool
   :returns: The Aggregation strategy configuration
   :rtype: Strategy


.. py:function:: bucket_optimized_coalesced(model: torch.nn.Module, state: xffl.distributed.distributed_state.DistributedState, use_multiple_cuda_streams: bool = False, use_contiguous_memory: bool = False) -> xffl.distributed.utils.Strategy

   Weights averaging through a stacking-based approach

   All layers are stacked into "buckets" according to their size and sent on different CUDA streams trying to divide the information sent equally between them

   :param model: PyTorch model
   :type model: nn.Module
   :param state: xFFL distributed state
   :type state: DistributedState
   :param use_multiple_cuda_streams: use multiple CUDA streams if available, defaults to False
   :type use_multiple_cuda_streams: bool
   :param use_contiguous_memory: convert tensors to a contiguous memory representation, defaults to False
   :type use_contiguous_memory: bool
   :returns: The Aggregation strategy configuration
   :rtype: Strategy


.. py:function:: bucket_optimized_flatten_(model: torch.nn.Module, state: xffl.distributed.distributed_state.DistributedState, use_multiple_cuda_streams: bool = False, use_contiguous_memory: bool = False) -> xffl.distributed.utils.Strategy

   Weights averaging through a stacking-based approach

   All layers are stacked into "buckets" according to their size and sent on different CUDA streams trying to divide the information sent equally between them

   :param model: PyTorch model
   :type model: nn.Module
   :param state: xFFL distributed state
   :type state: DistributedState
   :param use_multiple_cuda_streams: use multiple CUDA streams if available, defaults to False
   :type use_multiple_cuda_streams: bool
   :param use_contiguous_memory: convert tensors to a contiguous memory representation, defaults to False
   :type use_contiguous_memory: bool
   :returns: The Aggregation strategy configuration
   :rtype: Strategy


.. py:function:: bucket_optimized_coalesced_(model: torch.nn.Module, state: xffl.distributed.distributed_state.DistributedState, use_multiple_cuda_streams: bool = False, use_contiguous_memory: bool = False) -> xffl.distributed.utils.Strategy

   Weights averaging through a stacking-based approach

   All layers are stacked into "buckets" according to their size and sent on different CUDA streams trying to divide the information sent equally between them

   :param model: PyTorch model
   :type model: nn.Module
   :param state: xFFL distributed state
   :type state: DistributedState
   :param use_multiple_cuda_streams: use multiple CUDA streams if available, defaults to False
   :type use_multiple_cuda_streams: bool
   :param use_contiguous_memory: convert tensors to a contiguous memory representation, defaults to False
   :type use_contiguous_memory: bool
   :returns: The Aggregation strategy configuration
   :rtype: Strategy


.. py:function:: benchmark_aggregation(state: xffl.distributed.distributed_state.DistributedState, model: torch.nn.Module, iterations: int = 10, dump: Optional[str] = None) -> None

   Benchmark method for testing the available aggregation strategies

   :param state: xFFL distributed state
   :type state: DistributedState
   :param model: PyTorch model
   :type model: nn.Module
   :param iterations: Number of iterations to run each aggreagtion strategy, defaults to 10
   :type iterations: int, optional


