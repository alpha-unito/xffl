xffl.distributed.distributed_state
==================================

.. py:module:: xffl.distributed.distributed_state


Attributes
----------

.. autoapisummary::

   xffl.distributed.distributed_state.logger


Classes
-------

.. autoapisummary::

   xffl.distributed.distributed_state.DistributedState


Functions
---------

.. autoapisummary::

   xffl.distributed.distributed_state.create_device_mesh


Module Contents
---------------

.. py:data:: logger
   :type:  logging.Logger

   Default xFFL logger


.. py:class:: DistributedState

   This dataclass traces all the distributed environment parameters


   .. py:attribute:: backend
      :type:  Optional[torch.distributed.distributed_c10d.Backend]
      :value: None


      Communication backend



   .. py:attribute:: master_addr
      :type:  Optional[str]
      :value: None


      Rendez-vous address



   .. py:attribute:: master_port
      :type:  Optional[int]
      :value: None


      Rendez-vous port



   .. py:attribute:: rank
      :type:  Optional[int]
      :value: None


      Global rank



   .. py:attribute:: world_size
      :type:  Optional[int]
      :value: None


      Global world size



   .. py:attribute:: node_local_rank
      :type:  Optional[int]
      :value: None


      Rank of the process inside the local computing node



   .. py:attribute:: node_local_size
      :type:  Optional[int]
      :value: None


      node size of a computing node



   .. py:attribute:: node_rank
      :type:  Optional[int]
      :value: None


      Rank of the computing node with respect to all the other ones



   .. py:attribute:: node_world_size
      :type:  Optional[int]
      :value: None


      Global number of computing nodes involved in the training process



   .. py:attribute:: replica_local_rank
      :type:  Optional[int]
      :value: None


      Rank of the process inside the local replica sharding group



   .. py:attribute:: replica_local_size
      :type:  Optional[int]
      :value: None


      Group size of a replica sharding group



   .. py:attribute:: replica_rank
      :type:  Optional[int]
      :value: None


      Rank of the replica group with respect to all the other ones (eventually, inside the federated group)



   .. py:attribute:: replica_world_size
      :type:  Optional[Tuple[int, Ellipsis]]
      :value: None


      Global number of replica sharding groups involved in the training process (eventually, inside the federated groups)



   .. py:attribute:: federated_local_rank
      :type:  Optional[int]
      :value: None


      Rank of the process inside the local federated group



   .. py:attribute:: federated_local_size
      :type:  Optional[Tuple[int, Ellipsis]]
      :value: None


      Group size of a federated group (eventually, list of group sizes if the federation is asymmetric)



   .. py:attribute:: federated_rank
      :type:  Optional[int]
      :value: None


      Federated group rank with respect to all the other ones



   .. py:attribute:: federated_world_size
      :type:  Optional[int]
      :value: None


      Global number of federated groups involved in the training process



   .. py:attribute:: fsdp_mesh
      :type:  Optional[torch.distributed.device_mesh.DeviceMesh]
      :value: None


      FSDP device mesh



   .. py:attribute:: hsdp_mesh
      :type:  Optional[torch.distributed.device_mesh.DeviceMesh]
      :value: None


      HSDP device mesh



   .. py:attribute:: is_sender
      :type:  Optional[bool]
      :value: None


      True if the rank should communicate (All-Gather) across network cells, False otherwise



   .. py:attribute:: receive_from
      :type:  Optional[int]
      :value: None


      The rank from which to receive the averaged parameters (Broadcast)



   .. py:attribute:: federated_group
      :type:  Optional[Tuple[torch.distributed.ProcessGroup, Ellipsis]]
      :value: None


      Process group collecting ranks holding the same model's shard across federated groups



   .. py:attribute:: replica_group
      :type:  Optional[Tuple[torch.distributed.ProcessGroup, Ellipsis]]
      :value: None


      Process group collecting ranks holding the same model's shard inside federated groups



   .. py:attribute:: federation
      :type:  Optional[torch.distributed.ProcessGroup]
      :value: None


      Process group collecting all ranks participating in the same federated group



   .. py:attribute:: device_type
      :type:  Optional[torch.device]
      :value: None


      Chosen deployment device



   .. py:attribute:: current_device
      :type:  Optional[torch.device | int]
      :value: None


      Specific device currently in use by the process



   .. py:attribute:: init_device
      :type:  Optional[torch.device]
      :value: None


      Chosen initialization device



   .. py:attribute:: meta_initialization
      :type:  Optional[bool]
      :value: None


      True if meta initialization is enabled, False otherwise



   .. py:attribute:: streams
      :type:  Optional[Tuple[torch.cuda.Stream, Ellipsis]]
      :value: None


      Pool of available CUDA streams



   .. py:method:: __str__()


   .. py:method:: set_global(backend: torch.distributed.distributed_c10d.Backend, device_type: torch.device, master_addr: str, master_port: int, rank: int, world_size: int) -> None

      Set global process group information.

      :param backend: Communication backend to use
      :type backend: Backend
      :param device_type: Type of device to use
      :type device_type: Literal["cpu", "cuda"]
      :param master_addr: Address of the master node for the rendez-vous
      :type master_addr: str
      :param master_port: Port of the master node for the rendez-vous
      :type master_port: int
      :param rank: Global process rank
      :type rank: int
      :param world_size: Global world size
      :type world_size: int



   .. py:method:: set_exec_device(current_device: torch.device | int, streams: Optional[int] = None) -> None

      Set the devices of the distributed process group.

      :param current_device: Training device
      :type current_device: torch.device | int
      :param streams: Number of CUDA streams to instantiate, defaults to 4
      :type streams: int



   .. py:method:: set_init_device(init_device: Optional[torch.device], meta_initialization: bool = False) -> None

      Set the devices of the distributed process group.

      :param init_device: Initialization device
      :type init_device: torch.device
      :param meta_initialization: If meta device initialization is required
      :type meta_initialization: bool



   .. py:method:: set_node(node_local_rank: int, node_local_size: int, node_rank: int, node_world_size: int) -> None

      Set the process' information relative to the local node.

      :param node_local_rank: Local compute node rank
      :type node_local_rank: int
      :param node_local_size: World size of the local compute node
      :type node_local_size: int
      :param node_rank: Rank of the local compute node among all the available nodes in the training
      :type node_rank: int
      :param node_world_size: Number of compute nodes involved in the training process
      :type node_world_size: int



   .. py:method:: is_node_setup() -> bool

      Checks if the local compute node information is set up.

      :return: True if the local compute node information is set up, False otherwise
      :rtype: bool



   .. py:method:: _get_global_fsdp_mesh() -> Optional[torch.distributed.device_mesh.DeviceMesh]

      Returns a standard global FSDP device mesh.
      Do not call this method if global FSDP is not required.

      :return: A global FSDP device mesh if the distributed PyTorch environment is initialized, None otherwise
      :rtype: Optional[DeviceMesh]



   .. py:method:: set_fsdp(mesh: Optional[torch.distributed.device_mesh.DeviceMesh] = None) -> None

      Enable PyTorch's FSDP functionality.
      If no mesh specified, FSDP will be enabled on the global process group.

      :param mesh: An FSDP device mesh, defaults to None
      :type mesh: Optional[DeviceMesh]



   .. py:method:: is_fsdp_setup() -> bool

      Checks if FSDP is set up.

      :return: True if FSDP is set up, False otherwise
      :rtype: bool



   .. py:method:: _set_global_hsdp_mesh() -> Optional[torch.distributed.device_mesh.DeviceMesh]

      Returns a global HSD device mesh.
      Do not call this method if global HSD device is not required.

      :return: A global HSDP device mesh if the distributed PyTorch environment is initialized, None otherwise
      :rtype: Optional[DeviceMesh]



   .. py:method:: set_hsdp(hsdp: int) -> None

      Enable global PyTorch's HSDP functionality.

      :param hsdp: Size of an HSDP replica
      :type hsdp: int



   .. py:method:: _partial_hsdp_setup(hsdp: int) -> None

      Initialize PyTorch's HSDP parameters without creating the device mesh.

      :param hsdp: Size of an HSDP replica
      :type hsdp: int



   .. py:method:: _partial_hsdp_setup_manual(replica_local_rank: int, replica_local_size: int, replica_rank: int, replica_world_size: Tuple[int, Ellipsis]) -> None

      Partial set up of PyTorch's HSDP functionality; to complete it is necessary to instantiate also the HSDP device mesh.

      :param replica_local_rank: Rank of the current process within its model replica
      :type replica_local_rank: int
      :param replica_local_size: Local world size of a model replica
      :type replica_local_size: int
      :param replica_rank: Rank of the current model replica among the current federated group replica world size
      :type replica_rank: int
      :param replica_world_size: Number of replicas available for each federated group
      :type replica_world_size: Tuple[int,...]



   .. py:method:: is_hsdp_setup() -> bool

      Checks if HSDP is set up.
      Does not check the HSDP device mesh.

      :return: True if HSDP is set up, False otherwise
      :rtype: bool



   .. py:method:: unset_hsdp() -> None

      Unsets all HSDP related variables.



   .. py:method:: _set_rank_role() -> None


   .. py:method:: set_federated_scaling(federated_group_size: Tuple[int], hsdp: Optional[int] = None) -> None


   .. py:method:: _get_communicating_processes(federated_rank: int) -> Tuple[int, Ellipsis]


   .. py:method:: _set_symmetric_federated_scaling(federated_group_size: Tuple[int]) -> None

      Create the federated scaling process groups

      :param federated_group_size: Number of processes making up one federated group
      :type federated_group_size: int



   .. py:method:: _set_asymmetric_federated_scaling(federated_group_size: Tuple[int]) -> None

      Create the federated scaling process groups

      This process groups bring together all the ranks handling corresponding model's shards.
      E.g.: if a model is sharded among four processes and replicated across two process groups (i.e., device_mesh=[[0,1,2,3],[4,5,6,7]])
      then the federated scaling process groups correspond to the groups of processes having the same local rank (i.e., [[0,4][1,5][2,6][3,7]])

      :param federated_group_size: Number of processes making up one federated group
      :type federated_group_size: int



   .. py:method:: unset_federated_scaling() -> None

      Unset Federated Scaling parameters



   .. py:method:: is_federated_scaling_setup() -> bool

      Checks if Federated Scaling is set up.

      :return: True if Federated Scaling is set up, False otherwise
      :rtype: bool



   .. py:method:: create_process_group(ranks: Tuple[int, Ellipsis] | torch.Tensor, group_desc: Optional[str]) -> torch.distributed.ProcessGroup

      Creates a new process group with the specified ranks

      Only the interested rank can enter this method

      :param ranks: Ranks making up the group
      :type ranks: Tuple[int, ...]
      :param group_desc: Description of the process group
      :type group_desc: Optional[str]
      :return: Process group handle
      :rtype: ProcessGroup



.. py:function:: create_device_mesh(mesh_shape: Tuple[int, Ellipsis]) -> torch.Tensor

   Creates a Tensor of distributed process ranks with the specified dimensions

   :param mesh_shape: Dimensions of the mesh
   :type mesh_shape: Tuple[int, ...]
   :return: Tensor of ranks
   :rtype: torch.Tensor


