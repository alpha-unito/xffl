xffl.distributed.utils
======================

.. py:module:: xffl.distributed.utils

.. autoapi-nested-parse::

   Utility methods for distributed training



Attributes
----------

.. autoapisummary::

   xffl.distributed.utils.logger


Classes
-------

.. autoapisummary::

   xffl.distributed.utils.Strategy


Functions
---------

.. autoapisummary::

   xffl.distributed.utils._is_broadcast_necessary
   xffl.distributed.utils._setup_streams
   xffl.distributed.utils._all_reduce
   xffl.distributed.utils._all_reduce_and_broadcast


Module Contents
---------------

.. py:data:: logger
   :type:  logging.Logger

   Default xFFL logger


.. py:class:: Strategy

   Aggregation strategy class

   Each strategy is described as a mapping in which each element corresponds to a single communication

   :param mapping: Mapping between original layers index, layer parameter, appropriate context manager for communication, and CUDA stream index
   :type mapping: Tuple[Tuple[Tuple[int, ...], torch.Tensor, ContextManager, int], ...]
   :param reduce_op: All reduce operation
   :type reduce_op: dist.ReduceOp.RedOpType
   :param use_contiguous_memory: Convert tensors to contiguous memory before communication
   :type use_contiguous_memory: bool
   :param src: Source rank for broadcast communications
   :type src: int
   :param broadcast: If it is necessary to run broadcasts
   :type broadcast: bool
   :param state: xFFL distributed state
   :type state: DistributedState
   :param requires_copy: Specified strategy requires copying back the aggregated tensors, defaults to False
   :type requires_copy: bool
   :param param_list: Original model parameter list necessary for copying, defaults to None
   :type param_list: Optional[List[torch.Tensor]]


   .. py:attribute:: mapping
      :type:  Tuple[Tuple[Tuple[int, Ellipsis], torch.Tensor, ContextManager, int], Ellipsis]


   .. py:attribute:: reduce_op
      :type:  torch.distributed.ReduceOp.RedOpType


   .. py:attribute:: use_contiguous_memory
      :type:  bool


   .. py:attribute:: src
      :type:  int


   .. py:attribute:: broadcast
      :type:  bool


.. py:function:: _is_broadcast_necessary(state: xffl.distributed.distributed_state.DistributedState) -> bool

   Checks if the current rank needs to take part into a weights broadcast

   :param state: xFFL distributed state (rank, world_size, backend)
   :type state: DistributedState


.. py:function:: _setup_streams(use_multiple_cuda_streams: bool, state: xffl.distributed.distributed_state.DistributedState) -> Tuple[int, int, torch.distributed.ReduceOp.RedOpType, ContextManager]

   Sets up the CUDA streams infos and reduce operation for the aggregation

   :param use_multiple_cuda_streams: If multiple CUDA streams should be used, defaults to False
   :type use_multiple_cuda_streams: bool
   :param state: The xFFL distributed state
   :type state: DistributedState
   :return: The number of streams, the index of the current stream, the reduce operation type, and the context manager for the stream
   :rtype: Tuple[int, int, dist.ReduceOp.RedOpType, ContextManager]


.. py:function:: _all_reduce(strategy: Strategy, state: xffl.distributed.distributed_state.DistributedState) -> None

   AllReduce part of the aggregation - benchmark purpose only

   Executes an all-reduce operation to average the model's weights according to the specified strategy and current distributed state configuration

   :param strategy: Aggregation strategy
   :type strategy: Strategy
   :param state: xFFL distributed state
   :type state: DistributedState


.. py:function:: _all_reduce_and_broadcast(strategy: Strategy, state: xffl.distributed.distributed_state.DistributedState) -> None

   Communication part of the aggregation - complementary to the selected strategy

   Executes an all-reduce followed by a broadcast (if needed) to average the model's weights according to the specified strategy and current distributed state configuration

   :param strategy: Aggregation strategy
   :type strategy: Strategy
   :param state: xFFL distributed state
   :type state: DistributedState


