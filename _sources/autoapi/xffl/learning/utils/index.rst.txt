xffl.learning.utils
===================

.. py:module:: xffl.learning.utils

.. autoapi-nested-parse::

   Utility methods useful in a number of common DNN training scenarios



Attributes
----------

.. autoapisummary::

   xffl.learning.utils.logger


Functions
---------

.. autoapisummary::

   xffl.learning.utils.set_deterministic_execution
   xffl.learning.utils.set_nondeterministic_execution
   xffl.learning.utils.get_model_size
   xffl.learning.utils.get_model_size_in_bits
   xffl.learning.utils.seed_dataloader_worker
   xffl.learning.utils.set_activation_checkpointing
   xffl.learning.utils.preload


Module Contents
---------------

.. py:data:: logger
   :type:  logging.Logger

   Default xFFL logger


.. py:function:: set_deterministic_execution(seed: int) -> torch.Generator

   Set all the necessary RNGs to obtain reproducible executions

   This method sets random, numpy, torch and CUDA RNGs with the same seed.
   It also forces PyTorch's to use deterministic algorithms, reducing performance

   :param seed: Random seed
   :type seed: int
   :return: PyTorch RNG
   :rtype: torch.Generator


.. py:function:: set_nondeterministic_execution() -> None

   Deactivate deterministic execution and deterministic memory filling to improve performance


.. py:function:: get_model_size(model: torch.nn.Module | transformers.AutoModel) -> int

   Returns the model's trainable parameters number

   :param model: PyTorch model
   :type model: nn.Module
   :return: Number of trainable parameters
   :rtype: int


.. py:function:: get_model_size_in_bits(model: torch.nn.Module | transformers.AutoModel) -> int

   Returns the model's trainable parameters size in bits

   :param model: PyTorch model
   :type model: nn.Module
   :return: Size of trainable parameters in bits
   :rtype: int


.. py:function:: seed_dataloader_worker(worker_id: int) -> None

   Seeds PyTorch's data loader workers to guarantee reproducibility

   Since each data loader worker is a process, the underlying libraries have to be re-seeded in a reproducible way to guarantee reproducibility of the loaded data and that different workers do not have the same seed, thus loading the same data

   :param worker_id: Worker's id
   :type worker_id: int


.. py:function:: set_activation_checkpointing(model: torch.nn.Module | transformers.PreTrainedModel, layer: type = None) -> None

   Sets up activation (gradient) checkpointing

   This feature reduces maximum memory usage trading off more compute

   :param model: Model on which setting up the checkpointing
   :type model: nn.Module | PreTrainedModel
   :param layer: Layer to wrap, needed only by Torch models, defaults to None
   :type layer: Optional[nn.Module], optional


.. py:function:: preload(files: List[xffl.custom.types.PathLike]) -> None

   Pre-loads the given list of files and folders

   Particularly useful on HPC, where data can be moved near the computing nodes ahead of time

   :param files: Paths of the files and folders to be preloaded
   :type files: List[PathLike]
   :raises OSError, ValueError: If the subprocess run fails


