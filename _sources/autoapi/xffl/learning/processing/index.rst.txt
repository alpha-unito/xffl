xffl.learning.processing
========================

.. py:module:: xffl.learning.processing

.. autoapi-nested-parse::

   DNN training utility methods



Attributes
----------

.. autoapisummary::

   xffl.learning.processing.logger


Functions
---------

.. autoapisummary::

   xffl.learning.processing.distributed_training
   xffl.learning.processing.fsdp_evaluation


Module Contents
---------------

.. py:data:: logger
   :type:  logging.Logger

   Default xFFL logger


.. py:function:: distributed_training(model: torch.nn.Module | torch.distributed.fsdp.FullyShardedDataParallel, optimizer: torch.optim.Optimizer, train_dataloader: torch.utils.data.DataLoader, eval_dataloader: torch.utils.data.DataLoader, state: xffl.distributed.distributed.DistributedState, federated_batches: Optional[int] = None, validate: bool = True, epochs: int = 1, save_path: Optional[xffl.custom.types.PathLike] = None, output_model_name: Optional[str] = None, lr_scheduler: Optional[torch.optim.lr_scheduler.LRScheduler] = None, wandb_run: Optional[wandb.wandb_run.Run] = None, criterion=None) -> Dict[str, float]

   Generic training cycle for FSDP models

   :param model: Model to train
   :type model: nn.Module
   :param optimizer: Model's optimizer
   :type optimizer: Optimizer
   :param train_dataloader: Training dataset data loader
   :type train_dataloader: DataLoader
   :param eval_dataloader: Validation dataset data loader
   :type eval_dataloader: DataLoader
   :param state: Instantiated distributed state
   :type state: DistributedState
   :param federated_batches: Number of training batched to process between two federated averaging
   :type federated_batches: Optional[int]
   :param validate: Activate validation, defaults to True
   :type validate: bool, optional
   :param epochs: Number of epochs to train, defaults to 1
   :type epochs: int, optional
   :param save_path: Path where to save the trained model, defaults to None
   :type save_path: Optional[PathLike], optional
   :param output_model_name: Name to use for the saved trained model, defaults to None
   :type output_model_name: Optional[str], optional
   :param lr_scheduler: Learning rate scheduler, defaults to None
   :type lr_scheduler: Optional[LRScheduler], optional
   :param wandb_run: WandB run if wandb logging is desired, defaults to None
   :type wandb_run: Optional[wandb.Run], optional
   :return: Dictionary of metrics names and achieved values
   :rtype: Dict[str, float]


.. py:function:: fsdp_evaluation(model: torch.nn.Module, eval_dataloader: torch.utils.data.DataLoader, state: xffl.distributed.distributed.DistributedState, wandb_run: Optional[wandb.wandb_run.Run] = None, criterion=None) -> Tuple[torch.Tensor, torch.Tensor, List[float], List[float]]

   Generic evaluation cycle for FSDP models

   :param model: Model to evaluate
   :type model: nn.Module
   :param eval_dataloader: Validation dataset data loader
   :type eval_dataloader: DataLoader
   :param state: Instantiated distributed state
   :type state: DistributedState
   :param wandb_run: WandB run if wandb logging is desired, defaults to None
   :type wandb_run: Optional[wandb.Run], optional
   :return: perplexity, epoch loss, step loss, step perplexity
   :rtype: Tuple[float, float, float, float]


